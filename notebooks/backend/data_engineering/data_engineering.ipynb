{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f70ca6",
   "metadata": {},
   "source": [
    "# Root level Data Engineering needs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pyproject.toml\n",
    "[project]\n",
    "name = \"react_fastapi_railway\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Pytorch and Jax GPU docker container\"\n",
    "authors = [\n",
    "  { name = \"Geoffrey Hadfield\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  # Core web framework\n",
    "  \"fastapi>=0.104.0\",\n",
    "  \"uvicorn[standard]>=0.24.0\",\n",
    "  \"python-dotenv>=1.0.0\",\n",
    "\n",
    "  # Settings and validation\n",
    "  \"pydantic>=2.0.0\",\n",
    "  \"pydantic-settings>=2.0.0\",\n",
    "\n",
    "  # HTTP client and multipart parsing\n",
    "  \"httpx>=0.24.0\",\n",
    "  \"python-multipart>=0.0.6\",\n",
    "\n",
    "  # Data & ML basics\n",
    "  \"numpy>=1.24.0\",\n",
    "  \"pandas>=2.1.0\",\n",
    "  \"scikit-learn>=1.3.0\",\n",
    "  \"mlflow>=2.8.0\",\n",
    "\n",
    "  # (Your existing extras—keep if you still need them)\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.2.8,<1.3.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard==0.5.1\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",\n",
    "  \"numpyro>=0.18.0,<1.0.0\",\n",
    "  \"jax==0.6.0\",\n",
    "  \"jaxlib==0.6.0\",\n",
    "  \"pytensor>=2.18.3\",\n",
    "  \"aesara>=2.9.4\",\n",
    "  \"tqdm>=4.67.0\",\n",
    "  \"pyarrow>=12.0.0\",\n",
    "  \"optuna>=3.0.0\",\n",
    "  \"optuna-integration[mlflow]>=0.2.0\",\n",
    "  \"omegaconf>=2.3.0,<2.4.0\",\n",
    "  \"hydra-core>=1.3.2,<1.4.0\",\n",
    "  \"aiosqlite>=0.19.0\", \n",
    "  \"python-jose[cryptography]>=3.3.0\",\n",
    "  \"passlib[bcrypt]>=1.7.4\",\n",
    "  \"bcrypt==4.0.1\",  # Pin bcrypt version to resolve warning\n",
    "  # Rate limiting\n",
    "  \"fastapi-limiter>=0.1.5\",\n",
    "  \"aioredis>=2.0.0\",\n",
    "  \"httpx>=0.24.0\",\n",
    "  \"psutil>=5.0.0,<8.0.0\",\n",
    "  \"protobuf>=4.24.4\",\n",
    "  \"confluent-kafka>=2.3.0\",              # meets provider min\n",
    "  \"sqlalchemy>=1.4.49,<2.0.0\",           # py3.12 support, keeps MLflow happy\n",
    "  \"psycopg2-binary>=2.9.9\",\n",
    "  \"fastavro>=1.7.0\",\n",
    "  \"apache-airflow==2.10.4\",              # add core airflow\n",
    "  \"apache-airflow-providers-postgres>=6.2.1,<7.0.0\",\n",
    "  \"apache-airflow-providers-apache-kafka>=1.10.0,<2.0.0\",\n",
    "  \"apache-airflow-providers-standard>=1.4.1,<2.0.0\",\n",
    "  \"apache-airflow-providers-http>=4.5.0,<5.0.0\",\n",
    "  \"ipykernel>=6.25.0\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"invoke>=2.2\",\n",
    "]\n",
    "\n",
    "cuda = [\n",
    "  \"cupy-cuda12x>=12.0.0\",\n",
    "]\n",
    "\n",
    "[tool.pytensor]\n",
    "device    = \"cuda\"\n",
    "floatX    = \"float32\"\n",
    "allow_gc  = true\n",
    "optimizer = \"fast_run\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db7b13",
   "metadata": {},
   "source": [
    "# Astro/Airflow Local Dev Quickstart\n",
    "1. Initialize & Start the Project\n",
    "\n",
    "# From the directory where you want the project to live\n",
    "cd api/src\n",
    "mkdir -p airflow_project && cd airflow_project\n",
    "\n",
    "astro dev init            # Scaffold Airflow project (dags/, Dockerfile, etc.)\n",
    "astro dev start           # Build image & start all Airflow services (webserver, scheduler, DB)\n",
    "\n",
    "    If ports (8080/5432) are busy, set alternates before start:\n",
    "    astro config set webserver.port 8081 / astro config set postgres.port 5433.\n",
    "    Astronomer\n",
    "    Astronomer\n",
    "\n",
    "2. Everyday Lifecycle Commands\n",
    "\n",
    "Use these while iterating on DAGs and dependencies:\n",
    "\n",
    "astro dev stop            # Stop containers, keep project state\n",
    "astro dev restart         # Stop → rebuild image → start (after reqs/Dockerfile changes)\n",
    "\n",
    "    These two are your main “apply changes” loop during development.\n",
    "    Astronomer\n",
    "    Astronomer\n",
    "\n",
    "3. Inspect, Logs, & Diagnostics\n",
    "\n",
    "astro dev status          # Health & ports of services\n",
    "astro dev logs            # Combined logs (Ctrl+C to quit)\n",
    "astro dev tail scheduler  # Live-tail a specific service (scheduler/webserver/triggerer)\n",
    "astro dev ps              # Show running containers for this project\n",
    "astro dev top             # Process table inside a service container\n",
    "astro dev stats           # CPU/mem stats per container\n",
    "\n",
    "    Helpful when debugging start-up issues, stuck tasks, or resource pressure.\n",
    "    Astronomer\n",
    "    GitHub\n",
    "\n",
    "4. Force a DAG Reparse (No Waiting)\n",
    "\n",
    "astro dev run dags reserialize\n",
    "\n",
    "    Airflow auto-parses: new files ~5 min, edits ~30 s. This command forces an immediate parse.\n",
    "    Astronomer\n",
    "    Astronomer\n",
    "\n",
    "5. One-off DAG Test Runs\n",
    "\n",
    "astro run <dag-id>\n",
    "\n",
    "    Compiles and executes a single DAG in a throwaway worker container—fast feedback without touching the scheduler.\n",
    "    Astronomer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e36bc",
   "metadata": {},
   "source": [
    "# Within Airflow project, created files from astro dev init that we adjusted to fit our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/requirements.txt\n",
    "# Astro Runtime includes the following pre-installed providers packages: https://www.astronomer.io/docs/astro/runtime-image-architecture#provider-packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49361241",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/airflow_settings.yaml\n",
    "# This file allows you to configure Airflow Connections, Pools, and Variables in a single place for local development only.\n",
    "# NOTE: json dicts can be added to the conn_extra field as yaml key value pairs. See the example below.\n",
    "\n",
    "# For more information, refer to our docs: https://www.astronomer.io/docs/astro/cli/develop-project#configure-airflow_settingsyaml-local-development-only\n",
    "# For questions, reach out to: https://support.astronomer.io\n",
    "# For issues create an issue ticket here: https://github.com/astronomer/astro-cli/issues\n",
    "\n",
    "airflow:\n",
    "  connections:\n",
    "    - conn_id:\n",
    "      conn_type:\n",
    "      conn_host:\n",
    "      conn_schema:\n",
    "      conn_login:\n",
    "      conn_password:\n",
    "      conn_port:\n",
    "      conn_extra:\n",
    "        example_extra_field: example-value\n",
    "  pools:\n",
    "    - pool_name:\n",
    "      pool_slot:\n",
    "      pool_description:\n",
    "  variables:\n",
    "    - variable_name:\n",
    "      variable_value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f733f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/.env\n",
    "#not empty\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e922b",
   "metadata": {},
   "source": [
    "# Utils + Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9561890",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/utils/__init__.py\n",
    "\"\"\"\n",
    "Utils package for the NBA Player Valuation project.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8065cb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/utils/config.py\n",
    "\"\"\"\n",
    "Central configuration for the NBA‑Player‑Valuation project.\n",
    "All magic values live here so they can be tweaked without code edits.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ── Core directories ───────────────────────────────────────────────────────────\n",
    "def find_project_root(name: str = \"airflow_project\") -> Path:\n",
    "    \"\"\"\n",
    "    Walk up from this file (or cwd) until a directory named `name` is found.\n",
    "    Fallback to cwd if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        p = Path.cwd()\n",
    "    # walk through p and its parents\n",
    "    for parent in (p, *p.parents):\n",
    "        if parent.name == name or (parent / \".git\").is_dir():\n",
    "            return parent\n",
    "    # no match → fallback\n",
    "    return Path.cwd()\n",
    "\n",
    "# Allow explicit override\n",
    "if find_project_root():\n",
    "    PROJECT_ROOT = Path(find_project_root()).resolve() # / \"api/src/airflow_project\"\n",
    "else:\n",
    "    PROJECT_ROOT = find_project_root() # / \"api/src/airflow_project\"\n",
    "\n",
    "\n",
    "DATA_DIR: Path = Path(PROJECT_ROOT / \"data\")\n",
    "LOG_DIR: Path = Path(PROJECT_ROOT / \"logs\")\n",
    "DUCKDB_FILE: Path = Path(DATA_DIR / \"nba.duckdb\")\n",
    "\n",
    "# NBA stats API\n",
    "NBA_API_RPM: int = int(os.getenv(\"NBA_API_RPM\", \"12\"))  # requests per minute\n",
    "\n",
    "# Spotrac scraping\n",
    "SPOTRAC_BASE: str = \"https://www.spotrac.com/nba\"\n",
    "SPOTRAC_FREE_AGENTS: str = f\"{SPOTRAC_BASE}/free-agents/{{year}}/\"\n",
    "SPOTRAC_CAP_TRACKER: str = f\"{SPOTRAC_BASE}/cap/{{year}}/\"\n",
    "SPOTRAC_TAX_TRACKER: str = f\"{SPOTRAC_BASE}/tax/_/year/{{year}}/\"\n",
    "# Spotrac dedicated folder\n",
    "\n",
    "\n",
    "# Injury sources\n",
    "NBA_OFFICIAL_INJURY_URL: str = \"https://cdn.nba.com/static/json/injury/injury_report_{{date}}.json\"\n",
    "ROTOWIRE_RSS: str = \"https://www.rotowire.com/rss/news.php?sport=NBA\"\n",
    "\n",
    "# StatsD (optional)\n",
    "STATSD_HOST: str = os.getenv(\"STATSD_HOST\", \"localhost\")\n",
    "STATSD_PORT: int = int(os.getenv(\"STATSD_PORT\", \"8125\"))\n",
    "\n",
    "# ── Data ranges ────────────────────────────────────────────────────────────────\n",
    "SEASONS: range = range(2015, 2026)  # inclusive upper bound matches Spotrac sample\n",
    "\n",
    "# Thread pools & concurrency\n",
    "MAX_WORKERS: int = int(os.getenv(\"NPV_MAX_WORKERS\", \"8\")) \n",
    "\n",
    "\n",
    "# ── Core data directories ───────────────────────────────────────────────────────────\n",
    "RAW_DIR      : Path = DATA_DIR / \"raw\"\n",
    "DEBUG_DIR    : Path = DATA_DIR / \"debug\"\n",
    "EXPORTS_DIR  : Path = DATA_DIR / \"exports\"\n",
    "INJURY_DIR   : Path = DATA_DIR / \"injury_reports\"\n",
    "INJURY_DATASETS_DIR : Path = DATA_DIR / \"injury_datasets\"\n",
    "NBA_BASIC_ADVANCED_STATS_DIR : Path = DATA_DIR / \"nba_basic_advanced_stats\"\n",
    "ADVANCED_METRICS_DIR: Path = DATA_DIR / \"new_processed\" / \"advanced_metrics\"\n",
    "NBA_BASE_DATA_DIR : Path = DATA_DIR / \"nba_processed\"\n",
    "DEFENSE_DATA_DIR : Path = DATA_DIR / \"defense_metrics\"\n",
    "FINAL_DATASET_DIR : Path = DATA_DIR / \"merged_final_dataset\"\n",
    "PLAY_TYPES_DIR : Path = DATA_DIR / \"synergyplay_types\"\n",
    "SPOTRAC_DIR  : Path = DATA_DIR / \"spotrac_contract_data\"\n",
    "SILVER_DIR   : Path = SPOTRAC_DIR / \"silver\"\n",
    "FINAL_DIR    : Path = SPOTRAC_DIR / \"final\"\n",
    "SPOTRAC_DEBUG_DIR    : Path = SPOTRAC_DIR / \"debug\"\n",
    "SPOTRAC_RAW_DIR      : Path = SPOTRAC_DIR / \"raw\"\n",
    "\n",
    "# ── Helper functions (single source of truth) ────────────────────────────────\n",
    "def get_injury_base_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Return the canonical injury_reports root.  \n",
    "    ENV `INJURY_DATA_DIR` wins; otherwise we fall back to INJURY_DIR.\n",
    "    Always creates the dir so callers can assume it exists.\n",
    "    \"\"\"\n",
    "    base = Path(os.getenv(\"INJURY_DATA_DIR\", INJURY_DIR)).resolve()\n",
    "    return base\n",
    "\n",
    "def injury_path(*parts: str) -> Path:\n",
    "    \"\"\"Shorthand for get_injury_base_dir().joinpath(*parts).resolve().\"\"\"\n",
    "    return get_injury_base_dir().joinpath(*parts).resolve()\n",
    "\n",
    "# ── One‑shot: ensure all declared dirs exist at import‑time ───────────────────\n",
    "for _p in (\n",
    "    DATA_DIR, RAW_DIR, DEBUG_DIR, EXPORTS_DIR, INJURY_DIR,\n",
    "    SPOTRAC_DIR, SILVER_DIR, FINAL_DIR, SPOTRAC_DEBUG_DIR, SPOTRAC_RAW_DIR,\n",
    "    NBA_BASIC_ADVANCED_STATS_DIR, NBA_BASE_DATA_DIR, ADVANCED_METRICS_DIR,\n",
    "    DEFENSE_DATA_DIR, FINAL_DATASET_DIR,\n",
    "    PLAY_TYPES_DIR   # ← ensure synergy play‑types folder exists\n",
    "):\n",
    "    _p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"all directories:\")\n",
    "print(\"root directory:\")\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"RAW_DIR: {RAW_DIR}\")\n",
    "print(f\"DEBUG_DIR: {DEBUG_DIR}\")\n",
    "print(f\"EXPORTS_DIR: {EXPORTS_DIR}\")\n",
    "print(f\"INJURY_DIR: {INJURY_DIR}\")\n",
    "print(f\"INJURY_DATASETS_DIR: {INJURY_DATASETS_DIR}\")\n",
    "print(f\"NBA_BASIC_ADVANCED_STATS_DIR: {NBA_BASIC_ADVANCED_STATS_DIR}\")\n",
    "print(f\"NBA_BASE_DATA_DIR: {NBA_BASE_DATA_DIR}\")\n",
    "print(f\"ADVANCED_METRICS_DIR: {ADVANCED_METRICS_DIR}\")\n",
    "print(f\"DEFENSE_DATA_DIR: {DEFENSE_DATA_DIR}\")\n",
    "print(f\"FINAL_DATASET_DIR: {FINAL_DATASET_DIR}\")\n",
    "print(f\"PLAY_TYPES_DIR: {PLAY_TYPES_DIR}\")\n",
    "print(f\"SPOTRAC_DIR: {SPOTRAC_DIR}\")\n",
    "print(f\"SILVER_DIR: {SILVER_DIR}\")\n",
    "print(f\"FINAL_DIR: {FINAL_DIR}\")\n",
    "print(f\"SPOTRAC_DEBUG_DIR: {SPOTRAC_DEBUG_DIR}\")\n",
    "print(f\"SPOTRAC_RAW_DIR: {SPOTRAC_RAW_DIR}\")\n",
    "print(\"all directories:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfdb536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/utils/utils.py\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import duckdb, boto3, logging\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def configure_logging(level: int = logging.INFO, log_dir: str = \"logs\") -> None:\n",
    "    Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "    fmt = \"%(asctime)s [%(levelname)s] %(name)s :: %(message)s\"\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=fmt,\n",
    "        handlers=[\n",
    "            logging.FileHandler(Path(log_dir, \"data_pipeline.log\")),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def to_duck(table: str, parquet: Path) -> None:\n",
    "    con = duckdb.connect(database=\"nba.duckdb\", read_only=False)\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table} AS\n",
    "        SELECT * FROM parquet_scan('{parquet}')\n",
    "        \"\"\")\n",
    "    con.execute(f\"COPY (SELECT * FROM parquet_scan('{parquet}')) TO '{table}.parquet' (FORMAT PARQUET);\")\n",
    "    con.close()\n",
    "    logging.info(\"%s appended to DuckDB\", parquet)\n",
    "\n",
    "def maybe_upload(parquet: Path) -> None:\n",
    "    if os.getenv(\"AWS_ACCESS_KEY_ID\"):\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        bucket = os.getenv(\"NBA_S3_BUCKET\", \"nba-bronze\")\n",
    "        s3.upload_file(str(parquet), bucket, f\"bronze/{parquet.name}\")\n",
    "        logging.info(\"Uploaded %s to s3://%s/bronze/\", parquet.name, bucket)\n",
    "\n",
    "def _write_parquet_safe(df: pd.DataFrame, out_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Atomic parquet write to avoid partial files.\n",
    "    ALSO: drop any duplicate column names before writing.\n",
    "    \"\"\"\n",
    "    # 1) Drop duplicate columns by name (keep the first occurrence)\n",
    "    if df.columns.duplicated().any():\n",
    "        df = df.loc[:, ~df.columns.duplicated()]\n",
    "    # 2) Write to a temporary file and atomically replace\n",
    "    tmp = out_path.with_suffix(out_path.suffix + \".tmp\")\n",
    "    df.to_parquet(tmp, index=False)\n",
    "    os.replace(tmp, out_path)\n",
    "\n",
    "\n",
    "# --- NEW final writer ---\n",
    "def write_final_dataset(df: pd.DataFrame, out_path: Path) -> Path:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    _write_parquet_safe(df, out_path)\n",
    "    logging.info(\"Final merged parquet -> %s\", out_path)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80d29f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/utils/data_check_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/utils/data_check_utils.py\n",
    "\"\"\"\n",
    "Reusable data-pull + data-quality checks (clear-arg edition)\n",
    "\n",
    "Dimensions covered:\n",
    "- Min/Max summary, Nulls, Duplicates, Outliers (IQR), Consistency (rules),\n",
    "  Completeness, Accuracy (vs. reference or lookup lists), Validity (column rules),\n",
    "  Uniqueness (primary key).\n",
    "\n",
    "Style & references:\n",
    "- Naming follows PEP 8 / Google Python Style Guide.\n",
    "- Data-quality dimensions per common industry taxonomy.\n",
    "- Outliers via 1.5×IQR; duplicates via pandas .duplicated().\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Report dataclasses\n",
    "# =========================\n",
    "@dataclass\n",
    "class NullsReport:\n",
    "    per_column: pd.DataFrame  # columns: column, null_count, total_rows, null_pct\n",
    "    total_rows: int\n",
    "\n",
    "@dataclass\n",
    "class DuplicatesReport:\n",
    "    has_duplicates: bool\n",
    "    duplicate_row_count: int\n",
    "    sample_rows: pd.DataFrame\n",
    "\n",
    "@dataclass\n",
    "class OutliersReport:\n",
    "    per_column_summary: pd.DataFrame  # column, outlier_count, pct, lower_fence, upper_fence\n",
    "    sample_rows: pd.DataFrame\n",
    "\n",
    "@dataclass\n",
    "class ConsistencyReport:\n",
    "    violations_by_rule: Dict[str, int]\n",
    "    sample_rows_by_rule: Dict[str, pd.DataFrame]\n",
    "\n",
    "@dataclass\n",
    "class CompletenessReport:\n",
    "    required_columns: List[str]\n",
    "    nonnull_ratio_by_column: Dict[str, float]\n",
    "    failing_columns: List[str]\n",
    "\n",
    "@dataclass\n",
    "class AccuracyReport:\n",
    "    check_status_by_name: Dict[str, str]\n",
    "    sample_rows_by_check: Dict[str, pd.DataFrame]\n",
    "\n",
    "@dataclass\n",
    "class ValidityReport:\n",
    "    status: str  # \"passed\" | \"failed\" | \"skipped\"\n",
    "    details: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class UniquenessReport:\n",
    "    primary_key_columns: List[str]\n",
    "    has_violations: bool\n",
    "    duplicate_key_row_count: int\n",
    "    sample_duplicate_keys: pd.DataFrame\n",
    "\n",
    "@dataclass\n",
    "class RangeReport:\n",
    "    numeric_summary: pd.DataFrame   # column, count, min, max, mean, std\n",
    "\n",
    "@dataclass\n",
    "class DataQualityReport:\n",
    "    minmax: RangeReport\n",
    "    nulls: NullsReport\n",
    "    duplicates: DuplicatesReport\n",
    "    outliers: OutliersReport\n",
    "    consistency: ConsistencyReport\n",
    "    completeness: CompletenessReport\n",
    "    accuracy: AccuracyReport\n",
    "    validity: ValidityReport\n",
    "    uniqueness: UniquenessReport\n",
    "\n",
    "    def to_json(self, pretty: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Convert the report to JSON format.\n",
    "        \n",
    "        Args:\n",
    "            pretty: If True, format with indentation for readability\n",
    "            \n",
    "        Returns:\n",
    "            JSON string representation of the report\n",
    "        \"\"\"\n",
    "        def df_to_records(df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "            return df.to_dict(orient=\"records\") if not df.empty else []\n",
    "        \n",
    "        data = {\n",
    "            \"minmax\": {\"numeric_summary\": df_to_records(self.minmax.numeric_summary)},\n",
    "            \"nulls\": {\n",
    "                \"total_rows\": self.nulls.total_rows,\n",
    "                \"per_column\": df_to_records(self.nulls.per_column),\n",
    "            },\n",
    "            \"duplicates\": {\n",
    "                \"has_duplicates\": self.duplicates.has_duplicates,\n",
    "                \"duplicate_row_count\": self.duplicates.duplicate_row_count,\n",
    "                \"sample_rows\": df_to_records(self.duplicates.sample_rows),\n",
    "            },\n",
    "            \"outliers\": {\n",
    "                \"per_column_summary\": df_to_records(self.outliers.per_column_summary),\n",
    "                \"sample_rows\": df_to_records(self.outliers.sample_rows),\n",
    "            },\n",
    "            \"consistency\": {\n",
    "                \"violations_by_rule\": self.consistency.violations_by_rule,\n",
    "                \"sample_rows_by_rule\": {k: df_to_records(v) for k, v in self.consistency.sample_rows_by_rule.items()},\n",
    "            },\n",
    "            \"completeness\": {\n",
    "                \"required_columns\": self.completeness.required_columns,\n",
    "                \"nonnull_ratio_by_column\": self.completeness.nonnull_ratio_by_column,\n",
    "                \"failing_columns\": self.completeness.failing_columns,\n",
    "            },\n",
    "            \"accuracy\": {\n",
    "                \"check_status_by_name\": self.accuracy.check_status_by_name,\n",
    "                \"sample_rows_by_check\": {k: df_to_records(v) for k, v in self.accuracy.sample_rows_by_check.items()},\n",
    "            },\n",
    "            \"validity\": {\"status\": self.validity.status, \"details\": self.validity.details},\n",
    "            \"uniqueness\": {\n",
    "                \"primary_key_columns\": self.uniqueness.primary_key_columns,\n",
    "                \"has_violations\": self.uniqueness.has_violations,\n",
    "                \"duplicate_key_row_count\": self.uniqueness.duplicate_key_row_count,\n",
    "                \"sample_duplicate_keys\": df_to_records(self.uniqueness.sample_duplicate_keys),\n",
    "            },\n",
    "        }\n",
    "        return json.dumps(data, indent=2 if pretty else None)\n",
    "\n",
    "\n",
    "def _json_or_none(obj):\n",
    "    try:\n",
    "        return json.dumps(obj, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def report_to_rows(report: DataQualityReport) -> list[dict]:\n",
    "    \"\"\"Flatten DataQualityReport into long rows suitable for a single table.\"\"\"\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    # ---- Min/Max\n",
    "    for rec in report.minmax.numeric_summary.to_dict(\"records\"):\n",
    "        rows.append({\n",
    "            \"section\": \"minmax\", \"subsection\": rec[\"column\"], \"field\": \"count\", \"value\": rec[\"count\"], \"extra\": None\n",
    "        })\n",
    "        for k in (\"min\", \"max\", \"mean\", \"std\"):\n",
    "            rows.append({\"section\": \"minmax\", \"subsection\": rec[\"column\"], \"field\": k, \"value\": rec[k], \"extra\": None})\n",
    "\n",
    "    # ---- Nulls\n",
    "    for rec in report.nulls.per_column.to_dict(\"records\"):\n",
    "        rows.append({\n",
    "            \"section\": \"nulls\", \"subsection\": rec[\"column\"], \"field\": \"null_count\",\n",
    "            \"value\": rec[\"null_count\"], \"extra\": {\"total_rows\": rec[\"total_rows\"], \"null_pct\": rec[\"null_pct\"]}\n",
    "        })\n",
    "\n",
    "    # ---- Duplicates\n",
    "    rows.append({\n",
    "        \"section\": \"duplicates\", \"subsection\": \"_overall\", \"field\": \"has_duplicates\",\n",
    "        \"value\": bool(report.duplicates.has_duplicates),\n",
    "        \"extra\": {\"duplicate_row_count\": int(report.duplicates.duplicate_row_count),\n",
    "                  \"sample_rows\": report.duplicates.sample_rows.head(5).to_dict(\"records\")}\n",
    "    })\n",
    "\n",
    "    # ---- Outliers (IQR fences: Q1-1.5*IQR, Q3+1.5*IQR)\n",
    "    for rec in report.outliers.per_column_summary.to_dict(\"records\"):\n",
    "        rows.append({\n",
    "            \"section\": \"outliers\", \"subsection\": rec[\"column\"], \"field\": \"outlier_count\",\n",
    "            \"value\": rec.get(\"outlier_count\", 0),\n",
    "            \"extra\": {\"pct\": rec.get(\"pct\"), \"lower_fence\": rec.get(\"lower_fence\"), \"upper_fence\": rec.get(\"upper_fence\")}\n",
    "        })\n",
    "    if not report.outliers.sample_rows.empty:\n",
    "        rows.append({\n",
    "            \"section\": \"outliers\", \"subsection\": \"_samples\", \"field\": \"sample_rows\",\n",
    "            \"value\": None, \"extra\": report.outliers.sample_rows.head(5).to_dict(\"records\")\n",
    "        })\n",
    "\n",
    "    # ---- Consistency\n",
    "    for name, cnt in (report.consistency.violations_by_rule or {}).items():\n",
    "        rows.append({\"section\": \"consistency\", \"subsection\": name, \"field\": \"violations\", \"value\": int(cnt), \"extra\": None})\n",
    "        ex = report.consistency.sample_rows_by_rule.get(name)\n",
    "        if isinstance(ex, pd.DataFrame) and not ex.empty:\n",
    "            rows.append({\"section\": \"consistency\", \"subsection\": name, \"field\": \"sample_rows\",\n",
    "                         \"value\": None, \"extra\": ex.head(5).to_dict(\"records\")})\n",
    "\n",
    "    # ---- Completeness\n",
    "    rows.append({\n",
    "        \"section\": \"completeness\", \"subsection\": \"_overall\", \"field\": \"required_columns\",\n",
    "        \"value\": None, \"extra\": report.completeness.required_columns\n",
    "    })\n",
    "    for col, ratio in (report.completeness.nonnull_ratio_by_column or {}).items():\n",
    "        rows.append({\"section\": \"completeness\", \"subsection\": col, \"field\": \"nonnull_ratio\", \"value\": float(ratio), \"extra\": None})\n",
    "    if report.completeness.failing_columns:\n",
    "        rows.append({\"section\": \"completeness\", \"subsection\": \"_overall\", \"field\": \"failing_columns\",\n",
    "                     \"value\": None, \"extra\": report.completeness.failing_columns})\n",
    "\n",
    "    # ---- Accuracy\n",
    "    for name, status in (report.accuracy.check_status_by_name or {}).items():\n",
    "        rows.append({\"section\": \"accuracy\", \"subsection\": name, \"field\": \"status\", \"value\": status, \"extra\": None})\n",
    "        ex = report.accuracy.sample_rows_by_check.get(name)\n",
    "        if isinstance(ex, pd.DataFrame) and not ex.empty:\n",
    "            rows.append({\"section\": \"accuracy\", \"subsection\": name, \"field\": \"sample_rows\",\n",
    "                         \"value\": None, \"extra\": ex.head(5).to_dict(\"records\")})\n",
    "\n",
    "    # ---- Validity\n",
    "    rows.append({\"section\": \"validity\", \"subsection\": \"_overall\", \"field\": \"status\", \"value\": report.validity.status, \"extra\": None})\n",
    "    if report.validity.details:\n",
    "        rows.append({\"section\": \"validity\", \"subsection\": \"_overall\", \"field\": \"details\",\n",
    "                     \"value\": None, \"extra\": report.validity.details})\n",
    "\n",
    "    # ---- Uniqueness\n",
    "    rows.append({\"section\": \"uniqueness\", \"subsection\": \"_overall\", \"field\": \"primary_key_columns\",\n",
    "                 \"value\": None, \"extra\": report.uniqueness.primary_key_columns})\n",
    "    rows.append({\"section\": \"uniqueness\", \"subsection\": \"_overall\", \"field\": \"has_violations\",\n",
    "                 \"value\": bool(report.uniqueness.has_violations), \"extra\": None})\n",
    "    rows.append({\"section\": \"uniqueness\", \"subsection\": \"_overall\", \"field\": \"duplicate_key_row_count\",\n",
    "                 \"value\": int(report.uniqueness.duplicate_key_row_count), \"extra\": None})\n",
    "    if isinstance(report.uniqueness.sample_duplicate_keys, pd.DataFrame) and not report.uniqueness.sample_duplicate_keys.empty:\n",
    "        rows.append({\"section\": \"uniqueness\", \"subsection\": \"_samples\", \"field\": \"sample_duplicate_keys\",\n",
    "                     \"value\": None, \"extra\": report.uniqueness.sample_duplicate_keys.head(5).to_dict(\"records\")})\n",
    "\n",
    "    return rows\n",
    "\n",
    "def report_to_table(report: DataQualityReport) -> pd.DataFrame:\n",
    "    \"\"\"Return a tidy DataFrame with columns: section, subsection, field, value, extra(json).\"\"\"\n",
    "    rows = report_to_rows(report)\n",
    "    tbl = pd.DataFrame.from_records(rows, columns=[\"section\", \"subsection\", \"field\", \"value\", \"extra\"])\n",
    "    # store 'extra' as a compact JSON string for easy viewing\n",
    "    if \"extra\" in tbl.columns:\n",
    "        tbl[\"extra\"] = tbl[\"extra\"].map(_json_or_none)\n",
    "    return tbl.sort_values([\"section\", \"subsection\", \"field\"]).reset_index(drop=True)\n",
    "\n",
    "# ---------- Render helpers ----------\n",
    "def report_to_markdown_table(report: DataQualityReport) -> str:\n",
    "    \"\"\"Render the flattened table to Markdown (great for README/Slack).\"\"\"\n",
    "    df = report_to_table(report)\n",
    "    return df.to_markdown(index=False)\n",
    "\n",
    "def report_to_html_table(report: DataQualityReport, caption: str | None = None) -> str:\n",
    "    \"\"\"Render a compact HTML table (email/dashboard).\"\"\"\n",
    "    df = report_to_table(report)\n",
    "    styler = df.style.set_table_attributes('class=\"table table-sm\"').hide(axis=\"index\")\n",
    "    if caption:\n",
    "        styler = styler.set_caption(caption)\n",
    "    return styler.to_html()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Core check helpers\n",
    "# =========================\n",
    "def summarize_min_max(df: pd.DataFrame) -> RangeReport:\n",
    "    \"\"\"\n",
    "    Numeric min/max/mean/std summary. Returns empty summary if no numeric columns.\n",
    "    \"\"\"\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    if num.empty:\n",
    "        empty = pd.DataFrame(columns=[\"column\", \"count\", \"min\", \"max\", \"mean\", \"std\"])\n",
    "        return RangeReport(empty)\n",
    "    desc = pd.DataFrame({\n",
    "        \"column\": num.columns,\n",
    "        \"count\": num.count().values,\n",
    "        \"min\": num.min().values,\n",
    "        \"max\": num.max().values,\n",
    "        \"mean\": num.mean().values,\n",
    "        \"std\": num.std(ddof=1).values,\n",
    "    }).sort_values(\"column\").reset_index(drop=True)\n",
    "    return RangeReport(desc)\n",
    "\n",
    "\n",
    "def summarize_nulls(df: pd.DataFrame) -> NullsReport:\n",
    "    \"\"\"\n",
    "    Per-column null counts and percentages.\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    rows = []\n",
    "    for col in df.columns:\n",
    "        n = int(df[col].isna().sum())\n",
    "        rows.append({\n",
    "            \"column\": col,\n",
    "            \"null_count\": n,\n",
    "            \"total_rows\": total,\n",
    "            \"null_pct\": (n / total) if total else 0.0,\n",
    "        })\n",
    "    out = pd.DataFrame(rows).sort_values(\"null_pct\", ascending=False).reset_index(drop=True)\n",
    "    return NullsReport(per_column=out, total_rows=total)\n",
    "\n",
    "\n",
    "def find_duplicates(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    key_columns: Optional[Sequence[str]] = None,\n",
    "    sample_size: int = 10,\n",
    ") -> DuplicatesReport:\n",
    "    \"\"\"\n",
    "    Detect duplicate rows. If key_columns is provided, checks duplicates over that subset;\n",
    "    otherwise considers the entire row. Uses pandas .duplicated().\n",
    "    \"\"\"\n",
    "    mask = df.duplicated(subset=key_columns, keep=False)\n",
    "    dupes = df.loc[mask].head(sample_size)\n",
    "    count = int(mask.sum())\n",
    "    return DuplicatesReport(\n",
    "        has_duplicates=count > 0,\n",
    "        duplicate_row_count=count,\n",
    "        sample_rows=dupes\n",
    "    )\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    numeric_columns: Optional[Sequence[str]] = None,\n",
    "    sample_size: int = 10,\n",
    ") -> OutliersReport:\n",
    "    \"\"\"\n",
    "    IQR-based outliers per column: values < Q1-1.5*IQR or > Q3+1.5*IQR.\n",
    "    \"\"\"\n",
    "    num = df.select_dtypes(include=[np.number])\n",
    "    if numeric_columns:\n",
    "        num = num[[c for c in numeric_columns if c in num.columns]]\n",
    "    summary_rows = []\n",
    "    any_outlier_mask = pd.Series(False, index=df.index)\n",
    "    for col in num.columns:\n",
    "        s = num[col].dropna()\n",
    "        if s.empty:\n",
    "            continue\n",
    "        q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        mask = (num[col] < lower) | (num[col] > upper)\n",
    "        any_outlier_mask |= mask.fillna(False)\n",
    "        summary_rows.append({\n",
    "            \"column\": col,\n",
    "            \"outlier_count\": int(mask.sum()),\n",
    "            \"pct\": float(mask.mean()) if mask.size else 0.0,\n",
    "            \"lower_fence\": float(lower),\n",
    "            \"upper_fence\": float(upper),\n",
    "        })\n",
    "    summary = (pd.DataFrame(summary_rows)\n",
    "               if summary_rows else\n",
    "               pd.DataFrame(columns=[\"column\",\"outlier_count\",\"pct\",\"lower_fence\",\"upper_fence\"]))\n",
    "    summary = summary.sort_values(\"outlier_count\", ascending=False).reset_index(drop=True)\n",
    "    examples = df.loc[any_outlier_mask].head(sample_size)\n",
    "    return OutliersReport(per_column_summary=summary, sample_rows=examples)\n",
    "\n",
    "\n",
    "def check_consistency_rules(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    rules_by_name: Dict[str, Callable[[pd.DataFrame], pd.Series]],\n",
    "    sample_size: int = 5,\n",
    ") -> ConsistencyReport:\n",
    "    \"\"\"\n",
    "    Apply boolean-violation rules (fn returns mask of violations). Examples:\n",
    "    - season string format; set membership; cross-field dependencies.\n",
    "    \"\"\"\n",
    "    violations, examples = {}, {}\n",
    "    for name, fn in rules_by_name.items():\n",
    "        mask = fn(df)\n",
    "        cnt = int(mask.sum())\n",
    "        violations[name] = cnt\n",
    "        if cnt:\n",
    "            examples[name] = df.loc[mask].head(sample_size)\n",
    "    return ConsistencyReport(violations_by_rule=violations, sample_rows_by_rule=examples)\n",
    "\n",
    "\n",
    "def check_completeness(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    required_columns: Sequence[str],\n",
    "    min_nonnull_ratio: float = 1.0,\n",
    ") -> CompletenessReport:\n",
    "    \"\"\"\n",
    "    Ensure required columns exist and meet a non-null ratio (default 100%).\n",
    "    \"\"\"\n",
    "    ratios: Dict[str, float] = {}\n",
    "    failing: List[str] = []\n",
    "    total = len(df)\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            ratios[col] = 0.0\n",
    "            failing.append(col)\n",
    "            continue\n",
    "        ratio = 1.0 if total == 0 else float(df[col].notna().mean())\n",
    "        ratios[col] = ratio\n",
    "        if ratio < min_nonnull_ratio:\n",
    "            failing.append(col)\n",
    "    return CompletenessReport(\n",
    "        required_columns=list(required_columns),\n",
    "        nonnull_ratio_by_column=ratios,\n",
    "        failing_columns=failing,\n",
    "    )\n",
    "\n",
    "\n",
    "def check_accuracy(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    allowed_values_by_column: Optional[Dict[str, Iterable[Any]]] = None,\n",
    "    reference_table: Optional[pd.DataFrame] = None,\n",
    "    reference_join_keys: Optional[Sequence[str]] = None,\n",
    "    sample_size: int = 5,\n",
    ") -> AccuracyReport:\n",
    "    \"\"\"\n",
    "    Two simple accuracy patterns:\n",
    "      (A) allowed_values_by_column: domain/lookup checks for categorical fields.\n",
    "      (B) reference join coverage: ensure rows match a reference table on keys.\n",
    "    \"\"\"\n",
    "    checks: Dict[str, str] = {}\n",
    "    examples: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    if allowed_values_by_column:\n",
    "        for col, allowed in allowed_values_by_column.items():\n",
    "            if col in df.columns:\n",
    "                bad_mask = ~df[col].isin(list(allowed)) & df[col].notna()\n",
    "                cnt = int(bad_mask.sum())\n",
    "                checks[f\"lookup:{col}\"] = \"ok (0 violations)\" if cnt == 0 else f\"violations={cnt}\"\n",
    "                if cnt:\n",
    "                    examples[f\"lookup:{col}\"] = df.loc[bad_mask].head(sample_size)\n",
    "\n",
    "    if reference_table is not None and reference_join_keys:\n",
    "        left = df[list(reference_join_keys)].drop_duplicates()\n",
    "        right = reference_table[list(reference_join_keys)].drop_duplicates()\n",
    "        merged = left.merge(right, on=list(reference_join_keys), how=\"left\", indicator=True)\n",
    "        missing = merged[\"_merge\"] == \"left_only\"\n",
    "        cnt = int(missing.sum())\n",
    "        checks[\"reference_join_coverage\"] = \"ok (0 missing)\" if cnt == 0 else f\"missing={cnt}\"\n",
    "        if cnt:\n",
    "            examples[\"reference_join_coverage\"] = merged.loc[missing].drop(columns=[\"_merge\"]).head(sample_size)\n",
    "\n",
    "    return AccuracyReport(check_status_by_name=checks, sample_rows_by_check=examples)\n",
    "\n",
    "\n",
    "def check_validity(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    column_rules: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    ") -> ValidityReport:\n",
    "    \"\"\"\n",
    "    Lightweight validity (type/regex/range/membership). For strict contracts, consider Pandera/GE.\n",
    "    Example column_rules:\n",
    "    {\n",
    "      \"PLAYER_ID\":   {\"dtype\": \"int64\", \"ge\": 0},\n",
    "      \"SEASON\":      {\"regex\": r\"^\\\\d{4}-\\\\d{2}$\"},\n",
    "      \"SEASON_TYPE\": {\"in\": [\"Regular Season\",\"Playoffs\",\"Pre Season\",\"All Star\"]},\n",
    "      \"E_OFF_RATING\":{\"ge\": 0, \"le\": 200}\n",
    "    }\n",
    "    \"\"\"\n",
    "    if not column_rules:\n",
    "        return ValidityReport(status=\"skipped\", details={})\n",
    "\n",
    "    details = {\"errors\": []}\n",
    "    for col, rules in column_rules.items():\n",
    "        if col not in df.columns:\n",
    "            details[\"errors\"].append({\"column\": col, \"error\": \"missing column\"})\n",
    "            continue\n",
    "        s = df[col]\n",
    "        if \"dtype\" in rules:\n",
    "            expected = rules[\"dtype\"]\n",
    "            if str(s.dtype) != expected:\n",
    "                details[\"errors\"].append({\"column\": col, \"error\": f\"dtype {s.dtype} != {expected}\"})\n",
    "        if \"regex\" in rules:\n",
    "            pat = re.compile(rules[\"regex\"])\n",
    "            bad = s.dropna().astype(str).map(lambda x: not bool(pat.match(x)))\n",
    "            if bad.any():\n",
    "                details[\"errors\"].append({\"column\": col, \"error\": f\"regex_fail_count={int(bad.sum())}\"})\n",
    "        if \"in\" in rules:\n",
    "            allowed = set(rules[\"in\"])\n",
    "            bad = ~s.isin(allowed) & s.notna()\n",
    "            if bad.any():\n",
    "                details[\"errors\"].append({\"column\": col, \"error\": f\"in_fail_count={int(bad.sum())}\"})\n",
    "        if s.dtype.kind in \"fi\":\n",
    "            if \"ge\" in rules:\n",
    "                bad = s.dropna() < rules[\"ge\"]\n",
    "                if bad.any():\n",
    "                    details[\"errors\"].append({\"column\": col, \"error\": f\"ge_fail_count={int(bad.sum())}\"})\n",
    "            if \"le\" in rules:\n",
    "                bad = s.dropna() > rules[\"le\"]\n",
    "                if bad.any():\n",
    "                    details[\"errors\"].append({\"column\": col, \"error\": f\"le_fail_count={int(bad.sum())}\"})\n",
    "    status = \"passed\" if not details[\"errors\"] else \"failed\"\n",
    "    return ValidityReport(status=status, details=details)\n",
    "\n",
    "\n",
    "def check_uniqueness(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    primary_key_columns: Sequence[str],\n",
    "    sample_size: int = 10,\n",
    ") -> UniquenessReport:\n",
    "    \"\"\"\n",
    "    Primary-key-level uniqueness across `primary_key_columns`.\n",
    "    Uses pandas .duplicated(subset=...).\n",
    "    \"\"\"\n",
    "    if not primary_key_columns:\n",
    "        return UniquenessReport([], False, 0, pd.DataFrame())\n",
    "    mask = df.duplicated(subset=primary_key_columns, keep=False)\n",
    "    dup_count = int(mask.sum())\n",
    "    examples = df.loc[mask, list(primary_key_columns)].drop_duplicates().head(sample_size)\n",
    "    return UniquenessReport(\n",
    "        primary_key_columns=list(primary_key_columns),\n",
    "        has_violations=dup_count > 0,\n",
    "        duplicate_key_row_count=dup_count,\n",
    "        sample_duplicate_keys=examples,\n",
    "    )\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# Orchestrator: run all checks & build report\n",
    "# ==========================================\n",
    "def run_quality_suite(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    required_columns: Sequence[str],\n",
    "    primary_key_columns: Sequence[str],\n",
    "    outlier_numeric_columns: Optional[Sequence[str]] = None,\n",
    "    consistency_rules_by_name: Optional[Dict[str, Callable[[pd.DataFrame], pd.Series]]] = None,\n",
    "    allowed_values_by_column: Optional[Dict[str, Iterable[Any]]] = None,\n",
    "    reference_table: Optional[pd.DataFrame] = None,\n",
    "    reference_join_keys: Optional[Sequence[str]] = None,\n",
    "    validity_column_rules: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "    output_format: str = \"table\",\n",
    "    table_caption: Optional[str] = None,\n",
    ") -> Union[DataQualityReport, pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    One-call quality suite with explicit names and flexible output formatting.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to analyze\n",
    "        required_columns: columns that must exist and meet min non-null ratio (default 100%)\n",
    "        primary_key_columns: columns forming the business key (checked for uniqueness)\n",
    "        outlier_numeric_columns: numeric columns to scan for IQR outliers (None = all numeric)\n",
    "        consistency_rules_by_name: {rule_name: fn(df)->mask_of_violations}\n",
    "        allowed_values_by_column: {column: iterable of allowed values} for lookup/domain checks\n",
    "        reference_table: DataFrame to test join coverage/accuracy against\n",
    "        reference_join_keys: columns used to join df to reference_table\n",
    "        validity_column_rules: {column: {dtype/regex/in/ge/le}} lightweight validity rules\n",
    "        output_format: Output format - \"table\" (default), \"json\", \"markdown\", \"html\", or \"report\"\n",
    "        table_caption: Optional caption for HTML table format\n",
    "        \n",
    "    Returns:\n",
    "        - \"table\": pd.DataFrame with flattened results (default)\n",
    "        - \"json\": JSON string\n",
    "        - \"markdown\": Markdown table string  \n",
    "        - \"html\": HTML table string\n",
    "        - \"report\": Raw DataQualityReport object for programmatic access\n",
    "        \n",
    "    Examples:\n",
    "        # Default table format\n",
    "        df_results = run_quality_suite(df, required_columns=[\"id\"], primary_key_columns=[\"id\"])\n",
    "        \n",
    "        # JSON format for APIs\n",
    "        json_results = run_quality_suite(df, required_columns=[\"id\"], primary_key_columns=[\"id\"], \n",
    "                                       output_format=\"json\")\n",
    "        \n",
    "        # Markdown for documentation\n",
    "        md_results = run_quality_suite(df, required_columns=[\"id\"], primary_key_columns=[\"id\"], \n",
    "                                     output_format=\"markdown\")\n",
    "        \n",
    "        # Raw report object for complex logic\n",
    "        report = run_quality_suite(df, required_columns=[\"id\"], primary_key_columns=[\"id\"], \n",
    "                                 output_format=\"report\")\n",
    "    \"\"\"\n",
    "    # Validate output format\n",
    "    valid_formats = [\"table\", \"json\", \"markdown\", \"html\", \"report\"]\n",
    "    if output_format not in valid_formats:\n",
    "        raise ValueError(f\"output_format must be one of {valid_formats}, got '{output_format}'\")\n",
    "    \n",
    "    # Run all quality checks\n",
    "    minmax = summarize_min_max(df)\n",
    "    nulls = summarize_nulls(df)\n",
    "    duplicates = find_duplicates(df, key_columns=None)\n",
    "    outliers = detect_outliers_iqr(df, numeric_columns=outlier_numeric_columns)\n",
    "    consistency = check_consistency_rules(df, rules_by_name=(consistency_rules_by_name or {}))\n",
    "    completeness = check_completeness(df, required_columns=required_columns)\n",
    "    accuracy = check_accuracy(\n",
    "        df,\n",
    "        allowed_values_by_column=allowed_values_by_column,\n",
    "        reference_table=reference_table,\n",
    "        reference_join_keys=reference_join_keys,\n",
    "    )\n",
    "    validity = check_validity(df, column_rules=validity_column_rules)\n",
    "    uniqueness = check_uniqueness(df, primary_key_columns=primary_key_columns)\n",
    "\n",
    "    # Create the report object\n",
    "    report = DataQualityReport(\n",
    "        minmax=minmax,\n",
    "        nulls=nulls,\n",
    "        duplicates=duplicates,\n",
    "        outliers=outliers,\n",
    "        consistency=consistency,\n",
    "        completeness=completeness,\n",
    "        accuracy=accuracy,\n",
    "        validity=validity,\n",
    "        uniqueness=uniqueness,\n",
    "    )\n",
    "    \n",
    "    # Return in requested format\n",
    "    if output_format == \"report\":\n",
    "        return report\n",
    "    elif output_format == \"json\":\n",
    "        return report.to_json()\n",
    "    elif output_format == \"table\":\n",
    "        return report_to_table(report)\n",
    "    elif output_format == \"markdown\":\n",
    "        return report_to_markdown_table(report)\n",
    "    elif output_format == \"html\":\n",
    "        return report_to_html_table(report, caption=table_caption)\n",
    "    else:\n",
    "        # This shouldn't happen due to validation above, but just in case\n",
    "        raise ValueError(f\"Unsupported output format: {output_format}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Convenience functions for specific formats\n",
    "# ------------------------------------------\n",
    "def run_quality_suite_json(df: pd.DataFrame, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Convenience function to run quality suite and return JSON.\n",
    "    All kwargs are passed to run_quality_suite except output_format.\n",
    "    \"\"\"\n",
    "    kwargs.pop('output_format', None)  # Remove if present\n",
    "    return run_quality_suite(df, output_format=\"json\", **kwargs)\n",
    "\n",
    "\n",
    "def run_quality_suite_table(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience function to run quality suite and return DataFrame table.\n",
    "    All kwargs are passed to run_quality_suite except output_format.\n",
    "    \"\"\"\n",
    "    kwargs.pop('output_format', None)  # Remove if present\n",
    "    return run_quality_suite(df, output_format=\"table\", **kwargs)\n",
    "\n",
    "\n",
    "def run_quality_suite_markdown(df: pd.DataFrame, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Convenience function to run quality suite and return Markdown.\n",
    "    All kwargs are passed to run_quality_suite except output_format.\n",
    "    \"\"\"\n",
    "    kwargs.pop('output_format', None)  # Remove if present\n",
    "    return run_quality_suite(df, output_format=\"markdown\", **kwargs)\n",
    "\n",
    "\n",
    "def run_quality_suite_html(df: pd.DataFrame, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Convenience function to run quality suite and return HTML.\n",
    "    All kwargs are passed to run_quality_suite except output_format.\n",
    "    \"\"\"\n",
    "    kwargs.pop('output_format', None)  # Remove if present\n",
    "    return run_quality_suite(df, output_format=\"html\", **kwargs)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Backward-compatibility aliases (soft-deprecate)\n",
    "# ------------------------------------------\n",
    "def _alias_run_quality_suite(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    required_cols: Sequence[str],\n",
    "    pk: Sequence[str],\n",
    "    iqr_outlier_cols: Optional[Sequence[str]] = None,\n",
    "    consistency_rules: Optional[Dict[str, Callable[[pd.DataFrame], pd.Series]]] = None,\n",
    "    accuracy_lookups: Optional[Dict[str, Iterable[Any]]] = None,\n",
    "    accuracy_reference_df: Optional[pd.DataFrame] = None,\n",
    "    accuracy_join_key: Optional[Sequence[str]] = None,\n",
    "    validity_schema: Optional[Dict[str, Dict[str, Any]]] = None,\n",
    "    output_format: str = \"table\",\n",
    ") -> Union[DataQualityReport, pd.DataFrame, str]:\n",
    "    \"\"\"Deprecated arg names wrapper. Prefer run_quality_suite(...).\"\"\"\n",
    "    return run_quality_suite(\n",
    "        df,\n",
    "        required_columns=required_cols,\n",
    "        primary_key_columns=pk,\n",
    "        outlier_numeric_columns=iqr_outlier_cols,\n",
    "        consistency_rules_by_name=consistency_rules,\n",
    "        allowed_values_by_column=accuracy_lookups,\n",
    "        reference_table=accuracy_reference_df,\n",
    "        reference_join_keys=accuracy_join_key,\n",
    "        validity_column_rules=validity_schema,\n",
    "        output_format=output_format,\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example usage\n",
    "# =========================\n",
    "def _example_fetch() -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"PLAYER_ID\": [1, 1, 2, 3, 4],\n",
    "        \"PLAYER_NAME\": [\"A\", \"A\", \"B\", \"C\", None],\n",
    "        \"SEASON\": [\"2023-24\", \"2023-24\", \"2023-24\", \"2023-24\", \"BAD\"],\n",
    "        \"SEASON_TYPE\": [\"Regular Season\", \"Regular Season\", \"Regular Season\", \"Playoffs\", \"Regular Season\"],\n",
    "        \"E_OFF_RATING\": [110.0, 110.0, 5000.0, 108.2, 105.0],\n",
    "        \"TEAM_ID\": [10, 10, 11, None, 13],\n",
    "        \"TEAM_NAME\": [\"X\", \"X\", \"Y\", \"Z\", \"W\"],\n",
    "    })\n",
    "\n",
    "\n",
    "def _example_rules() -> Dict[str, Callable[[pd.DataFrame], pd.Series]]:\n",
    "    return {\n",
    "        \"season_format\": lambda df: ~df[\"SEASON\"].astype(str).str.match(r\"^\\d{4}-\\d{2}$\"),\n",
    "        \"team_id_name_coupling\": lambda df: (\n",
    "            (df[\"TEAM_ID\"].isna() & df[\"TEAM_NAME\"].notna()) |\n",
    "            (df[\"TEAM_ID\"].notna() & df[\"TEAM_NAME\"].isna())\n",
    "        ),\n",
    "        \"season_type_allowed\": lambda df: ~df[\"SEASON_TYPE\"].isin(\n",
    "            [\"Regular Season\", \"Playoffs\", \"Pre Season\", \"All Star\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def _example_validity_rules() -> Dict[str, Dict[str, Any]]:\n",
    "    return {\n",
    "        \"PLAYER_ID\": {\"dtype\": \"int64\", \"ge\": 0},\n",
    "        \"PLAYER_NAME\": {\"dtype\": \"object\"},\n",
    "        \"SEASON\": {\"regex\": r\"^\\d{4}-\\d{2}$\"},\n",
    "        \"SEASON_TYPE\": {\"in\": [\"Regular Season\", \"Playoffs\", \"Pre Season\", \"All Star\"]},\n",
    "        \"E_OFF_RATING\": {\"ge\": 0, \"le\": 200},\n",
    "    }\n",
    "\n",
    "\n",
    "def example_usage() -> None:\n",
    "    \"\"\"Demonstrates different output formats.\"\"\"\n",
    "    df = _example_fetch()\n",
    "    \n",
    "    # Default table format\n",
    "    print(\"=== TABLE FORMAT (DEFAULT) ===\")\n",
    "    table_result = run_quality_suite(\n",
    "        df,\n",
    "        required_columns=[\"PLAYER_ID\", \"PLAYER_NAME\", \"SEASON\", \"SEASON_TYPE\"],\n",
    "        primary_key_columns=[\"PLAYER_ID\", \"SEASON\", \"SEASON_TYPE\"],\n",
    "        outlier_numeric_columns=[\"E_OFF_RATING\"],\n",
    "        consistency_rules_by_name=_example_rules(),\n",
    "        allowed_values_by_column={\"SEASON_TYPE\": [\"Regular Season\",\"Playoffs\",\"Pre Season\",\"All Star\"]},\n",
    "        reference_table=pd.DataFrame({\"TEAM_ID\": [10, 11, 12, 13], \"TEAM_NAME\": [\"X\",\"Y\",\"Z\",\"W\"]}),\n",
    "        reference_join_keys=[\"TEAM_ID\"],\n",
    "        validity_column_rules=_example_validity_rules(),\n",
    "    )\n",
    "    print(table_result)\n",
    "    print(f\"Table shape: {table_result.shape}\")\n",
    "    \n",
    "    # JSON format\n",
    "    print(\"\\n=== JSON FORMAT ===\")\n",
    "    json_result = run_quality_suite_json(\n",
    "        df,\n",
    "        required_columns=[\"PLAYER_ID\", \"PLAYER_NAME\"],\n",
    "        primary_key_columns=[\"PLAYER_ID\"],\n",
    "    )\n",
    "    print(json_result[:500] + \"...\")\n",
    "    \n",
    "    # Markdown format\n",
    "    print(\"\\n=== MARKDOWN FORMAT ===\")\n",
    "    md_result = run_quality_suite_markdown(\n",
    "        df,\n",
    "        required_columns=[\"PLAYER_ID\"],\n",
    "        primary_key_columns=[\"PLAYER_ID\"],\n",
    "    )\n",
    "    print(md_result[:500] + \"...\")\n",
    "    \n",
    "    # Raw report for programmatic access\n",
    "    print(\"\\n=== REPORT OBJECT ===\")\n",
    "    report = run_quality_suite(\n",
    "        df,\n",
    "        required_columns=[\"PLAYER_ID\"],\n",
    "        primary_key_columns=[\"PLAYER_ID\"],\n",
    "        output_format=\"report\"\n",
    "    )\n",
    "    print(f\"Report type: {type(report)}\")\n",
    "    print(f\"Nulls found: {report.nulls.per_column.shape[0]} columns checked\")\n",
    "    print(f\"Duplicates found: {report.duplicates.has_duplicates}\")\n",
    "    \n",
    "    # Save outputs\n",
    "    out = Path(\"data/quality_reports\")\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save in different formats\n",
    "    (out / \"example_report.json\").write_text(json_result)\n",
    "    (out / \"example_report.md\").write_text(md_result)\n",
    "    table_result.to_csv(out / \"example_report.csv\", index=False)\n",
    "    \n",
    "    html_result = run_quality_suite_html(\n",
    "        df,\n",
    "        required_columns=[\"PLAYER_ID\"],\n",
    "        primary_key_columns=[\"PLAYER_ID\"],\n",
    "        table_caption=\"Data Quality Report - Player Metrics\"\n",
    "    )\n",
    "    (out / \"example_report.html\").write_text(html_result)\n",
    "    \n",
    "    print(f\"\\nReports saved to {out}/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8ebf2",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dffb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/eda/data_pull.py\n",
    "\"\"\"\n",
    "\n",
    "**Checks on the data source**\n",
    "Pull in the data_check_utils to utilize the below, all arguments are optional:\n",
    "report = run_quality_suite(\n",
    "    df,\n",
    "    required_columns=[\"PLAYER_ID\", \"PLAYER_NAME\", \"SEASON\", \"SEASON_TYPE\"],\n",
    "    primary_key_columns=[\"PLAYER_ID\", \"SEASON\", \"SEASON_TYPE\"],\n",
    "    outlier_numeric_columns=[\"E_OFF_RATING\"],\n",
    "    consistency_rules_by_name=_example_rules(),\n",
    "    allowed_values_by_column={\"SEASON_TYPE\": [\"Regular Season\",\"Playoffs\",\"Pre Season\",\"All Star\"]},\n",
    "    reference_table=pd.DataFrame({\"TEAM_ID\": [10, 11, 12, 13], \"TEAM_NAME\": [\"X\",\"Y\",\"Z\",\"W\"]}),\n",
    "    reference_join_keys=[\"TEAM_ID\"],\n",
    "    validity_column_rules=_example_validity_rules(),\n",
    ")\n",
    "    \n",
    "**Data Saving Utils**\n",
    "Pull in the utils for the logging/duckdb/s3/etc.\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7d904",
   "metadata": {},
   "source": [
    "# Dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/dags/exampledag.py\n",
    "\"\"\"\n",
    "## Astronaut ETL example DAG\n",
    "\n",
    "This DAG queries the list of astronauts currently in space from the\n",
    "Open Notify API and prints each astronaut's name and flying craft.\n",
    "\n",
    "There are two tasks, one to get the data from the API and save the results,\n",
    "and another to print the results. Both tasks are written in Python using\n",
    "Airflow's TaskFlow API, which allows you to easily turn Python functions into\n",
    "Airflow tasks, and automatically infer dependencies and pass data.\n",
    "\n",
    "The second task uses dynamic task mapping to create a copy of the task for\n",
    "each Astronaut in the list retrieved from the API. This list will change\n",
    "depending on how many Astronauts are in space, and the DAG will adjust\n",
    "accordingly each time it runs.\n",
    "\n",
    "For more explanation and getting started instructions, see our Write your\n",
    "first DAG tutorial: https://www.astronomer.io/docs/learn/get-started-with-airflow\n",
    "\n",
    "![Picture of the ISS](https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2010/02/space_station_over_earth/10293696-3-eng-GB/Space_Station_over_Earth_card_full.jpg)\n",
    "\"\"\"\n",
    "\n",
    "from airflow.sdk.definitions.asset import Asset\n",
    "from airflow.decorators import dag, task\n",
    "from pendulum import datetime\n",
    "import requests\n",
    "\n",
    "\n",
    "# Define the basic parameters of the DAG, like schedule and start_date\n",
    "@dag(\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    doc_md=__doc__,\n",
    "    default_args={\"owner\": \"Astro\", \"retries\": 3},\n",
    "    tags=[\"example\"],\n",
    ")\n",
    "def example_astronauts():\n",
    "    # Define tasks\n",
    "    @task(\n",
    "        # Define a dataset outlet for the task. This can be used to schedule downstream DAGs when this task has run.\n",
    "        outlets=[Asset(\"current_astronauts\")]\n",
    "    )  # Define that this task updates the `current_astronauts` Dataset\n",
    "    def get_astronauts(**context) -> list[dict]:\n",
    "        \"\"\"\n",
    "        This task uses the requests library to retrieve a list of Astronauts\n",
    "        currently in space. The results are pushed to XCom with a specific key\n",
    "        so they can be used in a downstream pipeline. The task returns a list\n",
    "        of Astronauts to be used in the next task.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            r = requests.get(\"http://api.open-notify.org/astros.json\")\n",
    "            r.raise_for_status()\n",
    "            number_of_people_in_space = r.json()[\"number\"]\n",
    "            list_of_people_in_space = r.json()[\"people\"]\n",
    "        except Exception:\n",
    "            print(\"API currently not available, using hardcoded data instead.\")\n",
    "            number_of_people_in_space = 12\n",
    "            list_of_people_in_space = [\n",
    "                {\"craft\": \"ISS\", \"name\": \"Oleg Kononenko\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Nikolai Chub\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Tracy Caldwell Dyson\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Matthew Dominick\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Michael Barratt\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Jeanette Epps\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Alexander Grebenkin\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Butch Wilmore\"},\n",
    "                {\"craft\": \"ISS\", \"name\": \"Sunita Williams\"},\n",
    "                {\"craft\": \"Tiangong\", \"name\": \"Li Guangsu\"},\n",
    "                {\"craft\": \"Tiangong\", \"name\": \"Li Cong\"},\n",
    "                {\"craft\": \"Tiangong\", \"name\": \"Ye Guangfu\"},\n",
    "            ]\n",
    "\n",
    "        context[\"ti\"].xcom_push(\n",
    "            key=\"number_of_people_in_space\", value=number_of_people_in_space\n",
    "        )\n",
    "        return list_of_people_in_space\n",
    "\n",
    "    @task\n",
    "    def print_astronaut_craft(greeting: str, person_in_space: dict) -> None:\n",
    "        \"\"\"\n",
    "        This task creates a print statement with the name of an\n",
    "        Astronaut in space and the craft they are flying on from\n",
    "        the API request results of the previous task, along with a\n",
    "        greeting which is hard-coded in this example.\n",
    "        \"\"\"\n",
    "        craft = person_in_space[\"craft\"]\n",
    "        name = person_in_space[\"name\"]\n",
    "\n",
    "        print(f\"{name} is currently in space flying on the {craft}! {greeting}\")\n",
    "\n",
    "    # Use dynamic task mapping to run the print_astronaut_craft task for each\n",
    "    # Astronaut in space\n",
    "    print_astronaut_craft.partial(greeting=\"Hello! :)\").expand(\n",
    "        person_in_space=get_astronauts()  # Define dependencies using TaskFlow API syntax\n",
    "    )\n",
    "\n",
    "\n",
    "# Instantiate the DAG\n",
    "example_astronauts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49753922",
   "metadata": {},
   "source": [
    "# Plugins\n",
    "- plugins: Add custom or community plugins for your project to this file. It is empty by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b572e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/plugins/custom_operator.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a42d3e",
   "metadata": {},
   "source": [
    "# testing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ac4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile api/src/airflow_project/tests/dags/test_dag_example.py\n",
    "\"\"\"Example DAGs test. This test ensures that all Dags have tags, retries set to two, and no import errors. This is an example pytest and may not be fit the context of your DAGs. Feel free to add and remove tests.\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "import pytest\n",
    "from airflow.models import DagBag\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def suppress_logging(namespace):\n",
    "    logger = logging.getLogger(namespace)\n",
    "    old_value = logger.disabled\n",
    "    logger.disabled = True\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        logger.disabled = old_value\n",
    "\n",
    "\n",
    "def get_import_errors():\n",
    "    \"\"\"\n",
    "    Generate a tuple for import errors in the dag bag\n",
    "    \"\"\"\n",
    "    with suppress_logging(\"airflow\"):\n",
    "        dag_bag = DagBag(include_examples=False)\n",
    "\n",
    "        def strip_path_prefix(path):\n",
    "            return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n",
    "\n",
    "        # prepend \"(None,None)\" to ensure that a test object is always created even if it's a no op.\n",
    "        return [(None, None)] + [\n",
    "            (strip_path_prefix(k), v.strip()) for k, v in dag_bag.import_errors.items()\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_dags():\n",
    "    \"\"\"\n",
    "    Generate a tuple of dag_id, <DAG objects> in the DagBag\n",
    "    \"\"\"\n",
    "    with suppress_logging(\"airflow\"):\n",
    "        dag_bag = DagBag(include_examples=False)\n",
    "\n",
    "    def strip_path_prefix(path):\n",
    "        return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n",
    "\n",
    "    return [(k, v, strip_path_prefix(v.fileloc)) for k, v in dag_bag.dags.items()]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"rel_path,rv\", get_import_errors(), ids=[x[0] for x in get_import_errors()]\n",
    ")\n",
    "def test_file_imports(rel_path, rv):\n",
    "    \"\"\"Test for import errors on a file\"\"\"\n",
    "    if rel_path and rv:\n",
    "        raise Exception(f\"{rel_path} failed to import with message \\n {rv}\")\n",
    "\n",
    "\n",
    "APPROVED_TAGS = {}\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"dag_id,dag,fileloc\", get_dags(), ids=[x[2] for x in get_dags()]\n",
    ")\n",
    "def test_dag_tags(dag_id, dag, fileloc):\n",
    "    \"\"\"\n",
    "    test if a DAG is tagged and if those TAGs are in the approved list\n",
    "    \"\"\"\n",
    "    assert dag.tags, f\"{dag_id} in {fileloc} has no tags\"\n",
    "    if APPROVED_TAGS:\n",
    "        assert not set(dag.tags) - APPROVED_TAGS\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"dag_id,dag, fileloc\", get_dags(), ids=[x[2] for x in get_dags()]\n",
    ")\n",
    "def test_dag_retries(dag_id, dag, fileloc):\n",
    "    \"\"\"\n",
    "    test if a DAG has retries set\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        dag.default_args.get(\"retries\", None) >= 2\n",
    "    ), f\"{dag_id} in {fileloc} must have task retries >= 2.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
