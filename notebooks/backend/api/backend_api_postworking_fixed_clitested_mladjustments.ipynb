{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1c6b95",
   "metadata": {},
   "source": [
    "# Root Level files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22383fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "# Centralized configuration for all environments\n",
    "# This file is the single source of truth for all configurable values\n",
    "# Environment variables still override these values (12-factor app compliance)\n",
    "\n",
    "default:\n",
    "  # MLflow Configuration\n",
    "  MLFLOW_EXPERIMENT: \"ml_fullstack_models\"\n",
    "  MLFLOW_TRACKING_URI: \"file:api/mlruns_local\"\n",
    "  MLFLOW_REGISTRY_URI: \"file:api/mlruns_local\"\n",
    "  RETAIN_RUNS_PER_MODEL: 5\n",
    "  MLFLOW_GC_AFTER_TRAIN: 1\n",
    "\n",
    "  # Rate Limiting Configuration\n",
    "  RATE_LIMIT_WINDOW: 60\n",
    "  RATE_LIMIT_WINDOW_LIGHT: 300\n",
    "  RATE_LIMIT_LOGIN_WINDOW: 20\n",
    "  ENABLE_RATE_LIMIT: 1\n",
    "\n",
    "  # Quality Gate Thresholds\n",
    "  QUALITY_GATE_ACCURACY_THRESHOLD: 0.85\n",
    "  QUALITY_GATE_F1_THRESHOLD: 0.85\n",
    "\n",
    "  # MLOps Configuration\n",
    "  ENVIRONMENT: \"development\"\n",
    "  REQUIRE_MODEL_APPROVAL: 0\n",
    "  AUTO_PROMOTE_TO_PRODUCTION: 0\n",
    "  ENABLE_MODEL_COMPARISON: 1\n",
    "  MODEL_AUDIT_ENFORCEMENT: \"warn\"\n",
    "  MAX_MODEL_VERSIONS_PER_MODEL: 10\n",
    "\n",
    "  # JAX/XLA Configuration\n",
    "  XLA_FLAGS: \"--xla_force_host_platform_device_count=1\"\n",
    "  PYTENSOR_FLAGS: \"device=cpu,floatX=float32\"\n",
    "\n",
    "  # Model Training Flags\n",
    "  SKIP_BACKGROUND_TRAINING: 0\n",
    "  AUTO_TRAIN_MISSING: 1\n",
    "  UNIT_TESTING: 0\n",
    "\n",
    "  # Debug Flags\n",
    "  DEBUG_RATELIMIT: 0\n",
    "\n",
    "  # Security\n",
    "  ACCESS_TOKEN_EXPIRE_MINUTES: 30\n",
    "\n",
    "  # Prediction caching (Redis)\n",
    "  CACHE_ENABLED: 0           # 0 = disabled, 1 = enabled\n",
    "  CACHE_TTL_MINUTES: 60      # how long to cache predictions (in minutes)\n",
    "\n",
    "  # Canonical API base URLs (NOT auto-exposed to frontend – no VITE_ prefix)\n",
    "  # Local developer FastAPI port\n",
    "  LOCAL_VITE_API_BASE: \"http://127.0.0.1:8000\"\n",
    "  # Staging base (can override below; fallback -> local if not set)\n",
    "  STAGING_VITE_API_BASE: \"http://127.0.0.1:8000\"\n",
    "  # Railway production public FastAPI base\n",
    "  RAILWAY_VITE_API_BASE: \"https://fastapi-production-1d13.up.railway.app\"\n",
    "\n",
    "dev:\n",
    "  # Database\n",
    "  DATABASE_URL: \"sqlite+aiosqlite:///./app.db\"\n",
    "  SECRET_KEY: \"dev-secret-key-change-in-production\"\n",
    "  VITE_API_URL: \"http://127.0.0.1:8000\"\n",
    "  \n",
    "  # CORS\n",
    "  ALLOWED_ORIGINS: \"http://localhost:3000,http://127.0.0.1:3000\"\n",
    "\n",
    "  # Redis Configuration\n",
    "  REDIS_URL: \"redis://localhost:6379\"\n",
    "\n",
    "  # Rate Limiting (relaxed for development)\n",
    "  RATE_LIMIT_DEFAULT: 120\n",
    "  RATE_LIMIT_CANCER: 60\n",
    "  RATE_LIMIT_LOGIN: 10\n",
    "  RATE_LIMIT_TRAINING: 5\n",
    "  RATE_LIMIT_WINDOW_LIGHT: 600\n",
    "\n",
    "  # MLflow Configuration (local development)\n",
    "  MLFLOW_TRACKING_URI: \"file:api/mlruns_local\"\n",
    "  MLFLOW_REGISTRY_URI: \"file:api/mlruns_local\"\n",
    "  RETAIN_RUNS_PER_MODEL: 10\n",
    "  MLFLOW_GC_AFTER_TRAIN: 0\n",
    "\n",
    "  # Model Training Flags (enabled for development)\n",
    "  SKIP_BACKGROUND_TRAINING: 0\n",
    "  AUTO_TRAIN_MISSING: 1\n",
    "\n",
    "  # Debug Flags (enabled for development)\n",
    "  DEBUG_RATELIMIT: 1\n",
    "\n",
    "  # Rate Limiting (disabled in development for easier testing)\n",
    "  ENABLE_RATE_LIMIT: 0\n",
    "\n",
    "  # MLOps Configuration (relaxed for development)\n",
    "  ENVIRONMENT: \"development\"\n",
    "  REQUIRE_MODEL_APPROVAL: 0\n",
    "  AUTO_PROMOTE_TO_PRODUCTION: 0\n",
    "  ENABLE_MODEL_COMPARISON: 1\n",
    "  MODEL_AUDIT_ENFORCEMENT: \"warn\"\n",
    "  MAX_MODEL_VERSIONS_PER_MODEL: 15\n",
    "  QUALITY_GATE_ACCURACY_THRESHOLD: 0.85\n",
    "  QUALITY_GATE_F1_THRESHOLD: 0.85\n",
    "\n",
    "  # Enable caching in dev, with short TTL for testing\n",
    "  CACHE_ENABLED: 1\n",
    "  CACHE_TTL_MINUTES: 5\n",
    "\n",
    "staging:\n",
    "  # Database\n",
    "  DATABASE_URL: \"sqlite+aiosqlite:///./app.db\"\n",
    "  SECRET_KEY: \"staging-secret-key-change-in-production\"\n",
    "  VITE_API_URL: \"http://127.0.0.1:8000\"\n",
    "  # CORS\n",
    "  ALLOWED_ORIGINS: \"https://staging-frontend.railway.app\"\n",
    "\n",
    "  # Redis Configuration\n",
    "  REDIS_URL: \"${REDIS_URL}\"\n",
    "\n",
    "  # Rate Limiting (stricter than dev)\n",
    "  RATE_LIMIT_DEFAULT: 60\n",
    "  RATE_LIMIT_CANCER: 30\n",
    "  RATE_LIMIT_LOGIN: 3\n",
    "  RATE_LIMIT_TRAINING: 2\n",
    "\n",
    "  # MLflow Configuration (staging environment)\n",
    "  MLFLOW_TRACKING_URI: \"file:/data/mlruns\"\n",
    "  MLFLOW_REGISTRY_URI: \"file:/data/mlruns\"\n",
    "\n",
    "  # Model Training Flags (enabled for self-healing)\n",
    "  SKIP_BACKGROUND_TRAINING: 0   # allow background loader\n",
    "  AUTO_TRAIN_MISSING: 1         # stub/bayesian self-train if missing\n",
    "\n",
    "  # Debug Flags (disabled in staging)\n",
    "  DEBUG_RATELIMIT: 0\n",
    "\n",
    "  # Rate Limiting (disabled in staging unless Redis is available)\n",
    "  ENABLE_RATE_LIMIT: 0\n",
    "\n",
    "  # MLOps Configuration (strict for staging)\n",
    "  ENVIRONMENT: \"staging\"\n",
    "  REQUIRE_MODEL_APPROVAL: 1\n",
    "  AUTO_PROMOTE_TO_PRODUCTION: 0\n",
    "  ENABLE_MODEL_COMPARISON: 1\n",
    "  MODEL_AUDIT_ENFORCEMENT: \"warn\"\n",
    "  MAX_MODEL_VERSIONS_PER_MODEL: 8\n",
    "  QUALITY_GATE_ACCURACY_THRESHOLD: 0.90\n",
    "  QUALITY_GATE_F1_THRESHOLD: 0.90\n",
    "\n",
    "  # Keep same defaults in staging\n",
    "  CACHE_ENABLED: 0\n",
    "  CACHE_TTL_MINUTES: 60\n",
    "\n",
    "  # Optionally override STAGING_VITE_API_BASE here if you later have a staging FastAPI URL\n",
    "  # STAGING_VITE_API_BASE: \"https://your-staging-fastapi.up.railway.app\"\n",
    "\n",
    "prod:\n",
    "  # Database\n",
    "  DATABASE_URL: \"sqlite+aiosqlite:///./app.db\"\n",
    "  SECRET_KEY: \"must-come-from-env\"\n",
    "  \n",
    "  # adjust this for prod\n",
    "  VITE_API_URL: \"https://fastapi-production-1d13.up.railway.app\"\n",
    "  # CORS\n",
    "  ALLOWED_ORIGINS: \"https://react-frontend-production-2805.up.railway.app\"\n",
    "\n",
    "  # Redis Configuration\n",
    "  REDIS_URL: \"${REDIS_URL}\"\n",
    "\n",
    "  # Rate Limiting (strictest settings)\n",
    "  RATE_LIMIT_DEFAULT: 60\n",
    "  RATE_LIMIT_CANCER: 30\n",
    "  RATE_LIMIT_LOGIN: 3\n",
    "  RATE_LIMIT_TRAINING: 2\n",
    "\n",
    "  # MLflow Configuration (production environment)\n",
    "  MLFLOW_TRACKING_URI: \"file:/data/mlruns\"\n",
    "  MLFLOW_REGISTRY_URI: \"file:/data/mlruns\"\n",
    "\n",
    "  # Model Training Flags \n",
    "  SKIP_BACKGROUND_TRAINING: 0\n",
    "  AUTO_TRAIN_MISSING: 1\n",
    "\n",
    "  # Debug Flags \n",
    "  DEBUG_RATELIMIT: 1\n",
    "\n",
    "  # Rate Limiting (enabled in production if Redis is available)\n",
    "  ENABLE_RATE_LIMIT: 1\n",
    "\n",
    "  # MLOps Configuration (strictest for production)\n",
    "  ENVIRONMENT: \"production\"\n",
    "  REQUIRE_MODEL_APPROVAL: 1\n",
    "  AUTO_PROMOTE_TO_PRODUCTION: 0\n",
    "  ENABLE_MODEL_COMPARISON: 1\n",
    "  MODEL_AUDIT_ENFORCEMENT: \"fail\"\n",
    "  MAX_MODEL_VERSIONS_PER_MODEL: 5\n",
    "  QUALITY_GATE_ACCURACY_THRESHOLD: 0.92\n",
    "  QUALITY_GATE_F1_THRESHOLD: 0.92\n",
    "  \n",
    "\n",
    "  # Generally off in prod unless toggled via env\n",
    "  CACHE_ENABLED: 0\n",
    "  CACHE_TTL_MINUTES: 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63246651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting package.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile package.json\n",
    "{\n",
    "  \"name\": \"fastapi-react-monorepo\",\n",
    "  \"private\": true,\n",
    "  \"type\": \"module\",\n",
    "  \"scripts\": {\n",
    "    \"env:clean\": \"node -e \\\"const fs = require('fs'); const { execSync } = require('child_process'); function forceDelete(dir) { try { if (process.platform === 'win32') { try { execSync('taskkill /F /IM python.exe 2>nul', { stdio: 'ignore' }); } catch {} } if (fs.existsSync(dir)) { fs.rmSync(dir, { recursive: true, force: true }); console.log('Cleaned .venv directory'); } else { console.log('No .venv directory to clean'); } } catch (err) { console.warn('Warning: Could not fully clean .venv directory:', err.message); console.log('This is usually not a problem - continuing...'); } } forceDelete('.venv');\\\"\",\n",
    "    \"env:create\": \"uv python install 3.12 && uv venv --python 3.12 .venv\",\n",
    "    \"env:sync\": \"cross-env VIRTUAL_ENV= uv pip install --python .venv/Scripts/python.exe --upgrade pip setuptools wheel && cross-env VIRTUAL_ENV= uv pip install --python .venv/Scripts/python.exe -e api\",\n",
    "    \"env:sync:unix\": \"cross-env VIRTUAL_ENV= uv pip install --python .venv/bin/python --upgrade pip setuptools wheel && cross-env VIRTUAL_ENV= uv pip install --python .venv/bin/python -e api\",\n",
    "    \"env:full\": \"npm run env:clean && npm run env:create && npm run env:sync && npm --prefix web ci && uv lock --upgrade\",\n",
    "    \"env:full:unix\": \"npm run env:clean && npm run env:create && npm run env:sync:unix && npm --prefix web ci\",\n",
    "    \"install:all\": \"npm run env:full\",\n",
    "    \"install:all:unix\": \"npm run env:full:unix\",\n",
    "    \"seed\": \"cross-env uv run --python .venv/Scripts/python.exe api/scripts/seed_user.py\",\n",
    "    \"seed:unix\": \"cross-env uv run --python .venv/bin/python api/scripts/seed_user.py\",\n",
    "\n",
    "    \"frontend:env:dev\": \"cross-env APP_ENV=dev node scripts/config-to-env.mjs dev\",\n",
    "    \"frontend:env:staging\": \"cross-env APP_ENV=staging node scripts/config-to-env.mjs staging\",\n",
    "    \"frontend:env:prod\": \"cross-env APP_ENV=prod node scripts/config-to-env.mjs prod\",\n",
    "\n",
    "    \"dev:dev\": \"npm run frontend:env:dev && concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\"npm run backend:reload\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"dev:staging\": \"npm run frontend:env:staging && concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\"npm run backend:staging\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"dev:prod\": \"npm run frontend:env:prod && concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\"npm run backend:prod\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "\n",
    "    \"dev\": \"node scripts/config-to-env.mjs && concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\"npm run backend:reload\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"dev:unix\": \"node scripts/config-to-env.mjs && concurrently -n \\\"API,WEB\\\" -c \\\"cyan,magenta\\\" \\\"npm run backend:reload:unix\\\" \\\"npm --prefix web run dev\\\"\",\n",
    "    \"env:switch\": \"node api/scripts/env-switch.mjs\",\n",
    "    \"backend:reload\": \"cross-env APP_ENV=dev VIRTUAL_ENV= uv run --python .venv/Scripts/python.exe uvicorn app.main:app --reload --app-dir api --env-file api/env.dev\",\n",
    "    \"backend:reload:unix\": \"cross-env APP_ENV=dev VIRTUAL_ENV= uv run --python .venv/bin/python uvicorn app.main:app --reload --app-dir api --env-file api/env.dev\",\n",
    "    \"backend\": \"cross-env VIRTUAL_ENV= uv run --python .venv/Scripts/python.exe uvicorn app.main:app --host 0.0.0.0 --port 8000 --app-dir api --env-file api/.env\",\n",
    "    \"backend:unix\": \"cross-env VIRTUAL_ENV= uv run --python .venv/bin/python uvicorn app.main:app --host 0.0.0.0 --port 8000 --app-dir api --env-file api/.env\",\n",
    "    \"backend:dev\": \"npm run env:switch env.dev      && cross-env APP_ENV=dev     npm run backend\",\n",
    "    \"backend:staging\": \"npm run config:sync && npm run ml:promote:staging iris_random_forest && npm run env:switch env.staging  && cross-env APP_ENV=staging npm run backend\",\n",
    "    \"backend:prod\": \"npm run config:sync && npm run ml:promote:prod iris_random_forest     && npm run env:switch env.prod     && cross-env APP_ENV=prod    npm run backend\",\n",
    "    \"frontend\": \"npm --prefix web run dev\",\n",
    "    \"ensure:models\": \"cross-env uv run --python .venv/Scripts/python.exe api/scripts/ensure_models.py\",\n",
    "    \"ensure:models:unix\": \"cross-env uv run --python .venv/bin/python api/scripts/ensure_models.py\",\n",
    "    \"build:web\": \"npm --prefix web run build\",\n",
    "    \"debug\": \"node -e \\\"setTimeout(() => process.exit(0), 3000)\\\" && curl -s http://127.0.0.1:8000/api/v1/health && echo. && curl -s -X POST -d \\\"username=alice&password=secret\\\" -H \\\"Content-Type: application/x-www-form-urlencoded\\\" http://127.0.0.1:8000/api/v1/token\",\n",
    "    \"validate:setup\": \"node -e \\\"console.log('Validating setup...'); const fs = require('fs'); const path = require('path'); const checks = [{name: '.venv exists', check: () => fs.existsSync('.venv')}, {name: 'api/pyproject.toml exists', check: () => fs.existsSync('api/pyproject.toml')}, {name: 'web/package.json exists', check: () => fs.existsSync('web/package.json')}, {name: 'web/node_modules exists', check: () => fs.existsSync('web/node_modules')}]; checks.forEach(({name, check}) => console.log(name + ':', check() ? '✓' : '✗')); console.log('Setup validation complete.');\\\"\",\n",
    "    \"frontend:clean\": \"node scripts/frontend-clean.js\",\n",
    "    \"frontend:install\": \"npm --prefix web ci\",\n",
    "    \"frontend:rebuild\": \"npm run frontend:clean && npm run frontend:install\",\n",
    "    \"frontend:rebuild-lock\": \"node scripts/frontend-clean.js --zap-lock && npm --prefix web install --package-lock-only && npm --prefix web ci\",\n",
    "    \"frontend:diagnose\": \"node scripts/frontend-diagnose.js\",\n",
    "    \"frontend:verify-lock\": \"node scripts/verify-frontend-lock.js\",\n",
    "    \"test:api\": \"cross-env uv run --python .venv/Scripts/python.exe pytest api/tests\",\n",
    "    \"test:api:unix\": \"cross-env uv run --python .venv/bin/python pytest api/tests\",\n",
    "    \"test:api:endpoints\": \"cross-env uv run --python .venv/Scripts/python.exe pytest api/tests/test_endpoints.py\",\n",
    "    \"test:api:endpoints:unix\": \"cross-env uv run --python .venv/bin/python pytest api/tests/test_endpoints.py\",\n",
    "    \"test:endpoints\": \"npm run test:api:endpoints\",\n",
    "    \"test:endpoints:unix\": \"npm run test:api:endpoints:unix\",\n",
    "    \"test:all\": \"npm run test:api\",\n",
    "    \"test:all:unix\": \"npm run test:api:unix\",\n",
    "    \"test:all:watch\": \"npm run test:all -- --watch\",\n",
    "    \"test:all:watch:unix\": \"npm run test:all:unix -- --watch\",\n",
    "    \"ml:promote:staging\": \"python api/scripts/promote.py staging\",\n",
    "    \"ml:promote:prod\": \"python api/scripts/promote.py prod\",\n",
    "    \"config:sync\": \"node api/scripts/config-sync.mjs\",\n",
    "    \"backend:staging:logreg\": \"npm run config:sync && npm run ml:promote:staging iris_logreg && npm run env:switch env.staging && cross-env APP_ENV=staging npm run backend\",\n",
    "    \"backend:prod:logreg\": \"npm run config:sync && npm run ml:promote:prod iris_logreg && npm run env:switch env.prod && cross-env APP_ENV=prod npm run backend\",\n",
    "    \"railway:auth\": \"powershell -ExecutionPolicy Bypass -File scripts/railway_login.ps1\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"concurrently\": \"^8.2.2\",\n",
    "    \"cross-env\": \"^7.0.3\",\n",
    "    \"rimraf\": \"^5.0.5\",\n",
    "    \"semver\": \"^7.6.0\",\n",
    "    \"yaml\": \"^2.8.0\"\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "17a95ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting invoke.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile invoke.yml\n",
    "# invoke.yml (simplified to delegate env mgmt to uv via npm scripts)\n",
    "tasks:\n",
    "  create-env:\n",
    "    - npm run env:full\n",
    "\n",
    "  update-env:\n",
    "    - npm run env:sync\n",
    "\n",
    "  dev:\n",
    "    - npm run dev\n",
    "\n",
    "  test:\n",
    "    - .venv\\Scripts\\python.exe -m pytest api/tests -v --cov=api --cov-report=xml\n",
    "\n",
    "  lint:\n",
    "    - .venv\\Scripts\\python.exe -m black --check api\n",
    "    - .venv\\Scripts\\python.exe -m isort --check-only api\n",
    "    - .venv\\Scripts\\python.exe -m flake8 api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b816b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gitignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gitignore\n",
    ".env\n",
    "dev.env\n",
    ".devcontainer/.env.runtime\n",
    "\n",
    "mlruns/\n",
    "mlflow_db/\n",
    "mlruns_local/\n",
    "\n",
    "node_modules/\n",
    "frontend/node_modules/\n",
    "\n",
    "archive/\n",
    ".venv\n",
    "uv.lock\n",
    "\n",
    "test_iris.json\n",
    "#.env.template\n",
    "\n",
    "# Railway CLI (never commit tokens)\n",
    ".railway/config.json\n",
    "\n",
    "archive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edd77621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile env.template\n",
    "ENV_NAME=\"react_fastapi_railway\"\n",
    "CUDA_TAG=\"12.8.0\"\n",
    "DOCKER_BUILDKIT=\"1\"\n",
    "HOST_JUPYTER_PORT=\"8890\"\n",
    "HOST_TENSORBOARD_PORT=\"6008\"\n",
    "HOST_EXPLAINER_PORT=\"8050\"\n",
    "HOST_STREAMLIT_PORT=\"8501\"\n",
    "HOST_MLFLOW_PORT=\"5000\"\n",
    "HOST_APP_PORT=\"5100\"\n",
    "HOST_BACKEND_DEV_PORT=\"5002\"\n",
    "MLFLOW_TRACKING_URI=\"http://mlflow:5000\"\n",
    "MLFLOW_VERSION=\"2.12.2\"\n",
    "PYTHON_VER=\"3.10\"\n",
    "JAX_PLATFORM_NAME=\"gpu\"\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=\"true\"\n",
    "XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"\n",
    "XLA_PYTHON_CLIENT_MEM_FRACTION=\"0.95\"\n",
    "XLA_FLAGS=\"--xla_force_host_platform_device_count=1\"\n",
    "JAX_DISABLE_JIT=\"false\"\n",
    "JAX_ENABLE_X64=\"false\"\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=\"false\"\n",
    "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=\"8589934592\"\n",
    "RAILWAY_API_TOKEN=\"\"\n",
    "RAILWAY_API_URL=\"\"\n",
    "RAILWAY_VITE_API_URL=\"https://fastapi-production-1d13.up.railway.app\"\n",
    "VITE_API_URL=http://127.0.0.1:8000/api/v1\n",
    "REACT_APP_API_URL=\"https://react-frontend-production-2805.up.railway.app\"\n",
    "SECRET_KEY=\"change-me-in-prod\"\n",
    "USERNAME_KEY=\"alice\"\n",
    "USER_PASSWORD=\"supersecretvalue\"\n",
    "DATABASE_URL=\"sqlite+aiosqlite:///./app.db\"\n",
    "RAILWAY_ENVIRONMENT=\"production\"\n",
    "RAILWAY_ENVIRONMENT_ID=\"fa10dc06-75ec-4c11-93d4-a0fde17996d0\"\n",
    "RAILWAY_ENVIRONMENT_NAME=\"production\"\n",
    "RAILWAY_PRIVATE_DOMAIN=\"empowering-appreciation.railway.internal\"\n",
    "RAILWAY_PROJECT_ID=\"fc9da558-31d6-4b28-9eda-2bbe56cc7390\"\n",
    "RAILWAY_PROJECT_NAME=\"responsible-abundance\"\n",
    "RAILWAY_SERVICE_ID=\"87c129ab-ba49-471a-88bb-853ace60180d\"\n",
    "RAILWAY_SERVICE_NAME=\"empowering-appreciation\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5d07af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyproject.toml\n",
    "[project]\n",
    "name = \"react_fastapi_railway\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Pytorch and Jax GPU docker container\"\n",
    "authors = [\n",
    "  { name = \"Geoffrey Hadfield\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  # Core web framework\n",
    "  \"fastapi>=0.104.0\",\n",
    "  \"uvicorn[standard]>=0.24.0\",\n",
    "  \"python-dotenv>=1.0.0\",\n",
    "\n",
    "  # Settings and validation\n",
    "  \"pydantic>=2.0.0\",\n",
    "  \"pydantic-settings>=2.0.0\",\n",
    "\n",
    "  # HTTP client and multipart parsing\n",
    "  \"httpx>=0.24.0\",\n",
    "  \"python-multipart>=0.0.6\",\n",
    "\n",
    "  # Data & ML basics\n",
    "  \"numpy>=1.24.0\",\n",
    "  \"pandas>=2.1.0\",\n",
    "  \"scikit-learn>=1.3.0\",\n",
    "  \"mlflow>=2.8.0\",\n",
    "\n",
    "  # (Your existing extras—keep if you still need them)\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.2.8,<1.3.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard==0.5.1\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",\n",
    "  \"numpyro>=0.18.0,<1.0.0\",\n",
    "  \"jax==0.6.0\",\n",
    "  \"jaxlib==0.6.0\",\n",
    "  \"pytensor>=2.18.3\",\n",
    "  \"aesara>=2.9.4\",\n",
    "  \"tqdm>=4.67.0\",\n",
    "  \"pyarrow>=12.0.0\",\n",
    "  \"optuna>=3.0.0\",\n",
    "  \"optuna-integration[mlflow]>=0.2.0\",\n",
    "  \"omegaconf>=2.3.0,<2.4.0\",\n",
    "  \"hydra-core>=1.3.2,<1.4.0\",\n",
    "  \"aiosqlite>=0.19.0\", \n",
    "  \"python-jose[cryptography]>=3.3.0\",\n",
    "  \"passlib[bcrypt]>=1.7.4\",\n",
    "  \"bcrypt==4.0.1\",  # Pin bcrypt version to resolve warning\n",
    "  # Rate limiting\n",
    "  \"fastapi-limiter>=0.1.5\",\n",
    "  \"aioredis>=2.0.0\",\n",
    "  \"httpx>=0.24.0\",\n",
    "  \"psutil>=5.0.0,<8.0.0\",\n",
    "  \"ipykernel>=6.25.0\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"invoke>=2.2\",\n",
    "]\n",
    "\n",
    "cuda = [\n",
    "  \"cupy-cuda12x>=12.0.0\",\n",
    "]\n",
    "\n",
    "[tool.pytensor]\n",
    "device    = \"cuda\"\n",
    "floatX    = \"float32\"\n",
    "allow_gc  = true\n",
    "optimizer = \"fast_run\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe14bb",
   "metadata": {},
   "source": [
    "# helpful scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0311da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/promote.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/promote.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "MLflow Model Promotion CLI\n",
    "\n",
    "Promotes models to staging or production environments by setting MLflow aliases.\n",
    "Usage: python scripts/promote.py [staging|prod] [model_name] [version]\n",
    "\n",
    "Examples:\n",
    "  python scripts/promote.py staging iris_random_forest\n",
    "  python scripts/promote.py prod iris_random_forest 3\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the api directory to Python path (now in api/scripts/)\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "api_path = ROOT.resolve()\n",
    "sys.path.insert(0, str(api_path))\n",
    "\n",
    "from app.services.ml.model_service import ModelService\n",
    "\n",
    "\n",
    "async def promote_model(environment: str, model_name: str, version: int = None):\n",
    "    \"\"\"Promote a model to the specified environment.\"\"\"\n",
    "    \n",
    "    # Map environment to stage\n",
    "    stage_map = {\n",
    "        'staging': 'Staging',\n",
    "        'prod': 'Production'\n",
    "    }\n",
    "    \n",
    "    if environment not in stage_map:\n",
    "        print(f\"❌ Invalid environment: {environment}. Must be 'staging' or 'prod'\")\n",
    "        return False\n",
    "    \n",
    "    target_stage = stage_map[environment]\n",
    "    alias = \"prod\" if environment == \"prod\" else \"staging\"\n",
    "    \n",
    "    print(f\"🚀 Promoting {model_name} to {environment} environment...\")\n",
    "    \n",
    "    try:\n",
    "        service = ModelService()\n",
    "        await service.initialize()\n",
    "        \n",
    "        print(f\"Promoting {model_name} to {target_stage}...\")\n",
    "        \n",
    "        result = await service.promote_model_to_stage(\n",
    "            model_name=model_name,\n",
    "            target_stage=target_stage,\n",
    "            version=version\n",
    "        )\n",
    "        \n",
    "        if result.get(\"promoted\"):\n",
    "            print(f\"✅ Successfully promoted {model_name} to {environment}\")\n",
    "            print(f\"   Version: {result.get('version', 'N/A')}\")\n",
    "            print(f\"   Alias: @{alias}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Failed to promote {model_name}: {result.get('error', 'Unknown error')}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to promote {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        # Print traceback without trying to serialize the exception object\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Promote MLflow models to staging or production\")\n",
    "    parser.add_argument(\"environment\", choices=[\"staging\", \"prod\"], \n",
    "                       help=\"Target environment (staging or prod)\")\n",
    "    parser.add_argument(\"model_name\", help=\"Name of the model to promote\")\n",
    "    parser.add_argument(\"version\", nargs=\"?\", type=int, \n",
    "                       help=\"Specific version to promote (optional)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    success = asyncio.run(promote_model(\n",
    "        environment=args.environment,\n",
    "        model_name=args.model_name,\n",
    "        version=args.version\n",
    "    ))\n",
    "    \n",
    "    if success:\n",
    "        print(\"🎉 Model promotion completed successfully!\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(\"💥 Model promotion failed!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7cdcfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/railway_login.ps1\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/railway_login.ps1\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Load RAILWAY_API_TOKEN from root .env (one level up from this scripts/ folder)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "$scriptDir = Split-Path -Parent $MyInvocation.MyCommand.Definition\n",
    "$envFile   = Join-Path $scriptDir '..\\.env'\n",
    "\n",
    "if (Test-Path $envFile) {\n",
    "    Write-Host \"Reading .env from $envFile\"\n",
    "    Get-Content $envFile | ForEach-Object {\n",
    "        # matches lines like RAILWAY_API_TOKEN=abc or RAILWAY_API_TOKEN=\"abc\"\n",
    "        if ($_ -match '^\\s*RAILWAY_API_TOKEN\\s*=\\s*\"?(.+?)\"?\\s*$') {\n",
    "            $token = $Matches[1]\n",
    "            $Env:RAILWAY_API_TOKEN = $token\n",
    "            Write-Host \"Loaded RAILWAY_API_TOKEN from .env\"\n",
    "            return\n",
    "        }\n",
    "    }\n",
    "    if (-not $Env:RAILWAY_API_TOKEN) {\n",
    "        Write-Warning \"RAILWAY_API_TOKEN not found in .env. You may need to add it.\"\n",
    "    }\n",
    "} else {\n",
    "    Write-Warning \".env file not found at $envFile\"\n",
    "}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Clear existing Railway CLI configuration\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "Remove-Item -Force \"$Env:USERPROFILE\\.railway\\config.json\" -ErrorAction SilentlyContinue\n",
    "Test-Path \"$Env:USERPROFILE\\.railway\\config.json\"  # Should return False\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Clear any old env vars\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "Remove-Item Env:RAILWAY_TOKEN      -ErrorAction SilentlyContinue\n",
    "Remove-Item Env:RAILWAY_API_TOKEN  -ErrorAction SilentlyContinue\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Re-set from .env (in case Remove-Item wiped it)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "if (Test-Path $envFile) {\n",
    "    Get-Content $envFile | ForEach-Object {\n",
    "        if ($_ -match '^\\s*RAILWAY_API_TOKEN\\s*=\\s*\"?(.+?)\"?\\s*$') {\n",
    "            $Env:RAILWAY_API_TOKEN = $Matches[1]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Railway logout/login using the loaded token\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "railway logout\n",
    "railway whoami  # should show “not logged in”\n",
    "railway login  # non-interactively picks up $Env:RAILWAY_API_TOKEN\n",
    "railway whoami  # verify you’re back in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14354135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/env-switch.mjs\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/env-switch.mjs\n",
    "#!/usr/bin/env node\n",
    "/**\n",
    " * Environment switching utility\n",
    " * \n",
    " * Copies environment files to api/.env for cross-platform compatibility\n",
    " * Usage: node scripts/env-switch.mjs [env-file]\n",
    " * \n",
    " * Examples:\n",
    " *   node scripts/env-switch.mjs api/env.prod\n",
    " *   node scripts/env-switch.mjs api/env.staging\n",
    " */\n",
    "\n",
    "import { copyFileSync, existsSync } from 'fs';\n",
    "import { fileURLToPath } from 'url';\n",
    "import { dirname, join } from 'path';\n",
    "\n",
    "const __filename = fileURLToPath(import.meta.url);\n",
    "const __dirname = dirname(__filename);\n",
    "\n",
    "// Parse arguments - first arg after script name\n",
    "const [, , src = 'env.dev'] = process.argv;\n",
    "\n",
    "// Resolve relative to script directory (now in api/scripts/)\n",
    "const srcPath = join(__dirname, '..', src);\n",
    "\n",
    "if (!existsSync(srcPath)) {\n",
    "  console.error(`❌ Environment file not found: ${srcPath}`);\n",
    "  console.error('Available files:');\n",
    "  console.error('  env.dev');\n",
    "  console.error('  env.staging');\n",
    "  console.error('  env.prod');\n",
    "  process.exit(1);\n",
    "}\n",
    "\n",
    "try {\n",
    "  const targetPath = join(__dirname, '..', '.env');\n",
    "  copyFileSync(srcPath, targetPath);\n",
    "  console.log(`✔  switched to ${src}`);\n",
    "} catch (error) {\n",
    "  console.error(`❌ Failed to copy environment file: ${error.message}`);\n",
    "  process.exit(1);\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4736581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/config-sync.mjs\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/config-sync.mjs\n",
    "#!/usr/bin/env node\n",
    "/**\n",
    " * Copy root config.yaml → api/config.yaml\n",
    " * Usage: node api/scripts/config-sync.mjs\n",
    " */\n",
    "\n",
    "import { copyFileSync, existsSync } from 'fs';\n",
    "import { fileURLToPath }      from 'url';\n",
    "import { dirname, join }      from 'path';\n",
    "\n",
    "const __filename = fileURLToPath(import.meta.url);\n",
    "const __dirname  = dirname(__filename);\n",
    "\n",
    "const rootYaml = join(__dirname, '..', '..', 'config.yaml');\n",
    "const destYaml = join(__dirname, '..', 'config.yaml');\n",
    "\n",
    "if (!existsSync(rootYaml)) {\n",
    "  console.error(`❌  config.yaml not found at ${rootYaml}`);\n",
    "  process.exit(1);\n",
    "}\n",
    "\n",
    "try {\n",
    "  copyFileSync(rootYaml, destYaml);\n",
    "  console.log(`✔  overwritten api/config.yaml from root config.yaml`);\n",
    "} catch (error) {\n",
    "  console.error(`❌  Failed to copy config: ${error.message}`);\n",
    "  process.exit(1);\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c963bec",
   "metadata": {},
   "source": [
    "# Api Folder Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "877952d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/env.dev\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/env.dev\n",
    "# Local developer overrides ONLY. Most non-secret config now lives in config.yaml.\n",
    "# Copy to .env.dev (ignored) and source manually if needed.\n",
    "\n",
    "# Secrets (DO NOT COMMIT real values)\n",
    "SECRET_KEY=dev-secret-key-change-in-production\n",
    "DATABASE_URL=sqlite+aiosqlite:///./app.db\n",
    "\n",
    "# Tokens (local testing)\n",
    "USERNAME_KEY=alice\n",
    "USER_PASSWORD=supersecretvalue\n",
    "\n",
    "# Override environment if needed\n",
    "APP_ENV=dev\n",
    "\n",
    "# Optional local MLflow server (uncomment to override YAML)\n",
    "#MLFLOW_TRACKING_URI=http://127.0.0.1:5000\n",
    "\n",
    "# Redis (local)\n",
    "REDIS_URL=redis://localhost:6379 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6be8176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/env.staging\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/env.staging\n",
    "# Local developer overrides ONLY. Most non-secret config now lives in config.yaml.\n",
    "# Copy to .env.staging (ignored) and source manually if needed.\n",
    "\n",
    "# Secrets (DO NOT COMMIT real values)\n",
    "SECRET_KEY=staging-secret-key-change-in-production\n",
    "DATABASE_URL=sqlite+aiosqlite:///./app.db\n",
    "\n",
    "# Tokens (local testing)\n",
    "USERNAME_KEY=alice\n",
    "USER_PASSWORD=supersecretvalue\n",
    "\n",
    "# Override environment if needed\n",
    "APP_ENV=staging\n",
    "# Enable production fallback for model loading\n",
    "ALLOW_PROD_RUN_FALLBACK=1\n",
    "\n",
    "# Optional local MLflow server (uncomment to override YAML)\n",
    "#MLFLOW_TRACKING_URI=http://127.0.0.1:5000\n",
    "\n",
    "# Redis (local)\n",
    "#REDIS_URL=redis://localhost:6379 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48d36137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/env.prod\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/env.prod\n",
    "# Production environment overrides. Most non-secret config lives in config.yaml.\n",
    "# This file is for Railway production deployment.\n",
    "\n",
    "# Secrets (DO NOT COMMIT real values - use Railway variables)\n",
    "SECRET_KEY=production-secret-key-change-in-production\n",
    "DATABASE_URL=sqlite+aiosqlite:///./app.db\n",
    "\n",
    "# Tokens (production)\n",
    "USERNAME_KEY=alice\n",
    "USER_PASSWORD=supersecretvalue\n",
    "\n",
    "# Production environment\n",
    "APP_ENV=prod\n",
    "ENVIRONMENT=production\n",
    "\n",
    "# Enable production fallback for model loading\n",
    "ALLOW_PROD_RUN_FALLBACK=1\n",
    "\n",
    "# MLflow tracking (Railway will override)\n",
    "#MLFLOW_TRACKING_URI=file:./mlruns_local\n",
    "\n",
    "# Redis (Railway will provide)\n",
    "#REDIS_URL=redis://localhost:6379 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8cfd1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/pyproject.toml\n",
    "[project]\n",
    "name = \"api\"\n",
    "version = \"1.0.0\"\n",
    "description = \"FastAPI backend with React frontend\"\n",
    "requires-python = \">=3.11,<3.13\"  # Constrain to Python 3.11-3.12 for stability\n",
    "dependencies = [\n",
    "    \"fastapi>=0.104.0,<0.106.0\",\n",
    "    \"uvicorn>=0.24.0,<0.26.0\",\n",
    "    \"sqlalchemy>=2.0.23,<2.1.0\",\n",
    "    \"aiosqlite>=0.19.0,<0.21.0\",\n",
    "    \"python-jose[cryptography]>=3.3.0,<3.4.0\",\n",
    "    \"passlib[bcrypt]>=1.7.4,<1.8.0\",\n",
    "    \"python-multipart>=0.0.6,<0.1.0\",\n",
    "    \"pydantic>=2.4.2,<2.6.0\",\n",
    "    \"pydantic-settings>=2.0.0,<2.2.0\",\n",
    "    \"bcrypt==4.0.1\",  # Pin bcrypt version to resolve warning\n",
    "    # Rate limiting - Updated to use redis.asyncio\n",
    "    \"fastapi-limiter>=0.1.5,<0.2.0\",\n",
    "    \"redis>=5.0.0,<6.0.0\",  # Replaces aioredis - asyncio support merged into redis-py\n",
    "    \"httpx>=0.24.0,<0.26.0\",\n",
    "    # ML dependencies - Updated for Python 3.12 compatibility\n",
    "    \"mlflow>=2.8.0,<2.10.0\",  # Fixed: removed invalid [sqlalchemy] extra\n",
    "    \"scikit-learn>=1.5.2,<1.8.0\",\n",
    "    \"pandas>=2.0.0,<2.2.0\",\n",
    "    \"numpy>=1.26.0,<2.4.0\",  # Ensure numpy >=1.26.0 for Python 3.12+ compatibility\n",
    "    \"scipy>=1.12.0,<1.17.0\",\n",
    "    # PyMC stack - Updated for better compatibility\n",
    "    \"pymc>=5.10.0,<5.17.0\",  # Use newer PyMC version\n",
    "    \"pytensor>=2.18.0,<2.26.0\",  # Explicitly constrain pytensor to compatible version\n",
    "    \"arviz>=0.17.0,<0.19.0\",  # Updated arviz version\n",
    "    \"requests>=2.31.0,<2.33.0\",\n",
    "    \"pyyaml>=6.0,<7.0\",\n",
    "    # JAX stack - Updated for compatibility\n",
    "    \"jax[cpu]>=0.4.20,<0.5.0\",  # Use stable JAX version\n",
    "    \"jaxlib>=0.4.20,<0.5.0\",    # Match jaxlib with jax\n",
    "    \"numpyro>=0.15.0,<0.16.0\",  # Updated numpyro\n",
    "    \"psutil>=5.0.0,<8.0.0\",\n",
    "    # Additional dependencies for compatibility\n",
    "    \"packaging>=21.0\",\n",
    "    \"toml>=0.10.2\",\n",
    "    # Add setuptools explicitly for Python 3.12+\n",
    "    \"setuptools>=68.0.0\",  # Ensures setuptools availability\n",
    "    \"ipykernel>=6.25.0\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "    \"pytest>=7.0.0,<8.0.0\",\n",
    "    \"pytest-asyncio>=0.21.0,<0.23.0\",\n",
    "    \"httpx>=0.24.0,<0.26.0\"\n",
    "]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"hatchling\", \"setuptools>=68.0.0\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[tool.hatch.build.targets.wheel]\n",
    "packages = [\"app\"]\n",
    "\n",
    "# Environment management configuration\n",
    "[tool.dependency-check]\n",
    "python-version-range = \">=3.11,<3.13\"\n",
    "strict-mode = false\n",
    "critical-packages = [\n",
    "    \"numpy\", \"scipy\", \"scikit-learn\", \"pandas\", \"mlflow\", \"pymc\", \"pytensor\"\n",
    "]\n",
    "\n",
    "[tool.model-compatibility]\n",
    "# Updated compatibility matrix\n",
    "supported-sklearn-versions = [\"1.5.2\", \"1.6.0\", \"1.7.0\"]\n",
    "supported-numpy-versions = [\"1.26.0\", \"1.26.4\", \"2.0.0\", \"2.1.0\", \"2.2.0\", \"2.3.1\"]\n",
    "supported-scipy-versions = [\"1.12.0\", \"1.13.0\", \"1.14.0\", \"1.15.0\", \"1.16.0\"]\n",
    "supported-pymc-versions = [\"5.10.0\", \"5.11.0\", \"5.12.0\", \"5.13.0\", \"5.14.0\", \"5.15.0\", \"5.16.0\"]\n",
    "supported-pytensor-versions = [\"2.18.0\", \"2.19.0\", \"2.20.0\", \"2.21.0\", \"2.22.0\", \"2.23.0\", \"2.24.0\", \"2.25.0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12f1f765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/railway.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/railway.json\n",
    "{\n",
    "  \"$schema\": \"https://railway.app/railway.schema.json\",\n",
    "  \"build\": { \"builder\": \"NIXPACKS\" },\n",
    "  \"deploy\": {\n",
    "    \"startCommand\": \"bash ./start.sh\",\n",
    "    \"healthcheckPath\": \"/api/v1/health\",\n",
    "    \"healthcheckInterval\": 10,\n",
    "    \"healthcheckTimeout\": 300,\n",
    "    \"restartPolicyType\": \"ON_FAILURE\",\n",
    "    \"restartPolicyMaxRetries\": 10\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e68f0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/start.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/start.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "\n",
    "# ============================================================================\n",
    "# start.sh – Robust FastAPI launcher for Railway / Nixpacks\n",
    "#\n",
    "# Adds:\n",
    "#   • Runtime diagnostics: show PWD + tree slices so we can see what made it\n",
    "#     into the image.\n",
    "#   • Defensive path discovery for helper scripts (seed_user.py, ensure_models.py)\n",
    "#   • Env toggles: SEED_ON_BOOT=0   WARM_MODELS_ON_BOOT=0\n",
    "#   • Fail‑soft seeding (never crash container if seed script missing)\n",
    "#   • Inline Python fallback seeder (no external file needed)\n",
    "# ============================================================================\n",
    "\n",
    "# ----- helpers ---------------------------------------------------------------\n",
    "_here=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"   # directory containing start.sh\n",
    "_root=\"$(cd \"${_here}/..\" && pwd)\"                      # repo root (one up from api/)\n",
    "_api=\"${_here}\"                                         # alias; clarity\n",
    "\n",
    "log()  { echo -e \"$*\" >&2; }\n",
    "die()  { log \"❌  $*\"; exit 1; }\n",
    "\n",
    "# ----- sanity env ------------------------------------------------------------\n",
    "if [[ -z \"${PORT:-}\" ]]; then\n",
    "  die \"PORT not set – Railway always provides it.\"\n",
    "fi\n",
    "if [[ -z \"${SECRET_KEY:-}\" ]]; then\n",
    "  die \"SECRET_KEY is not set for the backend service – aborting.\"\n",
    "fi\n",
    "\n",
    "log \"🚀  FastAPI boot; PORT=$PORT  PY=$(python -V)\"\n",
    "# redact secret in log dump\n",
    "env | grep -E 'RAILWAY_|PORT|DATABASE_URL|APP_ENV|ENVIRONMENT|APP_CONFIG_FILE|SECRET_KEY' \\\n",
    "  | sed 's/SECRET_KEY=.*/SECRET_KEY=***/' >&2\n",
    "\n",
    "# ----- runtime context debug (first deploy triage) ---------------------------\n",
    "log \"📂 Runtime dirs:\"\n",
    "log \"  script dir : ${_here}\"\n",
    "log \"  repo root  : ${_root}\"\n",
    "log \"  api dir    : ${_api}\"\n",
    "log \"  pwd        : $(pwd)\"\n",
    "log \"  $(ls -alh . | wc -l) entries in pwd\"\n",
    "\n",
    "log \"📂 Listing ./api (if present):\"\n",
    "ls -alh ./api || log \"  (no ./api directory)\"\n",
    "\n",
    "log \"📂 Listing ${_api}/scripts:\"\n",
    "ls -alh \"${_api}/scripts\" || log \"  (no scripts directory)\"\n",
    "\n",
    "# ----- optional local .env ---------------------------------------------------\n",
    "# (Load after Railway vars so local dev overrides work; harmless if missing)\n",
    "if [[ -f \"${_root}/.env\" ]]; then\n",
    "  # shellcheck disable=SC2046\n",
    "  export $(grep -Ev '^#' \"${_root}/.env\" | xargs) || true\n",
    "fi\n",
    "\n",
    "# --- after env dump ----------------------------------------------------------\n",
    "# Set sensible default on Railway if APP_ENV not specified\n",
    "if [[ -z \"${APP_ENV:-}\" ]]; then\n",
    "  export APP_ENV=\"staging\"      # sensible default on Railway\n",
    "  echo \"🔧  APP_ENV not set – defaulting to staging\"\n",
    "fi\n",
    "\n",
    "# Ensure MLflow datastore exists\n",
    "MLFLOW_ROOT=\"/data/mlruns\"\n",
    "mkdir -p \"$MLFLOW_ROOT\"\n",
    "export MLFLOW_TRACKING_URI=\"file:${MLFLOW_ROOT}\"\n",
    "export MLFLOW_REGISTRY_URI=\"file:${MLFLOW_ROOT}\"\n",
    "echo \"📁  MLflow store => $MLFLOW_ROOT\"\n",
    "\n",
    "# ============================================================================\n",
    "# Database seed\n",
    "# ============================================================================\n",
    "seed_db () {\n",
    "  if [[ \"${SEED_ON_BOOT:-1}\" != \"1\" ]]; then\n",
    "    log \"ℹ️  SEED_ON_BOOT=0 – skipping DB seed.\"\n",
    "    return 0\n",
    "  fi\n",
    "\n",
    "  local cand\n",
    "  local found=\"\"\n",
    "  # Try (in order): absolute script dir, repo‑root/api/scripts, pwd-relative\n",
    "  for cand in \\\n",
    "      \"${_api}/scripts/seed_user.py\" \\\n",
    "      \"${_root}/api/scripts/seed_user.py\" \\\n",
    "      \"./api/scripts/seed_user.py\" \\\n",
    "      \"./scripts/seed_user.py\"\n",
    "  do\n",
    "    if [[ -f \"$cand\" ]]; then\n",
    "      found=\"$cand\"\n",
    "      break\n",
    "    fi\n",
    "  done\n",
    "\n",
    "  if [[ -n \"$found\" ]]; then\n",
    "    log \"🫘  Seeding DB via $found\"\n",
    "    # Never crash container if seed script blows up\n",
    "    if ! python \"$found\"; then\n",
    "      log \"⚠️  Seed script failed (non-fatal).\"\n",
    "    fi\n",
    "    return 0\n",
    "  fi\n",
    "\n",
    "  # --- inline fallback seeder ------------------------------------------------\n",
    "  log \"⚠️  seed_user.py not found in image – using inline DB seeder.\"\n",
    "  python - <<'EOF' || log \"⚠️  Inline seed failed (continuing).\"\n",
    "import os, asyncio\n",
    "from passlib.context import CryptContext\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\n",
    "from sqlalchemy.orm import DeclarativeMeta\n",
    "\n",
    "# Dynamically import app.models (installed site‑package OK)\n",
    "try:\n",
    "    from app.models import Base, User  # type: ignore\n",
    "except Exception as e:\n",
    "    print(f\"INLINE SEEDER: cannot import app.models ({e}) – aborting seed.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "USERNAME = os.getenv(\"USERNAME_KEY\", \"alice\")\n",
    "PASSWORD = os.getenv(\"USER_PASSWORD\", \"supersecretvalue\")\n",
    "DB_URL   = os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n",
    "\n",
    "pwd = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "engine = create_async_engine(DB_URL)\n",
    "session_factory = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "async def _seed():\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "    async with session_factory() as db:\n",
    "        result = await db.execute(select(User).where(User.username == USERNAME))\n",
    "        user = result.scalar_one_or_none()\n",
    "        hashed = pwd.hash(PASSWORD)\n",
    "        if user:\n",
    "            user.hashed_password = hashed\n",
    "            action = \"Updated\"\n",
    "        else:\n",
    "            db.add(User(username=USERNAME, hashed_password=hashed))\n",
    "            action = \"Created\"\n",
    "        await db.commit()\n",
    "        print(f\"INLINE SEEDER: {action} user {USERNAME}\")\n",
    "\n",
    "asyncio.run(_seed())\n",
    "EOF\n",
    "}\n",
    "\n",
    "seed_db   # run it\n",
    "\n",
    "# ============================================================================\n",
    "# Optional model warm‑start\n",
    "# ============================================================================\n",
    "warm_models () {\n",
    "  if [[ \"${WARM_MODELS_ON_BOOT:-1}\" != \"1\" ]]; then\n",
    "    log \"ℹ️  WARM_MODELS_ON_BOOT=0 – skipping model warm‑start.\"\n",
    "    return 0\n",
    "  fi\n",
    "\n",
    "  local warm=\"${_api}/scripts/ensure_models.py\"\n",
    "  if [[ -f \"$warm\" ]]; then\n",
    "    log \"🏗️  Pre-training / warming models via $warm ...\"\n",
    "    if ! python \"$warm\"; then\n",
    "      log \"⚠️  pre-train failed – stub will load in API process.\"\n",
    "    fi\n",
    "  else\n",
    "    log \"ℹ️  No ensure_models.py found – skipping pre-train.\"\n",
    "  fi\n",
    "}\n",
    "warm_models\n",
    "\n",
    "# ============================================================================\n",
    "# Launch API\n",
    "# ============================================================================\n",
    "log \"▶️  Launching uvicorn (app.main:app; app-dir=${_api}) ...\"\n",
    "exec uvicorn app.main:app \\\n",
    "  --host 0.0.0.0 --port \"$PORT\" \\\n",
    "  --proxy-headers --forwarded-allow-ips=\"*\" --log-level info \\\n",
    "  --app-dir \"${_api}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2dcbfaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/seed_user.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/seed_user.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Idempotent DB seed script.\n",
    "\n",
    "Creates or updates the default user defined by USERNAME_KEY / USER_PASSWORD\n",
    "environment variables. Safe to re-run.\n",
    "\n",
    "* Logs resolved DATABASE_URL & working directory for debugging container builds.\n",
    "* Tolerates missing .env (warn only).\n",
    "* Safe imports when run out of tree (adds sibling parent to sys.path if needed).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "from passlib.context import CryptContext\n",
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker\n",
    "from sqlalchemy import select\n",
    "\n",
    "# ----- diagnostics -----------------------------------------------------------\n",
    "HERE = Path(__file__).resolve()\n",
    "API_DIR = HERE.parents[1]\n",
    "ROOT = HERE.parents[2]\n",
    "print(f\"[seed_user] __file__={HERE}\")\n",
    "print(f\"[seed_user] api_dir={API_DIR}\")\n",
    "print(f\"[seed_user] root={ROOT}\")\n",
    "print(f\"[seed_user] cwd={Path.cwd()}\")\n",
    "\n",
    "# ----- optional .env load ----------------------------------------------------\n",
    "ENV_PATH = ROOT / \".env\"\n",
    "if ENV_PATH.is_file():\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv(ENV_PATH, encoding=\"utf-8\")\n",
    "        print(f\"[seed_user] loaded .env from {ENV_PATH}\")\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"[seed_user] ⚠️ .env not UTF-8 – skipped\")\n",
    "else:\n",
    "    print(f\"[seed_user] (no .env at {ENV_PATH})\")\n",
    "\n",
    "# ----- import models ---------------------------------------------------------\n",
    "# Ensure <api_dir> is importable when script run from repo root\n",
    "if str(API_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(API_DIR))\n",
    "\n",
    "try:\n",
    "    from app.models import Base, User  # type: ignore\n",
    "except Exception as exc:  # pragma: no cover - debug path\n",
    "    print(f\"[seed_user] ❌ cannot import app.models: {exc}\")\n",
    "    raise\n",
    "\n",
    "# ----- config env ------------------------------------------------------------\n",
    "USERNAME = os.getenv(\"USERNAME_KEY\", \"alice\")\n",
    "PASSWORD = os.getenv(\"USER_PASSWORD\", \"supersecretvalue\")\n",
    "DB_URL   = os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n",
    "\n",
    "print(f\"[seed_user] DATABASE_URL={DB_URL}\")\n",
    "print(f\"[seed_user] USERNAME={USERNAME}\")\n",
    "\n",
    "pwd = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "engine = create_async_engine(DB_URL)\n",
    "Session = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "async def main() -> None:\n",
    "    async with engine.begin() as conn:\n",
    "        await conn.run_sync(Base.metadata.create_all)\n",
    "\n",
    "    async with Session() as db:\n",
    "        result = await db.execute(select(User).where(User.username == USERNAME))\n",
    "        user = result.scalar_one_or_none()\n",
    "        hashed = pwd.hash(PASSWORD)\n",
    "\n",
    "        if user:\n",
    "            user.hashed_password = hashed\n",
    "            action = \"Updated\"\n",
    "        else:\n",
    "            db.add(User(username=USERNAME, hashed_password=hashed))\n",
    "            action = \"Created\"\n",
    "        await db.commit()\n",
    "        print(f\"[seed_user] {action} user {USERNAME}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a19a73d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/scripts/ensure_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/scripts/ensure_models.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Optional warm‑start script for Railway.\n",
    "\n",
    "(Logging additions for deploy debugging.)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "HERE = Path(__file__).resolve()\n",
    "API_DIR = HERE.parents[1]\n",
    "ROOT = HERE.parents[2]\n",
    "log.info(\"ensure_models: __file__=%s\", HERE)\n",
    "log.info(\"ensure_models: api_dir=%s\", API_DIR)\n",
    "log.info(\"ensure_models: root=%s\", ROOT)\n",
    "log.info(\"ensure_models: cwd=%s\", Path.cwd())\n",
    "\n",
    "# Ensure we can import `app.*` when executed as a script from repo root\n",
    "if str(API_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(API_DIR))\n",
    "\n",
    "from app.services.ml.model_service import model_service  # type: ignore\n",
    "\n",
    "\n",
    "async def _main() -> None:\n",
    "    # Add lightweight experiment bootstrap\n",
    "    try:\n",
    "        import mlflow\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        \n",
    "        # Ensure the experiment exists before any model loading\n",
    "        client = MlflowClient()\n",
    "        exp = client.get_experiment_by_name(\"ml_fullstack_models\")\n",
    "        if exp is None:\n",
    "            exp_id = client.create_experiment(\"ml_fullstack_models\")\n",
    "            log.info(\"ensure_models: created experiment ml_fullstack_models (ID: %s)\", exp_id)\n",
    "        else:\n",
    "            log.info(\"ensure_models: found existing experiment ml_fullstack_models (ID: %s)\", exp.experiment_id)\n",
    "    except Exception as e:\n",
    "        log.warning(\"ensure_models: experiment bootstrap failed: %s\", e)\n",
    "    \n",
    "    log.info(\"ensure_models: initialize() …\")\n",
    "    await model_service.initialize()\n",
    "    log.info(\"ensure_models: startup(auto_train=False) …\")\n",
    "    await model_service.startup(auto_train=False)\n",
    "    log.info(\"ensure_models: status=%s\", model_service.status)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(_main())\n",
    "    except Exception as exc:\n",
    "        log.exception(\"ensure_models failed: %s\", exc)\n",
    "        # Non-zero exit? Returning 0 keeps container booting;\n",
    "        # change to `raise` if you want hard fail.\n",
    "        sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "850e919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/__init__.py\n",
    "# api/app/utils/__init__.py\n",
    "\"\"\"\n",
    "Utility functions for the FastAPI application.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "548cf2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/utils/env_sanitizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/utils/env_sanitizer.py\n",
    "# api/app/utils/env_sanitizer.py\n",
    "\"\"\"\n",
    "Early‑process clean‑up of env variables that mis‑configure JAX / PyTensor.\n",
    "Import *before* anything touches JAX / PyMC.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, logging, importlib.util\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "_VALID_XLA_PREFIXES = (\"--xla_\", \"--mmap_\", \"--tfrt_\")\n",
    "\n",
    "def _clean_xla_flags() -> None:\n",
    "    \"\"\"Remove invalid XLA_FLAGS tokens that cause crashes.\"\"\"\n",
    "    val = os.getenv(\"XLA_FLAGS\")\n",
    "    if not val:\n",
    "        return\n",
    "    tokens = [t for t in val.split() if t]\n",
    "    bad = [t for t in tokens if not t.startswith(_VALID_XLA_PREFIXES)]\n",
    "    if bad:\n",
    "        log.warning(\"🧹 Removing invalid XLA_FLAGS tokens: %s\", bad)\n",
    "        tokens = [t for t in tokens if t not in bad]\n",
    "    if tokens:\n",
    "        os.environ[\"XLA_FLAGS\"] = \" \".join(tokens)\n",
    "    else:        # was just '--'\n",
    "        os.environ.pop(\"XLA_FLAGS\", None)\n",
    "\n",
    "def _downgrade_jax_backend() -> None:\n",
    "    \"\"\"Force JAX to use CPU if GPU is requested but not available.\"\"\"\n",
    "    # Check if GPU backend is explicitly requested\n",
    "    platform_name = os.getenv(\"JAX_PLATFORM_NAME\", \"\").lower()\n",
    "    if platform_name in (\"gpu\", \"cuda\"):\n",
    "        # Check if CUDA runtime is actually available\n",
    "        cuda_spec = importlib.util.find_spec(\"jaxlib.cuda_extension\")\n",
    "        if cuda_spec is None:\n",
    "            log.warning(\"⚠️ No CUDA runtime found – forcing JAX_PLATFORM_NAME=cpu\")\n",
    "            os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "        else:\n",
    "            log.info(\"✅ CUDA runtime detected, keeping GPU backend\")\n",
    "\n",
    "def _force_pytensor_cpu() -> None:\n",
    "    \"\"\"Force PyTensor to use CPU device to avoid C++ compilation issues.\"\"\"\n",
    "    # Only set if not already configured\n",
    "    if \"PYTENSOR_FLAGS\" not in os.environ:\n",
    "        os.environ[\"PYTENSOR_FLAGS\"] = \"device=cpu,floatX=float32\"\n",
    "        log.info(\"🔧 Set PyTensor to CPU device\")\n",
    "    \n",
    "    # Also set legacy config for compatibility\n",
    "    if \"DEVICE\" not in os.environ:\n",
    "        os.environ[\"DEVICE\"] = \"cpu\"\n",
    "\n",
    "def _disable_pytensor_compilation() -> None:\n",
    "    \"\"\"Completely disable PyTensor C compilation to avoid MSVC issues.\"\"\"\n",
    "    # Force PyTensor to use Python backend instead of C compilation\n",
    "    os.environ[\"PYTENSOR_FLAGS\"] = \"device=cpu,floatX=float32\"\n",
    "    \n",
    "    # Disable C compilation entirely\n",
    "    os.environ[\"PYTENSOR_COMPILE_OPTIMIZER\"] = \"fast_compile\"\n",
    "    os.environ[\"PYTENSOR_COMPILE_MODE\"] = \"FAST_COMPILE\"\n",
    "    \n",
    "    # Force Python backend for PyTensor (no C compilation)\n",
    "    os.environ[\"PYTENSOR_LINKER\"] = \"py\"\n",
    "    \n",
    "    log.info(\"🔧 Disabled PyTensor C compilation, using Python backend\")\n",
    "\n",
    "def _check_cuda_environment() -> None:\n",
    "    \"\"\"Log CUDA-related environment variables for debugging.\"\"\"\n",
    "    cuda_vars = {k: v for k, v in os.environ.items() \n",
    "                 if 'CUDA' in k or 'GPU' in k or 'JAX' in k}\n",
    "    if cuda_vars:\n",
    "        log.info(\"🔍 CUDA/JAX environment variables: %s\", cuda_vars)\n",
    "\n",
    "def fix_ml_backends() -> None:\n",
    "    \"\"\"\n",
    "    Comprehensive fix for JAX/PyTensor backend configuration.\n",
    "    \n",
    "    This function should be called **once** at the very top of app.main\n",
    "    before any JAX or PyMC imports.\n",
    "    \"\"\"\n",
    "    log.info(\"🔧 Sanitizing ML backend configuration...\")\n",
    "    \n",
    "    _check_cuda_environment()\n",
    "    _clean_xla_flags()\n",
    "    _downgrade_jax_backend()\n",
    "    _force_pytensor_cpu()\n",
    "    _disable_pytensor_compilation()\n",
    "    \n",
    "    log.info(\"✅ ML backend sanitization complete\")\n",
    "\n",
    "# Legacy function for backward compatibility\n",
    "def fix_xla_flags() -> None:\n",
    "    \"\"\"Legacy function - now calls the comprehensive fix.\"\"\"\n",
    "    fix_ml_backends() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dda1108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/middleware/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/middleware/__init__.py\n",
    "# Middleware package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "887532d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/middleware/concurrency.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/middleware/concurrency.py\n",
    "\"\"\"\n",
    "Semaphore‑based concurrency limiter implemented as **pure ASGI middleware**.\n",
    "Avoids BaseHTTPMiddleware, so it never triggers the Starlette EndOfStream bug.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from typing import Callable, Set\n",
    "from starlette.types import ASGIApp, Scope, Receive, Send\n",
    "\n",
    "class ConcurrencyLimiter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        app: ASGIApp,\n",
    "        *,\n",
    "        max_concurrent: int = 4,\n",
    "        heavy_endpoints: Set[str] | None = None,\n",
    "    ) -> None:\n",
    "        self.app = app\n",
    "        self._sem = asyncio.Semaphore(max_concurrent)\n",
    "        self.heavy_endpoints = heavy_endpoints or {\n",
    "            \"/api/v1/cancer/predict\",\n",
    "            \"/api/v1/iris/train\",\n",
    "            \"/api/v1/cancer/train\",\n",
    "        }\n",
    "\n",
    "    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:\n",
    "        # Only apply to HTTP requests that match our list\n",
    "        if scope[\"type\"] != \"http\" or scope[\"path\"] not in self.heavy_endpoints:\n",
    "            await self.app(scope, receive, send)\n",
    "            return\n",
    "\n",
    "        async with self._sem:\n",
    "            await self.app(scope, receive, send) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d4377fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/deps/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/deps/__init__.py\n",
    "# Rate limiting dependencies package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0186617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/deps/limits.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/deps/limits.py\n",
    "# api/app/deps/limits.py  ✅ FIXED\n",
    "\"\"\"\n",
    "Rate‑limiting helpers – now **fail‑safe** when Redis is absent and\n",
    "compatible with fastapi‑limiter ≥ 0.1.5 which passes *request* and *response*.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from fastapi_limiter.depends import RateLimiter\n",
    "from fastapi_limiter import FastAPILimiter\n",
    "from starlette.requests import Request\n",
    "from starlette.responses import Response  # NEW\n",
    "from ..core.config import settings\n",
    "\n",
    "# ── helpers ──────────────────────────────────────────────────────────────\n",
    "async def _path_aware_ip(request: Request) -> str:\n",
    "    fwd = request.headers.get(\"X-Forwarded-For\")\n",
    "    ip = (fwd.split(\",\")[0].strip() if fwd else request.client.host)\n",
    "    return f\"{ip}:{request.scope['path']}\"\n",
    "\n",
    "async def _user_or_ip(request: Request) -> str:\n",
    "    auth = request.headers.get(\"Authorization\", \"\")\n",
    "    if auth.startswith(\"Bearer \"):\n",
    "        return auth[7:]\n",
    "    return await _path_aware_ip(request)\n",
    "\n",
    "async def _no_limit(_: Request, __: Response) -> None:  # SIG UPDATED\n",
    "    \"\"\"Dummy dependency when rate‑limiting is disabled.\"\"\"\n",
    "    return None\n",
    "\n",
    "# ── factory that returns a dependency compatible with new signature ─────\n",
    "def _safe_limiter(times: int, seconds: int, identifier):\n",
    "    base = RateLimiter(times=times, seconds=seconds, identifier=identifier)\n",
    "\n",
    "    async def wrapper(request: Request, response: Response):  # SIG UPDATED\n",
    "        # If Redis is unavailable (e.g. tests), skip gracefully\n",
    "        if FastAPILimiter.redis is None:\n",
    "            return None\n",
    "        return await base(request, response)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "# ── public dependencies ---------------------------------------------------\n",
    "if settings.ENABLE_RATE_LIMIT:\n",
    "    default_limit  = _safe_limiter(settings.RATE_LIMIT_DEFAULT,\n",
    "                                   settings.RATE_LIMIT_WINDOW, _user_or_ip)\n",
    "    light_limit    = _safe_limiter(settings.RATE_LIMIT_DEFAULT * 2,\n",
    "                                   settings.RATE_LIMIT_WINDOW_LIGHT, _user_or_ip)\n",
    "    heavy_limit    = _safe_limiter(settings.RATE_LIMIT_CANCER,\n",
    "                                   settings.RATE_LIMIT_WINDOW, _user_or_ip)\n",
    "    login_limit    = _safe_limiter(settings.RATE_LIMIT_LOGIN + 1,\n",
    "                                   settings.RATE_LIMIT_LOGIN_WINDOW, _path_aware_ip)\n",
    "    training_limit = _safe_limiter(settings.RATE_LIMIT_TRAINING,\n",
    "                                   settings.RATE_LIMIT_WINDOW * 5, _user_or_ip)\n",
    "else:\n",
    "    # All no‑ops when rate‑limiting disabled\n",
    "    default_limit = light_limit = heavy_limit = login_limit = training_limit = _no_limit\n",
    "\n",
    "def get_redis():\n",
    "    \"\"\"Helper for tests/metrics.\"\"\"\n",
    "    return FastAPILimiter.redis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4f0005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/core/env_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/core/env_utils.py\n",
    "\"\"\"\n",
    "Environment utilities for canonical environment mapping.\n",
    "\n",
    "This module provides consistent environment name mapping to eliminate\n",
    "drift between different environment tokens used across the system.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "\n",
    "_ENV_CANON_MAP = {\n",
    "    \"dev\": \"development\",\n",
    "    \"development\": \"development\",\n",
    "    \"local\": \"development\",\n",
    "    \"staging\": \"staging\",\n",
    "    \"stage\": \"staging\",\n",
    "    \"preprod\": \"staging\",\n",
    "    \"prod\": \"production\",\n",
    "    \"production\": \"production\",\n",
    "    \"live\": \"production\",\n",
    "}\n",
    "\n",
    "def canonical_env(name: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Map any common environment token to a canonical value:\n",
    "    development | staging | production.\n",
    "\n",
    "    Uses APP_ENV first, then ENVIRONMENT, then defaults to development.\n",
    "    \n",
    "    Args:\n",
    "        name: Environment name to canonicalize. If None, reads from APP_ENV or ENVIRONMENT env vars.\n",
    "        \n",
    "    Returns:\n",
    "        Canonical environment name: 'development', 'staging', or 'production'\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = os.getenv(\"APP_ENV\") or os.getenv(\"ENVIRONMENT\") or \"development\"\n",
    "    return _ENV_CANON_MAP.get(name.lower().strip(), \"development\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a5b7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/core/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/core/config.py\n",
    "\"\"\"\n",
    "Centralized configuration loader.\n",
    "\n",
    "Load order (highest precedence last):\n",
    "    1. Built-in safe defaults (field defaults below)\n",
    "    2. config.yaml: default block\n",
    "    3. config.yaml: <APP_ENV> block overlay    # APP_ENV=dev|staging|prod (aliases ok)\n",
    "    4. Real environment variables               # 12-factor override for secrets\n",
    "\n",
    "Robust to different container layouts (e.g., running from repo root,\n",
    "from within `api/`, or in packaged wheels) by *searching* for config.yaml\n",
    "instead of assuming a fixed number of parent hops.\n",
    "\n",
    "Usage:\n",
    "    from app.core.config import settings\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import functools\n",
    "import yaml\n",
    "import logging\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "from .env_utils import canonical_env\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Environment tokens                                                         #\n",
    "# --------------------------------------------------------------------------- #\n",
    "_APP_ENV_RAW = os.getenv(\"APP_ENV\", \"dev\")          # raw token (may be 'dev', 'prod', 'production', etc.)\n",
    "APP_ENV_CANON = canonical_env(_APP_ENV_RAW)         # canonicalized: development/staging/production\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Config path discovery                                                      #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def _candidate_paths() -> Iterable[pathlib.Path]:\n",
    "    \"\"\"\n",
    "    Yield candidate config.yaml paths in priority order.\n",
    "\n",
    "    Order:\n",
    "      1. $APP_CONFIG_FILE if set\n",
    "      2. Walk upward from this file looking for `config.yaml`\n",
    "      3. Walk upward looking for `api/config.yaml`\n",
    "\n",
    "    We cap the climb at 6 directory levels to avoid runaway loops if something is\n",
    "    oddly symlinked in containers.\n",
    "    \"\"\"\n",
    "    # 1️⃣ Explicit override\n",
    "    override = os.getenv(\"APP_CONFIG_FILE\")\n",
    "    if override:\n",
    "        p = pathlib.Path(override).expanduser().resolve()\n",
    "        yield p\n",
    "\n",
    "    # 2️⃣ Upward search for config.yaml\n",
    "    here = pathlib.Path(__file__).resolve()\n",
    "    for parent in list(here.parents)[:6]:\n",
    "        yield parent / \"config.yaml\"\n",
    "\n",
    "    # 3️⃣ Upward search for api/config.yaml\n",
    "    for parent in list(here.parents)[:6]:\n",
    "        yield parent / \"api\" / \"config.yaml\"\n",
    "\n",
    "\n",
    "def _discover_config_path() -> pathlib.Path:\n",
    "    \"\"\"Return the first existing candidate path or raise with diagnostics.\"\"\"\n",
    "    tried: List[str] = []\n",
    "    for cand in _candidate_paths():\n",
    "        tried.append(str(cand))\n",
    "        if cand.is_file():\n",
    "            return cand\n",
    "\n",
    "    msg_lines = [\n",
    "        \"config.yaml not found. Searched the following paths (in order):\",\n",
    "        *(\"  - \" + s for s in tried),\n",
    "        \"Set APP_CONFIG_FILE to override.\"\n",
    "    ]\n",
    "    raise FileNotFoundError(\"\\n\".join(msg_lines))\n",
    "\n",
    "\n",
    "# Resolve at import time *once*. We intentionally do **not** resolve lazily in\n",
    "# _load_yaml() so that import failures are immediate & obvious.\n",
    "CONFIG_PATH = _discover_config_path()\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Type coercion helpers                                                      #\n",
    "# --------------------------------------------------------------------------- #\n",
    "_BOOL_KEYS = {\n",
    "    \"REQUIRE_MODEL_APPROVAL\",\n",
    "    \"AUTO_PROMOTE_TO_PRODUCTION\",\n",
    "    \"ENABLE_MODEL_COMPARISON\",\n",
    "    \"MLFLOW_GC_AFTER_TRAIN\",\n",
    "    \"SKIP_BACKGROUND_TRAINING\",\n",
    "    \"AUTO_TRAIN_MISSING\",\n",
    "    \"UNIT_TESTING\",\n",
    "    \"DEBUG_RATELIMIT\",\n",
    "    \"CACHE_ENABLED\",\n",
    "    \"ENABLE_RATE_LIMIT\",\n",
    "}\n",
    "\n",
    "def _coerce_types(d: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize YAML-loaded values prior to BaseSettings construction.\n",
    "\n",
    "    * Convert ints {0,1} to bool for keys in _BOOL_KEYS.\n",
    "    * Strip whitespace on string values.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        if k in _BOOL_KEYS:\n",
    "            if isinstance(v, bool):\n",
    "                out[k] = v\n",
    "            elif isinstance(v, int):\n",
    "                out[k] = bool(v)\n",
    "            elif isinstance(v, str) and v.strip().isdigit():\n",
    "                out[k] = bool(int(v.strip()))\n",
    "            else:\n",
    "                out[k] = v\n",
    "        elif isinstance(v, str):\n",
    "            out[k] = v.strip()\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "@functools.lru_cache\n",
    "def _load_yaml(env_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Read config.yaml (or overridden path) and merge default + env block.\n",
    "\n",
    "    `env_token` may be 'dev','prod','staging' *or* any alias; we canonicalize.\n",
    "    \"\"\"\n",
    "    raw = yaml.safe_load(CONFIG_PATH.read_text()) or {}\n",
    "    if \"default\" not in raw:\n",
    "        raise KeyError(f\"{CONFIG_PATH} must contain a 'default' section\")\n",
    "\n",
    "    canon = canonical_env(env_token)\n",
    "    # Map canonical name -> possible YAML block keys (accepting short aliases)\n",
    "    candidate_keys = {\n",
    "        \"development\": (\"dev\", \"development\"),\n",
    "        \"staging\": (\"staging\", \"stage\", \"preprod\"),\n",
    "        \"production\": (\"prod\", \"production\", \"live\"),\n",
    "    }[canon]\n",
    "\n",
    "    env_block = {}\n",
    "    for key in candidate_keys:\n",
    "        if key in raw:\n",
    "            env_block = raw[key]\n",
    "            break\n",
    "\n",
    "    merged = {**raw[\"default\"], **(env_block or {})}\n",
    "    merged = _coerce_types(merged)\n",
    "\n",
    "    # ensure ENVIRONMENT reflects canonical env (downstream uses this)\n",
    "    merged[\"ENVIRONMENT\"] = canon\n",
    "\n",
    "    if os.getenv(\"CONFIG_DEBUG\") == \"1\":\n",
    "        log.info(\"CONFIG_DEBUG: loaded %s (env=%s canonical=%s)\", CONFIG_PATH, env_token, canon)\n",
    "        log.info(\"CONFIG_DEBUG: merged keys=%s\", sorted(merged.keys()))\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "class _Settings(BaseSettings):\n",
    "    # --- core fields (defaults are last-resort safe fallbacks) ---------------\n",
    "    DATABASE_URL: str = \"sqlite+aiosqlite:///./app.db\"\n",
    "    SECRET_KEY: str | None = None\n",
    "    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30\n",
    "    ALLOWED_ORIGINS: str = \"*\"\n",
    "    REDIS_URL: str = \"redis://localhost:6379\"\n",
    "\n",
    "    # Rate Limiting\n",
    "    RATE_LIMIT_WINDOW: int = 60\n",
    "    RATE_LIMIT_WINDOW_LIGHT: int = 300\n",
    "    RATE_LIMIT_LOGIN_WINDOW: int = 20\n",
    "    RATE_LIMIT_DEFAULT: int = 60\n",
    "    RATE_LIMIT_CANCER: int = 30\n",
    "    RATE_LIMIT_LOGIN: int = 3\n",
    "    RATE_LIMIT_TRAINING: int = 2\n",
    "\n",
    "    # MLflow\n",
    "    MLFLOW_EXPERIMENT: str = \"ml_fullstack_models\"\n",
    "    MLFLOW_TRACKING_URI: str = \"file:./mlruns_local\"\n",
    "    MLFLOW_REGISTRY_URI: str = \"file:./mlruns_local\"\n",
    "    RETAIN_RUNS_PER_MODEL: int = 5\n",
    "    MLFLOW_GC_AFTER_TRAIN: bool = True\n",
    "\n",
    "    # Model Training\n",
    "    SKIP_BACKGROUND_TRAINING: bool = False\n",
    "    AUTO_TRAIN_MISSING: bool = True\n",
    "    UNIT_TESTING: bool = False\n",
    "\n",
    "    # Debug\n",
    "    DEBUG_RATELIMIT: bool = False\n",
    "\n",
    "    # Rate Limiting\n",
    "    ENABLE_RATE_LIMIT: bool = True\n",
    "\n",
    "    # ML backends\n",
    "    XLA_FLAGS: str = \"--xla_force_host_platform_device_count=1\"\n",
    "    PYTENSOR_FLAGS: str = \"device=cpu,floatX=float32\"\n",
    "\n",
    "    # MLOps gating\n",
    "    ENVIRONMENT: str = \"development\"        # overlaid by YAML\n",
    "    QUALITY_GATE_ACCURACY_THRESHOLD: float = 0.85\n",
    "    QUALITY_GATE_F1_THRESHOLD: float = 0.85\n",
    "    REQUIRE_MODEL_APPROVAL: bool = False\n",
    "    AUTO_PROMOTE_TO_PRODUCTION: bool = False\n",
    "    ENABLE_MODEL_COMPARISON: bool = True\n",
    "    MODEL_AUDIT_ENFORCEMENT: str = \"warn\"\n",
    "    MAX_MODEL_VERSIONS_PER_MODEL: int = 5\n",
    "\n",
    "    # Prediction caching\n",
    "    CACHE_ENABLED: bool = False\n",
    "    CACHE_TTL_MINUTES: int = 60\n",
    "\n",
    "    # Computed field (not from env)\n",
    "    ENVIRONMENT_CANONICAL: str = APP_ENV_CANON  # injected in build()\n",
    "    \n",
    "    # Optional prod run fallback toggle\n",
    "    ALLOW_PROD_RUN_FALLBACK: bool = False\n",
    "\n",
    "    model_config = SettingsConfigDict(env_prefix=\"\", case_sensitive=False, extra=\"ignore\")\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls) -> \"_Settings\":  # type: ignore[override]\n",
    "        # 1. merge YAML default + env block (based on APP_ENV or default 'dev')\n",
    "        data = _load_yaml(_APP_ENV_RAW)\n",
    "\n",
    "        # 2. Honour explicit ENVIRONMENT env‑var if provided (e.g. Railway)\n",
    "        if \"ENVIRONMENT\" in os.environ:\n",
    "            data[\"ENVIRONMENT\"] = os.environ[\"ENVIRONMENT\"].strip()\n",
    "\n",
    "        # 3. Build settings (env vars overlay)\n",
    "        inst: \"_Settings\" = cls(**data)\n",
    "\n",
    "        # 4. FINAL canonical value after all overlays\n",
    "        inst.ENVIRONMENT_CANONICAL = canonical_env(inst.ENVIRONMENT)\n",
    "\n",
    "        # 5. Set the prod run fallback toggle from env var\n",
    "        inst.ALLOW_PROD_RUN_FALLBACK = bool(int(os.getenv(\"ALLOW_PROD_RUN_FALLBACK\", \"0\")))\n",
    "\n",
    "        log.info(\n",
    "            \"📄 Loaded config (ENV=%s ⇒ %s, allow_prod_run_fallback=%s)\",\n",
    "            inst.ENVIRONMENT,\n",
    "            inst.ENVIRONMENT_CANONICAL,\n",
    "            inst.ALLOW_PROD_RUN_FALLBACK,\n",
    "        )\n",
    "        return inst\n",
    "\n",
    "\n",
    "# public singleton\n",
    "settings: _Settings = _Settings.build()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49871cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/crud.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/crud.py\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from .models import User\n",
    "\n",
    "async def get_user_by_username(db: AsyncSession, username: str):\n",
    "    stmt = select(User).where(User.username == username)\n",
    "    res = await db.execute(stmt)\n",
    "    return res.scalar_one_or_none() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61dc31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/models.py\n",
    "from sqlalchemy import Column, Integer, String\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    username = Column(String, unique=True, index=True)\n",
    "    hashed_password = Column(String) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a86ef40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/db.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/db.py\n",
    "# api/app/db.py\n",
    "from contextlib import asynccontextmanager\n",
    "import os, logging, asyncio\n",
    "from sqlalchemy.ext.asyncio import (\n",
    "    AsyncSession,\n",
    "    create_async_engine,\n",
    "    async_sessionmaker,\n",
    ")\n",
    "from fastapi_limiter import FastAPILimiter\n",
    "from redis import asyncio as redis\n",
    "from .models import Base\n",
    "from .services.ml.model_service import model_service\n",
    "from .core.config import settings\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Database engine & session factory (module-level singletons – cheap & safe)\n",
    "# ---------------------------------------------------------------------------\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite+aiosqlite:///./app.db\")\n",
    "engine = create_async_engine(DATABASE_URL, echo=False, future=True)\n",
    "AsyncSessionLocal = async_sessionmaker(engine, expire_on_commit=False)\n",
    "\n",
    "# Global readiness flag\n",
    "_app_ready: bool = False\n",
    "\n",
    "def get_app_ready():\n",
    "    \"\"\"Get the current app ready status.\"\"\"\n",
    "    return _app_ready\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FastAPI lifespan – runs ONCE at startup / shutdown\n",
    "# ---------------------------------------------------------------------------\n",
    "@asynccontextmanager\n",
    "async def lifespan(app):\n",
    "    \"\"\"\n",
    "    Application lifespan context-manager.\n",
    "\n",
    "    * creates DB tables\n",
    "    * initialises ML models\n",
    "    * (NEW) wires Redis-backed rate-limiter\n",
    "    * sets global _app_ready flag\n",
    "    * disposes resources on shutdown\n",
    "    \"\"\"\n",
    "    global _app_ready\n",
    "\n",
    "    logger.info(\"🗄️  Initializing database…  URL=%s\", DATABASE_URL)\n",
    "    try:\n",
    "        async with engine.begin() as conn:\n",
    "            # DDL is safe here; it blocks startup until complete\n",
    "            await conn.run_sync(Base.metadata.create_all)\n",
    "        logger.info(\"✅ Database tables created/verified successfully\")\n",
    "\n",
    "        # ── NEW: Initialize FastAPI-Limiter BEFORE serving traffic ──────────\n",
    "        if settings.ENABLE_RATE_LIMIT:\n",
    "            try:\n",
    "                # 1️⃣ Check for an explicit REDIS_URL env var (Railway will supply this)\n",
    "                env_url = os.getenv(\"REDIS_URL\")\n",
    "\n",
    "                # 2️⃣ In production, prefer the env var; else use settings.REDIS_URL\n",
    "                if settings.ENVIRONMENT_CANONICAL == \"production\" and env_url:\n",
    "                    redis_url = env_url\n",
    "                else:\n",
    "                    redis_url = settings.REDIS_URL\n",
    "\n",
    "                redis_conn = redis.from_url(\n",
    "                    redis_url,\n",
    "                    encoding=\"utf-8\",\n",
    "                    decode_responses=True,\n",
    "                )\n",
    "                await FastAPILimiter.init(redis_conn, prefix=\"ratelimit\")\n",
    "                logger.info(\"🚦 Rate-limiter initialised (Redis %s)\", redis_url)\n",
    "\n",
    "                # Optional: clean slate for CI\n",
    "                if os.getenv(\"FLUSH_TEST_LIMITS\") == \"1\":\n",
    "                    try:\n",
    "                        flushed = await redis_conn.flushdb()\n",
    "                        logger.info(\"🧹 Redis FLUSHDB executed for test run, status=%s\", flushed)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(\"Could not flush Redis in test mode: %s\", e)\n",
    "            except Exception as e:\n",
    "                logger.warning(\"⚠️  Rate-limiter init failed: %s – continuing without limits\", e)\n",
    "        else:\n",
    "            logger.info(\"⚠️  Rate limiting disabled by config\")\n",
    "\n",
    "        # Initialize application readiness\n",
    "        logger.info(\"🚀 Startup event starting - _app_ready=%s\", _app_ready)\n",
    "\n",
    "        if settings.UNIT_TESTING:\n",
    "            logger.info(\"🔒 UNIT_TESTING=1 – startup hooks bypassed\")\n",
    "            _app_ready = True\n",
    "            logger.info(\"✅ _app_ready set to True (unit testing)\")\n",
    "        else:\n",
    "            try:\n",
    "                # Initialize ModelService first\n",
    "                logger.info(\"🔧 Initializing ModelService\")\n",
    "                await model_service.initialize()\n",
    "                logger.info(\"✅ ModelService initialized successfully\")\n",
    "\n",
    "                # Start background training tasks\n",
    "                logger.info(\"🔄 Starting background training tasks\")\n",
    "                asyncio.create_task(model_service.startup())\n",
    "                logger.info(\"✅ Background training tasks started\")\n",
    "\n",
    "                # Set ready to true after initialization (models will load in background)\n",
    "                _app_ready = True\n",
    "                logger.info(\"🚀 FastAPI ready – _app_ready=%s, health probes will pass immediately\", _app_ready)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(\"❌ Startup event failed: %s\", e)\n",
    "                import traceback\n",
    "                logger.error(\"❌ Startup traceback: %s\", traceback.format_exc())\n",
    "                # Set ready to true anyway so the API can serve requests\n",
    "                _app_ready = True\n",
    "                logger.warning(\"⚠️  Setting _app_ready=True despite startup errors\")\n",
    "\n",
    "        logger.info(\"🎯 Lifespan startup complete - _app_ready=%s\", _app_ready)\n",
    "        yield\n",
    "    finally:\n",
    "        logger.info(\"🔒 Shutting down…\")\n",
    "        try:\n",
    "            await FastAPILimiter.close()           # NEW – graceful shutdown\n",
    "        except Exception:\n",
    "            pass\n",
    "        await engine.dispose()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dependency injection helper\n",
    "# ---------------------------------------------------------------------------\n",
    "async def get_db() -> AsyncSession:\n",
    "    \"\"\"Yield a new DB session per request.\"\"\"\n",
    "    async with AsyncSessionLocal() as session:\n",
    "        yield session\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9fe2096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/security.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/security.py\n",
    "from __future__ import annotations\n",
    "import os, logging, secrets\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional\n",
    "\n",
    "from fastapi import Depends, HTTPException, status, Request\n",
    "from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\n",
    "from jose import jwt, JWTError\n",
    "from passlib.context import CryptContext\n",
    "from pydantic import BaseModel\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.  SECRET_KEY ***must*** be provided in the environment in production.\n",
    "# ---------------------------------------------------------------------------\n",
    "SECRET_KEY = os.getenv(\"SECRET_KEY\")\n",
    "if not SECRET_KEY:\n",
    "    log.critical(\n",
    "        \"ENV variable SECRET_KEY is missing -- generating a temporary key. \"\n",
    "        \"ALL issued JWTs will be invalid after a pod restart! \"\n",
    "        \"Set it in Railway → Variables to disable this warning.\"\n",
    "    )\n",
    "    SECRET_KEY = secrets.token_urlsafe(32)   # fallback only for dev\n",
    "\n",
    "ALGORITHM = \"HS256\"\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv(\"ACCESS_TOKEN_EXPIRE_MINUTES\", 30))\n",
    "\n",
    "pwd_ctx = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n",
    "oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/api/v1/token\")\n",
    "\n",
    "class TokenData(BaseModel):\n",
    "    username: Optional[str] = None\n",
    "\n",
    "class LoginPayload(BaseModel):\n",
    "    username: str\n",
    "    password: str\n",
    "\n",
    "async def get_credentials(request: Request) -> LoginPayload:\n",
    "    \"\"\"\n",
    "    Accept either JSON **or** classic form‑encoded credentials.\n",
    "\n",
    "    Order of precedence:\n",
    "    1. If the request media‑type is JSON → parse it with Pydantic.\n",
    "    2. Else parse as form-encoded data.\n",
    "    \"\"\"\n",
    "    content_type = request.headers.get(\"content-type\", \"\")\n",
    "    \n",
    "    if content_type.startswith(\"application/json\"):\n",
    "        # JSON branch\n",
    "        try:\n",
    "            body = await request.json()\n",
    "            return LoginPayload(**body)\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "                detail=f\"Invalid JSON credentials: {e}\",\n",
    "            )\n",
    "    else:\n",
    "        # Form-encoded branch\n",
    "        try:\n",
    "            form_data = await request.form()\n",
    "            username = form_data.get(\"username\")\n",
    "            password = form_data.get(\"password\")\n",
    "            \n",
    "            if not username or not password:\n",
    "                raise HTTPException(\n",
    "                    status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "                    detail=\"username and password are required\"\n",
    "                )\n",
    "            \n",
    "            return LoginPayload(username=username, password=password)\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "                detail=f\"Invalid form credentials: {e}\",\n",
    "            )\n",
    "\n",
    "def verify_password(raw: str, hashed: str) -> bool:\n",
    "    return pwd_ctx.verify(raw, hashed)\n",
    "\n",
    "def get_password_hash(pw: str) -> str:\n",
    "    return pwd_ctx.hash(pw)\n",
    "\n",
    "def create_access_token(subject: str) -> str:\n",
    "    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n",
    "    return jwt.encode({\"sub\": subject, \"exp\": expire}, SECRET_KEY, algorithm=ALGORITHM)\n",
    "\n",
    "async def get_current_user(token: str = Depends(oauth2_scheme)) -> str:\n",
    "    try:\n",
    "        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])\n",
    "        username: str = payload.get(\"sub\")\n",
    "        if not username:\n",
    "            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED)\n",
    "        return username\n",
    "    except JWTError as exc:\n",
    "        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED) from exc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6e5ee",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "643c33a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/data_engineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/data_engineering.py\n",
    "#to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "774c4e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/trainers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/trainers/__init__.py\n",
    "# api/src/trainers/__init__.py\n",
    "from .base import BaseTrainer, TrainResult\n",
    "\n",
    "__all__ = [\"BaseTrainer\", \"TrainResult\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3bb93b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/trainers/base.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/trainers/base.py\n",
    "# api/src/trainers/base.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, Optional, Protocol\n",
    "import mlflow\n",
    "\n",
    "@dataclass\n",
    "class TrainResult:\n",
    "    run_id: str\n",
    "    metrics: Dict[str, float]\n",
    "    artifacts: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "class SupportsPyFunc(Protocol):\n",
    "    # Minimal protocol if custom loader is needed later\n",
    "    def predict(self, X): ...\n",
    "\n",
    "class BaseTrainer:\n",
    "    \"\"\"\n",
    "    Minimal trainer abstraction:\n",
    "      * implement `train(**hyperparams)` returning TrainResult\n",
    "      * optionally override default_hyperparams()\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    model_type: str = \"generic\"\n",
    "\n",
    "    def default_hyperparams(self) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def merge_hyperparams(self, overrides: Dict[str, Any] | None) -> Dict[str, Any]:\n",
    "        params = self.default_hyperparams().copy()\n",
    "        if overrides:\n",
    "            params.update({k: v for k, v in overrides.items() if v is not None})\n",
    "        return params\n",
    "\n",
    "    def train(self, **hyperparams) -> TrainResult:  # pragma: no cover - interface\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Optional hook – if a trainer needs a special load path\n",
    "    def load_pyfunc(self, run_uri: str):\n",
    "        return mlflow.pyfunc.load_model(run_uri) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83792f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/trainers/iris_rf_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/trainers/iris_rf_trainer.py\n",
    "# api/src/trainers/iris_rf_trainer.py\n",
    "from __future__ import annotations\n",
    "from .base import BaseTrainer, TrainResult\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import mlflow\n",
    "\n",
    "class IrisRandomForestTrainer(BaseTrainer):\n",
    "    name = \"iris_random_forest\"\n",
    "    model_type = \"classification\"\n",
    "\n",
    "    def default_hyperparams(self):\n",
    "        return {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": None,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "    def train(self, **overrides) -> TrainResult:\n",
    "        hp = self.merge_hyperparams(overrides)\n",
    "        iris = load_iris(as_frame=True)\n",
    "        X, y = iris.data, iris.target\n",
    "        Xtr, Xte, ytr, yte = train_test_split(\n",
    "            X, y, test_size=0.25, stratify=y, random_state=hp[\"random_state\"]\n",
    "        )\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=hp[\"n_estimators\"],\n",
    "            max_depth=hp[\"max_depth\"],\n",
    "            random_state=hp[\"random_state\"],\n",
    "            n_jobs=-1,\n",
    "            class_weight=\"balanced\",\n",
    "        ).fit(Xtr, ytr)\n",
    "\n",
    "        preds = rf.predict(Xte)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(yte, preds),\n",
    "            \"f1_macro\": f1_score(yte, preds, average=\"macro\"),\n",
    "            \"precision_macro\": precision_score(yte, preds, average=\"macro\"),\n",
    "            \"recall_macro\": recall_score(yte, preds, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        class _Wrapper(mlflow.pyfunc.PythonModel):\n",
    "            def __init__(self, model, cols):\n",
    "                self.model = model\n",
    "                self.cols = cols\n",
    "            def predict(self, context, model_input, params=None):\n",
    "                import pandas as pd, numpy as np\n",
    "                df = model_input if isinstance(model_input, pd.DataFrame) else pd.DataFrame(model_input, columns=self.cols)\n",
    "                return self.model.predict_proba(df)\n",
    "\n",
    "        with mlflow.start_run(run_name=self.name) as run:\n",
    "            mlflow.log_params({k: v for k, v in hp.items()})\n",
    "            mlflow.log_metrics(metrics)\n",
    "            sig = mlflow.models.signature.infer_signature(X, rf.predict_proba(X))\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=\"model\",\n",
    "                python_model=_Wrapper(rf, list(X.columns)),\n",
    "                registered_model_name=self.name,\n",
    "                input_example=X.head(),\n",
    "                signature=sig,\n",
    "            )\n",
    "            return TrainResult(run_id=run.info.run_id, metrics=metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "757b7b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/registry/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/registry/__init__.py\n",
    "# api/src/registry/__init__.py\n",
    "from .registry import register, all_names, get, load_from_entry_point\n",
    "from .types import TrainerSpec\n",
    "\n",
    "__all__ = [\"register\", \"all_names\", \"get\", \"load_from_entry_point\", \"TrainerSpec\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a5dd8ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/registry/types.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/registry/types.py\n",
    "# api/src/registry/types.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Type, Dict, Any\n",
    "from ..trainers.base import BaseTrainer\n",
    "\n",
    "@dataclass\n",
    "class TrainerSpec:\n",
    "    name: str\n",
    "    cls: Type[BaseTrainer]\n",
    "    default_params: Dict[str, Any] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27aae736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/registry/registry.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/registry/registry.py\n",
    "# api/src/registry/registry.py\n",
    "from __future__ import annotations\n",
    "from importlib import import_module\n",
    "from typing import Dict, Iterable, Type\n",
    "from .types import TrainerSpec\n",
    "from ..trainers.base import BaseTrainer\n",
    "\n",
    "_REGISTRY: Dict[str, TrainerSpec] = {}\n",
    "\n",
    "def register(spec: TrainerSpec) -> None:\n",
    "    _REGISTRY[spec.name] = spec\n",
    "\n",
    "def all_names() -> Iterable[str]:\n",
    "    return _REGISTRY.keys()\n",
    "\n",
    "def get(name: str) -> TrainerSpec:\n",
    "    return _REGISTRY[name]\n",
    "\n",
    "def load_from_entry_point(dotted: str, name: str | None = None):\n",
    "    \"\"\"\n",
    "    Load 'pkg.module:ClassName' into registry.\n",
    "    \"\"\"\n",
    "    mod_path, cls_name = dotted.split(\":\")\n",
    "    mod = import_module(mod_path)\n",
    "    cls: Type[BaseTrainer] = getattr(mod, cls_name)\n",
    "    inst_name = name or getattr(cls, \"name\", cls_name.lower())\n",
    "    spec = TrainerSpec(name=inst_name, cls=cls, default_params=cls().default_hyperparams())\n",
    "    register(spec)\n",
    "    return spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2561ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/train.py\n",
    "from typing import Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from .bayes import BayesCancerParams\n",
    "\n",
    "class IrisTrainRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Kick off Iris model training.\n",
    "\n",
    "    • `model_type` – 'rf' (Random‑Forest) | 'logreg'  \n",
    "    • `hyperparams` – optional scikit‑learn overrides, e.g. {\"n_estimators\": 500}  \n",
    "    • `async_training` – true ⇒ returns job_id immediately\n",
    "    \"\"\"\n",
    "    model_type: str = Field(\n",
    "        default=\"rf\",\n",
    "        description=\"Which Iris trainer to run: 'rf' or 'logreg'\"\n",
    "    )\n",
    "    hyperparams: Optional[Dict[str, Any]] = Field(\n",
    "        default=None,\n",
    "        description=\"Optional hyper‑parameter overrides\"\n",
    "    )\n",
    "    async_training: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Run in background and return job ID\"\n",
    "    )\n",
    "\n",
    "class CancerTrainRequest(BaseModel):\n",
    "    \"\"\"\n",
    "    Train Breast‑Cancer classifiers.\n",
    "\n",
    "    • `model_type` – 'bayes' (hier‑Bayes) | 'stub' (quick LogisticRegression)  \n",
    "    • `params` – validated Bayesian hyper‑parameters (only used when model_type='bayes')  \n",
    "    • `async_training` – background flag\n",
    "    \"\"\"\n",
    "    model_type: str = Field(\n",
    "        default=\"bayes\",\n",
    "        description=\"Which cancer model to train: 'bayes' or 'stub'\"\n",
    "    )\n",
    "    params: Optional[BayesCancerParams] = Field(\n",
    "        default=None,\n",
    "        description=\"Bayesian hyper‑parameters; ignored for stub model\"\n",
    "    )\n",
    "    async_training: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Run in background and return job ID\"\n",
    "    )\n",
    "\n",
    "class BayesTrainRequest(BaseModel):\n",
    "    \"\"\"Request model for Bayesian cancer model training\"\"\"\n",
    "    params: Optional[BayesCancerParams] = Field(\n",
    "        default=None, \n",
    "        description=\"Bayesian hyperparameters. If None, uses defaults.\"\n",
    "    )\n",
    "    async_training: bool = Field(\n",
    "        default=False,\n",
    "        description=\"If True, returns job_id immediately. If False, waits for completion.\"\n",
    "    )\n",
    "\n",
    "class BayesTrainResponse(BaseModel):\n",
    "    \"\"\"Response model for Bayesian training\"\"\"\n",
    "    run_id: str = Field(description=\"MLflow run ID\")\n",
    "    job_id: Optional[str] = Field(default=None, description=\"Background job ID if async\")\n",
    "    status: str = Field(description=\"Training status: 'completed', 'queued', 'failed'\")\n",
    "    message: Optional[str] = Field(default=None, description=\"Status message or error\")\n",
    "\n",
    "class BayesConfigResponse(BaseModel):\n",
    "    \"\"\"Response model for Bayesian configuration endpoint\"\"\"\n",
    "    defaults: BayesCancerParams = Field(description=\"Default hyperparameters\")\n",
    "    bounds: dict = Field(description=\"Parameter bounds for UI controls\")\n",
    "    descriptions: dict = Field(description=\"Parameter descriptions for tooltips\")\n",
    "    runtime_estimate: dict = Field(description=\"Runtime estimation factors\")\n",
    "\n",
    "class BayesRunMetrics(BaseModel):\n",
    "    \"\"\"Response model for Bayesian run metrics\"\"\"\n",
    "    run_id: str\n",
    "    accuracy: float\n",
    "    rhat_max: Optional[float] = None\n",
    "    ess_bulk_min: Optional[float] = None\n",
    "    ess_tail_min: Optional[float] = None\n",
    "    waic: Optional[float] = None\n",
    "    loo: Optional[float] = None\n",
    "    status: str\n",
    "    warnings: list[str] = Field(default_factory=list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0cc65796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/cancer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/cancer.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class CancerFeatures(BaseModel):\n",
    "    \"\"\"Breast cancer diagnostic features.\"\"\"\n",
    "    mean_radius: float = Field(..., description=\"Mean of distances from center to points on perimeter\")\n",
    "    mean_texture: float = Field(..., description=\"Standard deviation of gray-scale values\")\n",
    "    mean_perimeter: float = Field(..., description=\"Mean size of the core tumor\")\n",
    "    mean_area: float = Field(..., description=\"Mean area of the core tumor\")\n",
    "    mean_smoothness: float = Field(..., description=\"Mean of local variation in radius lengths\")\n",
    "    mean_compactness: float = Field(..., description=\"Mean of perimeter^2 / area - 1.0\")\n",
    "    mean_concavity: float = Field(..., description=\"Mean of severity of concave portions of the contour\")\n",
    "    mean_concave_points: float = Field(..., description=\"Mean for number of concave portions of the contour\")\n",
    "    mean_symmetry: float = Field(..., description=\"Mean symmetry\")\n",
    "    mean_fractal_dimension: float = Field(..., description=\"Mean for 'coastline approximation' - 1\")\n",
    "    \n",
    "    # SE features (standard error)\n",
    "    se_radius: float = Field(..., description=\"Standard error of radius\")\n",
    "    se_texture: float = Field(..., description=\"Standard error of texture\")\n",
    "    se_perimeter: float = Field(..., description=\"Standard error of perimeter\")\n",
    "    se_area: float = Field(..., description=\"Standard error of area\")\n",
    "    se_smoothness: float = Field(..., description=\"Standard error of smoothness\")\n",
    "    se_compactness: float = Field(..., description=\"Standard error of compactness\")\n",
    "    se_concavity: float = Field(..., description=\"Standard error of concavity\")\n",
    "    se_concave_points: float = Field(..., description=\"Standard error of concave points\")\n",
    "    se_symmetry: float = Field(..., description=\"Standard error of symmetry\")\n",
    "    se_fractal_dimension: float = Field(..., description=\"Standard error of fractal dimension\")\n",
    "    \n",
    "    # Worst features\n",
    "    worst_radius: float = Field(..., description=\"Worst radius\")\n",
    "    worst_texture: float = Field(..., description=\"Worst texture\")\n",
    "    worst_perimeter: float = Field(..., description=\"Worst perimeter\")\n",
    "    worst_area: float = Field(..., description=\"Worst area\")\n",
    "    worst_smoothness: float = Field(..., description=\"Worst smoothness\")\n",
    "    worst_compactness: float = Field(..., description=\"Worst compactness\")\n",
    "    worst_concavity: float = Field(..., description=\"Worst concavity\")\n",
    "    worst_concave_points: float = Field(..., description=\"Worst concave points\")\n",
    "    worst_symmetry: float = Field(..., description=\"Worst symmetry\")\n",
    "    worst_fractal_dimension: float = Field(..., description=\"Worst fractal dimension\")\n",
    "\n",
    "class CancerPredictRequest(BaseModel):\n",
    "    \"\"\"Cancer prediction request (allows 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"bayes\", description=\"Model type: 'bayes', 'logreg', or 'rf'\")\n",
    "    samples: List[CancerFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"Breast-cancer feature vectors\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "    posterior_samples: Optional[int] = Field(\n",
    "        None, ge=10, le=10_000, description=\"Posterior draws for uncertainty\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class CancerPredictResponse(BaseModel):\n",
    "    \"\"\"Cancer prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted diagnosis (M=malignant, B=benign)\")\n",
    "    probabilities: List[float] = Field(..., description=\"Probability of malignancy\")\n",
    "    uncertainties: Optional[List[float]] = Field(None, description=\"Uncertainty estimates (if requested)\")\n",
    "    input_received: List[CancerFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1fd0f1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/iris.py\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class IrisFeatures(BaseModel):\n",
    "    \"\"\"Iris measurement features.\"\"\"\n",
    "    sepal_length: float = Field(..., description=\"Sepal length in cm\", ge=0, le=10)\n",
    "    sepal_width: float = Field(..., description=\"Sepal width in cm\", ge=0, le=10)\n",
    "    petal_length: float = Field(..., description=\"Petal length in cm\", ge=0, le=10)\n",
    "    petal_width: float = Field(..., description=\"Petal width in cm\", ge=0, le=10)\n",
    "\n",
    "class IrisPredictRequest(BaseModel):\n",
    "    \"\"\"Iris prediction request (accepts legacy 'rows' alias).\"\"\"\n",
    "    model_type: str = Field(\"rf\", description=\"Model type: 'rf' or 'logreg'\")\n",
    "    samples: List[IrisFeatures] = Field(\n",
    "        ...,\n",
    "        description=\"List of iris measurements\",\n",
    "        alias=\"rows\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        populate_by_name = True\n",
    "        extra = \"forbid\"\n",
    "\n",
    "class IrisPredictResponse(BaseModel):\n",
    "    \"\"\"Iris prediction response.\"\"\"\n",
    "    predictions: List[str] = Field(..., description=\"Predicted iris species\")\n",
    "    probabilities: List[List[float]] = Field(..., description=\"Class probabilities\")\n",
    "    input_received: List[IrisFeatures] = Field(..., description=\"Echo of input features\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "904e29bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/schemas/bayes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/schemas/bayes.py\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import Optional\n",
    "\n",
    "class BayesCancerParams(BaseModel):\n",
    "    draws: int = Field(1000, ge=200, le=20_000, description=\"Posterior draws retained\")\n",
    "    tune: int = Field(1000, ge=200, le=20_000, description=\"Tuning (warmup) steps\")\n",
    "    target_accept: float = Field(0.95, ge=0.80, le=0.999, description=\"NUTS target acceptance\")\n",
    "    compute_waic: bool = Field(True, description=\"Attempt WAIC (may be slow)\")\n",
    "    compute_loo: bool = Field(False, description=\"Attempt LOO (slower); auto-off by default\")\n",
    "    max_rhat_warn: float = Field(1.01, ge=1.0, le=1.1)\n",
    "    min_ess_warn: int = Field(400, ge=50, le=5000)\n",
    "\n",
    "    @validator(\"tune\")\n",
    "    def tune_reasonable(cls, v, values):\n",
    "        if \"draws\" in values and v < 0.2 * values[\"draws\"]:\n",
    "            # gentle warning, not rejection\n",
    "            pass\n",
    "        return v\n",
    "\n",
    "    def to_kwargs(self):\n",
    "        return {\n",
    "            \"draws\": self.draws,\n",
    "            \"tune\": self.tune,\n",
    "            \"target_accept\": self.target_accept,\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "70bc6c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/__init__.py\n",
    "\"\"\"\n",
    "ML sub-package – exposes built-in trainers so the service can import\n",
    "`app.ml.builtin_trainers` with an absolute import.\n",
    "\"\"\"\n",
    "\n",
    "from .builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_iris_logreg,\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    \"train_iris_random_forest\",\n",
    "    \"train_iris_logreg\",\n",
    "    \"train_breast_cancer_bayes\",\n",
    "    \"train_breast_cancer_stub\",\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "374475aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/utils.py\n",
    "# api/app/ml/utils.py\n",
    "\n",
    "def configure_pytensor_compiler(*_, **__):\n",
    "    \"\"\"\n",
    "    Stub kept for backward‑compatibility.\n",
    "\n",
    "    The project now uses the **JAX backend**, so PyTensor never calls a C\n",
    "    compiler.  This function therefore does nothing and always returns True.\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "# ─── LEGACY ALIAS ──────────────────────────────────────────────────────────\n",
    "# Some early-boot modules import \"find_compiler\", so we alias it here\n",
    "find_compiler = configure_pytensor_compiler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72a78a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/ml/builtin_trainers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/ml/builtin_trainers.py\n",
    "# api/ml/builtin_trainers.py\n",
    "\"\"\"\n",
    "Built-in trainers for Iris RF and Breast-Cancer Bayesian LogReg.\n",
    "Executed automatically by ModelService when a model is missing.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from pathlib import Path\n",
    "import mlflow, mlflow.sklearn, mlflow.pyfunc\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import pickle\n",
    "import warnings\n",
    "import subprocess\n",
    "import os\n",
    "import platform\n",
    "from app.core.config import settings\n",
    "\n",
    "# Conditional imports for heavy dependencies\n",
    "if os.getenv(\"UNIT_TESTING\") != \"1\" and os.getenv(\"SKIP_BACKGROUND_TRAINING\") != \"1\":\n",
    "    import pymc as pm\n",
    "    import arviz as az\n",
    "else:\n",
    "    pm = None\n",
    "    az = None\n",
    "\n",
    "# --- ADD THIS NEAR THE TOP (after imports) ----------------------------------\n",
    "def _ensure_experiment(name: str = \"ml_fullstack_models\") -> str:\n",
    "    \"\"\"\n",
    "    Guarantee that `name` exists and return its experiment_id.\n",
    "    Handles the MLflow race where set_experiment returns a dangling ID\n",
    "    if the experiment folder has not been written yet.\n",
    "    \"\"\"\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    exp = client.get_experiment_by_name(name)\n",
    "    if exp is None:\n",
    "        exp_id = client.create_experiment(name)\n",
    "    else:\n",
    "        exp_id = exp.experiment_id\n",
    "    mlflow.set_experiment(name)          # marks it the active one\n",
    "    return exp_id\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Honour whatever Settings or the shell already provided; then\n",
    "# fall back if the host part cannot be resolved quickly.\n",
    "# ------------------------------------------------------------------\n",
    "from urllib.parse import urlparse\n",
    "import socket, time\n",
    "\n",
    "def _fast_resolve(uri: str) -> bool:\n",
    "    if uri.startswith(\"http\"):\n",
    "        host = urlparse(uri).hostname\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            socket.getaddrinfo(host, None, proto=socket.IPPROTO_TCP)\n",
    "            return (time.perf_counter() - t0) < 0.05\n",
    "        except socket.gaierror:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# MLflow tracking URI is now resolved in ModelService.initialize()\n",
    "# Trainers assume MLflow is already configured and experiments exist\n",
    "logger.info(\"📦 Trainers ready - MLflow URI will be resolved at service startup\")\n",
    "\n",
    "MLFLOW_EXPERIMENT = \"ml_fullstack_models\"\n",
    "\n",
    "# Remove side-effectful MLflow calls at import time\n",
    "# Experiments will be created on-demand in each trainer function\n",
    "\n",
    "# --- psutil health probe ----------------------------------------------------\n",
    "def _psutil_healthy() -> bool:\n",
    "    \"\"\"\n",
    "    Return True if psutil imports cleanly *and* exposes a working Process() object.\n",
    "    We cache the result because repeated checks are cheap but noisy in logs.\n",
    "    \"\"\"\n",
    "    global _PSUTIL_HEALTH_CACHE\n",
    "    try:\n",
    "        return _PSUTIL_HEALTH_CACHE\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    ok = False\n",
    "    try:\n",
    "        import psutil  # type: ignore\n",
    "        ok = hasattr(psutil, \"Process\")\n",
    "        if ok:\n",
    "            try:\n",
    "                _ = psutil.Process().pid  # touch native layer\n",
    "            except Exception:  # bad native ext\n",
    "                ok = False\n",
    "    except Exception:\n",
    "        ok = False\n",
    "\n",
    "    _PSUTIL_HEALTH_CACHE = ok\n",
    "    if not ok:\n",
    "        logger.warning(\"🩺 psutil unhealthy – disabling sklearn/joblib parallelism (n_jobs=1).\")\n",
    "    return ok\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  IRIS – point-estimate Random-Forest (enhanced with better parameters)\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_iris_random_forest(\n",
    "    n_estimators: int = 300,\n",
    "    max_depth: int | None = None,\n",
    "    random_state: int = 42,\n",
    ") -> str:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import mlflow, mlflow.pyfunc\n",
    "\n",
    "    # 1️⃣  ALWAYS ensure the experiment exists *inside* the function\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    safe_jobs = -1 if _psutil_healthy() else 1\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state,\n",
    "        n_jobs=safe_jobs,\n",
    "        class_weight=\"balanced\",\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    preds = rf.predict(X_te)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_te, preds),\n",
    "        \"f1_macro\": f1_score(y_te, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(y_te, preds, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(y_te, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    class IrisRFWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model):\n",
    "            if hasattr(model, \"n_jobs\"):\n",
    "                model.n_jobs = 1\n",
    "            self.model = model\n",
    "            self._cols = list(X.columns)\n",
    "\n",
    "        def _df(self, arr):\n",
    "            import pandas as pd, numpy as np\n",
    "            if isinstance(arr, pd.DataFrame):\n",
    "                return arr\n",
    "            return pd.DataFrame(np.asarray(arr), columns=self._cols)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            X_ = self._df(model_input)\n",
    "            return self.model.predict_proba(X_)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"iris_random_forest\") as run:\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_params({\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"random_state\": random_state,\n",
    "            \"safe_n_jobs\": safe_jobs,\n",
    "        })\n",
    "\n",
    "        sig = mlflow.models.signature.infer_signature(X, rf.predict_proba(X))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",            # ✅ correct kw-arg\n",
    "            python_model=IrisRFWrapper(rf),\n",
    "            registered_model_name=\"iris_random_forest\",\n",
    "            input_example=X.head(),\n",
    "            signature=sig,\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  IRIS – logistic-regression trainer (NEW)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_iris_logreg(\n",
    "    C: float = 1.0,\n",
    "    max_iter: int = 400,\n",
    "    random_state: int = 42,\n",
    ") -> str:\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import mlflow, mlflow.pyfunc\n",
    "\n",
    "    # 1️⃣  ALWAYS ensure the experiment exists *inside* the function\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    iris = load_iris(as_frame=True)\n",
    "    X, y = iris.data, iris.target\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    safe_jobs = -1 if _psutil_healthy() else 1\n",
    "    clf = LogisticRegression(\n",
    "        C=C,\n",
    "        max_iter=max_iter,\n",
    "        multi_class=\"multinomial\",\n",
    "        solver=\"lbfgs\",\n",
    "        n_jobs=safe_jobs,\n",
    "        random_state=random_state,\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    preds = clf.predict(X_te)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_te, preds),\n",
    "        \"f1_macro\": f1_score(y_te, preds, average=\"macro\"),\n",
    "        \"precision_macro\": precision_score(y_te, preds, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(y_te, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    class IrisLogRegWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model):\n",
    "            if hasattr(model, \"n_jobs\"):\n",
    "                model.n_jobs = 1\n",
    "            self.model = model\n",
    "            self._cols = list(X.columns)\n",
    "\n",
    "        def _df(self, arr):\n",
    "            import pandas as pd, numpy as np\n",
    "            if isinstance(arr, pd.DataFrame):\n",
    "                return arr\n",
    "            return pd.DataFrame(np.asarray(arr), columns=self._cols)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            X_ = self._df(model_input)\n",
    "            return self.model.predict_proba(X_)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"iris_logreg\") as run:\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_params({\n",
    "            \"C\": C,\n",
    "            \"max_iter\": max_iter,\n",
    "            \"random_state\": random_state,\n",
    "            \"safe_n_jobs\": safe_jobs,\n",
    "        })\n",
    "\n",
    "        sig = mlflow.models.signature.infer_signature(X, clf.predict_proba(X))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",            # ✅ correct kw-arg\n",
    "            python_model=IrisLogRegWrapper(clf),\n",
    "            registered_model_name=\"iris_logreg\",\n",
    "            input_example=X.head(),\n",
    "            signature=sig,\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER STUB – ultra-fast fallback model\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_breast_cancer_stub(random_state: int = 42) -> str:\n",
    "    \"\"\"\n",
    "    Ultra-fast fallback binary LogisticRegression on the breast-cancer dataset.\n",
    "\n",
    "    Serializes safely on Windows by forcing `n_jobs=1` if psutil unhealthy,\n",
    "    and exports MLflow PythonModel w/ modern signature that returns P(malignant).\n",
    "    References: joblib parallelism + psutil; MLflow PythonModel signature. :contentReference[oaicite:21]{index=21}\n",
    "    \"\"\"\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import mlflow, tempfile, pickle, pandas as pd\n",
    "\n",
    "    # 1️⃣  ALWAYS ensure the experiment exists *inside* the function\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    safe_jobs = -1 if _psutil_healthy() else 1\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        max_iter=200, n_jobs=safe_jobs, random_state=random_state\n",
    "    ).fit(Xtr, ytr)\n",
    "\n",
    "    class CancerStubWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, model):\n",
    "            if hasattr(model, \"n_jobs\"):\n",
    "                try:\n",
    "                    model.n_jobs = 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self.model = model\n",
    "            self._cols = list(X.columns)\n",
    "\n",
    "        def _df(self, arr):\n",
    "            import pandas as pd, numpy as np\n",
    "            if isinstance(arr, pd.DataFrame):\n",
    "                return arr\n",
    "            return pd.DataFrame(np.asarray(arr), columns=self._cols)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            proba = self.model.predict_proba(self._df(model_input))\n",
    "            return proba[:, 1]  # malignant probability\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            return self.model.predict_proba(self._df(X))\n",
    "\n",
    "    acc = accuracy_score(yte, clf.predict(Xte))\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_stub\") as run:\n",
    "        # Log config hash for reproducibility and drift detection\n",
    "        from app.core.config import settings as _s\n",
    "        import hashlib, json\n",
    "        _cfg_hash = hashlib.sha256(json.dumps(_s.model_dump(), sort_keys=True).encode()).hexdigest()\n",
    "        mlflow.log_param(\"train_config_hash\", _cfg_hash)\n",
    "\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_param(\"safe_n_jobs\", safe_jobs)\n",
    "        wrapper = CancerStubWrapper(clf)\n",
    "        sig = mlflow.models.signature.infer_signature(X, wrapper.predict(None, X))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\",\n",
    "            python_model=wrapper,\n",
    "            registered_model_name=\"breast_cancer_stub\",\n",
    "            input_example=X.head(),\n",
    "            signature=sig,\n",
    "        )\n",
    "        return run.info.run_id\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#  BREAST-CANCER – hierarchical Bayesian logistic regression\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_breast_cancer_bayes(\n",
    "    draws: int = 1000,\n",
    "    tune: int = 1000,\n",
    "    target_accept: float = 0.99,\n",
    "    params_obj=None,   # optional BayesCancerParams\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Hierarchical Bayesian logistic regression with varying intercepts.\n",
    "\n",
    "    Adds:\n",
    "      * Validated hyperparams via BayesCancerParams (if provided)\n",
    "      * Convergence diagnostics: R-hat, bulk/tail ESS\n",
    "      * Optional WAIC / LOO (guarded; can be disabled)\n",
    "      * Logged warnings if thresholds exceeded\n",
    "    \"\"\"\n",
    "    import pymc as pm\n",
    "    import pandas as pd, numpy as np\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import mlflow, tempfile, pickle\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Schema override if provided\n",
    "    if params_obj is not None:\n",
    "        draws = params_obj.draws\n",
    "        tune = params_obj.tune\n",
    "        target_accept = params_obj.target_accept\n",
    "        compute_waic = params_obj.compute_waic\n",
    "        compute_loo = params_obj.compute_loo\n",
    "        max_rhat_warn = params_obj.max_rhat_warn\n",
    "        min_ess_warn = params_obj.min_ess_warn\n",
    "    else:\n",
    "        compute_waic = True\n",
    "        compute_loo = False\n",
    "        max_rhat_warn = 1.01\n",
    "        min_ess_warn = 400\n",
    "\n",
    "    logger.info(\n",
    "        \"BayesCancer: draws=%d tune=%d target_accept=%.3f waic=%s loo=%s\",\n",
    "        draws, tune, target_accept, compute_waic, compute_loo\n",
    "    )\n",
    "\n",
    "    _ensure_experiment(MLFLOW_EXPERIMENT)\n",
    "\n",
    "    X_df, y = load_breast_cancer(as_frame=True, return_X_y=True)\n",
    "    quint, edges = pd.qcut(X_df[\"mean texture\"], 5, labels=False, retbins=True)\n",
    "    g = np.asarray(quint, dtype=\"int64\")\n",
    "    scaler = StandardScaler().fit(X_df)\n",
    "    Xs = scaler.transform(X_df)\n",
    "\n",
    "    coords = {\"group\": np.arange(5)}\n",
    "    with pm.Model(coords=coords) as m:\n",
    "        α = pm.Normal(\"α\", 0.0, 1.0, dims=\"group\")\n",
    "        β = pm.Normal(\"β\", 0.0, 1.0, shape=Xs.shape[1])\n",
    "        logit = α[g] + pm.math.dot(Xs, β)\n",
    "        pm.Bernoulli(\"obs\", logit_p=logit, observed=y)\n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=4,\n",
    "            nuts_sampler=\"numpyro\",\n",
    "            target_accept=target_accept,\n",
    "            progressbar=False,\n",
    "        )\n",
    "\n",
    "    # Diagnostics\n",
    "    import arviz as az\n",
    "    rhat = az.rhat(idata).to_array().values.max()\n",
    "    ess_bulk = az.ess(idata, method=\"bulk\").to_array().values.min()\n",
    "    ess_tail = az.ess(idata, method=\"tail\").to_array().values.min()\n",
    "    waic_val = None\n",
    "    loo_val = None\n",
    "    try:\n",
    "        if compute_waic:\n",
    "            waic_val = float(az.waic(idata).waic)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"WAIC computation failed: %s\", e)\n",
    "    try:\n",
    "        if compute_loo:\n",
    "            loo_val = float(az.loo(idata).loo)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"LOO computation failed: %s\", e)\n",
    "\n",
    "    # Wrapper\n",
    "    class _HierBayesWrapper(mlflow.pyfunc.PythonModel):\n",
    "        def __init__(self, trace, sc, ed, cols):\n",
    "            self.trace, self.scaler, self.edges, self.cols = trace, sc, ed, cols\n",
    "\n",
    "        def _quint(self, df):\n",
    "            col = \"mean texture\"\n",
    "            if col not in df.columns and \"mean_texture\" in df.columns:\n",
    "                df = df.rename(columns={\"mean_texture\": col})\n",
    "            tex = df[col].to_numpy()\n",
    "            return np.clip(np.digitize(tex, self.edges, right=False), 0, 4)\n",
    "\n",
    "        def predict(self, context, model_input, params=None):\n",
    "            df = model_input if isinstance(model_input, pd.DataFrame) else pd.DataFrame(model_input, columns=self.cols)\n",
    "            xs = self.scaler.transform(df)\n",
    "            g = self._quint(df)\n",
    "            αg = self.trace.posterior[\"α\"].median((\"chain\", \"draw\")).values\n",
    "            β = self.trace.posterior[\"β\"].median((\"chain\", \"draw\")).values\n",
    "            log = αg[g] + np.dot(xs, β)\n",
    "            return 1.0 / (1.0 + np.exp(-log))\n",
    "\n",
    "    wrapper = _HierBayesWrapper(idata, scaler, edges[1:-1], X_df.columns.tolist())\n",
    "    preds = wrapper.predict(None, X_df)\n",
    "    acc = float(((preds > 0.5).astype(int) == y).mean())\n",
    "\n",
    "    # Threshold warnings\n",
    "    if rhat > max_rhat_warn:\n",
    "        logger.warning(\"R-hat exceeds threshold: %.4f > %.2f\", rhat, max_rhat_warn)\n",
    "    if ess_bulk < min_ess_warn:\n",
    "        logger.warning(\"Bulk ESS below threshold: %.1f < %d\", ess_bulk, min_ess_warn)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td, mlflow.start_run(run_name=\"breast_cancer_bayes\") as run:\n",
    "        from app.core.config import settings as _s\n",
    "        import hashlib, json\n",
    "        _cfg_hash = hashlib.sha256(json.dumps(_s.model_dump(), sort_keys=True).encode()).hexdigest()\n",
    "\n",
    "        # Metrics\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"rhat_max\", rhat)\n",
    "        mlflow.log_metric(\"ess_bulk_min\", ess_bulk)\n",
    "        mlflow.log_metric(\"ess_tail_min\", ess_tail)\n",
    "        if waic_val is not None:\n",
    "            mlflow.log_metric(\"waic\", waic_val)\n",
    "        if loo_val is not None:\n",
    "            mlflow.log_metric(\"loo\", loo_val)\n",
    "\n",
    "        # Params\n",
    "        mlflow.log_param(\"train_config_hash\", _cfg_hash)\n",
    "        mlflow.log_param(\"draws\", draws)\n",
    "        mlflow.log_param(\"tune\", tune)\n",
    "        mlflow.log_param(\"target_accept\", target_accept)\n",
    "        mlflow.log_param(\"compute_waic\", compute_waic)\n",
    "        mlflow.log_param(\"compute_loo\", compute_loo)\n",
    "\n",
    "        sc_path = Path(td) / \"scaler.pkl\"\n",
    "        pickle.dump(scaler, open(sc_path, \"wb\"))\n",
    "        mlflow.pyfunc.log_model(\n",
    "            \"model\",\n",
    "            python_model=wrapper,\n",
    "            artifacts={\"scaler\": str(sc_path)},\n",
    "            registered_model_name=\"breast_cancer_bayes\",\n",
    "            input_example=X_df.head(),\n",
    "            signature=mlflow.models.signature.infer_signature(X_df, wrapper.predict(None, X_df)),\n",
    "        )\n",
    "        return run.info.run_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe3394a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/services/ml/model_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/services/ml/model_service.py\n",
    "\"\"\"\n",
    "Model service – self-healing startup with background training.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import asyncio, logging, os, time, socket, shutil, subprocess, hashlib, json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow, pandas as pd, numpy as np\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# IMPORTANT IMPORT NOTE\n",
    "# ------------------------------------------------------------------\n",
    "# This module lives in app/services/ml/.\n",
    "# To reach the sibling top-level package app/core/ we must step\n",
    "# *two* levels up (services/ml -> services -> app) before importing.\n",
    "# Rather than counting dots ('from ...core.config import settings'),\n",
    "# we choose *absolute imports* for clarity & stability across refactors.\n",
    "# See Python import system docs + Real Python discussion on why absolute\n",
    "# imports are preferred for larger projects.  :contentReference[oaicite:25]{index=25}\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from app.core.config import settings\n",
    "from app.ml.builtin_trainers import (\n",
    "    train_iris_random_forest,\n",
    "    train_iris_logreg,  # NEW\n",
    "    train_breast_cancer_bayes,\n",
    "    train_breast_cancer_stub,\n",
    ")\n",
    "\n",
    "# NEW imports for registry integration\n",
    "from importlib import import_module\n",
    "try:\n",
    "    from src.registry import registry as dynamic_registry  # noqa\n",
    "except Exception:\n",
    "    dynamic_registry = None  # tolerant if path not yet packaged\n",
    "\n",
    "\n",
    "# from ..core.config import settings\n",
    "# from ..ml.builtin_trainers import (\n",
    "#     train_iris_random_forest,\n",
    "#     train_iris_logreg,  # NEW\n",
    "#     train_breast_cancer_bayes,\n",
    "#     train_breast_cancer_stub,\n",
    "# )\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- safe sklearn predict_proba helper ---------------------------------------\n",
    "def _safe_sklearn_proba(estimator, X, *, log_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Call estimator.predict_proba(X) but recover from environments where\n",
    "    joblib/loky -> psutil introspection explodes (e.g., AttributeError: psutil.Process).\n",
    "\n",
    "    Strategy:\n",
    "    1. Try fast path.\n",
    "    2. On AttributeError mentioning psutil (or any RuntimeError from joblib),\n",
    "       set estimator.n_jobs = 1 if present and retry serially.\n",
    "    3. As a last resort, call estimator.predict(X) and synthesize 1-hot probs.\n",
    "\n",
    "    Returns a NumPy array of shape (n_samples, n_classes).\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    from joblib import parallel_backend\n",
    "\n",
    "    # Make sure we have an array / DataFrame scikit can handle\n",
    "    X_ = X\n",
    "\n",
    "    # 1st attempt --------------------------------------------------------------\n",
    "    try:\n",
    "        return estimator.predict_proba(X_)\n",
    "    except Exception as e:  # broad → we inspect below\n",
    "        msg = str(e)\n",
    "        bad_psutil = \"psutil\" in msg and \"Process\" in msg\n",
    "        if not bad_psutil:\n",
    "            logger.warning(\"%s predict_proba failed (%s) – retry single-threaded\",\n",
    "                           log_prefix, e)\n",
    "\n",
    "        # 2nd attempt: force serial backend -----------------------------------\n",
    "        try:\n",
    "            if hasattr(estimator, \"n_jobs\"):\n",
    "                try:\n",
    "                    estimator.n_jobs = 1\n",
    "                except Exception:  # read-only attr\n",
    "                    pass\n",
    "            with parallel_backend(\"threading\", n_jobs=1):\n",
    "                return estimator.predict_proba(X_)\n",
    "        except Exception as e2:\n",
    "            logger.error(\"%s serial predict_proba failed (%s) – fallback to classes\",\n",
    "                         log_prefix, e2)\n",
    "\n",
    "    # 3rd attempt: derive 1-hot from predict ----------------------------------\n",
    "    try:\n",
    "        preds = estimator.predict(X_)\n",
    "        preds = _np.asarray(preds, dtype=int)\n",
    "        n_classes = getattr(estimator, \"n_classes_\", preds.max() + 1)\n",
    "        probs = _np.zeros((preds.size, n_classes), dtype=float)\n",
    "        probs[_np.arange(preds.size), preds] = 1.0\n",
    "        return probs\n",
    "    except Exception as e3:\n",
    "        logger.exception(\"%s fallback predict also failed (%s)\", log_prefix, e3)\n",
    "        raise  # Let caller handle\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Cancer column mapping: Pydantic field names ➜ training column names\n",
    "# ---------------------------------------------------------------------------\n",
    "_CANCER_COLMAP: dict[str, str] = {\n",
    "    # Means\n",
    "    \"mean_radius\": \"mean radius\",\n",
    "    \"mean_texture\": \"mean texture\",\n",
    "    \"mean_perimeter\": \"mean perimeter\",\n",
    "    \"mean_area\": \"mean area\",\n",
    "    \"mean_smoothness\": \"mean smoothness\",\n",
    "    \"mean_compactness\": \"mean compactness\",\n",
    "    \"mean_concavity\": \"mean concavity\",\n",
    "    \"mean_concave_points\": \"mean concave points\",\n",
    "    \"mean_symmetry\": \"mean symmetry\",\n",
    "    \"mean_fractal_dimension\": \"mean fractal dimension\",\n",
    "    # SE\n",
    "    \"se_radius\": \"radius error\",\n",
    "    \"se_texture\": \"texture error\",\n",
    "    \"se_perimeter\": \"perimeter error\",\n",
    "    \"se_area\": \"area error\",\n",
    "    \"se_smoothness\": \"smoothness error\",\n",
    "    \"se_compactness\": \"compactness error\",\n",
    "    \"se_concavity\": \"concavity error\",\n",
    "    \"se_concave_points\": \"concave points error\",\n",
    "    \"se_symmetry\": \"symmetry error\",\n",
    "    \"se_fractal_dimension\": \"fractal dimension error\",\n",
    "    # Worst\n",
    "    \"worst_radius\": \"worst radius\",\n",
    "    \"worst_texture\": \"worst texture\",\n",
    "    \"worst_perimeter\": \"worst perimeter\",\n",
    "    \"worst_area\": \"worst area\",\n",
    "    \"worst_smoothness\": \"worst smoothness\",\n",
    "    \"worst_compactness\": \"worst compactness\",\n",
    "    \"worst_concavity\": \"worst concavity\",\n",
    "    \"worst_concave_points\": \"worst concave points\",\n",
    "    \"worst_symmetry\": \"worst symmetry\",\n",
    "    \"worst_fractal_dimension\": \"worst fractal dimension\",\n",
    "}\n",
    "\n",
    "def _rename_cancer_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure DataFrame columns match the training schema used by MLflow artefacts.\n",
    "    Unknown columns are left untouched so legacy models still work.\n",
    "    \"\"\"\n",
    "    return df.rename(columns=_CANCER_COLMAP)\n",
    "\n",
    "# Trainer mapping for self-healing\n",
    "TRAINERS = {\n",
    "    \"iris_random_forest\": train_iris_random_forest,\n",
    "    \"iris_logreg\":        train_iris_logreg,  # NEW\n",
    "    \"breast_cancer_bayes\": train_breast_cancer_bayes,\n",
    "    \"breast_cancer_stub\":  train_breast_cancer_stub,\n",
    "}\n",
    "\n",
    "class ModelService:\n",
    "    \"\"\"\n",
    "    Self-healing model service that loads existing models and schedules\n",
    "    background training for missing ones.\n",
    "    \"\"\"\n",
    "\n",
    "    _EXECUTOR = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._unit_test_mode = settings.UNIT_TESTING\n",
    "        self.initialized = False\n",
    "\n",
    "        # 🚫 Heavy clients only when NOT unit-testing\n",
    "        self.client = None if self._unit_test_mode else None  # Will be set in initialize()\n",
    "        self.mlflow_client = None\n",
    "\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        # registry bootstrap flags\n",
    "        self._registry_loaded = False\n",
    "        self.status: Dict[str, str] = {\n",
    "            \"iris_random_forest\": \"missing\",\n",
    "            \"iris_logreg\":        \"missing\",  # NEW\n",
    "            \"breast_cancer_bayes\": \"missing\",\n",
    "            \"breast_cancer_stub\": \"missing\",\n",
    "        }\n",
    "\n",
    "    # --- Registry integration (increment 1) ---------------------------------\n",
    "    def _init_registry_once(self):\n",
    "        if self._registry_loaded:\n",
    "            return\n",
    "        if dynamic_registry is None:\n",
    "            logger.info(\"Registry package not available yet; skipping dynamic trainer loading.\")\n",
    "            self._registry_loaded = True\n",
    "            return\n",
    "        try:\n",
    "            # Hardcode first migrated trainer; later we will iterate YAML directory.\n",
    "            from src.registry.registry import load_from_entry_point  # type: ignore\n",
    "            load_from_entry_point(\"src.trainers.iris_rf_trainer:IrisRandomForestTrainer\")\n",
    "            self._registry_loaded = True\n",
    "            logger.info(\"Dynamic registry initialized with trainers: %s\",\n",
    "                        list(dynamic_registry.all_names()))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Failed to initialize registry: %s\", e)\n",
    "            self._registry_loaded = True  # prevent retry storm\n",
    "\n",
    "    def _get_trainer_or_none(self, name: str):\n",
    "        if not self._registry_loaded:\n",
    "            self._init_registry_once()\n",
    "        try:\n",
    "            from src.registry.registry import get as reg_get  # type: ignore\n",
    "            return reg_get(name)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    async def train_via_registry(self, name: str, overrides: Dict[str, Any] | None = None) -> Optional[str]:\n",
    "        spec = self._get_trainer_or_none(name)\n",
    "        if spec is None:\n",
    "            logger.info(\"No registry trainer for %s\", name)\n",
    "            return None\n",
    "        trainer = spec.cls()\n",
    "        # merge overrides\n",
    "        params = trainer.merge_hyperparams(overrides or {})\n",
    "        loop = asyncio.get_running_loop()\n",
    "        logger.info(\"Training %s via registry with params=%s\", name, params)\n",
    "        result = await loop.run_in_executor(self._EXECUTOR, lambda: trainer.train(**params))\n",
    "        # After training, force reload of production candidate (latest run fallback)\n",
    "        await self._try_load(name)\n",
    "        return result.run_id\n",
    "\n",
    "    def _resolve_tracking_uri(self) -> str:\n",
    "        \"\"\"\n",
    "        Resolve MLflow tracking URI with graceful fallback:\n",
    "          1. Explicit env var MLFLOW_TRACKING_URI\n",
    "          2. settings.MLFLOW_TRACKING_URI\n",
    "          3. local file store 'file:./mlruns_local'\n",
    "        DNS / connection problems downgrade to local file store.\n",
    "        \"\"\"\n",
    "        import socket, urllib.parse, mlflow\n",
    "        candidates = []\n",
    "        if os.getenv(\"MLFLOW_TRACKING_URI\"):\n",
    "            candidates.append((\"env\", os.getenv(\"MLFLOW_TRACKING_URI\")))\n",
    "        candidates.append((\"settings\", settings.MLFLOW_TRACKING_URI))\n",
    "        candidates.append((\"fallback\", \"file:./mlruns_local\"))\n",
    "\n",
    "        for origin, uri in candidates:\n",
    "            parsed = urllib.parse.urlparse(uri)\n",
    "            if parsed.scheme in (\"http\", \"https\"):\n",
    "                host = parsed.hostname\n",
    "                try:\n",
    "                    socket.getaddrinfo(host, parsed.port or 80)\n",
    "                    logger.info(\"MLflow URI ok (%s): %s\", origin, uri)\n",
    "                    return uri\n",
    "                except socket.gaierror as e:\n",
    "                    logger.warning(\"MLflow URI unresolved (%s=%s) -> %s\", origin, uri, e)\n",
    "            else:\n",
    "                # file store always acceptable\n",
    "                logger.info(\"MLflow file store selected (%s): %s\", origin, uri)\n",
    "                return uri\n",
    "\n",
    "        return \"file:./mlruns_local\"\n",
    "\n",
    "    async def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Connect to MLflow – fall back to local file store if the configured\n",
    "        tracking URI is unreachable *or* the client is missing critical methods\n",
    "        (e.g. when mlflow-skinny accidentally shadows the full package).\n",
    "        \"\"\"\n",
    "        if self.initialized:\n",
    "            return\n",
    "\n",
    "        # Log critical dependency versions for diagnostics\n",
    "        try:\n",
    "            import pytensor\n",
    "            logger.info(\"📦 PyTensor version: %s\", pytensor.__version__)\n",
    "        except ImportError:\n",
    "            logger.warning(\"⚠️  PyTensor not available\")\n",
    "        except Exception as e:\n",
    "            logger.warning(\"⚠️  Could not determine PyTensor version: %s\", e)\n",
    "\n",
    "        def _needs_fallback(client) -> bool:\n",
    "            # any missing attr is a strong signal we are on mlflow-skinny\n",
    "            return not callable(getattr(client, \"list_experiments\", None))\n",
    "\n",
    "        try:\n",
    "            resolved = self._resolve_tracking_uri()\n",
    "            mlflow.set_tracking_uri(resolved)\n",
    "            logger.info(\"Using tracking URI: %s\", resolved)\n",
    "            self.mlflow_client = MlflowClient(resolved)\n",
    "\n",
    "            if _needs_fallback(self.mlflow_client):\n",
    "                raise AttributeError(\"list_experiments not implemented – skinny build detected\")\n",
    "\n",
    "            # minimal probe (cheap & always present)\n",
    "            self.mlflow_client.search_experiments(max_results=1)\n",
    "            logger.info(\"🟢  Connected to MLflow @ %s\", resolved)\n",
    "\n",
    "        except (MlflowException, socket.gaierror, AttributeError) as exc:\n",
    "            logger.warning(\"🔄  Falling back to local MLflow store – %s\", exc)\n",
    "            mlflow.set_tracking_uri(\"file:./mlruns_local\")\n",
    "            self.mlflow_client = MlflowClient(\"file:./mlruns_local\")\n",
    "            logger.info(\"📂  Using local file store ./mlruns_local\")\n",
    "\n",
    "        await self._load_models()\n",
    "        self.initialized = True\n",
    "\n",
    "    async def _load_models(self) -> None:\n",
    "        \"\"\"Load existing models from MLflow.\"\"\"\n",
    "        for name in [\"iris_random_forest\", \"iris_logreg\",\n",
    "                     \"breast_cancer_bayes\", \"breast_cancer_stub\"]:\n",
    "            try:\n",
    "                await self._try_load(name)\n",
    "            except Exception as exc:\n",
    "                logger.error(\"❌  load %s failed: %s\", name, exc)\n",
    "\n",
    "    async def startup(self, auto_train: bool | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Faster: serve stub immediately; heavy Bayesian job in background.\n",
    "        \"\"\"\n",
    "        if self._unit_test_mode:\n",
    "            logger.info(\"🔒 UNIT_TESTING=1 – skipping model loading\")\n",
    "            return                      # 👉 nothing else runs\n",
    "\n",
    "        # Initialize MLflow connection first\n",
    "        await self.initialize()\n",
    "\n",
    "        if settings.SKIP_BACKGROUND_TRAINING and not settings.AUTO_TRAIN_MISSING:\n",
    "            logger.warning(\"⏩ Both training flags disabled – models must already exist\")\n",
    "            # We still *try* to load existing artefacts so prod works\n",
    "            await self._try_load(\"iris_random_forest\")\n",
    "            await self._try_load(\"iris_logreg\")\n",
    "            await self._try_load(\"breast_cancer_bayes\")\n",
    "            return\n",
    "\n",
    "        auto = auto_train if auto_train is not None else settings.AUTO_TRAIN_MISSING\n",
    "        logger.info(\"🔄 Model-service startup (auto_train=%s)\", auto)\n",
    "\n",
    "        # Registry-aware load for migrated models\n",
    "        self._init_registry_once()\n",
    "\n",
    "        # Try dynamic load first for iris_random_forest\n",
    "        loaded_rf = await self._try_load(\"iris_random_forest\")\n",
    "        if not loaded_rf and auto:\n",
    "            # prefer registry path\n",
    "            run_id = await self.train_via_registry(\"iris_random_forest\")\n",
    "            if run_id:\n",
    "                await self._try_load(\"iris_random_forest\")\n",
    "\n",
    "        # Legacy deterministic model (to be migrated later)\n",
    "        if not await self._try_load(\"iris_logreg\") and auto:\n",
    "            logger.info(\"Training iris logistic-regression (legacy path)…\")\n",
    "            await asyncio.get_running_loop().run_in_executor(\n",
    "                self._EXECUTOR, train_iris_logreg\n",
    "            )\n",
    "            await self._try_load(\"iris_logreg\")\n",
    "\n",
    "        # Bayesian path unchanged (will migrate later)\n",
    "        if not await self._try_load(\"breast_cancer_bayes\"):\n",
    "            if not await self._try_load(\"breast_cancer_stub\") and auto:\n",
    "                logger.info(\"Training stub cancer model …\")\n",
    "                await asyncio.get_running_loop().run_in_executor(\n",
    "                    self._EXECUTOR, train_breast_cancer_stub\n",
    "                )\n",
    "                await self._try_load(\"breast_cancer_stub\")\n",
    "            if auto and not settings.SKIP_BACKGROUND_TRAINING:\n",
    "                logger.info(\"Scheduling Bayesian retrain in background\")\n",
    "                asyncio.create_task(\n",
    "                    self._train_and_reload(\"breast_cancer_bayes\", train_breast_cancer_bayes)\n",
    "                )\n",
    "\n",
    "    async def _try_load(self, name: str) -> bool:\n",
    "        \"\"\"Try to load a model and update status.\"\"\"\n",
    "        try:\n",
    "            model = await self._load_production_model(name)\n",
    "            if model:\n",
    "                self.models[name] = model\n",
    "                self.status[name] = \"loaded\"\n",
    "                logger.info(\"✅ %s loaded\", name)\n",
    "                return True\n",
    "            self.status.setdefault(name, \"missing\")\n",
    "            return False\n",
    "        except Exception as exc:\n",
    "            logger.error(\"❌  load %s failed: %s\", name, exc)\n",
    "            self.status[name] = \"failed\"\n",
    "            self.status[f\"{name}_last_error\"] = str(exc)\n",
    "            return False\n",
    "\n",
    "    async def _train_and_reload(self, name: str, trainer) -> None:\n",
    "        \"\"\"Train a model in background and reload it, with verbose phase logs.\"\"\"\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            logger.info(\"🏗️  BEGIN training %s\", name)\n",
    "            self.status[name] = \"training\"\n",
    "\n",
    "            loop = asyncio.get_running_loop()\n",
    "            await loop.run_in_executor(self._EXECUTOR, trainer)\n",
    "\n",
    "            logger.info(\"📦 Training %s complete in %.1fs – re-loading\", name,\n",
    "                        time.perf_counter() - t0)\n",
    "            model = await self._load_production_model(name)\n",
    "            if not model:\n",
    "                raise RuntimeError(f\"{name} trained but could not be re-loaded\")\n",
    "\n",
    "            self.models[name] = model\n",
    "            self.status[name] = \"loaded\"\n",
    "\n",
    "            # Trigger retention clean‑up in background\n",
    "            loop = asyncio.get_running_loop()\n",
    "            loop.run_in_executor(self._EXECUTOR,\n",
    "                                 lambda: asyncio.run(self._cleanup_runs(name)))\n",
    "            logger.info(\"✅ %s trained & loaded\", name)\n",
    "\n",
    "        except Exception as exc:\n",
    "            self.status[name] = \"failed\"\n",
    "            logger.error(\"❌ %s failed: %s\", name, exc, exc_info=True)  # ← keeps trace\n",
    "            # NEW: persist last_error for UI / debug endpoint\n",
    "            self.status[f\"{name}_last_error\"] = str(exc)\n",
    "\n",
    "# --- DROP-IN REPLACEMENT ----------------------------------------------------\n",
    "    async def _load_production_model(self, name: str):\n",
    "        \"\"\"\n",
    "        Load the canonical production model with alias support *and* perform an\n",
    "        **environment audit** of the recorded vs. runtime dependencies.\n",
    "\n",
    "        We DO NOT install anything automatically.  Instead we:\n",
    "            • attempt to load in the usual fallback order (@prod → @staging → Production stage → latest run)\n",
    "            • after a *successful* load, call `_audit_model_env(uri, name)` to diff\n",
    "              the model's logged environment spec against the current runtime\n",
    "              (importlib.metadata) and record mismatches in `self.status`.\n",
    "\n",
    "        The audit is *diagnostic* unless an optional enforcement policy is enabled\n",
    "        via env/config (MODEL_ENV_ENFORCEMENT = warn|fail|retrain).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Loaded MLflow model instance *or* None if nothing could be loaded, or\n",
    "        load was refused under \"fail\" policy for env mismatch.\n",
    "        \"\"\"\n",
    "        import sys\n",
    "        import mlflow\n",
    "        from mlflow.tracking.artifact_utils import _download_artifact_from_uri\n",
    "        from packaging.version import Version, InvalidVersion\n",
    "        import importlib.metadata as im\n",
    "        import json\n",
    "        import os\n",
    "\n",
    "        # Use MLOps configuration for enforcement policy\n",
    "        policy = settings.MODEL_AUDIT_ENFORCEMENT.lower()\n",
    "\n",
    "        def _warn_model_env(uri: str) -> None:\n",
    "            # unchanged best‑effort header check (Python version)\n",
    "            try:\n",
    "                local_dir = _download_artifact_from_uri(uri)\n",
    "                mlmodel_path = Path(local_dir) / \"MLmodel\"\n",
    "                if not mlmodel_path.is_file():\n",
    "                    return\n",
    "                import yaml\n",
    "                meta = yaml.safe_load(mlmodel_path.read_text())\n",
    "                py_model_ver = (\n",
    "                    meta.get(\"python_env\", {}).get(\"python\")\n",
    "                    or meta.get(\"flavors\", {})\n",
    "                        .get(\"python_function\", {})\n",
    "                        .get(\"loader_module_python_version\")\n",
    "                )\n",
    "                runtime_py = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "                if py_model_ver and not py_model_ver.startswith(runtime_py):\n",
    "                    logger.warning(\n",
    "                        \"⚠️ %s logged under Python %s but runtime is %s; \"\n",
    "                        \"deserialization may fail. Consider retraining.\",\n",
    "                        name, py_model_ver, runtime_py\n",
    "                    )\n",
    "            except Exception as e:  # best-effort\n",
    "                logger.debug(\"env check failed for %s (%s)\", uri, e)\n",
    "\n",
    "        def _audit_model_env(uri: str, model_name: str) -> dict:\n",
    "            \"\"\"\n",
    "            Return a dict: {pkg: {'required': spec, 'current': ver, 'match': bool, 'severity': str}}\n",
    "            Only logs the pip‑install command if there are mismatches.\n",
    "            \"\"\"\n",
    "            import logging\n",
    "            from pathlib import Path\n",
    "            from packaging.version import Version, InvalidVersion\n",
    "            import importlib.metadata as im\n",
    "            from mlflow.pyfunc import get_model_dependencies\n",
    "\n",
    "            audit: dict[str, dict] = {}\n",
    "\n",
    "            # 1️⃣ Suppress MLflow's own INFO log for pip install\n",
    "            pyfunc_logger = logging.getLogger(\"mlflow.pyfunc\")\n",
    "            old_level = pyfunc_logger.level\n",
    "            pyfunc_logger.setLevel(logging.WARNING)\n",
    "            try:\n",
    "                try:\n",
    "                    deps_path = get_model_dependencies(uri)\n",
    "                except Exception:\n",
    "                    deps_path = None\n",
    "            finally:\n",
    "                pyfunc_logger.setLevel(old_level)\n",
    "\n",
    "            # 2️⃣ Read the pip requirements.txt\n",
    "            req_lines: list[str] = []\n",
    "            if deps_path and Path(deps_path).is_file():\n",
    "                for ln in Path(deps_path).read_text().splitlines():\n",
    "                    ln = ln.strip()\n",
    "                    if not ln or ln.startswith(\"#\"):\n",
    "                        continue\n",
    "                    req_lines.append(ln)\n",
    "\n",
    "            # 3️⃣ Build the audit by comparing to runtime versions\n",
    "            for spec in req_lines:\n",
    "                pkg = spec.split(\"@\", 1)[0].split(\";\", 1)[0].strip()\n",
    "                pkg_lc = pkg.lower().replace(\"_\", \"-\")\n",
    "\n",
    "                req_ver = None\n",
    "                if \"==\" in spec:\n",
    "                    req_ver = spec.split(\"==\", 1)[1].strip()\n",
    "                elif \">=\" in spec:\n",
    "                    req_ver = spec.split(\">=\", 1)[1].strip()\n",
    "\n",
    "                cur_ver = None\n",
    "                try:\n",
    "                    cur_ver = im.version(pkg_lc)\n",
    "                except Exception:\n",
    "                    cur_ver = None\n",
    "\n",
    "                match = True\n",
    "                sev = \"OK\"\n",
    "                if req_ver:\n",
    "                    try:\n",
    "                        v_req = Version(req_ver)\n",
    "                        if cur_ver:\n",
    "                            v_cur = Version(cur_ver)\n",
    "                            if v_cur.major != v_req.major:\n",
    "                                sev, match = \"MAJOR_DRT\", False\n",
    "                            elif v_cur != v_req:\n",
    "                                sev, match = \"MINOR_DRT\", False\n",
    "                        else:\n",
    "                            sev, match = \"MISSING\", False\n",
    "                    except InvalidVersion:\n",
    "                        pass\n",
    "                elif cur_ver is None:\n",
    "                    sev, match = \"MISSING\", False\n",
    "\n",
    "                audit[pkg_lc] = {\n",
    "                    \"required\": req_ver,\n",
    "                    \"current\": cur_ver,\n",
    "                    \"match\": match,\n",
    "                    \"severity\": sev,\n",
    "                }\n",
    "\n",
    "            # 4️⃣ Record audit in service status\n",
    "            self.status[f\"{model_name}_dep_audit\"] = audit\n",
    "\n",
    "            # 5️⃣ Only show pip‑install hint if there *are* mismatches\n",
    "            if deps_path and any(not rec[\"match\"] for rec in audit.values()):\n",
    "                pyfunc_logger.info(\n",
    "                    \"To install the dependencies that were used to train the model, \"\n",
    "                    \"run the following command: 'pip install -r %s'\",\n",
    "                    deps_path,\n",
    "                )\n",
    "\n",
    "            # 6️⃣ MLOps policy enforcement based on environment\n",
    "            if policy in (\"fail\", \"retrain\"):\n",
    "                critical = (\"numpy\", \"scipy\", \"scikit-learn\", \"psutil\")\n",
    "                majors = [\n",
    "                    pkg\n",
    "                    for pkg, rec in audit.items()\n",
    "                    if pkg in critical and rec[\"severity\"] == \"MAJOR_DRT\"\n",
    "                ]\n",
    "                if majors:\n",
    "                    msg = f\"Critical env drift for {model_name}: {majors}\"\n",
    "                    logger.error(msg)\n",
    "                    if policy == \"fail\":\n",
    "                        self.status[f\"{model_name}_last_error\"] = msg\n",
    "                        return {\"_REFUSE_LOAD\": True}\n",
    "                    elif policy == \"retrain\":\n",
    "                        logger.warning(\n",
    "                            \"Scheduling background retrain for %s due to env drift\",\n",
    "                            model_name,\n",
    "                        )\n",
    "                        asyncio.create_task(\n",
    "                            self._train_and_reload(model_name, TRAINERS[model_name])\n",
    "                        )\n",
    "\n",
    "            return audit\n",
    "\n",
    "        client = self.mlflow_client\n",
    "\n",
    "        # MLOps-aware loading order based on environment\n",
    "        env_canon = settings.ENVIRONMENT_CANONICAL\n",
    "        if env_canon == \"production\":\n",
    "            # Production: strict order - only Production stage or @prod alias\n",
    "            attempts = [\n",
    "                (\"@prod\", f\"models:/{name}@prod\"),\n",
    "                (\"Production stage\", None),  # handle below\n",
    "            ]\n",
    "        elif env_canon == \"staging\":\n",
    "            # Staging: allow staging versions for testing\n",
    "            attempts = [\n",
    "                (\"@staging\", f\"models:/{name}@staging\"),\n",
    "                (\"@prod\", f\"models:/{name}@prod\"),\n",
    "                (\"Production stage\", None),  # handle below\n",
    "            ]\n",
    "        else:\n",
    "            # Development: most permissive\n",
    "            attempts = [\n",
    "                (\"@prod\", f\"models:/{name}@prod\"),\n",
    "                (\"@staging\", f\"models:/{name}@staging\"),\n",
    "                (\"Production stage\", None),  # handle below\n",
    "                (\"latest run\", None),        # handle below\n",
    "            ]\n",
    "\n",
    "        # 1️⃣ Try aliases first ------------------------------------------------------\n",
    "        for alias_name, uri in attempts[:2]:  # Only try aliases\n",
    "            if uri is None:\n",
    "                continue\n",
    "            try:\n",
    "                _warn_model_env(uri)\n",
    "                logger.info(\"↪︎  Loading %s from alias %s\", name, alias_name)\n",
    "                mdl = mlflow.pyfunc.load_model(uri)\n",
    "                audit = _audit_model_env(uri, name)\n",
    "                if audit.get(\"_REFUSE_LOAD\"):\n",
    "                    logger.warning(\"Refusing %s from %s under policy; continuing fallbacks\", name, alias_name)\n",
    "                else:\n",
    "                    # --- config hash drift check -------------------------------------------------\n",
    "                    try:\n",
    "                        # Candidate may be run-based or version-based; we try to extract run_id param from MLflow model flavor metadata.\n",
    "                        # Fallback: skip silently.\n",
    "                        from mlflow import get_tracking_uri\n",
    "                        tracking_client = self.mlflow_client\n",
    "                        # try to read run params if we have run context\n",
    "                        # NOTE: _download_artifact_from_uri gave us `uri`; if it's a runs:/ URI we can parse run_id\n",
    "                        if uri.startswith(\"runs:/\"):\n",
    "                            run_id = uri.split(\"/\", 2)[1]\n",
    "                            run = tracking_client.get_run(run_id)\n",
    "                            train_hash = run.data.params.get(\"train_config_hash\")\n",
    "                            if train_hash:\n",
    "                                from app.core.config import settings as _s\n",
    "                                cur_hash = hashlib.sha256(json.dumps(_s.model_dump(), sort_keys=True).encode()).hexdigest()\n",
    "                                if train_hash != cur_hash:\n",
    "                                    logger.warning(\n",
    "                                        \"⚠️ Config drift for %s: train_hash=%s current=%s\",\n",
    "                                        model_name, train_hash[:8], cur_hash[:8]\n",
    "                                    )\n",
    "                    except Exception as _e_hash:  # best-effort\n",
    "                        logger.debug(\"Config drift check skipped: %s\", _e_hash)\n",
    "                    return mdl\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Alias %s not available for %s: %s\", alias_name, name, e)\n",
    "\n",
    "        # 2️⃣ Try Production stage ---------------------------------------------------\n",
    "        try:\n",
    "            versions = client.search_model_versions(f\"name='{name}' AND stage='Production'\")\n",
    "            if versions:\n",
    "                version = versions[0].version\n",
    "                uri = f\"models:/{name}/{version}\"\n",
    "                _warn_model_env(uri)\n",
    "                logger.info(\"↪︎  Loading %s from registry: Production v%s\", name, version)\n",
    "                mdl = mlflow.pyfunc.load_model(uri)\n",
    "                audit = _audit_model_env(uri, name)\n",
    "                if audit.get(\"_REFUSE_LOAD\"):\n",
    "                    logger.warning(\"Refusing %s Production v%s under policy; continuing fallbacks\", name, version)\n",
    "                else:\n",
    "                    return mdl\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Production stage not available for %s: %s\", name, e)\n",
    "\n",
    "        # 3️⃣ Try Staging stage (only in dev/staging) ------------------------------\n",
    "        if settings.ENVIRONMENT != \"production\":\n",
    "            try:\n",
    "                versions = client.search_model_versions(f\"name='{name}' AND stage='Staging'\")\n",
    "                if versions:\n",
    "                    version = versions[0].version\n",
    "                    uri = f\"models:/{name}/{version}\"\n",
    "                    _warn_model_env(uri)\n",
    "                    logger.info(\"↪︎  Loading %s from registry: Staging v%s\", name, version)\n",
    "                    mdl = mlflow.pyfunc.load_model(uri)\n",
    "                    audit = _audit_model_env(uri, name)\n",
    "                    if audit.get(\"_REFUSE_LOAD\"):\n",
    "                        logger.warning(\"Refusing %s Staging v%s under policy; continuing fallbacks\", name, version)\n",
    "                    else:\n",
    "                        return mdl\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Staging stage not available for %s: %s\", name, e)\n",
    "\n",
    "        # 4️⃣  (Possible) Fallback to latest run – now allowed in prod too\n",
    "        allow_run_fallback = (\n",
    "            settings.ENVIRONMENT_CANONICAL != \"production\"\n",
    "            or settings.ALLOW_PROD_RUN_FALLBACK\n",
    "        )\n",
    "        if allow_run_fallback:\n",
    "            try:\n",
    "                runs = []\n",
    "                for exp in client.search_experiments():\n",
    "                    runs.extend(client.search_runs(\n",
    "                        [exp.experiment_id],\n",
    "                        f\"tags.mlflow.runName = '{name}'\",\n",
    "                        order_by=[\"attributes.start_time DESC\"],\n",
    "                        max_results=1))\n",
    "                if runs:\n",
    "                    uri = f\"runs:/{runs[0].info.run_id}/model\"\n",
    "                    logger.warning(\n",
    "                        \"⚠️  %s: alias/stage missing – loading *latest run* (%s) \"\n",
    "                        \"because ALLOW_PROD_RUN_FALLBACK=%d\",\n",
    "                        name, runs[0].info.run_id, allow_run_fallback,\n",
    "                    )\n",
    "                    _warn_model_env(uri)\n",
    "                    mdl = mlflow.pyfunc.load_model(uri)\n",
    "                    audit = _audit_model_env(uri, name)\n",
    "                    if audit.get(\"_REFUSE_LOAD\"):\n",
    "                        logger.warning(\"Refusing %s latest run under policy\", name)\n",
    "                    else:\n",
    "                        return mdl\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Latest‑run fallback failed for %s: %s\", name, e)\n",
    "\n",
    "        logger.error(\"❌ No suitable model found for %s after all fallbacks\", name)\n",
    "        return None\n",
    "# --- END DROP-IN REPLACEMENT -------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "    async def evaluate_model_quality(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        candidate_run_id: str,\n",
    "        test_data_path: Optional[str] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate a candidate model against production baseline.\n",
    "\n",
    "        This implements quality gates for MLOps:\n",
    "        1. Load production model (if exists)\n",
    "        2. Load candidate model from run_id\n",
    "        3. Evaluate both on test set\n",
    "        4. Return comparison metrics\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to evaluate\n",
    "            candidate_run_id: MLflow run ID of candidate model\n",
    "            test_data_path: Optional path to test data (uses built-in if None)\n",
    "\n",
    "        Returns:\n",
    "            Dict with evaluation results and promotion decision\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "        import pandas as pd\n",
    "\n",
    "        logger.info(\"🔍 Evaluating quality gate for %s (candidate: %s)\", model_name, candidate_run_id)\n",
    "\n",
    "        # Load test data\n",
    "        if model_name.startswith(\"iris\"):\n",
    "            from sklearn.datasets import load_iris\n",
    "            from sklearn.model_selection import train_test_split\n",
    "\n",
    "            iris = load_iris(as_frame=True)\n",
    "            X, y = iris.data, iris.target\n",
    "            _, X_test, _, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "        elif model_name.startswith(\"breast_cancer\"):\n",
    "            from sklearn.datasets import load_breast_cancer\n",
    "            from sklearn.model_selection import train_test_split\n",
    "\n",
    "            X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "            _, X_test, _, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "            X_test = _rename_cancer_columns(X_test)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "\n",
    "        # Load candidate model\n",
    "        try:\n",
    "            candidate_uri = f\"runs:/{candidate_run_id}/model\"\n",
    "            candidate_model = mlflow.pyfunc.load_model(candidate_uri)\n",
    "            logger.info(\"✅ Loaded candidate model from %s\", candidate_run_id)\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Failed to load candidate model: %s\", e)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Failed to load candidate model: {e}\",\n",
    "                \"candidate_metrics\": None,\n",
    "                \"production_metrics\": None\n",
    "            }\n",
    "\n",
    "        # Evaluate candidate\n",
    "        try:\n",
    "            if model_name.startswith(\"iris\"):\n",
    "                y_pred = candidate_model.predict(X_test)\n",
    "                if len(y_pred.shape) == 2:  # probabilities\n",
    "                    y_pred = y_pred.argmax(axis=1)\n",
    "            else:  # cancer model\n",
    "                y_pred_proba = candidate_model.predict(X_test)\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "            candidate_metrics = {\n",
    "                \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "                \"f1_macro\": f1_score(y_test, y_pred, average=\"macro\"),\n",
    "                \"precision_macro\": precision_score(y_test, y_pred, average=\"macro\"),\n",
    "                \"recall_macro\": recall_score(y_test, y_pred, average=\"macro\")\n",
    "            }\n",
    "            logger.info(\"📊 Candidate metrics: %s\", candidate_metrics)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Failed to evaluate candidate: %s\", e)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Failed to evaluate candidate: {e}\",\n",
    "                \"candidate_metrics\": None,\n",
    "                \"production_metrics\": None\n",
    "            }\n",
    "\n",
    "        # Try to load production model for comparison\n",
    "        production_metrics = None\n",
    "        try:\n",
    "            prod_model = await self._load_production_model(model_name)\n",
    "            if prod_model:\n",
    "                if model_name.startswith(\"iris\"):\n",
    "                    y_pred_prod = prod_model.predict(X_test)\n",
    "                    if len(y_pred_prod.shape) == 2:  # probabilities\n",
    "                        y_pred_prod = y_pred_prod.argmax(axis=1)\n",
    "                else:  # cancer model\n",
    "                    y_pred_proba_prod = prod_model.predict(X_test)\n",
    "                    y_pred_prod = (y_pred_proba_prod > 0.5).astype(int)\n",
    "\n",
    "                production_metrics = {\n",
    "                    \"accuracy\": accuracy_score(y_test, y_pred_prod),\n",
    "                    \"f1_macro\": f1_score(y_test, y_pred_prod, average=\"macro\"),\n",
    "                    \"precision_macro\": precision_score(y_test, y_pred_prod, average=\"macro\"),\n",
    "                    \"recall_macro\": recall_score(y_test, y_pred_prod, average=\"macro\")\n",
    "                }\n",
    "                logger.info(\"📊 Production metrics: %s\", production_metrics)\n",
    "            else:\n",
    "                logger.info(\"📊 No production model found for comparison\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(\"⚠️  Could not evaluate production model: %s\", e)\n",
    "\n",
    "                # Quality gate decision\n",
    "        promoted = False\n",
    "        reason = \"\"\n",
    "\n",
    "        if production_metrics:\n",
    "            # Compare against production baseline\n",
    "            acc_improvement = candidate_metrics[\"accuracy\"] - production_metrics[\"accuracy\"]\n",
    "            f1_improvement = candidate_metrics[\"f1_macro\"] - production_metrics[\"f1_macro\"]\n",
    "\n",
    "            # Quality gate: must maintain or improve performance\n",
    "            if acc_improvement >= -0.01 and f1_improvement >= -0.01:  # Allow 1% degradation\n",
    "                promoted = True\n",
    "                reason = f\"Performance maintained (acc: {acc_improvement:+.3f}, f1: {f1_improvement:+.3f})\"\n",
    "            else:\n",
    "                reason = f\"Performance degraded (acc: {acc_improvement:+.3f}, f1: {f1_improvement:+.3f})\"\n",
    "        else:\n",
    "            # No production baseline - use absolute thresholds from settings\n",
    "            if candidate_metrics[\"accuracy\"] >= settings.QUALITY_GATE_ACCURACY_THRESHOLD \\\n",
    "               and candidate_metrics[\"f1_macro\"] >= settings.QUALITY_GATE_F1_THRESHOLD:\n",
    "                promoted = True\n",
    "                reason = f\"Meets minimum thresholds (acc: {candidate_metrics['accuracy']:.3f} >= {settings.QUALITY_GATE_ACCURACY_THRESHOLD}, f1: {candidate_metrics['f1_macro']:.3f} >= {settings.QUALITY_GATE_F1_THRESHOLD})\"\n",
    "            else:\n",
    "                reason = f\"Below minimum thresholds (acc: {candidate_metrics['accuracy']:.3f} < {settings.QUALITY_GATE_ACCURACY_THRESHOLD} or f1: {candidate_metrics['f1_macro']:.3f} < {settings.QUALITY_GATE_F1_THRESHOLD})\"\n",
    "\n",
    "        result = {\n",
    "            \"promoted\": promoted,\n",
    "            \"reason\": reason,\n",
    "            \"candidate_metrics\": candidate_metrics,\n",
    "            \"production_metrics\": production_metrics,\n",
    "            \"candidate_run_id\": candidate_run_id,\n",
    "            \"model_name\": model_name\n",
    "        }\n",
    "\n",
    "        # log evaluation metadata back to MLflow\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "            client.set_tag(candidate_run_id, f\"quality_gate:{settings.ENVIRONMENT_CANONICAL}\",\n",
    "                           \"PASSED\" if promoted else \"FAILED\")\n",
    "            client.set_tag(candidate_run_id, \"quality_gate_reason\", reason)\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Failed to set MLflow quality gate tags: %s\", e)\n",
    "\n",
    "        logger.info(\"🎯 Quality gate result: %s - %s\", \"PASSED\" if promoted else \"FAILED\", reason)\n",
    "        return result\n",
    "\n",
    "    async def promote_model_to_staging(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        run_id: str\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Promote a model to staging after quality gate passes.\n",
    "\n",
    "        This is the core MLOps promotion logic:\n",
    "        1. Evaluate model quality\n",
    "        2. If passed, register as staging version\n",
    "        3. Set @staging alias\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to promote\n",
    "            run_id: MLflow run ID of the candidate model\n",
    "\n",
    "        Returns:\n",
    "            Dict with promotion result\n",
    "        \"\"\"\n",
    "        logger.info(\"🚀 Starting promotion process for %s (run: %s)\", model_name, run_id)\n",
    "\n",
    "        # Evaluate quality gate\n",
    "        eval_result = await self.evaluate_model_quality(model_name, run_id)\n",
    "\n",
    "        if not eval_result[\"promoted\"]:\n",
    "            logger.warning(\"❌ Quality gate failed for %s: %s\", model_name, eval_result.get(\"reason\", \"Unknown\"))\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": eval_result.get(\"error\", eval_result.get(\"reason\", \"Quality gate failed\")),\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "\n",
    "        # Promote to staging\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "            candidate_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "            # Create new model version\n",
    "            mv = client.create_model_version(\n",
    "                name=model_name,\n",
    "                source=candidate_uri,\n",
    "                run_id=run_id\n",
    "            )\n",
    "\n",
    "            # Transition to Staging\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=mv.version,\n",
    "                stage=\"Staging\"\n",
    "            )\n",
    "\n",
    "            # Set @staging alias\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"staging\",\n",
    "                version=mv.version\n",
    "            )\n",
    "\n",
    "            logger.info(\"✅ Successfully promoted %s to staging (version %s)\", model_name, mv.version)\n",
    "\n",
    "            return {\n",
    "                \"promoted\": True,\n",
    "                \"version\": mv.version,\n",
    "                \"stage\": \"Staging\",\n",
    "                \"alias\": \"staging\",\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(\"❌ Failed to promote %s to staging: %s\", model_name, error_msg)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Promotion failed: {error_msg}\",\n",
    "                \"evaluation\": eval_result\n",
    "            }\n",
    "\n",
    "    async def promote_model_to_production(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        version: Optional[int] = None,\n",
    "        approved_by: Optional[str] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Promote a staging model to production.\n",
    "\n",
    "        This can be called manually or automatically:\n",
    "        1. If version specified, promote that specific version\n",
    "        2. Otherwise, promote the current @staging alias\n",
    "        3. Set @prod alias for atomic promotion\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to promote\n",
    "            version: Specific version to promote (optional)\n",
    "\n",
    "        Returns:\n",
    "            Dict with promotion result\n",
    "        \"\"\"\n",
    "        logger.info(\"🚀 Promoting %s to production (version: %s)\", model_name, version or \"staging\")\n",
    "\n",
    "        # Enforce human approval in production if required\n",
    "        if settings.REQUIRE_MODEL_APPROVAL and settings.ENVIRONMENT_CANONICAL == \"production\":\n",
    "            if not approved_by:\n",
    "                return {\n",
    "                    \"promoted\": False,\n",
    "                    \"error\": \"Approval required: pass approved_by=<user> when promoting to production.\",\n",
    "                }\n",
    "\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "\n",
    "            if version is None:\n",
    "                # Get the current staging version\n",
    "                staging_versions = client.search_model_versions(\n",
    "                    f\"name='{model_name}' AND stage='Staging'\"\n",
    "                )\n",
    "                if not staging_versions:\n",
    "                    return {\n",
    "                        \"promoted\": False,\n",
    "                        \"error\": f\"No staging version found for {model_name}\"\n",
    "                    }\n",
    "                version = staging_versions[0].version\n",
    "\n",
    "            # Transition to Production\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=version,\n",
    "                stage=\"Production\"\n",
    "            )\n",
    "\n",
    "            # Set @prod alias for atomic promotion\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"prod\",\n",
    "                version=version\n",
    "            )\n",
    "\n",
    "            # record approval metadata as tags\n",
    "            try:\n",
    "                client.set_model_version_tag(model_name, version, \"approved_by\", str(approved_by or \"n/a\"))\n",
    "                client.set_model_version_tag(model_name, version, \"approved_env\", str(settings.ENVIRONMENT_CANONICAL))\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Could not tag model version approval: %s\", e)\n",
    "\n",
    "            logger.info(\"✅ Successfully promoted %s to production (version %s)\", model_name, version)\n",
    "\n",
    "            return {\n",
    "                \"promoted\": True,\n",
    "                \"version\": version,\n",
    "                \"stage\": \"Production\",\n",
    "                \"alias\": \"prod\"\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(\"❌ Failed to promote %s to production: %s\", model_name, error_msg)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Production promotion failed: {error_msg}\"\n",
    "            }\n",
    "\n",
    "    async def promote_model_to_stage(\n",
    "        self, \n",
    "        model_name: str,\n",
    "        target_stage: str,\n",
    "        version: Optional[int] = None,\n",
    "        approved_by: Optional[str] = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Promote a model to a specific stage (staging or production).\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the model to promote\n",
    "            target_stage: Target stage ('Staging' or 'Production')\n",
    "            version: Specific version to promote (optional)\n",
    "            approved_by: User who approved the promotion (optional)\n",
    "\n",
    "        Returns:\n",
    "            Dict with promotion result\n",
    "        \"\"\"\n",
    "        logger.info(\"🚀 Promoting %s to %s (version: %s)\", model_name, target_stage, version or \"latest\")\n",
    "\n",
    "        # Validate target stage\n",
    "        if target_stage not in [\"Staging\", \"Production\"]:\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"Invalid target stage: {target_stage}. Must be 'Staging' or 'Production'\"\n",
    "            }\n",
    "\n",
    "        # Enforce human approval in production if required\n",
    "        if target_stage == \"Production\" and settings.REQUIRE_MODEL_APPROVAL and settings.ENVIRONMENT_CANONICAL == \"production\":\n",
    "            if not approved_by:\n",
    "                return {\n",
    "                    \"promoted\": False,\n",
    "                    \"error\": \"Approval required: pass approved_by=<user> when promoting to production.\",\n",
    "                }\n",
    "\n",
    "        try:\n",
    "            client = self.mlflow_client\n",
    "\n",
    "            if version is None:\n",
    "                # Get the latest version\n",
    "                versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "                if not versions:\n",
    "                    return {\n",
    "                        \"promoted\": False,\n",
    "                        \"error\": f\"No versions found for {model_name}\"\n",
    "                    }\n",
    "                version = versions[0].version\n",
    "\n",
    "            # Set appropriate alias (modern approach - skip deprecated stage transitions)\n",
    "            alias = \"prod\" if target_stage == \"Production\" else \"staging\"\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=alias,\n",
    "                version=version\n",
    "            )\n",
    "\n",
    "            # record approval metadata as tags\n",
    "            try:\n",
    "                client.set_model_version_tag(model_name, version, \"approved_by\", approved_by or \"n/a\")\n",
    "                client.set_model_version_tag(model_name, version, \"approved_env\", settings.ENVIRONMENT_CANONICAL)\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Could not tag model version approval: %s\", e)\n",
    "\n",
    "            logger.info(\"✅ Successfully promoted %s to %s (version %s)\", model_name, target_stage, version)\n",
    "\n",
    "            return {\n",
    "                \"promoted\": True,\n",
    "                \"version\": version,\n",
    "                \"stage\": target_stage,\n",
    "                \"alias\": alias\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            logger.error(\"❌ Failed to promote %s to %s: %s\", model_name, target_stage, error_msg)\n",
    "            return {\n",
    "                \"promoted\": False,\n",
    "                \"error\": f\"{target_stage} promotion failed: {error_msg}\"\n",
    "            }\n",
    "\n",
    "    async def get_model_metrics(self, model_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve metrics for all versions of a registered model.\n",
    "\n",
    "        This enables MLOps comparison between different model versions\n",
    "        for quality gate decisions and promotion workflows.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the registered model\n",
    "\n",
    "        Returns:\n",
    "            List of dicts with version info and metrics for each model version\n",
    "        \"\"\"\n",
    "        client = self.mlflow_client\n",
    "        versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "        results = []\n",
    "\n",
    "        for v in versions:\n",
    "            run_id = v.run_id\n",
    "            try:\n",
    "                run = client.get_run(run_id)\n",
    "                # Convert MLflow Metric objects to plain Python values\n",
    "                metrics = {}\n",
    "                for key, metric in run.data.metrics.items():\n",
    "                    metrics[key] = float(metric.value) if hasattr(metric, 'value') else float(metric)\n",
    "                # Add metadata for better MLOps context\n",
    "                tags = run.data.tags\n",
    "                creation_timestamp = v.creation_timestamp\n",
    "                last_updated_timestamp = v.last_updated_timestamp\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not fetch metrics for {model_name} v{v.version}: {e}\")\n",
    "                metrics = {}\n",
    "                tags = {}\n",
    "                creation_timestamp = None\n",
    "                last_updated_timestamp = None\n",
    "\n",
    "            results.append({\n",
    "                \"version\": int(v.version),\n",
    "                \"stage\": v.current_stage,\n",
    "                \"run_id\": run_id,\n",
    "                \"metrics\": metrics,\n",
    "                \"tags\": tags,\n",
    "                \"creation_timestamp\": creation_timestamp,\n",
    "                \"last_updated_timestamp\": last_updated_timestamp,\n",
    "                \"description\": v.description or \"\"\n",
    "            })\n",
    "\n",
    "        # Sort by version number for consistent ordering\n",
    "        results.sort(key=lambda x: x[\"version\"])\n",
    "        return results\n",
    "\n",
    "    async def compare_model_versions(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        version_a: int, \n",
    "        version_b: int\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compare two specific model versions for MLOps decision making.\n",
    "\n",
    "        Args:\n",
    "            model_name: Name of the registered model\n",
    "            version_a: First version to compare\n",
    "            version_b: Second version to compare\n",
    "\n",
    "        Returns:\n",
    "            Dict with comparison results and recommendation\n",
    "        \"\"\"\n",
    "        client = self.mlflow_client\n",
    "\n",
    "        # Get both versions\n",
    "        try:\n",
    "            version_a_info = client.get_model_version(model_name, version_a)\n",
    "            version_b_info = client.get_model_version(model_name, version_b)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Could not fetch model versions: {e}\",\n",
    "                \"comparison\": None\n",
    "            }\n",
    "\n",
    "        # Get metrics for both versions\n",
    "        metrics_a = await self._get_version_metrics(version_a_info.run_id)\n",
    "        metrics_b = await self._get_version_metrics(version_b_info.run_id)\n",
    "\n",
    "        # Compare key metrics\n",
    "        comparison = {}\n",
    "        for metric in [\"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "            if metric in metrics_a and metric in metrics_b:\n",
    "                val_a = metrics_a[metric]\n",
    "                val_b = metrics_b[metric]\n",
    "                diff = val_b - val_a\n",
    "                comparison[metric] = {\n",
    "                    \"version_a\": val_a,\n",
    "                    \"version_b\": val_b,\n",
    "                    \"difference\": diff,\n",
    "                    \"improvement\": diff > 0\n",
    "                }\n",
    "\n",
    "        # Determine recommendation\n",
    "        improvements = sum(1 for comp in comparison.values() if comp[\"improvement\"])\n",
    "        total_metrics = len(comparison)\n",
    "\n",
    "        if total_metrics == 0:\n",
    "            recommendation = \"insufficient_data\"\n",
    "        elif improvements == total_metrics:\n",
    "            recommendation = \"promote_version_b\"\n",
    "        elif improvements == 0:\n",
    "            recommendation = \"keep_version_a\"\n",
    "        else:\n",
    "            recommendation = \"mixed_results\"\n",
    "\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"version_a\": {\n",
    "                \"version\": version_a,\n",
    "                \"stage\": version_a_info.current_stage,\n",
    "                \"run_id\": version_a_info.run_id,\n",
    "                \"metrics\": metrics_a\n",
    "            },\n",
    "            \"version_b\": {\n",
    "                \"version\": version_b,\n",
    "                \"stage\": version_b_info.current_stage,\n",
    "                \"run_id\": version_b_info.run_id,\n",
    "                \"metrics\": metrics_b\n",
    "            },\n",
    "            \"comparison\": comparison,\n",
    "            \"recommendation\": recommendation,\n",
    "            \"summary\": f\"Version B improves {improvements}/{total_metrics} metrics\"\n",
    "        }\n",
    "\n",
    "    async def _get_version_metrics(self, run_id: str) -> Dict[str, float]:\n",
    "        \"\"\"Helper to get metrics for a specific run.\"\"\"\n",
    "        try:\n",
    "            run = self.mlflow_client.get_run(run_id)\n",
    "            # Convert MLflow Metric objects to plain Python values\n",
    "            metrics = {}\n",
    "            for key, metric in run.data.metrics.items():\n",
    "                metrics[key] = float(metric.value) if hasattr(metric, 'value') else float(metric)\n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not fetch metrics for run {run_id}: {e}\")\n",
    "            return {}\n",
    "\n",
    "    # Manual training endpoints (for UI)\n",
    "    async def train_iris(self, model_type: str = \"rf\") -> None:\n",
    "        \"\"\"\n",
    "        Train either the Random Forest or the Logistic Regression\n",
    "        on the Iris dataset, per the caller's choice.\n",
    "        \"\"\"\n",
    "        if model_type == \"rf\":\n",
    "            name, trainer = \"iris_random_forest\", TRAINERS[\"iris_random_forest\"]\n",
    "        else:  # \"logreg\"\n",
    "            name, trainer = \"iris_logreg\", TRAINERS[\"iris_logreg\"]\n",
    "\n",
    "        # reuse your existing helper\n",
    "        await self._train_and_reload(name, trainer)\n",
    "\n",
    "    async def train_cancer(self, model_type: str = \"bayes\") -> None:\n",
    "        \"\"\"\n",
    "        Train either the Bayesian (PyMC) or stub (LogReg)\n",
    "        on the Breast Cancer dataset, per the caller's choice.\n",
    "        \"\"\"\n",
    "        if model_type == \"bayes\":\n",
    "            name, trainer = \"breast_cancer_bayes\", TRAINERS[\"breast_cancer_bayes\"]\n",
    "        else:  # \"stub\"\n",
    "            name, trainer = \"breast_cancer_stub\", TRAINERS[\"breast_cancer_stub\"]\n",
    "\n",
    "        await self._train_and_reload(name, trainer)\n",
    "\n",
    "    async def train_bayes_cancer_with_params(self, params=None) -> str:\n",
    "        \"\"\"\n",
    "        Train Bayesian cancer model with validated parameters.\n",
    "        Returns the MLflow run ID.\n",
    "        \"\"\"\n",
    "        from app.ml.builtin_trainers import train_breast_cancer_bayes\n",
    "        \n",
    "        # Run training with parameters\n",
    "        run_id = train_breast_cancer_bayes(params_obj=params)\n",
    "        \n",
    "        # Reload the model after training\n",
    "        await self._try_load(\"breast_cancer_bayes\")\n",
    "        \n",
    "        return run_id\n",
    "\n",
    "    # Predict methods (unchanged from your previous version)\n",
    "    async def predict_iris(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"rf\",\n",
    "    ) -> Tuple[List[str], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        Predict Iris species from measurements.\n",
    "\n",
    "        Hardens input normalization & always uses a serial, psutil‑safe path\n",
    "        to compute class probabilities to avoid joblib/loky crashes when\n",
    "        psutil is broken. Also ensures feature names are preserved to silence\n",
    "        scikit‑learn's 'X does not have valid feature names' warning. :contentReference[oaicite:23]{index=23}\n",
    "        \"\"\"\n",
    "        if model_type not in (\"rf\", \"logreg\"):\n",
    "            raise ValueError(\"model_type must be 'rf' or 'logreg'\")\n",
    "\n",
    "        model_name = \"iris_random_forest\" if model_type == \"rf\" else \"iris_logreg\"\n",
    "        model = self.models.get(model_name)\n",
    "        if not model:\n",
    "            raise RuntimeError(f\"{model_name} not loaded\")\n",
    "\n",
    "        # construct DF w/ training column names in correct order\n",
    "        X_df = pd.DataFrame(\n",
    "            [{\n",
    "                \"sepal length (cm)\":  f[\"sepal_length\"],\n",
    "                \"sepal width (cm)\":   f[\"sepal_width\"],\n",
    "                \"petal length (cm)\":  f[\"petal_length\"],\n",
    "                \"petal width (cm)\":   f[\"petal_width\"],\n",
    "            } for f in features]\n",
    "        )\n",
    "        logger.debug(\"predict_iris(%s) columns=%s\", model_name, X_df.columns.tolist())\n",
    "\n",
    "        # ALWAYS unwrap and call safe helper (skip top-level pyfunc)\n",
    "        base = model\n",
    "        try:\n",
    "            py_model = model.unwrap_python_model()  # mlflow ≥2\n",
    "            if hasattr(py_model, \"model\"):\n",
    "                base = py_model.model\n",
    "            else:\n",
    "                base = py_model\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        probs = _safe_sklearn_proba(base, X_df, log_prefix=model_name)\n",
    "\n",
    "        # convert to names\n",
    "        import numpy as _np\n",
    "        probs = _np.asarray(probs, dtype=float)\n",
    "        if probs.ndim == 1:  # defensive\n",
    "            # promote to 3-class; treat as class-0 vs rest\n",
    "            z = _np.zeros((probs.size, 3), dtype=float)\n",
    "            z[:, 0] = probs\n",
    "            z[:, 1:] = (1 - probs) / 2\n",
    "            probs = z\n",
    "        preds = probs.argmax(axis=1)\n",
    "        class_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "        pred_names = [class_names[int(i)] for i in preds]\n",
    "        return pred_names, probs.tolist()\n",
    "\n",
    "\n",
    "    async def predict_cancer(\n",
    "        self,\n",
    "        features: List[Dict[str, float]],\n",
    "        model_type: str = \"bayes\",\n",
    "        posterior_samples: Optional[int] = None,\n",
    "    ) -> Tuple[List[str], List[float], Optional[List[Tuple[float, float]]]]:\n",
    "        \"\"\"\n",
    "        Predict breast cancer diagnosis.\n",
    "\n",
    "        For stub (sklearn) path we unwrap & call psutil‑safe helper to avoid\n",
    "        loky/psutil crashes; for bayes path we call model.predict() directly.\n",
    "        MLflow PythonModel wrappers now expose modern signature. :contentReference[oaicite:24]{index=24}\n",
    "        \"\"\"\n",
    "        if model_type == \"bayes\":\n",
    "            model = self.models.get(\"breast_cancer_bayes\") or self.models.get(\"breast_cancer_stub\")\n",
    "            using_bayes = \"breast_cancer_bayes\" in self.models and model is self.models[\"breast_cancer_bayes\"]\n",
    "        elif model_type == \"stub\":\n",
    "            model = self.models.get(\"breast_cancer_stub\")\n",
    "            using_bayes = False\n",
    "        else:\n",
    "            raise ValueError(\"model_type must be 'bayes' or 'stub'\")\n",
    "        if not model:\n",
    "            raise RuntimeError(\"No cancer model available\")\n",
    "\n",
    "        X_df_raw = pd.DataFrame(features)\n",
    "        X_df = _rename_cancer_columns(X_df_raw)\n",
    "\n",
    "        if using_bayes and hasattr(model, \"predict\"):\n",
    "            probs = model.predict(X_df)\n",
    "        else:\n",
    "            # unwrap & safe path\n",
    "            base = model\n",
    "            try:\n",
    "                py_model = model.unwrap_python_model()\n",
    "                base = getattr(py_model, \"model\", py_model)\n",
    "            except Exception:\n",
    "                pass\n",
    "            probs_full = _safe_sklearn_proba(base, X_df, log_prefix=\"breast_cancer_stub\")\n",
    "            probs = probs_full[:, 1] if probs_full.ndim == 2 else probs_full\n",
    "\n",
    "        labels = [\"malignant\" if p > 0.5 else \"benign\" for p in probs]\n",
    "\n",
    "        ci = None\n",
    "        if posterior_samples and using_bayes:\n",
    "            try:\n",
    "                # Access the underlying python model to get the trace\n",
    "                python_model = model.unwrap_python_model()\n",
    "\n",
    "                # Access posterior samples for uncertainty quantification\n",
    "                draws = python_model.trace.posterior\n",
    "                αg = draws[\"α\"].stack(samples=(\"chain\", \"draw\"))\n",
    "                β = draws[\"β\"].stack(samples=(\"chain\", \"draw\"))\n",
    "\n",
    "                # Get group indices and standardized features\n",
    "                g = python_model._quint(X_df)\n",
    "                Xs = python_model.scaler.transform(X_df)\n",
    "\n",
    "                # Compute posterior predictive samples\n",
    "                logits = αg.values[:, g] + np.dot(β.values.T, Xs.T)      # shape (S, N)\n",
    "                pp = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "                # Compute 95% credible intervals\n",
    "                lo, hi = np.percentile(pp, [2.5, 97.5], axis=0)\n",
    "                ci = list(zip(lo.tolist(), hi.tolist()))\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to compute uncertainty intervals: {e}\")\n",
    "                ci = None\n",
    "\n",
    "        return labels, probs.tolist(), ci\n",
    "\n",
    "    async def _cleanup_runs(self, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Keep the **newest N runs** for `model_name` and drop the rest, then\n",
    "        optionally invoke `mlflow gc` to purge artifact folders.\n",
    "\n",
    "        Runs marked *deleted* are still present on disk until GC executes,\n",
    "        so we always run GC when `settings.MLFLOW_GC_AFTER_TRAIN` is True.\n",
    "        \"\"\"\n",
    "        keep = max(settings.RETAIN_RUNS_PER_MODEL, 0)\n",
    "        try:\n",
    "            # 1️⃣ fetch runs newest→oldest\n",
    "            runs = self.mlflow_client.search_runs(\n",
    "                experiment_ids=[exp.experiment_id for exp in self.mlflow_client.search_experiments()],\n",
    "                filter_string=f\"tags.mlflow.runName = '{model_name}'\",\n",
    "                order_by=[\"attributes.start_time DESC\"],\n",
    "            )\n",
    "            if len(runs) <= keep:\n",
    "                logger.debug(\"No pruning needed for %s (runs=%d, keep=%d)\",\n",
    "                             model_name, len(runs), keep)\n",
    "                return\n",
    "\n",
    "            to_delete = runs[keep:]\n",
    "            for r in to_delete:\n",
    "                self.mlflow_client.delete_run(r.info.run_id)\n",
    "            logger.info(\"🗑️  Pruned %d old %s runs; kept %d\",\n",
    "                        len(to_delete), model_name, keep)\n",
    "\n",
    "            # 2️⃣ garbage‑collect artifacts\n",
    "            if settings.MLFLOW_GC_AFTER_TRAIN:\n",
    "                uri = mlflow.get_tracking_uri().removeprefix(\"file:\")\n",
    "                before = shutil.disk_usage(uri).used\n",
    "                subprocess.run(\n",
    "                    [\"mlflow\", \"gc\",\n",
    "                     \"--backend-store-uri\", uri,\n",
    "                     \"--artifact-store\", uri],\n",
    "                    check=True,\n",
    "                    stdout=subprocess.PIPE,\n",
    "                    stderr=subprocess.PIPE,\n",
    "                )\n",
    "                after = shutil.disk_usage(uri).used\n",
    "                logger.info(\"🧹 mlflow gc completed (%.2f MB → %.2f MB)\",\n",
    "                            before/1e6, after/1e6)\n",
    "\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Cleanup for %s failed: %s\", model_name, exc)\n",
    "\n",
    "    async def vacuum_store(self) -> None:\n",
    "        \"\"\"Force a *store‑wide* `mlflow gc` (use from cron jobs).\"\"\"\n",
    "        try:\n",
    "            uri = mlflow.get_tracking_uri().removeprefix(\"file:\")\n",
    "            before = shutil.disk_usage(uri).used\n",
    "            subprocess.run(\n",
    "                [\"mlflow\", \"gc\",\n",
    "                 \"--backend-store-uri\", uri,\n",
    "                 \"--artifact-store\", uri],\n",
    "                check=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "            )\n",
    "            after = shutil.disk_usage(uri).used\n",
    "            logger.info(\"🧹 Store-wide vacuum completed (%.2f MB → %.2f MB)\",\n",
    "                        before/1e6, after/1e6)\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Store vacuum failed: %s\", exc)\n",
    "\n",
    "\n",
    "# Global singleton\n",
    "model_service = ModelService()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "01981457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/app/main.py\n",
    "import logging\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from fastapi import FastAPI, Request, Depends, BackgroundTasks, status, HTTPException\n",
    "from fastapi.security import OAuth2PasswordRequestForm\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# ── NEW: Fix ML backend configuration before any JAX imports ───────────────────────────\n",
    "from .utils.env_sanitizer import fix_ml_backends\n",
    "fix_ml_backends()\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ── NEW: Rate limiting imports ─────────────────────────────────────────────────────────\n",
    "from fastapi_limiter import FastAPILimiter\n",
    "from redis import asyncio as redis\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# ── NEW: Concurrency limiting imports ────────────────────────────────────────────────\n",
    "from .middleware.concurrency import ConcurrencyLimiter\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from .db import lifespan, get_db, get_app_ready\n",
    "from .security import create_access_token, get_current_user, verify_password\n",
    "from .crud import get_user_by_username\n",
    "from .schemas.iris import IrisPredictRequest, IrisPredictResponse, IrisFeatures\n",
    "from .schemas.cancer import CancerPredictRequest, CancerPredictResponse, CancerFeatures\n",
    "from .schemas.train import IrisTrainRequest, CancerTrainRequest, BayesTrainRequest, BayesTrainResponse, BayesConfigResponse, BayesRunMetrics\n",
    "from .services.ml.model_service import model_service\n",
    "from .core.config import settings\n",
    "from .deps.limits import default_limit, heavy_limit, login_limit, training_limit, light_limit\n",
    "from .security import LoginPayload, get_credentials\n",
    "\n",
    "# ── NEW: guarantee log directory exists ───────────────────────────\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "# ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ── NEW: Redis cache client for prediction caching ────────────────────────────────────\n",
    "# Use the same Redis URL logic as in db.py for consistency\n",
    "if settings.CACHE_ENABLED:\n",
    "    env_url = os.getenv(\"REDIS_URL\")\n",
    "    if settings.ENVIRONMENT_CANONICAL == \"production\" and env_url:\n",
    "        redis_url = env_url\n",
    "    else:\n",
    "        redis_url = settings.REDIS_URL\n",
    "\n",
    "    cache = redis.from_url(\n",
    "        redis_url,\n",
    "        encoding=\"utf-8\",\n",
    "        decode_responses=True,\n",
    "    )\n",
    "    logger.info(\"📦 Prediction caching enabled (Redis %s)\", redis_url)\n",
    "else:\n",
    "    cache = None\n",
    "    logger.info(\"📦 Prediction caching disabled by config\")\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Pydantic models\n",
    "class Payload(BaseModel):\n",
    "    count: int\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    data: Payload\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "    input_received: Payload  # Echo back the input for verification\n",
    "\n",
    "class Token(BaseModel):\n",
    "    access_token: str\n",
    "    token_type: str\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"FastAPI + React ML App\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/api/v1/docs\",\n",
    "    redoc_url=\"/api/v1/redoc\",\n",
    "    openapi_url=\"/api/v1/openapi.json\",\n",
    "    swagger_ui_parameters={\"persistAuthorization\": True},\n",
    "    lifespan=lifespan,  # register startup/shutdown events\n",
    ")\n",
    "\n",
    "# ── Rate limiting is now initialized in lifespan() ────────────────────────────────────\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Configure CORS with environment-based origins\n",
    "origins_env = settings.ALLOWED_ORIGINS\n",
    "origins: list[str] = [o.strip() for o in origins_env.split(\",\")] if origins_env != \"*\" else [\"*\"]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # In production, replace with specific origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ── NEW: Add concurrency limiting middleware ──────────────────────────────────────────\n",
    "app.add_middleware(ConcurrencyLimiter, max_concurrent=4)\n",
    "# ────────────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_process_time_header(request: Request, call_next):\n",
    "    \"\"\"Measure request time and add X-Process-Time header.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    response = await call_next(request)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    response.headers[\"X-Process-Time\"] = f\"{elapsed:.4f}\"\n",
    "    return response\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/api/v1/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Basic health check - always returns 200 if server is running.\"\"\"\n",
    "    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n",
    "\n",
    "@app.get(\"/api/v1/hello\")\n",
    "async def hello(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"Simple endpoint for token validation.\"\"\"\n",
    "    return {\"message\": f\"Hello {current_user}!\", \"status\": \"authenticated\"}\n",
    "\n",
    "@app.get(\"/api/v1/ready\")\n",
    "async def ready():\n",
    "    \"\"\"Basic readiness check.\"\"\"\n",
    "    return {\"ready\": get_app_ready()}\n",
    "\n",
    "@app.get(\"/api/v1/ready/frontend\")\n",
    "async def ready_frontend() -> dict:\n",
    "    \"\"\"\n",
    "    Frontend-safe readiness payload.\n",
    "    Returns only small, stable fields the React SPA depends on.\n",
    "    This avoids the large nested dependency audit data that was causing frontend crashes.\n",
    "    \"\"\"\n",
    "    ready_for_login = get_app_ready()\n",
    "    loaded = set(model_service.models.keys())\n",
    "    return {\n",
    "        \"ready\": ready_for_login,\n",
    "        \"models\": {\n",
    "            \"iris\": \"iris_random_forest\" in loaded or \"iris_logreg\" in loaded,\n",
    "            \"cancer\": \"breast_cancer_bayes\" in loaded or \"breast_cancer_stub\" in loaded,\n",
    "        },\n",
    "        \"has_bayes\": \"breast_cancer_bayes\" in loaded,\n",
    "        \"has_stub\": \"breast_cancer_stub\" in loaded,\n",
    "        \"all_models_loaded\": all(\n",
    "            model in loaded \n",
    "            for model in [\"iris_random_forest\", \"breast_cancer_bayes\"]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "@app.post(\"/api/v1/token\", response_model=Token, dependencies=[Depends(login_limit)])\n",
    "async def login(\n",
    "    creds: LoginPayload = Depends(get_credentials),\n",
    "    db: AsyncSession = Depends(get_db),\n",
    "):\n",
    "    \"\"\"\n",
    "    Issue a JWT. Accepts **either**\n",
    "    • JSON {\"username\": \"...\", \"password\": \"...\"}  *or*\n",
    "    • classic x‑www‑form‑urlencoded.\n",
    "    \"\"\"\n",
    "    # 1️⃣ readiness gate\n",
    "    if not get_app_ready():\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Backend still loading models. Try again in a moment.\",\n",
    "            headers={\"Retry‑After\": \"10\"},\n",
    "        )\n",
    "\n",
    "    # 2️⃣ verify credentials\n",
    "    user = await get_user_by_username(db, creds.username)\n",
    "    if not user or not verify_password(creds.password, user.hashed_password):\n",
    "        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "                            detail=\"Invalid credentials\")\n",
    "\n",
    "    # 3️⃣ issue token\n",
    "    token = create_access_token(subject=user.username)\n",
    "    return Token(access_token=token, token_type=\"bearer\")\n",
    "\n",
    "# --- PATCH: ready_full -------------------------------------------------------\n",
    "@app.get(\"/api/v1/ready/full\")\n",
    "async def ready_full(debug: Optional[bool] = False) -> dict:\n",
    "    \"\"\"\n",
    "    Extended readiness probe with environment drift summary.\n",
    "\n",
    "    Query param:\n",
    "      debug=1  -> include filtered_status map for troubleshooting.\n",
    "    \"\"\"\n",
    "    ready_for_login = get_app_ready()\n",
    "    expected = {\"iris_random_forest\", \"breast_cancer_bayes\"}  # minimal contract\n",
    "    loaded = set(model_service.models.keys())\n",
    "\n",
    "    # ----- helpers -----------------------------------------------------------\n",
    "    def _is_meta(k: str) -> bool:\n",
    "        return k.endswith(\"_dep_audit\") or k.endswith(\"_last_error\")\n",
    "\n",
    "    def _model_status_items():\n",
    "        for k, v in model_service.status.items():\n",
    "            if _is_meta(k):\n",
    "                continue\n",
    "            yield k, v\n",
    "\n",
    "    # ----- env drift summary -------------------------------------------------\n",
    "    drift = {}\n",
    "    for m in (\"iris_random_forest\", \"iris_logreg\", \"breast_cancer_bayes\", \"breast_cancer_stub\"):\n",
    "        audit = model_service.status.get(f\"{m}_dep_audit\", {})\n",
    "        critical = any(\n",
    "            (pkg in (\"numpy\", \"scipy\", \"scikit-learn\", \"psutil\")) and rec.get(\"severity\") == \"MAJOR_DRT\"\n",
    "            for pkg, rec in audit.items()\n",
    "        )\n",
    "        drift[m] = {\"critical_drift\": critical, \"details\": audit}\n",
    "\n",
    "    # ----- core fields -------------------------------------------------------\n",
    "    filtered_status = dict(_model_status_items())\n",
    "    all_models_loaded = all(v == \"loaded\" for v in filtered_status.values())\n",
    "    training = [k for k, v in filtered_status.items() if v == \"training\"]\n",
    "\n",
    "    response = {\n",
    "        \"ready\": ready_for_login,\n",
    "        \"model_status\": model_service.status,  # raw (includes meta)\n",
    "        \"env_drift\": drift,\n",
    "        \"all_models_loaded\": all_models_loaded,\n",
    "        \"models\": {m: (m in loaded) for m in expected},\n",
    "        \"training\": training,\n",
    "    }\n",
    "\n",
    "    if debug:\n",
    "        response[\"status_filtered\"] = filtered_status\n",
    "        response[\"status_counts\"] = {\n",
    "            \"raw\": len(model_service.status),\n",
    "            \"filtered\": len(filtered_status),\n",
    "        }\n",
    "\n",
    "    # Log response size for debugging\n",
    "    if debug:\n",
    "        import json\n",
    "        response_size = len(json.dumps(response))\n",
    "        logger.info(\"READY_FULL debug: payload size=%d bytes\", response_size)\n",
    "\n",
    "    logger.debug(\"READY endpoint – _app_ready=%s\", ready_for_login)\n",
    "    return response\n",
    "# --- END PATCH ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# ── Alias routes (no auth, not shown in OpenAPI) ────────────────────────────\n",
    "@app.get(\"/ready/full\", include_in_schema=False)\n",
    "async def ready_full_alias():\n",
    "    \"\"\"Alias for front-end calls that miss the /api/v1 prefix.\"\"\"\n",
    "    return await ready_full()\n",
    "\n",
    "@app.get(\"/health\", include_in_schema=False)\n",
    "async def health_alias():\n",
    "    \"\"\"Alias for plain /health (SPA hits it before it knows the prefix).\"\"\"\n",
    "    return await health_check()\n",
    "\n",
    "@app.post(\"/token\", include_in_schema=False)\n",
    "async def login_alias(request: Request):\n",
    "    \"\"\"\n",
    "    Alias: accept /token like /api/v1/token.\n",
    "    Keeps the OAuth2PasswordRequestForm semantics without exposing clutter in docs.\n",
    "    \"\"\"\n",
    "    from fastapi import Form\n",
    "\n",
    "    # Parse form data manually to match OAuth2PasswordRequestForm behavior\n",
    "    form_data = await request.form()\n",
    "    username = form_data.get(\"username\")\n",
    "    password = form_data.get(\"password\")\n",
    "\n",
    "    if not username or not password:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n",
    "            detail=\"username and password are required\"\n",
    "        )\n",
    "\n",
    "    # Create a mock OAuth2PasswordRequestForm object\n",
    "    class MockForm:\n",
    "        def __init__(self, username, password):\n",
    "            self.username = username\n",
    "            self.password = password\n",
    "\n",
    "    mock_form = MockForm(username, password)\n",
    "\n",
    "    # Reuse the existing login logic\n",
    "    db = await get_db().__anext__()\n",
    "    return await login(mock_form, db)\n",
    "\n",
    "@app.post(\"/iris/predict\", include_in_schema=False)\n",
    "async def iris_predict_alias(request: Request):\n",
    "    \"\"\"Alias for /api/v1/iris/predict\"\"\"\n",
    "    from .schemas.iris import IrisPredictRequest\n",
    "\n",
    "    # Parse JSON body\n",
    "    body = await request.json()\n",
    "    iris_request = IrisPredictRequest(**body)\n",
    "\n",
    "    # Reuse the existing prediction logic without authentication for testing\n",
    "    background_tasks = BackgroundTasks()\n",
    "    current_user = \"test_user\"  # Skip authentication for alias endpoints\n",
    "    return await predict_iris(iris_request, background_tasks, current_user)\n",
    "\n",
    "@app.post(\"/cancer/predict\", include_in_schema=False)\n",
    "async def cancer_predict_alias(request: Request):\n",
    "    \"\"\"Alias for /api/v1/cancer/predict\"\"\"\n",
    "    from .schemas.cancer import CancerPredictRequest\n",
    "\n",
    "    # Parse JSON body\n",
    "    body = await request.json()\n",
    "    cancer_request = CancerPredictRequest(**body)\n",
    "\n",
    "    # Reuse the existing prediction logic without authentication for testing\n",
    "    background_tasks = BackgroundTasks()\n",
    "    current_user = \"test_user\"  # Skip authentication for alias endpoints\n",
    "    return await predict_cancer(cancer_request, background_tasks, current_user)\n",
    "\n",
    "# ----- on-demand training endpoints ----------------------------------\n",
    "@app.post(\"/api/v1/iris/train\", status_code=202, dependencies=[Depends(training_limit)])\n",
    "async def train_iris(\n",
    "    request: IrisTrainRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Kick off training of the chosen Iris model.\n",
    "    \"\"\"\n",
    "    background_tasks.add_task(\n",
    "        model_service.train_iris,\n",
    "        request.model_type\n",
    "    )\n",
    "    return {\"status\": f\"started iris training ({request.model_type})\"}\n",
    "\n",
    "@app.post(\"/api/v1/cancer/train\", status_code=202, dependencies=[Depends(training_limit)])\n",
    "async def train_cancer(\n",
    "    request: CancerTrainRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Kick off training of the chosen Cancer model.\n",
    "    \"\"\"\n",
    "    background_tasks.add_task(\n",
    "        model_service.train_cancer,\n",
    "        request.model_type\n",
    "    )\n",
    "    return {\"status\": f\"started cancer training ({request.model_type})\"}\n",
    "\n",
    "@app.get(\"/api/v1/cancer/bayes/config\", response_model=BayesConfigResponse)\n",
    "async def get_bayes_config(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"\n",
    "    Get Bayesian training configuration for frontend form generation.\n",
    "    \"\"\"\n",
    "    from .schemas.bayes import BayesCancerParams\n",
    "    \n",
    "    defaults = BayesCancerParams()\n",
    "    \n",
    "    return BayesConfigResponse(\n",
    "        defaults=defaults,\n",
    "        bounds={\n",
    "            \"draws\": {\"min\": 200, \"max\": 20000},\n",
    "            \"tune\": {\"min\": 200, \"max\": 20000},\n",
    "            \"target_accept\": {\"min\": 0.80, \"max\": 0.999},\n",
    "            \"max_rhat_warn\": {\"min\": 1.0, \"max\": 1.1},\n",
    "            \"min_ess_warn\": {\"min\": 50, \"max\": 5000},\n",
    "        },\n",
    "        descriptions={\n",
    "            \"draws\": \"Number of posterior draws retained. More draws = better MCSE but longer runtime.\",\n",
    "            \"tune\": \"Warmup steps for NUTS adaptation. Should be ≥ 0.2 * draws for good convergence.\",\n",
    "            \"target_accept\": \"Target acceptance rate. Higher values reduce divergences but increase runtime.\",\n",
    "            \"compute_waic\": \"Compute Widely Applicable Information Criterion. Fast but may be less robust than LOO.\",\n",
    "            \"compute_loo\": \"Compute Leave-One-Out cross-validation. More reliable but slower.\",\n",
    "        },\n",
    "        runtime_estimate={\n",
    "            \"base_seconds_per_sample\": 0.001,  # rough estimate\n",
    "            \"chains\": 4,\n",
    "            \"overhead_seconds\": 5.0,  # model setup, data loading, etc.\n",
    "        }\n",
    "    )\n",
    "\n",
    "@app.post(\"/api/v1/cancer/bayes/train\", response_model=BayesTrainResponse, dependencies=[Depends(training_limit)])\n",
    "async def train_bayes_cancer(\n",
    "    request: BayesTrainRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Train Bayesian cancer model with validated hyperparameters.\n",
    "    \"\"\"\n",
    "    if not get_app_ready():\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"Backend still loading models. Try again in a moment.\",\n",
    "            headers={\"Retry‑After\": \"10\"},\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        if request.async_training:\n",
    "            # Queue for background processing\n",
    "            job_id = f\"bayes_{int(time.time())}\"\n",
    "            background_tasks.add_task(\n",
    "                model_service.train_bayes_cancer_with_params, \n",
    "                request.params\n",
    "            )\n",
    "            return BayesTrainResponse(\n",
    "                run_id=\"\",  # will be set when job completes\n",
    "                job_id=job_id,\n",
    "                status=\"queued\",\n",
    "                message=\"Training queued for background processing\"\n",
    "            )\n",
    "        else:\n",
    "            # Synchronous training\n",
    "            run_id = await model_service.train_bayes_cancer_with_params(request.params)\n",
    "            return BayesTrainResponse(\n",
    "                run_id=run_id,\n",
    "                status=\"completed\",\n",
    "                message=\"Training completed successfully\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        logger.error(\"Bayesian training failed: %s\", e)\n",
    "        return BayesTrainResponse(\n",
    "            run_id=\"\",\n",
    "            status=\"failed\",\n",
    "            message=str(e)\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/cancer/bayes/runs/{run_id}\", response_model=BayesRunMetrics)\n",
    "async def get_bayes_run_metrics(\n",
    "    run_id: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Get metrics for a specific Bayesian training run.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mlflow\n",
    "        run = mlflow.get_run(run_id)\n",
    "        metrics = run.data.metrics\n",
    "        params = run.data.params\n",
    "        \n",
    "        warnings = []\n",
    "        if metrics.get(\"rhat_max\", 0) > 1.01:\n",
    "            warnings.append(f\"R-hat exceeds threshold: {metrics['rhat_max']:.4f} > 1.01\")\n",
    "        if metrics.get(\"ess_bulk_min\", 0) < 400:\n",
    "            warnings.append(f\"Bulk ESS below threshold: {metrics['ess_bulk_min']:.1f} < 400\")\n",
    "        \n",
    "        return BayesRunMetrics(\n",
    "            run_id=run_id,\n",
    "            accuracy=metrics.get(\"accuracy\", 0.0),\n",
    "            rhat_max=metrics.get(\"rhat_max\"),\n",
    "            ess_bulk_min=metrics.get(\"ess_bulk_min\"),\n",
    "            ess_tail_min=metrics.get(\"ess_tail_min\"),\n",
    "            waic=metrics.get(\"waic\"),\n",
    "            loo=metrics.get(\"loo\"),\n",
    "            status=\"completed\",\n",
    "            warnings=warnings\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to get run metrics for %s: %s\", run_id, e)\n",
    "        raise HTTPException(status_code=404, detail=f\"Run {run_id} not found or metrics unavailable\")\n",
    "\n",
    "# ----- debug endpoints ----------------------------------\n",
    "@app.get(\"/api/v1/debug/ready\")\n",
    "async def debug_ready():\n",
    "    \"\"\"Debug endpoint to verify configuration loading.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"ready\",\n",
    "        \"environment\": settings.ENVIRONMENT,\n",
    "        \"rate_limits\": {\n",
    "            \"default\": settings.RATE_LIMIT_DEFAULT,\n",
    "            \"cancer\": settings.RATE_LIMIT_CANCER,\n",
    "            \"login\": settings.RATE_LIMIT_LOGIN,\n",
    "            \"training\": settings.RATE_LIMIT_TRAINING,\n",
    "            \"window\": settings.RATE_LIMIT_WINDOW,\n",
    "            \"window_light\": settings.RATE_LIMIT_WINDOW_LIGHT,\n",
    "        },\n",
    "        \"quality_gates\": {\n",
    "            \"accuracy_threshold\": settings.QUALITY_GATE_ACCURACY_THRESHOLD,\n",
    "            \"f1_threshold\": settings.QUALITY_GATE_F1_THRESHOLD,\n",
    "        },\n",
    "        \"mlflow\": {\n",
    "            \"experiment\": settings.MLFLOW_EXPERIMENT,\n",
    "            \"tracking_uri\": settings.MLFLOW_TRACKING_URI,\n",
    "            \"registry_uri\": settings.MLFLOW_REGISTRY_URI,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"skip_background\": settings.SKIP_BACKGROUND_TRAINING,\n",
    "            \"auto_train_missing\": settings.AUTO_TRAIN_MISSING,\n",
    "        },\n",
    "        \"debug\": {\n",
    "            \"debug_ratelimit\": settings.DEBUG_RATELIMIT,\n",
    "        }\n",
    "    }\n",
    "\n",
    "# --- effective config debug --------------------------------------------------\n",
    "@app.get(\"/api/v1/debug/effective-config\")\n",
    "async def effective_config(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"\n",
    "    Inspect the *effective* runtime configuration (after YAML + env overrides).\n",
    "\n",
    "    Sensitive fields are redacted. Use to debug environment drift across\n",
    "    dev/staging/production deployments.\n",
    "    \"\"\"\n",
    "    from app.core.config import settings\n",
    "\n",
    "    redacted = {\"SECRET_KEY\", \"DATABASE_URL\"}\n",
    "    cfg = settings.model_dump()\n",
    "    for k in list(cfg):\n",
    "        if k.upper() in redacted and cfg[k] is not None:\n",
    "            cfg[k] = \"***redacted***\"\n",
    "    return {\n",
    "        \"environment\": settings.ENVIRONMENT_CANONICAL,\n",
    "        \"config\": cfg,\n",
    "    }\n",
    "\n",
    "# ----- MLOps endpoints (new) ----------------------------------------\n",
    "@app.post(\"/api/v1/mlops/evaluate/{model_name}\")\n",
    "async def evaluate_model(\n",
    "    model_name: str,\n",
    "    run_id: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a candidate model against production baseline.\n",
    "\n",
    "    This endpoint is used by CI/CD pipelines to implement quality gates.\n",
    "    The model is evaluated on a fixed test set and compared to production.\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} evaluating model {model_name} (run: {run_id})\")\n",
    "\n",
    "    result = await model_service.evaluate_model_quality(model_name, run_id)\n",
    "    return result\n",
    "\n",
    "@app.post(\"/api/v1/mlops/promote/{model_name}/staging\")\n",
    "async def promote_to_staging(\n",
    "    model_name: str,\n",
    "    run_id: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Promote a model to staging after quality gate evaluation.\n",
    "\n",
    "    This endpoint:\n",
    "    1. Evaluates the model quality\n",
    "    2. If passed, registers as staging version\n",
    "    3. Sets @staging alias for atomic promotion\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} promoting {model_name} to staging (run: {run_id})\")\n",
    "\n",
    "    result = await model_service.promote_model_to_staging(model_name, run_id)\n",
    "    return result\n",
    "\n",
    "@app.post(\"/api/v1/mlops/promote/{model_name}/production\")\n",
    "async def promote_to_production(\n",
    "    model_name: str,\n",
    "    version: Optional[int] = None,\n",
    "    approved_by: Optional[str] = None,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Promote a staging model to production.\n",
    "\n",
    "    This can be called manually or by CI/CD:\n",
    "    - If version specified, promotes that specific version\n",
    "    - Otherwise, promotes the current @staging alias\n",
    "    - Sets @prod alias for atomic promotion\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} promoting {model_name} to production (version: {version})\")\n",
    "\n",
    "    result = await model_service.promote_model_to_production(model_name, version, approved_by)\n",
    "    return result\n",
    "\n",
    "@app.post(\"/api/v1/mlops/reload-model\")\n",
    "async def reload_model(\n",
    "    model_name: Optional[str] = None,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Hot-reload models from MLflow registry.\n",
    "\n",
    "    This endpoint allows the container to pick up new models\n",
    "    without restarting the entire service. Useful for:\n",
    "    - CI/CD deployments that update models\n",
    "    - Manual model promotions\n",
    "    - Testing new model versions\n",
    "\n",
    "    Args:\n",
    "        model_name: Specific model to reload (optional, reloads all if None)\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} reloading models (specific: {model_name})\")\n",
    "\n",
    "    try:\n",
    "        if model_name:\n",
    "            # Reload specific model\n",
    "            success = await model_service._try_load(model_name)\n",
    "            if success:\n",
    "                return {\n",
    "                    \"reloaded\": True,\n",
    "                    \"model\": model_name,\n",
    "                    \"status\": model_service.status.get(model_name, \"unknown\")\n",
    "                }\n",
    "            else:\n",
    "                raise HTTPException(\n",
    "                    status_code=404,\n",
    "                    detail=f\"Failed to reload model {model_name}\"\n",
    "                )\n",
    "        else:\n",
    "            # Reload all models\n",
    "            reloaded = []\n",
    "            failed = []\n",
    "\n",
    "            for name in [\"iris_random_forest\", \"iris_logreg\", \n",
    "                        \"breast_cancer_bayes\", \"breast_cancer_stub\"]:\n",
    "                try:\n",
    "                    success = await model_service._try_load(name)\n",
    "                    if success:\n",
    "                        reloaded.append(name)\n",
    "                    else:\n",
    "                        failed.append(name)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to reload {name}: {e}\")\n",
    "                    failed.append(name)\n",
    "\n",
    "            return {\n",
    "                \"reloaded\": len(failed) == 0,\n",
    "                \"reloaded_models\": reloaded,\n",
    "                \"failed_models\": failed,\n",
    "                \"status\": model_service.status\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model reload failed: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Model reload failed: {e}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/status\")\n",
    "async def mlops_status(current_user: str = Depends(get_current_user)):\n",
    "    \"\"\"\n",
    "    Get MLOps status including model versions and stages.\n",
    "\n",
    "    Returns comprehensive information about:\n",
    "    - Model loading status\n",
    "    - Registry versions and stages\n",
    "    - Alias assignments\n",
    "    - Training status\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} requesting MLOps status\")\n",
    "\n",
    "    try:\n",
    "        client = model_service.mlflow_client\n",
    "\n",
    "        # Get model registry information\n",
    "        registry_info = {}\n",
    "        for model_name in [\"iris_random_forest\", \"iris_logreg\", \n",
    "                          \"breast_cancer_bayes\", \"breast_cancer_stub\"]:\n",
    "            try:\n",
    "                versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "                registry_info[model_name] = {\n",
    "                    \"versions\": len(versions),\n",
    "                    \"stages\": {},\n",
    "                    \"aliases\": {}\n",
    "                }\n",
    "\n",
    "                # Group by stage\n",
    "                for v in versions:\n",
    "                    stage = v.current_stage\n",
    "                    if stage not in registry_info[model_name][\"stages\"]:\n",
    "                        registry_info[model_name][\"stages\"][stage] = []\n",
    "                    registry_info[model_name][\"stages\"][stage].append({\n",
    "                        \"version\": v.version,\n",
    "                        \"run_id\": v.run_id,\n",
    "                        \"created_at\": v.creation_timestamp\n",
    "                    })\n",
    "\n",
    "                # Get aliases\n",
    "                try:\n",
    "                    aliases = client.get_registered_model_aliases(model_name)\n",
    "                    registry_info[model_name][\"aliases\"] = {\n",
    "                        alias: version for alias, version in aliases.items()\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Could not get aliases for {model_name}: {e}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not get registry info for {model_name}: {e}\")\n",
    "                registry_info[model_name] = {\"error\": str(e)}\n",
    "\n",
    "        return {\n",
    "            \"model_status\": model_service.status,\n",
    "            \"loaded_models\": list(model_service.models.keys()),\n",
    "            \"registry_info\": registry_info,\n",
    "            \"app_ready\": get_app_ready(),\n",
    "            \"mlflow_uri\": settings.MLFLOW_TRACKING_URI\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"MLOps status failed: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"MLOps status failed: {e}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/models/{model_name}/metrics\")\n",
    "async def get_model_metrics(\n",
    "    model_name: str,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Get metrics for all versions of a registered model.\n",
    "\n",
    "    This endpoint enables MLOps comparison between different model versions\n",
    "    for quality gate decisions and promotion workflows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metrics = await model_service.get_model_metrics(model_name)\n",
    "        if not metrics:\n",
    "            raise HTTPException(\n",
    "                status_code=404, \n",
    "                detail=f\"No registered model found with name '{model_name}'\"\n",
    "            )\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"versions\": metrics,\n",
    "            \"total_versions\": len(metrics)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching metrics for {model_name}: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to fetch model metrics: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/models/{model_name}/compare\")\n",
    "async def compare_model_versions(\n",
    "    model_name: str,\n",
    "    version_a: int,\n",
    "    version_b: int,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare two specific model versions for MLOps decision making.\n",
    "\n",
    "    This endpoint helps determine which model version performs better\n",
    "    across key metrics like accuracy, F1-score, precision, and recall.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        comparison = await model_service.compare_model_versions(\n",
    "            model_name, version_a, version_b\n",
    "        )\n",
    "\n",
    "        if \"error\" in comparison:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=comparison[\"error\"]\n",
    "            )\n",
    "\n",
    "        return comparison\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error comparing versions for {model_name}: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to compare model versions: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/mlops/models/{model_name}/quality-gate\")\n",
    "async def check_quality_gate(\n",
    "    model_name: str,\n",
    "    version: Optional[int] = None,\n",
    "    current_user: str = Depends(get_current_user)\n",
    "):\n",
    "    \"\"\"\n",
    "    Check if a model version passes quality gates.\n",
    "\n",
    "    This endpoint evaluates a model against production baseline\n",
    "    or absolute thresholds to determine if it's ready for promotion.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If no version specified, use the latest staging version\n",
    "        if version is None:\n",
    "            client = model_service.mlflow_client\n",
    "            staging_versions = client.search_model_versions(\n",
    "                f\"name='{model_name}' AND stage='Staging'\"\n",
    "            )\n",
    "            if not staging_versions:\n",
    "                raise HTTPException(\n",
    "                    status_code=404,\n",
    "                    detail=f\"No staging version found for {model_name}\"\n",
    "                )\n",
    "            version = staging_versions[0].version\n",
    "\n",
    "        # Get the run_id for this version\n",
    "        version_info = client.get_model_version(model_name, version)\n",
    "        run_id = version_info.run_id\n",
    "\n",
    "        # Evaluate quality gate\n",
    "        eval_result = await model_service.evaluate_model_quality(model_name, run_id)\n",
    "\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"version\": version,\n",
    "            \"run_id\": run_id,\n",
    "            \"quality_gate_result\": eval_result,\n",
    "            \"passes_gate\": eval_result[\"promoted\"],\n",
    "            \"reason\": eval_result[\"reason\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking quality gate for {model_name}: {e}\")\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Failed to check quality gate: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.get(\"/api/v1/iris/ready\")\n",
    "async def iris_ready():\n",
    "    \"\"\"Check if Iris model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"iris_random_forest\" in model_service.models}\n",
    "\n",
    "@app.get(\"/api/v1/cancer/ready\")\n",
    "async def cancer_ready():\n",
    "    \"\"\"Check if Cancer model is loaded and ready.\"\"\"\n",
    "    return {\"loaded\": \"breast_cancer_bayes\" in model_service.models}\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/iris/predict\",\n",
    "    response_model=IrisPredictResponse,\n",
    "    status_code=status.HTTP_200_OK,\n",
    "    dependencies=[Depends(light_limit)]\n",
    ")\n",
    "async def predict_iris(\n",
    "    request: IrisPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict iris species from measurements, with optional Redis caching.\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /iris/predict with {len(request.samples)} samples\")\n",
    "\n",
    "    # Ensure model is loaded\n",
    "    model_name = \"iris_random_forest\" if request.model_type == \"rf\" else \"iris_logreg\"\n",
    "    if model_name not in model_service.models:\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"Iris model still loading. Try again shortly.\",\n",
    "            headers={\"Retry-After\": \"30\"},\n",
    "        )\n",
    "\n",
    "    # Build cache key from primitives (avoid Pydantic models)\n",
    "    serialized_samples = [s.dict() for s in request.samples]\n",
    "    key = f\"iris:{request.model_type}:{json.dumps(serialized_samples, sort_keys=True)}\"\n",
    "\n",
    "    # Try Redis GET if caching enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        cached = await cache.get(key)\n",
    "        if cached:\n",
    "            logger.debug(\"Cache hit for key %s\", key)\n",
    "            return IrisPredictResponse(**json.loads(cached))\n",
    "\n",
    "    # Perform prediction\n",
    "    preds, probs = await model_service.predict_iris(\n",
    "        features=serialized_samples,\n",
    "        model_type=request.model_type,\n",
    "    )\n",
    "\n",
    "    # Prepare a fully-serializable result dict\n",
    "    result = {\n",
    "        \"predictions\": preds,\n",
    "        \"probabilities\": probs,\n",
    "        \"input_received\": serialized_samples,\n",
    "    }\n",
    "\n",
    "    # Store in cache if enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        ttl = settings.CACHE_TTL_MINUTES * 60\n",
    "        await cache.set(key, json.dumps(result), ex=ttl)\n",
    "\n",
    "    # Audit log in background\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=iris input={serialized_samples} output={preds}\"\n",
    "    )\n",
    "\n",
    "    return IrisPredictResponse(**result)\n",
    "\n",
    "@app.post(\n",
    "    \"/api/v1/cancer/predict\",\n",
    "    response_model=CancerPredictResponse,\n",
    "    status_code=status.HTTP_200_OK,\n",
    "    dependencies=[Depends(heavy_limit)]\n",
    ")\n",
    "async def predict_cancer(\n",
    "    request: CancerPredictRequest,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_user: str = Depends(get_current_user),\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict breast-cancer diagnosis, with optional Redis caching.\n",
    "    \"\"\"\n",
    "    logger.info(f\"User {current_user} called /cancer/predict with {len(request.samples)} samples\")\n",
    "\n",
    "    # Build cache key from primitives (includes posterior_samples)\n",
    "    serialized_samples = [s.dict() for s in request.samples]\n",
    "    key = (\n",
    "        f\"cancer:{request.model_type}:\"\n",
    "        f\"{request.posterior_samples or 0}:\"\n",
    "        f\"{json.dumps(serialized_samples, sort_keys=True)}\"\n",
    "    )\n",
    "\n",
    "    # Try Redis GET if caching enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        cached = await cache.get(key)\n",
    "        if cached:\n",
    "            logger.debug(\"Cache hit for key %s\", key)\n",
    "            return CancerPredictResponse(**json.loads(cached))\n",
    "\n",
    "    # Perform prediction\n",
    "    preds, probs, uncertainties = await model_service.predict_cancer(\n",
    "        features=serialized_samples,\n",
    "        model_type=request.model_type,\n",
    "        posterior_samples=request.posterior_samples,\n",
    "    )\n",
    "\n",
    "    # Prepare a fully-serializable result dict\n",
    "    result = {\n",
    "        \"predictions\": preds,\n",
    "        \"probabilities\": probs,\n",
    "        \"uncertainties\": uncertainties,\n",
    "        \"input_received\": serialized_samples,\n",
    "    }\n",
    "\n",
    "    # Store in cache if enabled\n",
    "    if settings.CACHE_ENABLED:\n",
    "        ttl = settings.CACHE_TTL_MINUTES * 60\n",
    "        await cache.set(key, json.dumps(result), ex=ttl)\n",
    "\n",
    "    # Audit log in background\n",
    "    background_tasks.add_task(\n",
    "        logger.info,\n",
    "        f\"[audit] user={current_user} endpoint=cancer input={serialized_samples} output={preds}\"\n",
    "    )\n",
    "\n",
    "    return CancerPredictResponse(**result) \n",
    "\n",
    "@app.get(\"/api/v1/debug/compiler\")\n",
    "async def debug_compiler():\n",
    "    \"\"\"\n",
    "    Debug endpoint to check JAX/NumPyro backend configuration.\n",
    "    Returns information about the JAX backend setup.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import jax\n",
    "        import numpyro\n",
    "        import pymc as pm\n",
    "\n",
    "        return {\n",
    "            \"backend\": \"jax_numpyro\",\n",
    "            \"jax_version\": jax.__version__,\n",
    "            \"numpyro_version\": numpyro.__version__,\n",
    "            \"pymc_version\": pm.__version__,\n",
    "            \"jax_devices\": str(jax.devices()),\n",
    "            \"jax_platform\": jax.default_backend(),\n",
    "            \"status\": \"jax_backend_configured\"\n",
    "        }\n",
    "    except ImportError as e:\n",
    "        return {\n",
    "            \"backend\": \"unknown\",\n",
    "            \"error\": f\"Import error: {e}\",\n",
    "            \"status\": \"missing_dependencies\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"backend\": \"unknown\", \n",
    "            \"error\": f\"Configuration error: {e}\",\n",
    "            \"status\": \"configuration_failed\"\n",
    "        }\n",
    "\n",
    "@app.get(\"/api/v1/debug/psutil\")\n",
    "async def debug_psutil():\n",
    "    \"\"\"\n",
    "    Debug endpoint to check psutil status and configuration.\n",
    "    Returns information about psutil module and its Process class.\n",
    "    \"\"\"\n",
    "    import sys, types\n",
    "    try:\n",
    "        import psutil\n",
    "        module_info = {\n",
    "            \"module_path\": getattr(psutil, \"__file__\", \"?\"),\n",
    "            \"version\": getattr(psutil, \"__version__\", \"?\"),\n",
    "            \"has_Process\": hasattr(psutil, \"Process\"),\n",
    "            \"sys_path\": sys.path\n",
    "        }\n",
    "\n",
    "        # Try a safe Process call\n",
    "        try:\n",
    "            proc = psutil.Process()\n",
    "            module_info[\"process_test\"] = {\n",
    "                \"success\": True,\n",
    "                \"pid\": proc.pid,\n",
    "                \"cpu_count\": psutil.cpu_count()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            module_info[\"process_test\"] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"status\": \"loaded\",\n",
    "            \"info\": module_info\n",
    "        }\n",
    "    except ImportError as e:\n",
    "        return {\n",
    "            \"status\": \"import_failed\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        } \n",
    "\n",
    "@app.get(\"/api/v1/debug/deps\")\n",
    "async def debug_deps():\n",
    "    \"\"\"\n",
    "    Report recorded vs. runtime dependency versions for each loaded model.\n",
    "\n",
    "    Uses audit data collected during ModelService._load_production_model().\n",
    "    Helpful when MLflow logs 'requirements_utils' mismatch warnings.\n",
    "\n",
    "    NOTE: purely diagnostic – no secrets.\n",
    "    \"\"\"\n",
    "    import importlib.metadata as im\n",
    "    runtime = {}\n",
    "    for pkg in (\"numpy\", \"scipy\", \"scikit-learn\", \"psutil\", \"pandas\"):\n",
    "        try:\n",
    "            runtime[pkg] = im.version(pkg)\n",
    "        except Exception:\n",
    "            runtime[pkg] = None\n",
    "\n",
    "    audits = {k: v for k, v in model_service.status.items() if k.endswith(\"_dep_audit\")}\n",
    "    return {\n",
    "        \"runtime\": runtime,\n",
    "        \"model_audits\": audits,\n",
    "        \"enforcement_policy\": os.getenv(\"MODEL_ENV_ENFORCEMENT\", \"warn\"),\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/api/v1/test/401\")\n",
    "async def test_401():\n",
    "    \"\"\"Test endpoint that returns 401 for testing session expiry.\"\"\"\n",
    "    raise HTTPException(\n",
    "        status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "        detail=\"Test 401 response\"\n",
    "    )\n",
    "\n",
    "# ── Debug‑only ratelimit helpers ─────────────────────────────────────────────\n",
    "from .deps.limits import get_redis, _user_or_ip as user_or_ip\n",
    "\n",
    "@app.post(\"/api/v1/debug/ratelimit/reset\", include_in_schema=False)\n",
    "async def rl_reset(request: Request):\n",
    "    \"\"\"\n",
    "    Flush **all** rate‑limit counters bound to the caller (JWT _or_ IP).\n",
    "\n",
    "    We match every fragment that contains the identifier to survive\n",
    "    future changes in FastAPI‑Limiter's key schema.\n",
    "    \"\"\"\n",
    "    r = get_redis()\n",
    "    if not r:\n",
    "        raise HTTPException(status_code=503, detail=\"Rate‑limiter not initialised\")\n",
    "\n",
    "    ident = await user_or_ip(request)\n",
    "    keys = await r.keys(f\"ratelimit:*{ident}*\")        # <— broader pattern\n",
    "    if keys:\n",
    "        await r.delete(*keys)\n",
    "    return {\"reset\": len(keys)}\n",
    "\n",
    "if settings.DEBUG_RATELIMIT:          # OFF by default\n",
    "    @app.get(\"/api/v1/debug/ratelimit/{bucket}\", include_in_schema=False)\n",
    "    async def rl_status(bucket: str, request: Request):\n",
    "        \"\"\"\n",
    "        Inspect Redis keys for the current identifier + bucket.\n",
    "        Handy for CI tests – **never enable in prod**.\n",
    "        \"\"\"\n",
    "        key_prefix = f\"ratelimit:{bucket}:{await user_or_ip(request)}\"\n",
    "        r = get_redis()\n",
    "        keys = await r.keys(f\"{key_prefix}*\")\n",
    "        values = await r.mget(keys) if keys else []\n",
    "        return dict(zip(keys, values)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3bb5d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/tests/test_protection_systems_fixed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/tests/test_protection_systems_fixed.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Updated test script for rate limiting and concurrency protection.\n",
    "Uses debug endpoints to reset rate limits between tests.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import httpx\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import pytest\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import redis.asyncio as aioredis\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    __test__ = False  # prevent pytest from collecting this class as tests\n",
    "\n",
    "    name: str\n",
    "    success: bool\n",
    "    expected_status: int\n",
    "    actual_status: int\n",
    "    response_time: float\n",
    "    rate_limit_headers: Dict[str, str]\n",
    "    error_message: str = \"\"\n",
    "\n",
    "\n",
    "class ProtectionTester:\n",
    "    \"\"\"Test both rate limiting and concurrency protection systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str | None = None):\n",
    "        self.base_url = base_url or os.getenv(\"BASE_URL\", \"http://localhost:8000\")\n",
    "        self.auth_token = None\n",
    "        self.results: List[TestResult] = []\n",
    "        \n",
    "    async def wait_until_ready(self, timeout: int = 30) -> bool:\n",
    "        \"\"\"Poll /api/v1/health until the backend responds 200 or timeout.\"\"\"\n",
    "        deadline = time.time() + timeout\n",
    "        while time.time() < deadline:\n",
    "            try:\n",
    "                async with httpx.AsyncClient() as c:\n",
    "                    r = await c.get(f\"{self.base_url}/api/v1/health\", timeout=3)\n",
    "                    if r.status_code == 200:\n",
    "                        return True\n",
    "            except httpx.ConnectError:\n",
    "                # Backend not up yet\n",
    "                pass\n",
    "            await asyncio.sleep(1)\n",
    "        return False\n",
    "\n",
    "    async def authenticate(self) -> bool:\n",
    "        \"\"\"Get JWT; waits for backend and surfaces clear errors.\"\"\"\n",
    "        if not await self.wait_until_ready():\n",
    "            print(f\"❌ Backend not reachable at {self.base_url}. \"\n",
    "                  f\"Start it with 'uvicorn api.app.main:app --reload' or \"\n",
    "                  f\"set BASE_URL to the correct host:port.\")\n",
    "            return False\n",
    "        try:\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                r = await client.post(\n",
    "                    f\"{self.base_url}/api/v1/token\",\n",
    "                    json={\"username\": \"alice\", \"password\": \"supersecretvalue\"},\n",
    "                    headers={\"Content-Type\": \"application/json\"},\n",
    "                    timeout=5.0,\n",
    "                )\n",
    "            if r.status_code == 200:\n",
    "                self.auth_token = r.json()[\"access_token\"]\n",
    "                print(\"✅ Authentication successful\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"❌ Authentication failed: {r.status_code} {r.text}\")\n",
    "                return False\n",
    "        except httpx.ConnectError as e:\n",
    "            print(f\"❌ Connection error: {e}. Is the API running on {self.base_url}?\")\n",
    "            return False\n",
    "    \n",
    "    async def reset_rate_limits(self) -> bool:\n",
    "        \"\"\"Reset rate limits for current user.\"\"\"\n",
    "        try:\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                headers = {\"Authorization\": f\"Bearer {self.auth_token}\"}\n",
    "                response = await client.post(\n",
    "                    f\"{self.base_url}/api/v1/debug/ratelimit/reset\",\n",
    "                    headers=headers,\n",
    "                    timeout=5.0\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    print(f\"✅ Rate limits reset: {result.get('reset', 0)} keys cleared\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"❌ Rate limit reset failed: {response.status_code}\")\n",
    "                    return False\n",
    "        except httpx.ConnectError as e:\n",
    "            print(f\"❌ Rate limit reset connection error: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Rate limit reset error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    async def test_redis_connection(self) -> TestResult:\n",
    "        \"\"\"Test that Redis is reachable and responding to PING.\"\"\"\n",
    "        start = time.time()\n",
    "        name = \"Redis PING\"\n",
    "        # First, fetch the effective-config to get the Redis URL\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            # include token so effective-config returns the real REDIS_URL\n",
    "            headers = {\"Authorization\": f\"Bearer {self.auth_token}\"}\n",
    "            cfg_resp = await client.get(\n",
    "                f\"{self.base_url}/api/v1/debug/effective-config\",\n",
    "                headers=headers,\n",
    "                timeout=5.0\n",
    "            )\n",
    "        if cfg_resp.status_code != 200:\n",
    "            return TestResult(\n",
    "                name=name,\n",
    "                success=False,\n",
    "                expected_status=200,\n",
    "                actual_status=cfg_resp.status_code,\n",
    "                response_time=time.time() - start,\n",
    "                rate_limit_headers={},\n",
    "                error_message=f\"Could not fetch config: {cfg_resp.status_code}\"\n",
    "            )\n",
    "\n",
    "        redis_url = cfg_resp.json().get(\"config\", {}).get(\"REDIS_URL\")\n",
    "        if not redis_url:\n",
    "            return TestResult(\n",
    "                name=name,\n",
    "                success=False,\n",
    "                expected_status=1,\n",
    "                actual_status=0,\n",
    "                response_time=time.time() - start,\n",
    "                rate_limit_headers={},\n",
    "                error_message=\"REDIS_URL missing in config\"\n",
    "            )\n",
    "\n",
    "        # Now ping Redis directly\n",
    "        try:\n",
    "            redis_conn = aioredis.from_url(redis_url, encoding=\"utf-8\", decode_responses=True)\n",
    "            pong = await redis_conn.ping()\n",
    "            response_time = time.time() - start\n",
    "            success = pong is True\n",
    "            return TestResult(\n",
    "                name=name,\n",
    "                success=success,\n",
    "                expected_status=1,\n",
    "                actual_status=1 if success else 0,\n",
    "                response_time=response_time,\n",
    "                rate_limit_headers={},\n",
    "                error_message=\"\" if success else \"PING returned False\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return TestResult(\n",
    "                name=name,\n",
    "                success=False,\n",
    "                expected_status=1,\n",
    "                actual_status=0,\n",
    "                response_time=time.time() - start,\n",
    "                rate_limit_headers={},\n",
    "                error_message=str(e)\n",
    "            )\n",
    "\n",
    "    async def make_request(\n",
    "        self, \n",
    "        endpoint: str, \n",
    "        method: str = \"GET\",\n",
    "        data: Dict[str, Any] = None,\n",
    "        expected_status: int = 200\n",
    "    ) -> TestResult:\n",
    "        \"\"\"Make a single request and return detailed result.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                headers = {}\n",
    "                if self.auth_token:\n",
    "                    headers[\"Authorization\"] = f\"Bearer {self.auth_token}\"\n",
    "                \n",
    "                if method == \"GET\":\n",
    "                    response = await client.get(\n",
    "                        f\"{self.base_url}{endpoint}\", \n",
    "                        headers=headers,\n",
    "                        timeout=10.0\n",
    "                    )\n",
    "                elif method == \"POST\":\n",
    "                    headers[\"Content-Type\"] = \"application/json\"\n",
    "                    response = await client.post(\n",
    "                        f\"{self.base_url}{endpoint}\", \n",
    "                        json=data, \n",
    "                        headers=headers,\n",
    "                        timeout=10.0\n",
    "                    )\n",
    "                else:\n",
    "                    return TestResult(\n",
    "                        name=f\"{method} {endpoint}\",\n",
    "                        success=False,\n",
    "                        expected_status=expected_status,\n",
    "                        actual_status=0,\n",
    "                        response_time=0,\n",
    "                        rate_limit_headers={},\n",
    "                        error_message=f\"Unsupported method: {method}\"\n",
    "                    )\n",
    "                \n",
    "                response_time = time.time() - start_time\n",
    "                \n",
    "                # Extract rate limit headers\n",
    "                rate_limit_headers = {\n",
    "                    k: v for k, v in response.headers.items() \n",
    "                    if k.lower().startswith('x-ratelimit') or k.lower() == 'retry-after'\n",
    "                }\n",
    "                \n",
    "                success = response.status_code == expected_status\n",
    "                \n",
    "                return TestResult(\n",
    "                    name=f\"{method} {endpoint}\",\n",
    "                    success=success,\n",
    "                    expected_status=expected_status,\n",
    "                    actual_status=response.status_code,\n",
    "                    response_time=response_time,\n",
    "                    rate_limit_headers=rate_limit_headers,\n",
    "                    error_message=\"\" if success else f\"Expected {expected_status}, got {response.status_code}\"\n",
    "                )\n",
    "                \n",
    "        except httpx.ConnectError as e:\n",
    "            return TestResult(\n",
    "                name=f\"{method} {endpoint}\",\n",
    "                success=False,\n",
    "                expected_status=expected_status,\n",
    "                actual_status=0,\n",
    "                response_time=time.time() - start_time,\n",
    "                rate_limit_headers={},\n",
    "                error_message=f\"Connection error: {e}. Is the API running on {self.base_url}?\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return TestResult(\n",
    "                name=f\"{method} {endpoint}\",\n",
    "                success=False,\n",
    "                expected_status=expected_status,\n",
    "                actual_status=0,\n",
    "                response_time=time.time() - start_time,\n",
    "                rate_limit_headers={},\n",
    "                error_message=str(e)\n",
    "            )\n",
    "    \n",
    "    async def test_rate_limiting(self) -> None:\n",
    "        \"\"\"Test rate limiting functionality.\"\"\"\n",
    "        print(\"\\n🔍 Testing Rate Limiting...\")\n",
    "        \n",
    "        # Test 1: Login rate limiting (IP-based)\n",
    "        print(\"\\n📊 Test 1: Login Rate Limiting (IP-based)\")\n",
    "        for i in range(5):  # Try 5 login attempts (limit is 3)\n",
    "            result = await self.make_request(\n",
    "                \"/api/v1/token\",\n",
    "                method=\"POST\",\n",
    "                data={\"username\": \"wrong\", \"password\": \"wrong\"},\n",
    "                expected_status=401 if i < 3 else 429\n",
    "            )\n",
    "            self.results.append(result)\n",
    "            print(f\"  Attempt {i+1}: {result.actual_status} (expected: {result.expected_status})\")\n",
    "            if result.rate_limit_headers:\n",
    "                print(f\"    Headers: {result.rate_limit_headers}\")\n",
    "        \n",
    "        # Test 2: Iris prediction rate limiting\n",
    "        print(\"\\n🌺 Test 2: Iris Prediction Rate Limiting\")\n",
    "        iris_data = {\n",
    "            \"model_type\": \"rf\",\n",
    "            \"samples\": [{\n",
    "                \"sepal_length\": 5.1,\n",
    "                \"sepal_width\": 3.5,\n",
    "                \"petal_length\": 1.4,\n",
    "                \"petal_width\": 0.2\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        # Make requests until we hit the limit\n",
    "        for i in range(125):  # Light limit is 120, so 125th should fail\n",
    "            result = await self.make_request(\n",
    "                \"/api/v1/iris/predict\",\n",
    "                method=\"POST\",\n",
    "                data=iris_data,\n",
    "                expected_status=200 if i < 120 else 429\n",
    "            )\n",
    "            self.results.append(result)\n",
    "            \n",
    "            if i % 20 == 0:  # Progress indicator\n",
    "                print(f\"  Progress: {i+1}/125 requests\")\n",
    "            \n",
    "            if result.actual_status == 429:\n",
    "                print(f\"  ✅ Rate limit hit at request {i+1}\")\n",
    "                if result.rate_limit_headers:\n",
    "                    print(f\"    Headers: {result.rate_limit_headers}\")\n",
    "                break\n",
    "        \n",
    "        # Test 3: Cancer prediction rate limiting (heavier limit)\n",
    "        print(\"\\n🔬 Test 3: Cancer Prediction Rate Limiting\")\n",
    "        cancer_data = {\n",
    "            \"model_type\": \"bayes\",\n",
    "            \"samples\": [{\n",
    "                \"mean_radius\": 14.13,\n",
    "                \"mean_texture\": 19.26,\n",
    "                \"mean_perimeter\": 91.97,\n",
    "                \"mean_area\": 654.89,\n",
    "                \"mean_smoothness\": 0.096,\n",
    "                \"mean_compactness\": 0.104,\n",
    "                \"mean_concavity\": 0.089,\n",
    "                \"mean_concave_points\": 0.048,\n",
    "                \"mean_symmetry\": 0.181,\n",
    "                \"mean_fractal_dimension\": 0.063,\n",
    "                \"se_radius\": 0.406,\n",
    "                \"se_texture\": 1.216,\n",
    "                \"se_perimeter\": 2.866,\n",
    "                \"se_area\": 40.34,\n",
    "                \"se_smoothness\": 0.007,\n",
    "                \"se_compactness\": 0.025,\n",
    "                \"se_concavity\": 0.032,\n",
    "                \"se_concave_points\": 0.012,\n",
    "                \"se_symmetry\": 0.020,\n",
    "                \"se_fractal_dimension\": 0.004,\n",
    "                \"worst_radius\": 16.27,\n",
    "                \"worst_texture\": 25.68,\n",
    "                \"worst_perimeter\": 107.26,\n",
    "                \"worst_area\": 880.58,\n",
    "                \"worst_smoothness\": 0.132,\n",
    "                \"worst_compactness\": 0.254,\n",
    "                \"worst_concavity\": 0.273,\n",
    "                \"worst_concave_points\": 0.114,\n",
    "                \"worst_symmetry\": 0.290,\n",
    "                \"worst_fractal_dimension\": 0.084\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        for i in range(35):  # Heavy limit is 30, so 35th should fail\n",
    "            result = await self.make_request(\n",
    "                \"/api/v1/cancer/predict\",\n",
    "                method=\"POST\",\n",
    "                data=cancer_data,\n",
    "                expected_status=200 if i < 30 else 429\n",
    "            )\n",
    "            self.results.append(result)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Progress: {i+1}/35 requests\")\n",
    "            \n",
    "            if result.actual_status == 429:\n",
    "                print(f\"  ✅ Rate limit hit at request {i+1}\")\n",
    "                if result.rate_limit_headers:\n",
    "                    print(f\"    Headers: {result.rate_limit_headers}\")\n",
    "                break\n",
    "    \n",
    "    async def test_concurrency_limiting(self) -> None:\n",
    "        \"\"\"Test concurrency limiting functionality.\"\"\"\n",
    "        print(\"\\n🔍 Testing Concurrency Limiting...\")\n",
    "        \n",
    "        # Reset rate limits before concurrency test\n",
    "        print(\"\\n🔄 Resetting rate limits for concurrency test...\")\n",
    "        if not await self.reset_rate_limits():\n",
    "            print(\"⚠️  Could not reset rate limits - test may fail\")\n",
    "        \n",
    "        # Test concurrent cancer predictions (heavy endpoint)\n",
    "        print(\"\\n🏋️ Test: Concurrent Cancer Predictions\")\n",
    "        \n",
    "        cancer_data = {\n",
    "            \"model_type\": \"bayes\",\n",
    "            \"samples\": [{\n",
    "                \"mean_radius\": 14.13,\n",
    "                \"mean_texture\": 19.26,\n",
    "                \"mean_perimeter\": 91.97,\n",
    "                \"mean_area\": 654.89,\n",
    "                \"mean_smoothness\": 0.096,\n",
    "                \"mean_compactness\": 0.104,\n",
    "                \"mean_concavity\": 0.089,\n",
    "                \"mean_concave_points\": 0.048,\n",
    "                \"mean_symmetry\": 0.181,\n",
    "                \"mean_fractal_dimension\": 0.063,\n",
    "                \"se_radius\": 0.406,\n",
    "                \"se_texture\": 1.216,\n",
    "                \"se_perimeter\": 2.866,\n",
    "                \"se_area\": 40.34,\n",
    "                \"se_smoothness\": 0.007,\n",
    "                \"se_compactness\": 0.025,\n",
    "                \"se_concavity\": 0.032,\n",
    "                \"se_concave_points\": 0.012,\n",
    "                \"se_symmetry\": 0.020,\n",
    "                \"se_fractal_dimension\": 0.004,\n",
    "                \"worst_radius\": 16.27,\n",
    "                \"worst_texture\": 25.68,\n",
    "                \"worst_perimeter\": 107.26,\n",
    "                \"worst_area\": 880.58,\n",
    "                \"worst_smoothness\": 0.132,\n",
    "                \"worst_compactness\": 0.254,\n",
    "                \"worst_concavity\": 0.273,\n",
    "                \"worst_concave_points\": 0.114,\n",
    "                \"worst_symmetry\": 0.290,\n",
    "                \"worst_fractal_dimension\": 0.084\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        async def make_concurrent_request(request_id: int) -> TestResult:\n",
    "            \"\"\"Make a single concurrent request.\"\"\"\n",
    "            return await self.make_request(\n",
    "                \"/api/v1/cancer/predict\",\n",
    "                method=\"POST\",\n",
    "                data=cancer_data,\n",
    "                expected_status=200\n",
    "            )\n",
    "        \n",
    "        # Launch 6 concurrent requests (max_concurrent=4)\n",
    "        print(\"  Launching 6 concurrent requests (max_concurrent=4)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        tasks = [make_concurrent_request(i) for i in range(6)]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Process results\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        response_times = []\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, TestResult):\n",
    "                self.results.append(result)\n",
    "                if result.success:\n",
    "                    successful += 1\n",
    "                    response_times.append(result.response_time)\n",
    "                else:\n",
    "                    failed += 1\n",
    "                print(f\"  Request {i+1}: {result.actual_status} ({result.response_time:.2f}s)\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"  Request {i+1}: Exception - {result}\")\n",
    "        \n",
    "        print(f\"\\n  📊 Concurrency Test Results:\")\n",
    "        print(f\"    Successful: {successful}/6\")\n",
    "        print(f\"    Failed: {failed}/6\")\n",
    "        print(f\"    Total time: {total_time:.2f}s\")\n",
    "        if response_times:\n",
    "            print(f\"    Avg response time: {sum(response_times)/len(response_times):.2f}s\")\n",
    "    \n",
    "    async def test_health_endpoints(self) -> None:\n",
    "        \"\"\"Test that health endpoints work without rate limiting.\"\"\"\n",
    "        print(\"\\n🔍 Testing Health Endpoints (No Rate Limiting)...\")\n",
    "        \n",
    "        # Test health endpoint\n",
    "        result = await self.make_request(\"/api/v1/health\")\n",
    "        self.results.append(result)\n",
    "        print(f\"  Health check: {result.actual_status}\")\n",
    "        \n",
    "        # Test ready endpoint\n",
    "        result = await self.make_request(\"/api/v1/ready\")\n",
    "        self.results.append(result)\n",
    "        print(f\"  Ready check: {result.actual_status}\")\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"Print comprehensive test summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 PROTECTION SYSTEMS TEST SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_tests = len(self.results)\n",
    "        successful_tests = sum(1 for r in self.results if r.success)\n",
    "        failed_tests = total_tests - successful_tests\n",
    "        \n",
    "        print(f\"Total Tests: {total_tests}\")\n",
    "        print(f\"Successful: {successful_tests}\")\n",
    "        print(f\"Failed: {failed_tests}\")\n",
    "        print(f\"Success Rate: {(successful_tests/total_tests)*100:.1f}%\")\n",
    "        \n",
    "        # Rate limiting analysis\n",
    "        rate_limit_tests = [r for r in self.results if \"predict\" in r.name or \"token\" in r.name]\n",
    "        rate_limit_429s = [r for r in rate_limit_tests if r.actual_status == 429]\n",
    "        \n",
    "        print(f\"\\n🔒 Rate Limiting:\")\n",
    "        print(f\"  Rate-limited requests: {len(rate_limit_tests)}\")\n",
    "        print(f\"  429 responses (expected): {len(rate_limit_429s)}\")\n",
    "        \n",
    "        # Concurrency analysis\n",
    "        concurrent_tests = [r for r in self.results if \"cancer/predict\" in r.name]\n",
    "        concurrent_success = [r for r in concurrent_tests if r.success]\n",
    "        \n",
    "        print(f\"\\n🏋️ Concurrency Limiting:\")\n",
    "        print(f\"  Concurrent requests: {len(concurrent_tests)}\")\n",
    "        print(f\"  Successful concurrent: {len(concurrent_success)}\")\n",
    "        \n",
    "        # Show failed tests\n",
    "        if failed_tests > 0:\n",
    "            print(f\"\\n❌ Failed Tests:\")\n",
    "            for result in self.results:\n",
    "                if not result.success:\n",
    "                    print(f\"  {result.name}: {result.error_message}\")\n",
    "        \n",
    "        # Redis analysis\n",
    "        redis_tests = [r for r in self.results if r.name == \"Redis PING\"]\n",
    "        if redis_tests:\n",
    "            rt = redis_tests[0]\n",
    "            print(f\"\\n🗄️ Redis Check: {'PASSED' if rt.success else 'FAILED'} \"\n",
    "                f\"in {rt.response_time:.3f}s\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main test function.\"\"\"\n",
    "    print(\"🚀 Starting Updated Protection Systems Test Suite...\")\n",
    "    print(\"📝 This version uses debug endpoints to reset rate limits between tests\")\n",
    "    print(f\"🔗 Using API base URL: {os.getenv('BASE_URL', 'http://localhost:8000')}\")\n",
    "    \n",
    "    tester = ProtectionTester()          # now picks up BASE_URL env automatically\n",
    "    \n",
    "    # Authenticate first\n",
    "    if not await tester.authenticate():\n",
    "        print(\"❌ Cannot proceed without authentication\")\n",
    "        return 1\n",
    "    \n",
    "    # New: Test Redis connectivity\n",
    "    redis_result = await tester.test_redis_connection()\n",
    "    tester.results.append(redis_result)\n",
    "    print(f\"\\n🔑 Test Redis: {'OK' if redis_result.success else 'FAIL'} \"\n",
    "        f\"({redis_result.actual_status}; {redis_result.error_message})\")\n",
    "\n",
    "    # Run all tests\n",
    "    await tester.test_health_endpoints()\n",
    "    await tester.test_rate_limiting()\n",
    "    await tester.test_concurrency_limiting()\n",
    "    \n",
    "    # Print summary\n",
    "    tester.print_summary()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    exit(asyncio.run(main())) \n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Pytest Test Functions\n",
    "# ============================================================================\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_wait_until_ready_times_out_immediately():\n",
    "    \"\"\"Verify wait_until_ready returns False if health never responds.\"\"\"\n",
    "    tester = ProtectionTester(base_url=\"http://invalid-host-that-does-not-exist\")\n",
    "    result = await tester.wait_until_ready(timeout=1)\n",
    "    assert result is False\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_protection_tester_initialization():\n",
    "    \"\"\"Test that ProtectionTester initializes correctly.\"\"\"\n",
    "    tester = ProtectionTester(base_url=\"http://test.example.com\")\n",
    "    assert tester.base_url == \"http://test.example.com\"\n",
    "    assert tester.auth_token is None\n",
    "    assert len(tester.results) == 0\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_test_result_creation():\n",
    "    \"\"\"Test that TestResult can be created correctly.\"\"\"\n",
    "    result = TestResult(\n",
    "        name=\"test_request\",\n",
    "        success=True,\n",
    "        expected_status=200,\n",
    "        actual_status=200,\n",
    "        response_time=0.1,\n",
    "        rate_limit_headers={\"X-RateLimit-Remaining\": \"99\"},\n",
    "        error_message=\"\"\n",
    "    )\n",
    "    assert result.name == \"test_request\"\n",
    "    assert result.success is True\n",
    "    assert result.expected_status == 200\n",
    "    assert result.actual_status == 200\n",
    "    assert result.response_time == 0.1\n",
    "    assert result.rate_limit_headers == {\"X-RateLimit-Remaining\": \"99\"}\n",
    "    assert result.error_message == \"\"\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_make_request_invalid_method():\n",
    "    \"\"\"Test that make_request handles invalid HTTP methods correctly.\"\"\"\n",
    "    tester = ProtectionTester(base_url=\"http://test.example.com\")\n",
    "    result = await tester.make_request(\n",
    "        \"/api/v1/test\",\n",
    "        method=\"INVALID\",\n",
    "        expected_status=200\n",
    "    )\n",
    "    assert result.success is False\n",
    "    assert \"Unsupported method: INVALID\" in result.error_message\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_authenticate_with_invalid_backend():\n",
    "    \"\"\"Test authentication fails when backend is not reachable.\"\"\"\n",
    "    tester = ProtectionTester(base_url=\"http://invalid-host-that-does-not-exist\")\n",
    "    result = await tester.authenticate()\n",
    "    assert result is False\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_reset_rate_limits_without_auth():\n",
    "    \"\"\"Test reset_rate_limits fails when not authenticated.\"\"\"\n",
    "    tester = ProtectionTester(base_url=\"http://test.example.com\")\n",
    "    result = await tester.reset_rate_limits()\n",
    "    assert result is False\n",
    "\n",
    "\n",
    "class TestProtectionTesterIntegration:\n",
    "    \"\"\"Integration tests for ProtectionTester class.\"\"\"\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_health_endpoints_no_rate_limiting(self):\n",
    "        \"\"\"Test that health endpoints are accessible without rate limiting.\"\"\"\n",
    "        # This test will only pass if the API is actually running\n",
    "        # and health endpoints are accessible\n",
    "        tester = ProtectionTester()\n",
    "        \n",
    "        # Try to make health requests (may fail if API not running)\n",
    "        health_result = await tester.make_request(\"/api/v1/health\")\n",
    "        ready_result = await tester.make_request(\"/api/v1/ready\")\n",
    "        \n",
    "        # At minimum, we should get some response (even if it's an error)\n",
    "        assert health_result.actual_status in [200, 404, 500, 0]  # 0 for connection errors\n",
    "        assert ready_result.actual_status in [200, 404, 500, 0]\n",
    "    \n",
    "    @pytest.mark.asyncio\n",
    "    async def test_redis_connection_without_auth(self):\n",
    "        \"\"\"Test redis connection check fails without authentication.\"\"\"\n",
    "        tester = ProtectionTester(base_url=\"http://test.example.com\")\n",
    "        result = await tester.test_redis_connection()\n",
    "        assert result.success is False\n",
    "        assert \"Could not fetch config\" in result.error_message "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
