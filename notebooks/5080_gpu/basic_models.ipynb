{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f5a53f",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression fits a straight line to data. The model assumes a linear relationship\n",
    "y=w1x1+w2x2+⋯+b y=w1​x1​+w2​x2​+⋯+b\n",
    "between features and target. For one feature, this is $y = mX + b$\n",
    "kdnuggets.com\n",
    ". The parameters (weights $w$ and bias $b$) are chosen to minimize the average squared error (MSE) between predictions and actual values:\n",
    "MSE(w,b)=1N∑i=1N(yi−y^i)2,y^i=wTxi+b.MSE(w,b)=N1​∑i=1N​(yi​−y^​i​)2,y^​i​=wTxi​+b.\n",
    "Minimizing MSE can be done by solving the normal equations (closed-form) or by gradient descent. For example, given data of hours trained vs points scored, we can iteratively update weights by computing gradients\n",
    "kdnuggets.com\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb877603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENV SNAPSHOT (selected) ===\n",
      "JAX_PLATFORM_NAME=''\n",
      "JAX_PLATFORMS=None\n",
      "CUDA_VISIBLE_DEVICES='0'\n",
      "XLA_FLAGS='--xla_force_host_platform_device_count=1'\n",
      "NVIDIA_VISIBLE_DEVICES='all'\n",
      "NVIDIA_DRIVER_CAPABILITIES='compute,utility'\n",
      "===============================\n",
      "\n",
      "=== NVIDIA-SMI ===\n",
      "Thu Aug 28 20:31:54 2025       \n",
      "GPU is visible to the container/process.\n",
      "==================\n",
      "\n",
      "=== PyTorch ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import textwrap\n",
    "\n",
    "def _run(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)\n",
    "        return True, out.strip()\n",
    "    except Exception as e:\n",
    "        return False, f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "def env_snapshot():\n",
    "    keys = [\n",
    "        \"JAX_PLATFORM_NAME\", \"JAX_PLATFORMS\", \"CUDA_VISIBLE_DEVICES\",\n",
    "        \"XLA_FLAGS\", \"NVIDIA_VISIBLE_DEVICES\", \"NVIDIA_DRIVER_CAPABILITIES\"\n",
    "    ]\n",
    "    print(\"\\n=== ENV SNAPSHOT (selected) ===\")\n",
    "    for k in keys:\n",
    "        v = os.environ.get(k)\n",
    "        print(f\"{k}={v!r}\")\n",
    "    print(\"===============================\\n\")\n",
    "\n",
    "def check_nvidia_smi():\n",
    "    ok, out = _run([\"nvidia-smi\"])\n",
    "    print(\"=== NVIDIA-SMI ===\")\n",
    "    if ok:\n",
    "        print(out.splitlines()[0])\n",
    "        print(\"GPU is visible to the container/process.\")\n",
    "    else:\n",
    "        print(\"No GPU visible (nvidia-smi failed).\")\n",
    "        print(out)\n",
    "    print(\"==================\\n\")\n",
    "    return ok\n",
    "\n",
    "def check_pytorch():\n",
    "    print(\"=== PyTorch ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"torch.__version__={torch.__version__}\")\n",
    "        print(f\"torch.version.cuda={torch.version.cuda}\")\n",
    "        print(f\"torch.cuda.is_available()={torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"torch.cuda.device_count()={torch.cuda.device_count()}\")\n",
    "            print(f\"current device name: {torch.cuda.get_device_name(0)}\")\n",
    "            a = torch.randn(1024, 1024, device=\"cuda\")\n",
    "            b = torch.randn(1024, 1024, device=\"cuda\")\n",
    "            c = (a @ b).sum().item()\n",
    "            print(\"small matmul check (CUDA): OK\")\n",
    "        else:\n",
    "            print(\"CUDA not available in PyTorch.\")\n",
    "    except Exception as e:\n",
    "        print(\"PyTorch check failed:\", repr(e))\n",
    "    print(\"==============\\n\")\n",
    "\n",
    "def check_jax():\n",
    "    print(\"=== JAX ===\")\n",
    "    try:\n",
    "        import jax, jaxlib  # noqa\n",
    "        print(f\"jax.__version__={jax.__version__}\")\n",
    "        try:\n",
    "            import jaxlib\n",
    "            print(f\"jaxlib.__version__={jaxlib.__version__}\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not import jaxlib:\", repr(e))\n",
    "\n",
    "        # Try to list devices safely\n",
    "        try:\n",
    "            devs = jax.devices()\n",
    "            if not devs:\n",
    "                print(\"No JAX devices found.\")\n",
    "            else:\n",
    "                for d in devs:\n",
    "                    print(f\"Device: kind={d.device_kind}, platform={d.platform}, id={d.id}\")\n",
    "            # Functional smoke test on default backend\n",
    "            from jax import numpy as jnp\n",
    "            x = jnp.ones((1024,1024), dtype=jnp.float32)\n",
    "            y = (x @ x.T).sum()\n",
    "            _ = y.block_until_ready()\n",
    "            print(\"small matmul check (JAX default backend): OK\")\n",
    "        except RuntimeError as re:\n",
    "            # Decode the common “Unknown backend: 'gpu'” case\n",
    "            msg = str(re)\n",
    "            print(\"jax.devices() raised RuntimeError:\", msg)\n",
    "            if \"Unknown backend: 'gpu'\" in msg:\n",
    "                print(textwrap.dedent(\"\"\"\n",
    "                    HINT: JAX was forced to use the 'gpu' platform but no GPU backend is present.\n",
    "                    Check for:\n",
    "                    • JAX_PLATFORM_NAME or JAX_PLATFORMS env vars forcing 'gpu'\n",
    "                    • CPU-only jaxlib installed instead of CUDA-enabled wheels\n",
    "                    • Container not launched with GPU (e.g., --gpus all)\n",
    "                    \"\"\").strip())\n",
    "        except Exception as e:\n",
    "            print(\"jax.devices() failed:\", repr(e))\n",
    "    except Exception as e:\n",
    "        print(\"JAX import failed:\", repr(e))\n",
    "    print(\"===========\\n\")\n",
    "\n",
    "def check_tensorflow():\n",
    "    print(\"=== TensorFlow ===\")\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        print(f\"tf.__version__={tf.__version__}\")\n",
    "        try:\n",
    "            gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "            print(f\"tf GPUs: {gpus}\")\n",
    "            if gpus:\n",
    "                # Minimal compute test (graphless)\n",
    "                with tf.device(\"/GPU:0\"):\n",
    "                    a = tf.random.normal((1024,1024))\n",
    "                    b = tf.random.normal((1024,1024))\n",
    "                    c = tf.reduce_sum(tf.matmul(a, b))\n",
    "                _ = c.numpy()\n",
    "                print(\"small matmul check (TensorFlow GPU): OK\")\n",
    "            else:\n",
    "                print(\"No TensorFlow GPU devices found.\")\n",
    "        except Exception as e:\n",
    "            print(\"TensorFlow device query failed:\", repr(e))\n",
    "    except Exception as e:\n",
    "        print(\"TensorFlow import failed:\", repr(e))\n",
    "    print(\"==================\\n\")\n",
    "\n",
    "def main():\n",
    "    env_snapshot()\n",
    "    gpu_visible = check_nvidia_smi()\n",
    "    check_pytorch()\n",
    "    check_jax()\n",
    "    check_tensorflow()\n",
    "\n",
    "    if not gpu_visible:\n",
    "        print(textwrap.dedent(\"\"\"\n",
    "        SUMMARY: GPU NOT VISIBLE IN CONTAINER/PROCESS\n",
    "        - Ensure Docker is started with GPU access:\n",
    "            docker run --gpus all ...\n",
    "          or in docker-compose.yml:\n",
    "            services:\n",
    "              app:\n",
    "                gpus: all\n",
    "                environment:\n",
    "                  - NVIDIA_VISIBLE_DEVICES=all\n",
    "                  - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
    "        - On Windows, ensure WSL2 + NVIDIA drivers + NVIDIA Container Toolkit.\n",
    "        \"\"\").strip())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b6613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight: 0.7740, bias: 0.0000 (seed=42)\n",
      "Training for 1000 iterations with lr=0.1\n",
      "\n",
      "Iter    1/1000, loss=1.7402, w=1.1726, b=0.1278\n",
      "Iter  100/1000, loss=0.1007, w=0.8981, b=0.8763\n",
      "Iter  200/1000, loss=0.0952, w=0.8587, b=1.0184\n",
      "Iter  300/1000, loss=0.0950, w=0.8516, b=1.0443\n",
      "Iter  400/1000, loss=0.0950, w=0.8503, b=1.0490\n",
      "Iter  500/1000, loss=0.0950, w=0.8501, b=1.0498\n",
      "Iter  600/1000, loss=0.0950, w=0.8500, b=1.0500\n",
      "Iter  700/1000, loss=0.0950, w=0.8500, b=1.0500\n",
      "Iter  800/1000, loss=0.0950, w=0.8500, b=1.0500\n",
      "Iter  900/1000, loss=0.0950, w=0.8500, b=1.0500\n",
      "Iter 1000/1000, loss=0.0950, w=0.8500, b=1.0500\n",
      "\n",
      "Training complete.\n",
      "Final weight: 0.8500, bias: 1.0500\n",
      "\n",
      "Predictions vs. Actual:\n",
      "  x = 1, predicted = 1.90, actual = 1.50\n",
      "  x = 2, predicted = 2.75, actual = 3.00\n",
      "  x = 3, predicted = 3.60, actual = 4.00\n",
      "  x = 4, predicted = 4.45, actual = 4.50\n",
      "  x = 5, predicted = 5.30, actual = 5.00\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# linear_regression_gradient_descent.py\n",
    "\n",
    "\"\"\"\n",
    "Linear Regression with Gradient Descent\n",
    "---------------------------------------\n",
    "This script demonstrates a simple linear regression model trained\n",
    "using gradient descent on example data (hours practiced vs points scored).\n",
    "\n",
    "Key Concepts:\n",
    "  • Initialization: Starting weight w and bias b; in convex problems\n",
    "    (like linear regression with MSE) the final solution is independent\n",
    "    of the start, but the convergence speed can vary.\n",
    "  • Bias term b: Model intercept, i.e. prediction when x=0.\n",
    "  • Shapes: \n",
    "      X is (N×1), w is (1,), y_pred is (N,), error is (N,),\n",
    "      dw is (1,), db is scalar.\n",
    "  • Gradient descent update:\n",
    "      dw = (1/N) * Xᵀ·(y_pred - y)\n",
    "      db = (1/N) * Σ(y_pred - y)\n",
    "      w ← w - lr·dw\n",
    "      b ← b - lr·db\n",
    "\n",
    "Adjustable via CLI flags:\n",
    "  --lr        Learning rate (default 0.1)\n",
    "  --iters     Number of iterations (default 1000)\n",
    "  --seed      Random seed for initializing w (default 42)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Linear regression via gradient descent\"\n",
    "    )\n",
    "    parser.add_argument(\"--lr\",    type=float, default=0.1,  help=\"Learning rate\")\n",
    "    parser.add_argument(\"--iters\", type=int,   default=1000, help=\"Training iterations\")\n",
    "    parser.add_argument(\"--seed\",  type=int,   default=42,   help=\"Random seed\")\n",
    "\n",
    "    # Ignore unknown args (e.g., Jupyter’s -f flag) to avoid parse errors\n",
    "    args, unknown = parser.parse_known_args()  \n",
    "    return args\n",
    "\n",
    "def main(lr, n_iters, seed):\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1. Create the data\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Feature matrix X: hours practiced (5 examples) as a column vector (shape: 5×1)\n",
    "    X = np.array([[1], [2], [3], [4], [5]])\n",
    "    # Target vector y: points scored (shape: (5,))\n",
    "    y = np.array([1.5, 3.0, 4.0, 4.5, 5.0])\n",
    "    N = len(y)  # number of samples\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2. Initialize parameters\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Set RNG seed for reproducibility of w initialization\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Weight vector w (shape: (1,)), random start\n",
    "    w = rng.random(size=1)\n",
    "    # Bias scalar b, start at zero (intercept term)\n",
    "    b = 0.0\n",
    "\n",
    "    print(f\"Initial weight: {w[0]:.4f}, bias: {b:.4f} (seed={seed})\")\n",
    "    print(f\"Training for {n_iters} iterations with lr={lr}\\n\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3–6. Training loop: forward pass, compute gradients, update params\n",
    "    # ---------------------------------------------------------------------\n",
    "    for i in range(1, n_iters + 1):\n",
    "        # 3. Forward pass: predictions (shape: (5,))\n",
    "        y_pred = X.dot(w) + b\n",
    "\n",
    "        # 4. Compute gradients of MSE loss:\n",
    "        #    dw = (1/N) * Xᵀ·(y_pred - y)   → shape: (1,)\n",
    "        #    db = (1/N) * Σ(y_pred - y)     → scalar\n",
    "        error = y_pred - y\n",
    "        dw = (1 / N) * X.T.dot(error)\n",
    "        db = (1 / N) * np.sum(error)\n",
    "\n",
    "        # 5. Parameter update: move against the gradient\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "\n",
    "        # Print progress periodically (every 10% of iterations)\n",
    "        if i % max(1, n_iters // 10) == 0 or i == 1:\n",
    "            loss = (error**2).mean()\n",
    "            print(f\"Iter {i:4d}/{n_iters}, loss={loss:.4f}, w={w[0]:.4f}, b={b:.4f}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 7. Inspect the trained model\n",
    "    # ---------------------------------------------------------------------\n",
    "    print(\"\\nTraining complete.\")\n",
    "    print(f\"Final weight: {w[0]:.4f}, bias: {b:.4f}\\n\")\n",
    "\n",
    "    # Compare predictions vs. actual values\n",
    "    print(\"Predictions vs. Actual:\")\n",
    "    for x_val, actual in zip(X.flatten(), y):\n",
    "        pred = w * x_val + b\n",
    "        print(f\"  x = {x_val:>1}, predicted = {pred[0]:.2f}, actual = {actual:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(lr=args.lr, n_iters=args.iters, seed=args.seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f16fb",
   "metadata": {},
   "source": [
    "Bayesian Linear Regression\n",
    "\n",
    "Unlike standard linear regression, Bayesian regression treats the weights as random variables with a prior distribution. We combine a Gaussian prior with the data likelihood to get a posterior over weights. In practice, taking a normal prior $p(w)\\propto \\exp(-\\frac{1}{2\\sigma^2}|w|^2)$ leads to a regularized (Ridge) regression: one adds a penalty $\\lambda|w|^2$ to the loss. Fitting then becomes finding a posterior mean of $w$. Conceptually:\n",
    "posterior ∝ exp⁡(−12σ2∥w∥2)⋅exp⁡(−N⋅MSE(w,b)/2).posterior ∝ exp(−2σ21​∥w∥2)⋅exp(−N⋅MSE(w,b)/2).\n",
    "The update rule adds $\\lambda w$ to the gradient (MAP estimate). This gives the same effect as Ridge regression. Bayesian inference can also yield uncertainty estimates for $w$, but those require sampling (beyond basic NumPy).\n",
    "\n",
    "\n",
    "\n",
    "Summary:\n",
    "By placing a zero‑mean Gaussian prior on the weights\n",
    "p(w)∝exp⁡(−12σ2∥w∥2),\n",
    "p(w)∝exp(−2σ21​∥w∥2),\n",
    "\n",
    "and combining it with the usual Gaussian likelihood of the data, the posterior becomes\n",
    "p(w∣X,y)  ∝  exp⁡(−12σ2∥w∥2) exp⁡(−N2 MSE(w,b)).\n",
    "p(w∣X,y)∝exp(−2σ21​∥w∥2)exp(−2N​MSE(w,b)).\n",
    "\n",
    "Finding the MAP estimate of this posterior is equivalent to minimizing\n",
    "MSE(w,b)+λ∥w∥2MSE(w,b)+λ∥w∥2,\n",
    "i.e. Ridge regression with penalty λ∝1/σ2λ∝1/σ2.\n",
    "In gradient descent, this adds a term λ wλw to the weight‐gradient, giving the update\n",
    "w  ←  w−η[∇w MSE(w,b)+λ w].\n",
    "w←w−η[∇w​MSE(w,b)+λw].\n",
    "\n",
    "Below we outline each step with citations, then show the full annotated Python file.\n",
    "Bayesian Linear Regression: Gaussian Prior & Posterior\n",
    "Gaussian Prior on Weights\n",
    "\n",
    "In Bayesian regression we treat the weight vector ww as a random variable with a Gaussian prior\n",
    "p(w)  ∝  exp⁡(−12σ2∥w∥2).\n",
    "p(w)∝exp(−2σ21​∥w∥2).\n",
    "\n",
    "This encodes our belief that large weights are unlikely\n",
    "Let’s talk about science!\n",
    "and is exactly the same as the penalty term in Ridge regression\n",
    "Medium\n",
    ".\n",
    "Likelihood & Posterior\n",
    "\n",
    "Assuming Gaussian noise on observations gives a likelihood\n",
    "∝exp⁡(−N2 MSE(w,b))∝exp(−2N​MSE(w,b)).\n",
    "Multiplying prior and likelihood yields the posterior\n",
    "p(w∣X,y)  ∝  exp⁡(−12σ2∥w∥2) exp⁡(−N2 MSE(w,b)).\n",
    "p(w∣X,y)∝exp(−2σ21​∥w∥2)exp(−2N​MSE(w,b)).\n",
    "\n",
    "Taking the negative log gives the familiar Ridge objective\n",
    "Computer Science at Princeton\n",
    ".\n",
    "MAP Estimate & Ridge Equivalence\n",
    "Loss with L2 Penalty\n",
    "\n",
    "The MAP estimate minimizes\n",
    "1N∑i(yi−y^i)2⏟MSE  +  λ ∥w∥2,\n",
    "MSE\n",
    "N1​i∑​(yi​−y^​i​)2​​+λ∥w∥2,\n",
    "\n",
    "where λ=12σ2λ=2σ21​\n",
    "Statistical Odds & Ends\n",
    ".\n",
    "This is the Ridge regression cost function\n",
    "Cross Validated\n",
    ".\n",
    "Gradient‐Descent Update\n",
    "\n",
    "For standard MSE,\n",
    "∇w MSE=1NXT(Xw+b−y)∇w​MSE=N1​XT(Xw+b−y).\n",
    "Adding the prior gives\n",
    "∇w [MSE+λ∥w∥2]=1NXT(Xw+b−y)  +  λ w.\n",
    "∇w​[MSE+λ∥w∥2]=N1​XT(Xw+b−y)+λw.\n",
    "\n",
    "Thus the weight update becomes\n",
    "w←w  −  η[1NXT(Xw+b−y)+λ w]\n",
    "w←w−η[N1​XT(Xw+b−y)+λw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fad220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP-estimated weight: 0.6800, bias: 1.5600\n",
      "\n",
      "Final predictions vs actual:\n",
      "  x=1, pred=2.24, actual=1.50\n",
      "  x=2, pred=2.92, actual=3.00\n",
      "  x=3, pred=3.60, actual=4.00\n",
      "  x=4, pred=4.28, actual=4.50\n",
      "  x=5, pred=4.96, actual=5.00\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# bayesian_linear_regression.py\n",
    "\n",
    "\"\"\"\n",
    "Bayesian (MAP) Linear Regression via Gradient Descent\n",
    "-----------------------------------------------------\n",
    "This script fits a line y = w*x + b using MAP estimation,\n",
    "which with a Gaussian prior on w is equivalent to Ridge regression.\n",
    "\n",
    "Key additions over plain linear regression:\n",
    "- A Gaussian prior p(w) ∝ exp(-||w||^2 / (2σ^2)) yields penalty λ||w||^2.\n",
    "- Gradient adds λ * w when updating weights.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # -----------------------------\n",
    "    # 1. Create example data\n",
    "    # -----------------------------\n",
    "    X = np.array([[1], [2], [3], [4], [5]])  # hours practiced\n",
    "    y = np.array([1.5, 3.0, 4.0, 4.5, 5.0])  # points scored\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Initialize parameters\n",
    "    # -----------------------------\n",
    "    w = np.random.rand(1)   # initial weight\n",
    "    b = 0.0                 # initial bias\n",
    "\n",
    "    # Hyperparameters\n",
    "    lr      = 0.1           # learning rate\n",
    "    n_iters = 1000          # training iterations\n",
    "    N       = len(y)        # number of samples\n",
    "\n",
    "    # Bayesian prior strength → Ridge penalty\n",
    "    lambda_ = 0.5           # regularization coefficient\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Training loop\n",
    "    # -----------------------------\n",
    "    for i in range(n_iters):\n",
    "        # Forward pass: predictions\n",
    "        y_pred = X.dot(w) + b\n",
    "\n",
    "        # Compute basic MSE gradients\n",
    "        error = y_pred - y                   # shape (N,)\n",
    "        dw_basic = (1/N) * X.T.dot(error)    # shape (1,)\n",
    "        db      = (1/N) * np.sum(error)      # scalar\n",
    "\n",
    "        # Add prior (Ridge) gradient: λ * w\n",
    "        dw = dw_basic + lambda_ * w\n",
    "\n",
    "        # Update parameters\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Results\n",
    "    # -----------------------------\n",
    "    print(f\"MAP-estimated weight: {w[0]:.4f}, bias: {b:.4f}\\n\")\n",
    "\n",
    "    print(\"Final predictions vs actual:\")\n",
    "    for x_val, actual in zip(X.flatten(), y):\n",
    "        pred = w * x_val + b\n",
    "        print(f\"  x={x_val}, pred={pred[0]:.2f}, actual={actual:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de5eee",
   "metadata": {},
   "source": [
    "# Hierarchical Time Series Regression\n",
    "What Is Hierarchical Time‑Series Regression?\n",
    "\n",
    "Hierarchical time‑series regression extends standard regression by introducing a multi‑level parameter structure:\n",
    "\n",
    "    Global level: Defines hyperpriors (e.g., overall mean intercept and slope) shared across all series.\n",
    "\n",
    "    Group level: Specifies per‑series parameters (e.g., each store’s intercept and trend), drawn from the global distributions.\n",
    "    Wikipedia\n",
    "\n",
    "This setup contrasts with:\n",
    "\n",
    "    Complete pooling, where one regression fits all data, ignoring series differences.\n",
    "\n",
    "    No pooling, where each series is modeled separately, risking overfitting when data are sparse.\n",
    "    arXiv\n",
    "\n",
    "Key Benefits of the Hierarchical Structure\n",
    "\n",
    "    Partial Pooling / Borrowing Strength\n",
    "    By sharing information through hyperpriors, series with few observations “borrow” data from the whole ensemble, reducing variance in parameter estimates\n",
    "    Your Name Here's Homepage\n",
    "    .\n",
    "\n",
    "    Adaptive Regularization\n",
    "    The degree of pooling is learned from the data: when series are similar, the model pools more; when they differ, it pools less\n",
    "    otexts.com\n",
    "    .\n",
    "\n",
    "    Improved Forecast Coherence\n",
    "    In strictly hierarchical hierarchies (e.g., country → region → store), forecasts are internally consistent: the sum of lower‑level forecasts matches higher‑level totals\n",
    "    CRAN\n",
    "    .\n",
    "\n",
    "    Uncertainty Quantification\n",
    "    Full Bayesian inference yields credible intervals for both global and series‑specific parameters, offering richer insights than point estimates alone\n",
    "    leg.ufpr.br\n",
    "    .\n",
    "\n",
    "Mathematical Foundations\n",
    "\n",
    "Let there be JJ series, each with data {(tj,i,yj,i)}i=1Nj{(tj,i​,yj,i​)}i=1Nj​​. A simple hierarchical linear model is:\n",
    "\n",
    "    Hyperpriors (Global Level)\n",
    "    μa∼N(0, σa2),μb∼N(0, σb2)\n",
    "    μa​∼N(0,σa2​),μb​∼N(0,σb2​)\n",
    "\n",
    "    Group-Level Priors (Per-Series)\n",
    "    aj∼N(μa, σa2),bj∼N(μb, σb2)\n",
    "    aj​∼N(μa​,σa2​),bj​∼N(μb​,σb2​)\n",
    "\n",
    "    Observation Model (Likelihood)\n",
    "    yj,i∼N(aj+bj tj,i,  σobs2)\n",
    "    yj,i​∼N(aj​+bj​tj,i​,σobs2​)\n",
    "\n",
    "    where σobs2σobs2​ captures noise in observations.\n",
    "    Wikipedia\n",
    "\n",
    "The posterior is then sampled (e.g., via MCMC) or approximated (e.g., variational inference) to infer all parameters jointly\n",
    "leg.ufpr.br\n",
    ".\n",
    "Practical Applications & Tools\n",
    "\n",
    "    Retail forecasting: Model sales for multiple stores or regions, ensuring aggregate forecasts match corporate totals while capturing individual store trends\n",
    "    otexts.com\n",
    "    .\n",
    "\n",
    "    Sensor networks: Pool data across sensors in different locations to improve calibration and detect anomalies\n",
    "    Reddit\n",
    "    .\n",
    "\n",
    "    Hierarchical reconciliation: Use methods like bottom‑up or optimal combination (MinT) to reconcile forecasts across aggregation levels, implemented in R’s hts and htsDegenerateR packages\n",
    "    CRAN\n",
    "    .\n",
    "\n",
    "    Bayesian frameworks:\n",
    "\n",
    "        PyMC: Structure hierarchical models with intuitive context managers and NUTS sampling\n",
    "        PyMC Discourse\n",
    "        .\n",
    "\n",
    "        Stan / NumPyro: Similar hierarchical syntax supporting both MCMC and variational inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [mu_a, sigma_a, mu_b, sigma_b, a, b, sigma_obs]\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64891ccb40dd4139a7f77cbe06356f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# hierarchical_time_series_regression.py\n",
    "\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "def simulate_data(n_groups=3, n_time=10, sigma=0.5, seed=42):\n",
    "    \"\"\"Simulate n_groups time series of length n_time with\n",
    "       group-specific intercepts and slopes.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    time = np.arange(n_time)\n",
    "    true_a = np.array([1.0, 2.0, 3.0])           # intercepts\n",
    "    true_b = np.array([0.5, -0.2, 0.1])          # slopes\n",
    "    y = np.zeros((n_groups, n_time))\n",
    "    for j in range(n_groups):\n",
    "        y[j] = true_a[j] + true_b[j] * time + rng.normal(0, sigma, size=n_time)\n",
    "    return time, y\n",
    "\n",
    "def fit_hierarchical_model(time, y):\n",
    "    n_groups, n_time = y.shape\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Hyperpriors for intercepts and slopes\n",
    "        mu_a    = pm.Normal(\"mu_a\", mu=0, sigma=10)\n",
    "        sigma_a = pm.HalfNormal(\"sigma_a\", sigma=1)\n",
    "        mu_b    = pm.Normal(\"mu_b\", mu=0, sigma=1)\n",
    "        sigma_b = pm.HalfNormal(\"sigma_b\", sigma=1)\n",
    "\n",
    "        # Group‑level (partial pooling)\n",
    "        a = pm.Normal(\"a\", mu=mu_a, sigma=sigma_a, shape=n_groups)\n",
    "        b = pm.Normal(\"b\", mu=mu_b, sigma=sigma_b, shape=n_groups)\n",
    "\n",
    "        # Observation noise\n",
    "        sigma_obs = pm.HalfNormal(\"sigma_obs\", sigma=1)\n",
    "\n",
    "        # Expected value per group and time\n",
    "        y_est = a[:, None] + b[:, None] * time\n",
    "\n",
    "        # Likelihood\n",
    "        pm.Normal(\"y_obs\", mu=y_est, sigma=sigma_obs, observed=y)\n",
    "\n",
    "        # Inference\n",
    "        trace = pm.sample(1000, tune=1000, target_accept=0.9, cores=2)\n",
    "    return model, trace\n",
    "\n",
    "def main():\n",
    "    # 1) Simulate data\n",
    "    time, y = simulate_data()\n",
    "    # 2) Fit model\n",
    "    model, trace = fit_hierarchical_model(time, y)\n",
    "    # 3) Summarize\n",
    "    # print(az.summary(trace, var_names=[\"mu_a\",\"sigma_a\",\"mu_b\",\"sigma_b\",\"sigma_obs\"]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db1fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5080 Laptop GPU\n",
      "LinearRegression(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "<generator object Module.parameters at 0x75f06390e110>\n",
      "OrderedDict([('linear.weight', tensor([[-0.9560]])), ('linear.bias', tensor([0.1967]))])\n",
      "tensor([-0.7592], grad_fn=<ViewBackward0>)\n",
      "tensor([-1.7152], grad_fn=<ViewBackward0>)\n",
      "tensor([-2.6711], grad_fn=<ViewBackward0>)\n",
      "tensor([-3.6271], grad_fn=<ViewBackward0>)\n",
      "tensor([-4.5830], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# check if gpu is available\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model = LinearRegression(1, 1)\n",
    "    print(model)\n",
    "    print(model.parameters())\n",
    "    print(model.state_dict())\n",
    "    print(model.forward(torch.tensor([1.0])))\n",
    "    print(model.forward(torch.tensor([2.0])))\n",
    "    print(model.forward(torch.tensor([3.0])))\n",
    "    print(model.forward(torch.tensor([4.0])))\n",
    "    print(model.forward(torch.tensor([5.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685aed37",
   "metadata": {},
   "source": [
    "# Classification Algorithms\n",
    "\n",
    "Classification predicts categories (e.g. win/lose, or player position). We cover several common classifiers.\n",
    "Logistic Regression\n",
    "\n",
    "Logistic regression models the probability of a binary outcome (0/1). It uses the sigmoid (logistic) function to squeeze a linear score into [0,1]:\n",
    "y^=f(x)=11+exp⁡(−η(x)),η(x)=wTx+b,y^​=f(x)=1+exp(−η(x))1​,η(x)=wTx+b,\n",
    "so $f(x)\\approx P(Y=1|x)$\n",
    "realpython.com\n",
    ". Here $\\eta(x)$ is called the log-odds (logit). The sigmoid ensures outputs near 0 or 1, suitable for classification. For example, to predict if a team wins (1) or loses (0) based on stats: we compute $\\eta = w\\cdot x+b$ and then probability $\\hat y = \\frac1{1+e^{-\\eta}}$\n",
    "realpython.com\n",
    ".\n",
    "\n",
    "To train logistic regression, we maximize the (log-)likelihood of the binary labels or equivalently minimize binary cross-entropy loss:\n",
    "L(w,b)=−1N∑i=1N[yilog⁡(y^i)+(1−yi)log⁡(1−y^i)].L(w,b)=−N1​∑i=1N​[yi​log(y^​i​)+(1−yi​)log(1−y^​i​)].\n",
    "Gradient descent updates are similar to linear regression but use the sigmoid derivative. The weight gradient is\n",
    "∇wL=1NXT(y^−y),∇w​L=N1​XT(y^​−y),\n",
    "and $b$-gradient is $\\frac{1}{N}\\sum (\\hat y - y)$. Implementation example with a small dataset (e.g. predicts win=1 if team score exceeds opponent’s by a threshold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71669026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.6447\n",
      "Epoch 50: loss=0.2672\n",
      "Epoch 100: loss=0.2644\n",
      "Epoch 150: loss=0.2697\n",
      "Epoch 200: loss=0.2683\n",
      "Epoch 250: loss=0.2665\n",
      "Epoch 300: loss=0.2749\n",
      "Epoch 350: loss=0.2641\n",
      "Epoch 400: loss=0.2662\n",
      "Epoch 450: loss=0.2684\n",
      "Final w,b: [1.77348886] 0.7236525850267588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(y, y_pred, w, reg_lambda):\n",
    "    # binary cross-entropy + L2 penalty\n",
    "    ce = -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "    reg = (reg_lambda / (2*len(y))) * np.sum(w**2)\n",
    "    return ce + reg\n",
    "\n",
    "def train_logistic(\n",
    "    X, y,\n",
    "    lr=0.01,\n",
    "    n_iters=1000,\n",
    "    batch_size=None,\n",
    "    reg_lambda=0.0,\n",
    "    momentum=0.0,\n",
    "    verbose=False\n",
    "):\n",
    "    N, m = X.shape\n",
    "    # 1) Initialize\n",
    "    w = np.random.randn(m) * 0.01\n",
    "    b = 0.0\n",
    "    v_w = np.zeros_like(w)\n",
    "    v_b = 0.0\n",
    "\n",
    "    for epoch in range(n_iters):\n",
    "        # 2) Optionally shuffle for SGD/mini-batch\n",
    "        idx = np.random.permutation(N)\n",
    "        X_sh, y_sh = X[idx], y[idx]\n",
    "\n",
    "        # 3) Determine batches\n",
    "        batches = (\n",
    "            [ (X_sh, y_sh) ] if batch_size is None\n",
    "            else [\n",
    "                (X_sh[i:i+batch_size], y_sh[i:i+batch_size])\n",
    "                for i in range(0, N, batch_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for X_batch, y_batch in batches:\n",
    "            # 4a) Forward pass\n",
    "            z = X_batch.dot(w) + b\n",
    "            y_pred = sigmoid(z)\n",
    "\n",
    "            # 4b) Gradients w/ L2\n",
    "            dw = (X_batch.T.dot(y_pred - y_batch) / len(y_batch)\n",
    "                  + (reg_lambda/len(y_batch))*w)\n",
    "            db = np.mean(y_pred - y_batch)\n",
    "\n",
    "            # 4c) Momentum update\n",
    "            v_w = momentum*v_w + lr*dw\n",
    "            v_b = momentum*v_b + lr*db\n",
    "            w -= v_w\n",
    "            b -= v_b\n",
    "\n",
    "        if verbose and epoch % (n_iters//10 or 1) == 0:\n",
    "            loss = compute_loss(y, sigmoid(X.dot(w)+b), w, reg_lambda)\n",
    "            print(f\"Epoch {epoch}: loss={loss:.4f}\")\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[50],[55],[60],[65],[70]])\n",
    "y = np.array([0,0,1,1,1])\n",
    "# Feature‐scale\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "w, b = train_logistic(\n",
    "    X, y,\n",
    "    lr=0.05,\n",
    "    n_iters=500,\n",
    "    batch_size=2,       # mini‐batch of 2\n",
    "    reg_lambda=0.1,     # L2 strength\n",
    "    momentum=0.9,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"Final w,b:\", w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6e8a1f",
   "metadata": {},
   "source": [
    "Bayesian Logistic Regression\n",
    "\n",
    "Bayesian logistic regression treats weights as random with a prior. Unlike “classical” logistic (which finds a single best $w$ by maximum likelihood), Bayesian methods infer a posterior distribution of $w$. In simple terms, we start with a prior $p(w)$ and update via Bayes’ rule:\n",
    "p(w∣X,y)∝p(y∣X,w) p(w).p(w∣X,y)∝p(y∣X,w)p(w).\n",
    "This means the posterior ∝ (likelihood × prior)\n",
    "stats.stackexchange.com\n",
    ". For example, with a normal prior on $w$, the MAP estimate adds an $L_2$ penalty to the loss. We can approximate the posterior by sampling or variational methods (beyond basic NumPy). In practice, Bayesian logistic allows us to quantify uncertainty in predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MAP ESTIMATE ===\n",
      "Weights: [1.03697116]\n",
      "Bias:    0.5087098425833299\n",
      "\n",
      "=== LAPLACE APPROXIMATION ===\n",
      "Posterior SDs: [0.7468874  1.01527945]\n",
      "\n",
      "=== FULL BAYES WITH PYMC ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "/app/.venv/lib/python3.10/site-packages/pytensor/link/c/cmodule.py:2968: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.\n",
      "This usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.\n",
      "Alternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.\n",
      "For more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library\n",
      "  warnings.warn(\n",
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Multiprocess sampling (2 chains in 2 jobs)\n",
      "NUTS: [w, b]\n",
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082ee05d9c444bc983458db70ece4923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 1 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean     sd  hdi_2.5%  hdi_97.5%  mcse_mean  mcse_sd  ess_bulk  \\\n",
      "w[0]  1.129  0.766    -0.343      2.697      0.013    0.012    3265.0   \n",
      "b     0.275  0.730    -1.190      1.647      0.013    0.011    3242.0   \n",
      "\n",
      "      ess_tail  r_hat  \n",
      "w[0]    2687.0    1.0  \n",
      "b       2723.0    1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: [obs]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1db77fa2824964858984a794bf4940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: ppc['obs'] shape = (2, 2000, 5)\n",
      "\n",
      "=== POSTERIOR‑PREDICTIVE WIN PROBS ===\n",
      "Rating 50 → P(win) ≈ 0.27\n",
      "Rating 55 → P(win) ≈ 0.39\n",
      "Rating 60 → P(win) ≈ 0.57\n",
      "Rating 65 → P(win) ≈ 0.71\n",
      "Rating 70 → P(win) ≈ 0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n• Stronger “regularization”: \\n    - In neg_log_post: set sigma2 < 1.0\\n    - In PyMC priors: use pm.Normal(..., sigma=<0.5 or less>)\\n• Switch to L1 (sparse) prior:\\n    - In PyMC: replace w = pm.Normal(...) with pm.Laplace('w', mu=0, b=<scale>)\\n• Faster approx:\\n    - Use Laplace only; draw samples from theta ~ N(opt.x, cov_map)\\n    - Swap in ADVI: pm.fit(method='advi', ...) instead of pm.sample()\\n• Scale to large data:\\n    - In pm.fit: use method='advi_minibatch', minibatch_size=..., n → millions\\n• Partial pooling / hierarchical:\\n    - Replace w ~ Normal(μ, τ) and add hyperpriors:\\n          μ_w = pm.Normal('μ_w', 0, 1)\\n          τ_w = pm.HalfNormal('τ_w', 1)\\n          w   = pm.Normal('w', μ=μ_w, sigma=τ_w, shape=m)\\n• Feature interactions:\\n    - Expand X: e.g. X_poly = np.hstack([X, X**2, ...])\\n• Diagnostics:\\n    - Check az.plot_trace(trace)\\n    - Compute R̂ via az.rhat(trace)\\n    - Compare WAIC / LOO: az.waic(trace), az.loo(trace)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "bayes_logreg.py\n",
    "\n",
    "A from‑scratch, step‑by‑step guide to Bayesian logistic regression.\n",
    "Includes:\n",
    "  • Data setup\n",
    "  • MAP estimation (L2 ‑ ridge)\n",
    "  • Laplace approximation around the MAP\n",
    "  • Full posterior sampling with PyMC (NUTS)\n",
    "  • Posterior predictive checks\n",
    "Adjustable parameters and notes indicated via comments throughout.\n",
    "\"\"\"\n",
    "\n",
    "# 0) Imports\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import math\n",
    "\n",
    "# 1) Data setup\n",
    "# ------------------------\n",
    "# Binary outcome y = win (1) / loss (0)\n",
    "X_raw = np.array([50, 55, 60, 65, 70])[:, None]\n",
    "y     = np.array([0,   0,  1,  1,  1])\n",
    "# Standardize features for stability\n",
    "X = (X_raw - X_raw.mean()) / X_raw.std()\n",
    "N, m = X.shape\n",
    "\n",
    "# 2) Helper functions\n",
    "# ------------------------\n",
    "def sigmoid(z):\n",
    "    \"\"\"Standard logistic sigmoid.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def neg_log_post(theta, X, y, sigma2=1.0):\n",
    "    \"\"\"\n",
    "    Negative log‑posterior for MAP:\n",
    "      theta = [w_0, ..., w_{m-1}, b]\n",
    "      prior: w ~ N(0, sigma2 * I)\n",
    "    Adjust sigma2 ↓ for stronger shrinkage.\n",
    "    \"\"\"\n",
    "    w, b = theta[:-1], theta[-1]\n",
    "    z = X.dot(w) + b\n",
    "    ll = np.sum(y * np.log(sigmoid(z)) + (1 - y) * np.log(1 - sigmoid(z)))\n",
    "    # log‑prior ∝ - 0.5 * w^T w / sigma2\n",
    "    log_prior = -0.5 * np.sum(w**2) / sigma2\n",
    "    return -(ll + log_prior)   # we minimize this\n",
    "\n",
    "# 3) MAP estimation\n",
    "# ------------------------\n",
    "# Initial guess: zero weights & bias\n",
    "theta0 = np.zeros(m + 1)\n",
    "# Run BFGS to find MAP\n",
    "opt = minimize(\n",
    "    neg_log_post, theta0,\n",
    "    args=(X, y, 1.0),       # tweak sigma2 here (e.g. 0.5 → stronger prior)\n",
    "    method='BFGS',\n",
    "    options={'gtol': 1e-6, 'maxiter': 1000}\n",
    ")\n",
    "w_map, b_map = opt.x[:-1], opt.x[-1]\n",
    "print(\"=== MAP ESTIMATE ===\")\n",
    "print(\"Weights:\", w_map)\n",
    "print(\"Bias:   \", b_map)\n",
    "\n",
    "# 4) Laplace approximation\n",
    "# ------------------------\n",
    "# Approximate posterior covariance ≈ inverse Hessian at MAP\n",
    "hess_inv = opt.hess_inv      # SciPy BFGS gives you this\n",
    "cov_map  = hess_inv           # shape (m+1, m+1)\n",
    "print(\"\\n=== LAPLACE APPROXIMATION ===\")\n",
    "print(\"Posterior SDs:\", np.sqrt(np.diag(cov_map)))\n",
    "\n",
    "# To draw from Laplace‑Gaussian:\n",
    "# theta_samples = np.random.multivariate_normal(opt.x, cov_map, size=5000)\n",
    "\n",
    "# 5) Full Bayesian inference with PyMC (NUTS)\n",
    "# ------------------------\n",
    "print(\"\\n=== FULL BAYES WITH PYMC ===\")\n",
    "with pm.Model() as bayes_logreg:\n",
    "    # priors: Normal(0, 1) for each weight & bias; adjust sd for stronger/weaker shrinkage\n",
    "    w = pm.Normal('w', mu=0, sigma=1, shape=m)\n",
    "    b = pm.Normal('b', mu=0, sigma=1)\n",
    "\n",
    "    # linear predictor and likelihood\n",
    "    logits = pm.math.dot(X, w) + b\n",
    "    pm.Bernoulli('obs', logit_p=logits, observed=y)\n",
    "\n",
    "    # Inference: NUTS sampler\n",
    "    trace = pm.sample(\n",
    "        draws=2000,       # post‑tuning samples per chain\n",
    "        tune=1000,        # burn‑in\n",
    "        target_accept=0.9,\n",
    "        chains=2,\n",
    "        return_inferencedata=True\n",
    "    )\n",
    "\n",
    "# Summarize posterior\n",
    "print(az.summary(trace, var_names=['w', 'b'], hdi_prob=0.95))\n",
    "\n",
    "# 6) Posterior‑predictive probabilities (robust fix)\n",
    "# --------------------------------------------------\n",
    "with bayes_logreg:\n",
    "    ppc = pm.sample_posterior_predictive(\n",
    "        trace,\n",
    "        var_names=['obs'],\n",
    "        return_inferencedata=False\n",
    "    )\n",
    "\n",
    "# Inspect the raw shape to guide our averaging\n",
    "print(\"DEBUG: ppc['obs'] shape =\", ppc['obs'].shape)\n",
    "\n",
    "# Collapse any extra sample dims into one, then average\n",
    "samples = ppc['obs']\n",
    "if samples.ndim > 2:\n",
    "    samples = samples.reshape(-1, samples.shape[-1])\n",
    "pp_win_probs = samples.mean(axis=0)\n",
    "\n",
    "# Print nicely by converting each to Python scalars\n",
    "print(\"\\n=== POSTERIOR‑PREDICTIVE WIN PROBS ===\")\n",
    "for xi, p in zip(X_raw.flatten(), pp_win_probs):\n",
    "    print(f\"Rating {int(xi):>2} → P(win) ≈ {float(p):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 7) Where to adjust next\n",
    "# ------------------------\n",
    "\"\"\"\n",
    "• Stronger “regularization”: \n",
    "    - In neg_log_post: set sigma2 < 1.0\n",
    "    - In PyMC priors: use pm.Normal(..., sigma=<0.5 or less>)\n",
    "• Switch to L1 (sparse) prior:\n",
    "    - In PyMC: replace w = pm.Normal(...) with pm.Laplace('w', mu=0, b=<scale>)\n",
    "• Faster approx:\n",
    "    - Use Laplace only; draw samples from theta ~ N(opt.x, cov_map)\n",
    "    - Swap in ADVI: pm.fit(method='advi', ...) instead of pm.sample()\n",
    "• Scale to large data:\n",
    "    - In pm.fit: use method='advi_minibatch', minibatch_size=..., n → millions\n",
    "• Partial pooling / hierarchical:\n",
    "    - Replace w ~ Normal(μ, τ) and add hyperpriors:\n",
    "          μ_w = pm.Normal('μ_w', 0, 1)\n",
    "          τ_w = pm.HalfNormal('τ_w', 1)\n",
    "          w   = pm.Normal('w', μ=μ_w, sigma=τ_w, shape=m)\n",
    "• Feature interactions:\n",
    "    - Expand X: e.g. X_poly = np.hstack([X, X**2, ...])\n",
    "• Diagnostics:\n",
    "    - Check az.plot_trace(trace)\n",
    "    - Compute R̂ via az.rhat(trace)\n",
    "    - Compare WAIC / LOO: az.waic(trace), az.loo(trace)\n",
    "\"\"\"\n",
    "\n",
    "# If you like, add final plotting or saving of results here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65804f",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN is a simple non-parametric classifier (or regressor) based on distance. Given a query point, it finds the k closest training samples (by Euclidean distance) and uses their labels. For classification, KNN predicts by majority vote of neighbors; for regression, by averaging neighbors\n",
    "digitalocean.com\n",
    ". For example, to classify a player as “good” or “average” (0/1), we might look at the three nearest players in feature space and choose the majority label.\n",
    "\n",
    "An implementation outline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32421ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted position for player [192.  88.]: F\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Synthetic dataset: [height (cm), weight (kg)] -> Position (F=Forward, G=Guard)\n",
    "players = np.array([\n",
    "    [198, 95, 'F'],   # a taller, heavier Forward\n",
    "    [202, 100, 'F'], \n",
    "    [180, 78, 'G'],   # a shorter, lighter Guard\n",
    "    [175, 68, 'G'],\n",
    "    [188, 85, 'F'],\n",
    "    [190, 90, 'F'],\n",
    "    [170, 72, 'G'],\n",
    "    [185, 80, 'F']\n",
    "])\n",
    "X_train = players[:, :2].astype(float)\n",
    "y_train = players[:, 2]\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    return math.sqrt(np.sum((p1 - p2)**2))\n",
    "\n",
    "def knn_predict(x_query, X_train, y_train, k):\n",
    "    # Compute distances from x_query to all training points\n",
    "    distances = [euclidean_distance(x_query, x) for x in X_train]\n",
    "    # Get the indices of the k smallest distances\n",
    "    nn_idx = np.argsort(distances)[:k]\n",
    "    nn_labels = y_train[nn_idx]\n",
    "    # Majority vote\n",
    "    vote = Counter(nn_labels).most_common(1)[0][0]\n",
    "    return vote\n",
    "\n",
    "# Predict position for a new player of height=192cm, weight=88kg using k=3\n",
    "new_player = np.array([192.0, 88.0])\n",
    "pred_position = knn_predict(new_player, X_train, y_train, k=3)\n",
    "print(f\"Predicted position for player {new_player}: {pred_position}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb7c59",
   "metadata": {},
   "source": [
    "Decision Trees\n",
    "Theory – Splitting Criteria and Tree Construction\n",
    "\n",
    "A Decision Tree is a flowchart-like tree structure where each internal node represents a decision on a feature, each branch is an outcome of that decision, and each leaf node represents a prediction (class label or numeric value). They can handle both classification and regression tasks and are valued for their interpretability (rules are easy to explain)\n",
    "medium.com\n",
    ".\n",
    "\n",
    "How it works: The tree is built by recursively splitting the data based on features. The algorithm (e.g., CART – Classification and Regression Tree) searches for the feature and threshold that best purifies the data at each step. Purity is measured by metrics like:\n",
    "\n",
    "    Gini Impurity: G=1−∑jpj2,G=1−∑j​pj2​, where pjpj​ is the fraction of examples of class j in a node. Gini is 0 for a perfectly pure node (all examples same class) and highest when classes are evenly mixed\n",
    "    quantdare.com\n",
    "    .\n",
    "\n",
    "    Entropy: H=−∑jpjlog⁡2pj,H=−∑j​pj​log2​pj​, another measure of impurity from information theory\n",
    "    quantdare.com\n",
    "    . Entropy is 0 when pure and maximal when classes are equally mixed (e.g., 50/50 gives entropy = 1 bit).\n",
    "\n",
    "    Information Gain: The decrease in entropy from a split. If a node has entropy H before splitting and the weighted entropy of children after splitting is H<sub>split</sub>, then Information Gain = H - H<sub>split</sub>\n",
    "    en.wikipedia.org\n",
    "    en.wikipedia.org\n",
    "    . The algorithm seeks the split that maximizes this gain (or equivalently, maximizes Gini reduction).\n",
    "\n",
    "For regression trees, a common criterion is to minimize the variance (MSE) within each partition.\n",
    "\n",
    "Splitting process: Starting at the root (all data), the tree chooses the best feature and threshold to split on (e.g., “if assists > 5 go left, else go right”), based on highest information gain\n",
    "en.wikipedia.org\n",
    "en.wikipedia.org\n",
    ". This creates two child nodes. The process repeats recursively: at each non-leaf node, evaluate all possible splits to find the best one. Splitting stops when a stopping criterion is met, e.g., maximum tree depth, minimum samples in a node, or no further impurity reduction (perfectly pure or all features used).\n",
    "\n",
    "Leaf predictions: In classification, a leaf is assigned a class (often the majority class of training examples in that leaf). In regression, the leaf predicts a numeric value (often the mean of targets in that leaf).\n",
    "\n",
    "Avoiding overfitting: Unrestricted trees can grow very deep and complex, perfectly fitting training data (zero impurity leaves), which often overfits. To prevent this, we use pre-pruning (stop splits early if data gets too sparse or depth too high) or post-pruning (grow a full tree, then prune back leaves that don’t generalize well via validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0af6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best split: (1, 28, 0.20833333333333326, 0.2777777777777777, 0.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example sports game data: [home_court, star_player_points] -> win(1)/lose(0)\n",
    "# home_court: 1 if playing at home, 0 if away\n",
    "data = np.array([\n",
    "    [1, 30, 1],  # home, star scored 30 -> win\n",
    "    [1, 15, 0],  # home, star scored 15 -> lose\n",
    "    [0, 25, 0],  # away, 25 -> lose\n",
    "    [0, 28, 0],  # away, 28 -> lose\n",
    "    [1, 8,  0],  # home, only 8 points -> lose\n",
    "    [0, 35, 1],  # away, 35 -> win (away win unusual but star played exceptionally)\n",
    "    [1, 20, 1],  # home, 20 -> win\n",
    "    [0, 10, 0]   # away, 10 -> lose\n",
    "])\n",
    "X = data[:, :2]\n",
    "y = data[:, 2]\n",
    "\n",
    "# Function to compute Gini impurity of a label array\n",
    "def gini_impurity(labels):\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    p = np.mean(labels)  # proportion of class 1\n",
    "    q = 1 - p            # proportion of class 0\n",
    "    return 1 - (p**2 + q**2)\n",
    "\n",
    "# Try all possible splits and find best (brute-force for demonstration)\n",
    "best_gini = 1.0\n",
    "best_rule = None\n",
    "for feature in [0, 1]:\n",
    "    values = np.unique(X[:, feature])\n",
    "    for val in values:\n",
    "        # For numeric feature, consider splitting <= val vs > val \n",
    "        left_idx = (X[:, feature] <= val)\n",
    "        right_idx = (X[:, feature] > val)\n",
    "        gini_left = gini_impurity(y[left_idx])\n",
    "        gini_right = gini_impurity(y[right_idx])\n",
    "        # weighted gini after split\n",
    "        n_left, n_right = np.sum(left_idx), np.sum(right_idx)\n",
    "        gini_split = (n_left * gini_left + n_right * gini_right) / len(y)\n",
    "        if gini_split < best_gini:\n",
    "            best_gini = gini_split\n",
    "            best_rule = (feature, val, gini_split, gini_left, gini_right)\n",
    "\n",
    "print(\"Best split:\", best_rule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113bacfc",
   "metadata": {},
   "source": [
    "Random Forests\n",
    "Theory – Ensemble of Decision Trees\n",
    "\n",
    "A Random Forest is an ensemble method that combines multiple decision trees to improve robustness and accuracy\n",
    "medium.com\n",
    ". The core idea is to build many decision trees on random subsets of the data and features, and then aggregate their predictions. It’s like getting a “wisdom of the crowd” from many tree models.\n",
    "\n",
    "Key elements of Random Forests:\n",
    "\n",
    "    Bootstrap Aggregating (Bagging): From the training set of size n, sample B new datasets of size n with replacement (each tree gets a bootstrap sample)\n",
    "    en.wikipedia.org\n",
    "    . Each tree is trained on its bootstrap sample. This introduces diversity among trees (each tree sees a slightly different dataset).\n",
    "\n",
    "    Random Feature Subset: When splitting a node, instead of considering all features, Random Forests pick a random subset of features and find the best split among those\n",
    "    en.wikipedia.org\n",
    "    en.wikipedia.org\n",
    "    . For classification, a common default is dd\n",
    "\n",
    "    ​ features (if there are d total features)\n",
    "    en.wikipedia.org\n",
    "    . This further decorrelates the trees, so they don’t all choose the same dominant feature.\n",
    "\n",
    "    Tree independence: Each tree is grown to full depth (or with minimal pruning) on its bootstrap data and feature subsets. The trees will overfit individually, but since they’re all different, their errors can cancel out when averaged.\n",
    "\n",
    "    Aggregation: For classification, the forest takes a majority vote among the trees’ predicted classes. For regression, it averages the predicted values of all trees\n",
    "    en.wikipedia.org\n",
    "    . This averaging significantly reduces variance compared to a single tree\n",
    "    en.wikipedia.org\n",
    "    en.wikipedia.org\n",
    "    , resulting in a more stable and accurate model.\n",
    "\n",
    "The random forest algorithm can be summarized as\n",
    "en.wikipedia.org\n",
    "en.wikipedia.org\n",
    ":\n",
    "\n",
    "    For b = 1 to B (number of trees):\n",
    "\n",
    "        Sample n examples from training data with replacement (bootstrap sample).\n",
    "\n",
    "        Train a decision tree TbTb​ on this sample. At each split, consider a random subset of features (of size k) instead of all d features, choose the best split among those.\n",
    "\n",
    "    To predict a new example:\n",
    "\n",
    "        Get the prediction from each tree Tb(x)Tb​(x).\n",
    "\n",
    "        Classification: output the majority-vote class. Regression: output the average 1B∑b=1BTb(x)B1​∑b=1B​Tb​(x)\n",
    "        en.wikipedia.org\n",
    "        .\n",
    "\n",
    "The combination of bagging and feature randomness makes Random Forests resistant to overfitting; even though each tree might overfit, their average can still generalize well\n",
    "en.wikipedia.org\n",
    ". They also provide useful features like out-of-bag (OOB) error (each training instance can be predicted by trees that didn’t see it in training, giving an internal validation estimate)\n",
    "en.wikipedia.org\n",
    "en.wikipedia.org\n",
    "and feature importance (measuring how much each feature split improved the purity on average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df460b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1 prediction on all games: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tree 2 prediction on all games: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tree 3 prediction on all games: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Random Forest combined prediction: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual outcomes:                [1, 0, 0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Let's manually create 3 bootstrap samples from our previous data\n",
    "np.random.seed(1)\n",
    "B = 3\n",
    "n = len(X)\n",
    "boot_samples = [data[np.random.choice(n, n, replace=True)] for _ in range(B)]\n",
    "\n",
    "# Train a simple stump (1-level tree) on each bootstrap by splitting on HomeCourt\n",
    "predictions = []\n",
    "for b_idx, sample in enumerate(boot_samples):\n",
    "    X_b, y_b = sample[:, :2], sample[:, 2]\n",
    "    # Train a stump: we'll use the best split we found before (HomeCourt <= 0)\n",
    "    # Determine majority class for home=0 and home=1 in this sample\n",
    "    home_mask = (X_b[:,0] > 0)\n",
    "    pred_home = 1 if np.mean(y_b[home_mask]) >= 0.5 else 0\n",
    "    pred_away = 1 if np.mean(y_b[~home_mask]) >= 0.5 else 0\n",
    "    # Define a simple prediction function for this stump\n",
    "    def tree_predict(instance):\n",
    "        return pred_home if instance[0] > 0 else pred_away\n",
    "    # Record predictions of this tree on original data\n",
    "    preds = [tree_predict(xi) for xi in X]\n",
    "    predictions.append(preds)\n",
    "    print(f\"Tree {b_idx+1} prediction on all games: {preds}\")\n",
    "\n",
    "# Combine predictions by majority vote\n",
    "predictions = np.array(predictions)  # shape (B, n)\n",
    "final_preds = (np.mean(predictions, axis=0) >= 0.5).astype(int)\n",
    "print(\"Random Forest combined prediction:\", final_preds.tolist())\n",
    "print(\"Actual outcomes:               \", y.astype(int).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88071edb",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines classify data by finding the hyperplane that maximizes the margin between classes\n",
    "analyticsvidhya.com\n",
    "analyticsvidhya.com\n",
    ". The margin is the distance from the hyperplane to the nearest data points (support vectors). Formally, SVM solves:\n",
    "min⁡w,b 12∥w∥2s.t.  yi(wTxi+b)≥1 ∀i,minw,b​ 21​∥w∥2s.t.yi​(wTxi​+b)≥1 ∀i,\n",
    "seeking the widest margin. The result is a decision boundary that separates classes as much as possible. In practice, soft margins and kernels allow handling noisy and non-linear data. SVM predictions are $\\text{sign}(w^T x + b)$.\n",
    "\n",
    "    Margin maximization: “The goal is to maximize the minimum distance” between any training point and the decision boundary\n",
    "    analyticsvidhya.com\n",
    "    .\n",
    "\n",
    "    Non-linear data: Kernels map data to higher dimensions so that a linear hyperplane can separate them. SVM is powerful for complex boundaries\n",
    "    analyticsvidhya.com\n",
    "    .\n",
    "\n",
    "A simple linear SVM can be implemented by minimizing the hinge loss $L_i=\\max(0,1-y_i(w^T x_i+b))$. Gradient descent would add updates only for misclassified or “margin-violating” points. (Detailed code is lengthy, but conceptually one iterates: if $y_i(w^T x_i+b)<1$, update $w$ by $w \\leftarrow w + \\eta y_i x_i$, otherwise apply regularization.) The key outcome is a robust classifier with maximum margin\n",
    "analyticsvidhya.com\n",
    "analyticsvidhya.com\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weight vector: [0.64964788 0.26271842]\n",
      "Learned bias: -4.6\n",
      "Decision function output for new game: 0.48726835737247143\n",
      "Predicted outcome: Win\n"
     ]
    }
   ],
   "source": [
    "# Synthetic data: [offense_rating, defense_rating] -> win (+1) or lose (-1)\n",
    "X = np.array([\n",
    "    [8, 5],   # strong offense, moderate defense -> win\n",
    "    [3, 7],   # weak offense, strong defense -> (assume) lose\n",
    "    [6, 6],   # balanced -> win\n",
    "    [4, 4],   # weak both -> lose\n",
    "])\n",
    "y = np.array([1, -1, 1, -1])  # +1 = win, -1 = lose\n",
    "\n",
    "# Initialize weights and bias\n",
    "w = np.zeros(X.shape[1])\n",
    "b = 0.0\n",
    "C = 1.0      # regularization parameter\n",
    "alpha = 0.1  # learning rate\n",
    "\n",
    "# Training using sub-gradient descent for hinge loss\n",
    "for epoch in range(1000):\n",
    "    for i in range(len(X)):\n",
    "        if y[i] * (np.dot(w, X[i]) + b) < 1:\n",
    "            # misclassified or within margin\n",
    "            w = w - alpha * (w - C * y[i] * X[i])  # sub-gradient: w term + hinge term\n",
    "            b = b + alpha * C * y[i]               # update for bias\n",
    "        else:\n",
    "            # correctly classified and outside margin\n",
    "            w = w - alpha * w                      # only regularization term\n",
    "            # b update has no hinge term when correctly classified\n",
    "\n",
    "# Results\n",
    "print(\"Learned weight vector:\", w)\n",
    "print(\"Learned bias:\", b)\n",
    "# Predict on a new example\n",
    "new_game = np.array([5, 7])\n",
    "score = np.dot(w, new_game) + b\n",
    "pred_label = 1 if score >= 0 else -1\n",
    "print(\"Decision function output for new game:\", score)\n",
    "print(\"Predicted outcome:\", \"Win\" if pred_label==1 else \"Lose\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110a733",
   "metadata": {},
   "source": [
    "Naive Bayes\n",
    "Theory – Bayes’ Theorem with Independence Assumption\n",
    "\n",
    "Naive Bayes is a probabilistic classifier based on applying Bayes’ Theorem with a strong assumption: all features are conditionally independent given the class\n",
    "geeksforgeeks.org\n",
    ". Despite this “naive” assumption (which is often false in reality), the classifier performs surprisingly well in many domains, especially text classification.\n",
    "\n",
    "Bayes’ Theorem reminds us that for class CC and features X=(x1,…,xn)X=(x1​,…,xn​):\n",
    "P(C∣X)=P(X∣C) P(C)P(X).P(C∣X)=P(X)P(X∣C)P(C)​.\n",
    "\n",
    "For classification, we compare P(C=k∣X)P(C=k∣X) across classes k and choose the largest. Because P(X)P(X) is the same for all classes, essentially we compute the unnormalized posteriors:\n",
    "P(C=k)∏i=1nP(xi∣C=k) ,P(C=k)∏i=1n​P(xi​∣C=k) ,\n",
    "and pick the argmax class\n",
    "geeksforgeeks.org\n",
    "geeksforgeeks.org\n",
    ".\n",
    "\n",
    "The naive independence assumption says:\n",
    "P(x1,x2,…,xn∣C=k)=∏i=1nP(xi∣C=k) :contentReference[oaicite:87]index=87.P(x1​,x2​,…,xn​∣C=k)=∏i=1n​P(xi​∣C=k) :contentReference[oaicite:87]index=87.\n",
    "This drastically simplifies the model, as we just need the distribution of each feature in each class.\n",
    "\n",
    "To train a Naive Bayes classifier:\n",
    "\n",
    "    Compute priors: P(C=k)P(C=k) is estimated by frequency (e.g., proportion of training examples in class k)\n",
    "    geeksforgeeks.org\n",
    "    .\n",
    "\n",
    "    Compute likelihoods: For each feature ii and class k, estimate P(xi∣C=k)P(xi​∣C=k). Depending on feature type:\n",
    "\n",
    "        If xixi​ is categorical (e.g., weather = sunny/overcast/rainy), we use the frequency in class k (with possible Laplace smoothing to handle zero frequencies).\n",
    "\n",
    "        If xixi​ is continuous, a common approach is to assume a Gaussian Naive Bayes: fit a Gaussian distribution N(μk,i,σk,i2)N(μk,i​,σk,i2​) for feature i in class k, and use its density P(xi∣C=k)=12πσk,iexp⁡(−(xi−μk,i)2/(2σk,i2))P(xi​∣C=k)=2π\n",
    "\n",
    "        ​σk,i​1​exp(−(xi​−μk,i​)2/(2σk,i2​)). Alternatively, use other distributions if appropriate.\n",
    "\n",
    "Prediction: Compute scorek=log⁡P(C=k)+∑i=1nlog⁡P(xi∣C=k)scorek​=logP(C=k)+∑i=1n​logP(xi​∣C=k) for each class k, then predict class = argmax_k score_k. (Log is used to avoid underflow and turn products into sums.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca87d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-posteriors: {'Curveball': -27.041563918557433, 'Fastball': -11.62369561051438}\n",
      "Predicted class for pitch [  90 2400]: Fastball\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Toy dataset: [speed (mph), spin_rate (rpm)] -> pitch type ('Fastball' or 'Curveball')\n",
    "data = np.array([\n",
    "    [95, 2200, 'Fastball'],\n",
    "    [98, 2100, 'Fastball'],\n",
    "    [92, 2300, 'Fastball'],\n",
    "    [75, 2500, 'Curveball'],\n",
    "    [78, 2700, 'Curveball'],\n",
    "    [73, 2600, 'Curveball']\n",
    "], dtype=object)\n",
    "\n",
    "X = data[:, :2].astype(float)\n",
    "y = data[:, 2]\n",
    "\n",
    "# Separate by class\n",
    "classes = np.unique(y)\n",
    "class_stats = {}  # will hold prior, mean, var for each class\n",
    "for c in classes:\n",
    "    X_c = X[y==c]\n",
    "    prior = len(X_c) / len(X)\n",
    "    mean = X_c.mean(axis=0)\n",
    "    var = X_c.var(axis=0, ddof=1)  # sample variance\n",
    "    class_stats[c] = {\"prior\": prior, \"mean\": mean, \"var\": var}\n",
    "\n",
    "# Define Gaussian PDF\n",
    "def gaussian_pdf(x, mean, var):\n",
    "    # univariate formula applied element-wise for multi-dim (assumes independence)\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * var)\n",
    "    exp = np.exp(- (x-mean)**2 / (2*var))\n",
    "    return coeff * exp\n",
    "\n",
    "# Prediction for a new pitch\n",
    "new_pitch = np.array([90, 2400])  # a pitch with 90 mph speed and 2400 rpm spin\n",
    "posteriors = {}\n",
    "for c, stats in class_stats.items():\n",
    "    # start with log prior\n",
    "    log_prob = np.log(stats[\"prior\"])\n",
    "    # add log likelihoods for each feature under Gaussian\n",
    "    log_prob += np.sum(np.log(gaussian_pdf(new_pitch, stats[\"mean\"], stats[\"var\"])))\n",
    "    posteriors[c] = log_prob\n",
    "\n",
    "print(\"Log-posteriors:\", posteriors)\n",
    "pred_class = max(posteriors, key=posteriors.get)\n",
    "print(f\"Predicted class for pitch {new_pitch}: {pred_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b34714",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting)\n",
    "Theory – Gradient Boosted Decision Trees\n",
    "\n",
    "XGBoost is an optimized implementation of gradient boosting decision trees – one of the most successful machine learning techniques in structured data. Gradient Boosting builds an ensemble of trees sequentially, where each new tree corrects errors of the previous ones\n",
    "geeksforgeeks.org\n",
    "geeksforgeeks.org\n",
    ". “Extreme” Gradient Boosting (XGBoost) introduces engineering optimizations and regularization that make it particularly powerful and fast.\n",
    "\n",
    "Gradient Boosting Recap: Unlike Random Forests (parallel ensemble), boosting is a sequential ensemble:\n",
    "\n",
    "    Start with an initial model F0(x)F0​(x), often a constant prediction (e.g., mean of targets for regression, or log-odds for classification)\n",
    "    geeksforgeeks.org\n",
    "    .\n",
    "\n",
    "    For each iteration t=1 to Tt=1 to T:\n",
    "\n",
    "        Compute the residuals (for regression) or negative gradients of the loss function (for general boosting) for each training example. These represent “where the model is getting things wrong”. For example, in regression with MSE loss, the residual is ri=yi−Ft−1(xi)ri​=yi​−Ft−1​(xi​).\n",
    "\n",
    "        Train a new decision tree ht(x)ht​(x) to predict these residuals (i.e., fit the tree to the errors made so far).\n",
    "\n",
    "        Add the new tree into the model with a scaling factor: Ft(x)=Ft−1(x)+η ht(x)Ft​(x)=Ft−1​(x)+ηht​(x). Here ηη is the learning rate (shrinkage factor) to control how much each tree contributes\n",
    "        geeksforgeeks.org\n",
    "        .\n",
    "\n",
    "    The final model FT(x)FT​(x) is the sum of all trees’ predictions\n",
    "    geeksforgeeks.org\n",
    "    geeksforgeeks.org\n",
    "    .\n",
    "\n",
    "Each tree is typically a small CART (often restricted depth), and the boosting process gradually improves performance. Essentially, the model is “learning from mistakes”: each tree addresses the shortcomings of the ensemble so far\n",
    "geeksforgeeks.org\n",
    "geeksforgeeks.org\n",
    ".\n",
    "\n",
    "XGBoost specifics: XGBoost includes additional features:\n",
    "\n",
    "    Regularization: It adds regularization terms to the objective to penalize complex trees (e.g., many leaves or large leaf weights)\n",
    "    geeksforgeeks.org\n",
    "    geeksforgeeks.org\n",
    "    . This helps prevent overfitting, making XGBoost often generalize better than an un-regularized boosting.\n",
    "\n",
    "    Second-order optimization: When fitting each tree, XGBoost uses both first and second derivatives of the loss (if available) to approximate the optimum tree more efficiently (a technique known as Newton boosting).\n",
    "\n",
    "    Column subsampling: Similar to Random Forest, it can sample features (columns) for each tree or split, adding extra randomness.\n",
    "\n",
    "    Hardware optimization: XGBoost is optimized in C++ for speed, uses cache-aware block structures, and supports parallelization of certain tasks (though boosting is inherently sequential, tree construction can use multiple cores).\n",
    "\n",
    "In summary, XGBoost’s objective at each iteration is: find tree ht(x)ht​(x) that maximally improves\n",
    "objt=∑i=1nl(yi,Ft−1(xi)+ht(xi))+Ω(ht),objt​=∑i=1n​l(yi​,Ft−1​(xi​)+ht​(xi​))+Ω(ht​),\n",
    "where ll is the loss (e.g., squared error or logistic loss) and Ω(ht)Ω(ht​) is the regularization for the new tree (depends on number of leaves and leaf weights)\n",
    "geeksforgeeks.org\n",
    "geeksforgeeks.org\n",
    ". It uses a greedy algorithm to grow the tree that maximizes this improvement (often using gain formulas derived from gradients and Hessians).\n",
    "Python Implementation (Toy Example)\n",
    "\n",
    "Implementing full XGBoost from scratch is very complex. Instead, we illustrate a simple gradient boosting procedure on a small dataset to demonstrate the concept:\n",
    "\n",
    "Suppose we have a dataset of team ratings (offense and defense) to predict point margin (a regression task: positive means win by that many points, negative means lose). We’ll do a few rounds of boosting with very simple trees (stumps) to see how the ensemble improves predictions step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b07a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial prediction (mean): [3.5 3.5 3.5 3.5]\n",
      "Round 1: feature0 <= 5, add {left:-4.0, right:4.0}, new pred = [ 7.5 -0.5  7.5 -0.5]\n",
      "Round 2: feature0 <= 3, add {left:-2.5, right:0.8}, new pred = [ 8.33333333  0.33333333  8.33333333 -3.        ]\n"
     ]
    }
   ],
   "source": [
    "# Toy dataset: [offense_rating, defense_rating] -> point_margin (actual score difference)\n",
    "X = np.array([[8, 6], [5, 7], [6, 6], [3, 4]]) \n",
    "y = np.array([10, 2, 5, -3])  # e.g., team1 won by 10, team2 won by 2, etc.\n",
    "\n",
    "# Initialize model with a prediction (say mean of y)\n",
    "y_mean = np.mean(y)\n",
    "pred = np.full_like(y, y_mean, dtype=float)\n",
    "print(\"Initial prediction (mean):\", pred)\n",
    "\n",
    "# Iteratively add \"trees\"\n",
    "for t in range(2):  # 2 boosting rounds for demo\n",
    "    residual = y - pred  # compute residuals\n",
    "    # Fit a simple tree (stump) to residual: we'll just pick one feature and threshold manually for simplicity\n",
    "    # Determine which feature better splits residuals\n",
    "    best_feat, best_thresh, best_pred_left, best_pred_right = None, None, None, None\n",
    "    best_error = float('inf')\n",
    "    for feat in [0, 1]:\n",
    "        vals = X[:, feat]\n",
    "        for thresh in np.unique(vals):\n",
    "            left_mask = (X[:, feat] <= thresh)\n",
    "            right_mask = (X[:, feat] > thresh)\n",
    "            if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "                continue\n",
    "            # predict residual mean in each region\n",
    "            pred_left = np.mean(residual[left_mask])\n",
    "            pred_right = np.mean(residual[right_mask])\n",
    "            # calculate squared error of this split\n",
    "            error = np.sum((residual[left_mask] - pred_left)**2) + np.sum((residual[right_mask] - pred_right)**2)\n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_feat = feat\n",
    "                best_thresh = thresh\n",
    "                best_pred_left = pred_left\n",
    "                best_pred_right = pred_right\n",
    "    # Update predictions\n",
    "    update = np.where(X[:, best_feat] <= best_thresh, best_pred_left, best_pred_right)\n",
    "    pred += update  # add this tree's predictions\n",
    "    print(f\"Round {t+1}: feature{best_feat} <= {best_thresh}, add {{left:{best_pred_left:.1f}, right:{best_pred_right:.1f}}}, new pred = {pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6f619",
   "metadata": {},
   "source": [
    "Neural Networks (Feedforward)\n",
    "Theory – Neural Architecture and Backpropagation\n",
    "\n",
    "Neural networks are inspired by the brain’s neuron connections and are a class of non-linear models that can approximate very complex functions. Here we consider a basic feedforward neural network (also called a multilayer perceptron, MLP) with an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "Architecture: Neurons in each layer take a weighted sum of inputs from the previous layer, apply an activation function, and pass the result to the next layer. For example, a simple 1-hidden-layer network for classification might be:\n",
    "\n",
    "    Input layer: takes feature vector xx (e.g., a player’s stats).\n",
    "\n",
    "    Hidden layer: computes h=f(W1x+b1)h=f(W1​x+b1​), where W1W1​ are weights and b1b1​ biases for layer1, and f(⋅)f(⋅) is a non-linear activation (e.g., ReLU or sigmoid). This produces hidden features hh.\n",
    "\n",
    "    Output layer: computes o=g(W2h+b2)o=g(W2​h+b2​), where gg could be a sigmoid for binary output, softmax for multi-class, or identity for regression. oo gives the network’s output (e.g., predicted probability of win, or predicted points).\n",
    "\n",
    "Example: If we input a vector of a player’s game stats, the hidden layer might learn abstract features like “fatigue level” or “momentum”, combining raw stats non-linearly, and the output could be a predicted chance of win.\n",
    "\n",
    "Training – Backpropagation: Neural networks are trained by defining a loss function (e.g., cross-entropy for classification or MSE for regression) and using gradient descent (or advanced optimizers) to adjust weights to minimize the loss. The gradients of the weights are computed by backpropagation, which is essentially the chain rule of calculus applied through the network layers\n",
    "geeksforgeeks.org\n",
    "geeksforgeeks.org\n",
    ":\n",
    "\n",
    "    Do a forward pass: compute outputs of each layer up to the final output.\n",
    "\n",
    "    Compute the loss compared to the true label.\n",
    "\n",
    "    Propagate the error gradient backward:\n",
    "\n",
    "        Compute gradient at output layer (difference between prediction and target, times derivative of output activation).\n",
    "\n",
    "        For each hidden layer in reverse, compute its gradient by taking the weighted sum of gradients from the layer above and multiplying by the derivative of its activation function (the chain rule).\n",
    "\n",
    "    Update weights with gradient descent: W:=W−α∂loss∂WW:=W−α∂W∂loss​ (similarly for biases).\n",
    "\n",
    "For instance, in a 1-hidden-layer network with sigmoid activations, if a2=σ(W2a1)a2​=σ(W2​a1​) is the output and we have squared error loss, the formula for weight updates would involve terms like (a2−y)σ′(W2a1)(a2​−y)σ′(W2​a1​) for output layer and propagate further to hidden layer weights\n",
    "geeksforgeeks.org\n",
    "geeksforgeeks.org\n",
    ".\n",
    "\n",
    "The key is that every weight in the network influences the loss and backprop computes those influences efficiently.\n",
    "\n",
    "Neural networks can capture complex interactions: in sports, this means a NN could learn something like “if player is tall and fast, that combination is especially valuable” through hidden neurons that multiply or interact features. However, NNs need lots of data to train reliably and careful tuning (learning rate, architecture size, etc.) – otherwise they might overfit or get stuck in poor minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a359bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities:\n",
      " [[0.4 0.4 0.2]\n",
      " [0.4 0.4 0.2]\n",
      " [0.4 0.4 0.2]\n",
      " [0.4 0.4 0.2]\n",
      " [0.4 0.4 0.2]]\n",
      "Predicted classes: [0 0 1 1 1]\n",
      "True classes:       [0 0 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Synthetic dataset: [height, weight] -> position (0=Guard, 1=Forward, 2=Center)\n",
    "X = np.array([\n",
    "    [180, 75],  # Guard-like\n",
    "    [185, 80],  # Guard/Forward borderline\n",
    "    [195, 90],  # Forward\n",
    "    [205, 100], # Forward/Center borderline\n",
    "    [210, 110]  # Center\n",
    "], dtype=float)\n",
    "y = np.array([0, 0, 1, 1, 2])  # position labels\n",
    "\n",
    "# One-hot encode y for 3 classes\n",
    "Y = np.zeros((len(y), 3))\n",
    "for i, cls in enumerate(y):\n",
    "    Y[i, cls] = 1\n",
    "\n",
    "# Network architecture: 2 -> 3 -> 3 (2 inputs, 3 hidden neurons, 3 output classes)\n",
    "np.random.seed(0)\n",
    "W1 = np.random.randn(2, 3) * 0.1  # weight matrix from input to hidden\n",
    "b1 = np.zeros(3)\n",
    "W2 = np.random.randn(3, 3) * 0.1  # weight matrix from hidden to output\n",
    "b2 = np.zeros(3)\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    expz = np.exp(z - np.max(z))\n",
    "    return expz / expz.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Training parameters\n",
    "alpha = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1 = X.dot(W1) + b1        # hidden layer linear combination\n",
    "    A1 = sigmoid(Z1)          # hidden layer activation\n",
    "    Z2 = A1.dot(W2) + b2      # output layer linear combination\n",
    "    A2 = softmax(Z2.reshape(len(X), 3))  # output probabilities for 3 classes\n",
    "    \n",
    "    # Compute loss (cross-entropy)\n",
    "    # (for monitoring; not used in backprop explicitly here)\n",
    "    loss = -np.sum(Y * np.log(A2)) / len(X)\n",
    "    \n",
    "    # Backpropagation\n",
    "    dZ2 = A2 - Y                        # gradient of loss w.rt Z2 (softmax-crossentropy)\n",
    "    dW2 = A1.T.dot(dZ2) / len(X)\n",
    "    db2 = np.mean(dZ2, axis=0)\n",
    "    dA1 = dZ2.dot(W2.T)                # propagate to hidden layer\n",
    "    dZ1 = dA1 * (A1 * (1 - A1))        # gradient through sigmoid (sigmoid' = A1*(1-A1))\n",
    "    dW1 = X.T.dot(dZ1) / len(X)\n",
    "    db1 = np.mean(dZ1, axis=0)\n",
    "    \n",
    "    # Gradient descent update\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "\n",
    "# After training, let's predict the class probabilities\n",
    "A1 = sigmoid(X.dot(W1) + b1)\n",
    "A2 = softmax((A1.dot(W2) + b2).reshape(len(X), 3))\n",
    "print(\"Predicted probabilities:\\n\", A2.round(2))\n",
    "print(\"Predicted classes:\", np.argmax(A2, axis=1))\n",
    "print(\"True classes:      \", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993b472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
