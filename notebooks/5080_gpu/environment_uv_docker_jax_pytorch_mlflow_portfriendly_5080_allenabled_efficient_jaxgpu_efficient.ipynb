{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure no other containers are running for dev container, if they are stop and remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\docker_projects\\cancerbayes_irisrf_ex_fastapi_react\\notebooks\\5080_gpu\n",
      "notebooks/5080_gpu does not exist\n",
      "c:\\docker_projects\\cancerbayes_irisrf_ex_fastapi_react\\notebooks\\5080_gpu\n",
      "Please run this script from the root of the repository\n",
      "e.g. python environment_uv_docker_jax_pytorch_mlflow_portfriendly_5080_allenabled_efficient.ipynb\n",
      "or cd to the root of the repository and run this script\n",
      "e.g. cd .. && python notebooks/5080_gpu/environment_uv_docker_jax_pytorch_mlflow_portfriendly_5080_allenabled_efficient.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getcwd())\n",
    "if os.path.exists(\"notebooks/5080_gpu\"):\n",
    "    os.chdir(\"notebooks/5080_gpu\")\n",
    "    print(os.getcwd())\n",
    "else:\n",
    "    print(\"notebooks/5080_gpu does not exist\")\n",
    "    print(os.getcwd())\n",
    "    print(\"Please run this script from the root of the repository\")\n",
    "    print(\"e.g. python environment_uv_docker_jax_pytorch_mlflow_portfriendly_5080_allenabled_efficient.ipynb\")\n",
    "    print(\"or cd to the root of the repository and run this script\")\n",
    "    print(\"e.g. cd .. && python notebooks/5080_gpu/environment_uv_docker_jax_pytorch_mlflow_portfriendly_5080_allenabled_efficient.ipynb\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.gitattributes\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.gitattributes\n",
    "# Enforce LF for shell scripts & Docker/compose to avoid bash/sh\\r errors\n",
    "*.sh     text eol=lf\n",
    "*.bash   text eol=lf\n",
    "Dockerfile text eol=lf\n",
    "*.dockerfile text eol=lf\n",
    "docker-compose*.yml text eol=lf\n",
    "*.env    text eol=lf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.dockerignore\n",
    "# VCS/editor\n",
    ".git\n",
    ".gitignore\n",
    ".vscode\n",
    ".idea\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Python\n",
    "**/__pycache__/\n",
    "**/*.py[cod]\n",
    "**/*.pyo\n",
    "*.log\n",
    "\n",
    "# Virtual envs\n",
    ".venv\n",
    "venv\n",
    "env\n",
    "\n",
    "# Data / artifacts\n",
    "data/\n",
    "mlruns/\n",
    "mlflow_db/\n",
    "**/.ipynb_checkpoints/\n",
    "\n",
    "# Big notebooks outputs\n",
    "notebooks/**/*.ipynb_checkpoints\n",
    "\n",
    "# OS / misc\n",
    "*.code-workspace\n",
    "\n",
    "# We still need the .devcontainer directory for the Dockerfile, scripts, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/.env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/.env.template \n",
    "ENV_NAME=cancer_bayes_iris_env\n",
    "CUDA_TAG=12.8.0\n",
    "DOCKER_BUILDKIT=1\n",
    "\n",
    "# Host ports\n",
    "HOST_JUPYTER_PORT=8896\n",
    "HOST_TENSORBOARD_PORT=6006\n",
    "HOST_EXPLAINER_PORT=8056\n",
    "HOST_STREAMLIT_PORT=8506\n",
    "HOST_MLFLOW_PORT=5006\n",
    "\n",
    "# Python & JAX\n",
    "PYTHON_VER=3.10\n",
    "\n",
    "# Leave blank to autodetect; set to 'gpu' or 'cpu' only if you must force it.\n",
    "JAX_PLATFORM_NAME=\n",
    "\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=true\n",
    "XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "XLA_FLAGS=--xla_force_host_platform_device_count=1\n",
    "JAX_DISABLE_JIT=false\n",
    "JAX_ENABLE_X64=false\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=8589934592\n",
    "\n",
    "# Runtime GPU targeting & Jupyter convenience\n",
    "JUPYTER_TOKEN=jupyter\n",
    "\n",
    "# Keep uv from targeting /workspace/.venv (a bind mount)\n",
    "UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "\n",
    "# NOTE: Torch CUDA wheel is auto-selected from CUDA_TAG at startup:\n",
    "#   12.1  -> cu121\n",
    "#   12.4  -> cu124\n",
    "#   12.6+ -> cu126 (works for 12.6 / 12.8 / 12.9)\n",
    "\n",
    "\n",
    "# â”€â”€ Kaggle authentication (choose ONE approach) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Option A: env vars (fastest for CI/containers)\n",
    "KAGGLE_USERNAME=geoffhadfield \n",
    "KAGGLE_KEY=your_api_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/devcontainer.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/devcontainer.json\n",
    "{\n",
    "  \"name\": \"cancer_bayes_iris_env_uv\",\n",
    "  \"dockerComposeFile\": [\"docker-compose.yml\"],\n",
    "  \"service\": \"datascience\",\n",
    "  \"workspaceFolder\": \"/workspace\",\n",
    "  \"shutdownAction\": \"stopCompose\",\n",
    "\n",
    "  \"customizations\": {\n",
    "    \"vscode\": {\n",
    "      \"extensions\": [\n",
    "        \"ms-python.python\",\n",
    "        \"ms-python.vscode-pylance\",\n",
    "        \"ms-toolsai.jupyter\",\n",
    "        \"ms-toolsai.jupyter-renderers\",\n",
    "        \"nvidia.nsight-vscode-edition\"\n",
    "      ],\n",
    "      \"settings\": {\n",
    "        \"telemetry.telemetryLevel\": \"off\",\n",
    "        \"python.telemetry.enabled\": false,\n",
    "        \"jupyter.telemetry.enabled\": false,\n",
    "        \"jupyter.experiments.enabled\": false,\n",
    "        \"update.mode\": \"manual\",\n",
    "        \"extensions.autoUpdate\": false,\n",
    "        \"extensions.autoCheckUpdates\": false,\n",
    "        \"remote.extensionKind\": {\n",
    "          \"ms-python.python\": [\"ui\"],\n",
    "          \"ms-python.vscode-pylance\": [\"ui\"],\n",
    "          \"ms-toolsai.jupyter\": [\"ui\"],\n",
    "          \"ms-toolsai.jupyter-renderers\": [\"ui\"],\n",
    "          \"nvidia.nsight-vscode-edition\": [\"ui\"]\n",
    "        },\n",
    "        \"python.defaultInterpreterPath\": \"/app/.venv/bin/python\",\n",
    "        \"jupyter.interactiveWindow.textEditor.executeSelection\": true,\n",
    "        \"jupyter.widgetScriptSources\": [\"jsdelivr.com\", \"unpkg.com\"]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "\n",
    "  \"containerEnv\": {\n",
    "    \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\",\n",
    "    \"NVIDIA_VISIBLE_DEVICES\": \"all\",\n",
    "    \"NVIDIA_DRIVER_CAPABILITIES\": \"compute,utility\",\n",
    "    \"CUDA_STACK\": \"pip-cuda\",\n",
    "    \"LD_PRELOAD\": \"/usr/lib/x86_64-linux-gnu/libjemalloc.so.2\",\n",
    "    \"MALLOC_ARENA_MAX\": \"1\",\n",
    "    \"MALLOC_TCACHE_MAX\": \"0\",\n",
    "    \"PYTORCH_NO_CUDA_MEMORY_CACHING\": \"1\",\n",
    "    \"XLA_PYTHON_CLIENT_PREALLOCATE\": \"false\",\n",
    "    \"XLA_PYTHON_CLIENT_MEM_FRACTION\": \"0.25\",\n",
    "    \"XLA_PYTHON_CLIENT_ALLOCATOR\": \"platform\",\n",
    "    \"PYTORCH_CUDA_ALLOC_CONF\": \"max_split_size_mb:512,expandable_segments:True\"\n",
    "  },\n",
    "\n",
    "  \"remoteEnv\": {\n",
    "    \"JUPYTER_ENABLE_LAB\": \"true\",\n",
    "    \"PATH\": \"${containerEnv:PATH}:/usr/local/cuda/bin\"\n",
    "  },\n",
    "\n",
    "  \"postCreateCommand\": \"/bin/sh -lc 'set -e; find .devcontainer -maxdepth 1 -name \\\"*.sh\\\" -exec sed -i \\\"s/\\\\r$//\\\" {} \\\\;; chmod +x .devcontainer/*.sh; bash .devcontainer/opening.sh'\",\n",
    "  \"postStartCommand\": \"/bin/sh -lc 'set -e; find .devcontainer -maxdepth 1 -name \\\"*.sh\\\" -exec sed -i \\\"s/\\\\r$//\\\" {} \\\\;; chmod +x .devcontainer/*.sh; bash .devcontainer/opening.sh --verify-only || true'\"\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/Dockerfile\n",
    "# .devcontainer/Dockerfile - Optimized for faster builds\n",
    "ARG CUDA_TAG=12.8.0\n",
    "# Use NVIDIA's runtime image with pre-installed CUDA libraries (saves 1+ GB downloads)\n",
    "FROM nvidia/cuda:${CUDA_TAG}-runtime-ubuntu22.04 as runtime-base\n",
    "FROM nvidia/cuda:${CUDA_TAG}-devel-ubuntu22.04\n",
    "\n",
    "ARG PYTHON_VER=3.10\n",
    "ARG ENV_NAME=cancer_bayes_iris_env\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Single RUN layer for system packages with optimized cache mounts\n",
    "RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \\\n",
    "    --mount=type=cache,target=/var/lib/apt,sharing=locked \\\n",
    "    apt-get update && apt-get install -y --no-install-recommends \\\n",
    "        # Essential tools\n",
    "        bash curl ca-certificates git procps htop util-linux \\\n",
    "        python3 python3-venv python3-pip python3-dev \\\n",
    "        # Build tools (minimal set)\n",
    "        build-essential cmake pkg-config \\\n",
    "        # Memory management \n",
    "        libjemalloc2 libjemalloc-dev \\\n",
    "        # Network tools for diagnostics\n",
    "        iproute2 net-tools lsof \\\n",
    "    && apt-get clean && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Node.js in single layer with cache mount\n",
    "RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \\\n",
    "    --mount=type=cache,target=/var/lib/apt,sharing=locked \\\n",
    "    curl -fsSL https://deb.nodesource.com/setup_18.x | bash - \\\n",
    "    && apt-get install -y nodejs \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install uv package manager (pinned version for reproducibility)\n",
    "COPY --from=ghcr.io/astral-sh/uv:0.7.12 /uv /uvx /bin/\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Create virtual environment early\n",
    "RUN uv venv .venv --python \"${PYTHON_VER}\" --prompt \"${ENV_NAME}\"\n",
    "\n",
    "ENV VIRTUAL_ENV=/app/.venv \\\n",
    "    PATH=\"/app/.venv/bin:${PATH}\" \\\n",
    "    UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "# Essential memory management (simplified from original complex setup)\n",
    "ENV LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2 \\\n",
    "    MALLOC_ARENA_MAX=1 \\\n",
    "    MALLOC_TCACHE_MAX=0 \\\n",
    "    PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "# GPU framework environment (JAX & PyTorch only, TensorFlow removed)\n",
    "ENV XLA_PYTHON_CLIENT_PREALLOCATE=false \\\n",
    "    XLA_PYTHON_CLIENT_MEM_FRACTION=0.25 \\\n",
    "    XLA_PYTHON_CLIENT_ALLOCATOR=platform \\\n",
    "    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True \\\n",
    "    JAX_PREALLOCATION_SIZE_LIMIT_BYTES=8589934592\n",
    "\n",
    "# Install core Python packages first (better caching)\n",
    "RUN --mount=type=cache,target=/root/.cache/uv \\\n",
    "    uv pip install --no-cache-dir jupyterlab ipykernel\n",
    "\n",
    "# CUDA symlink setup (simplified)\n",
    "RUN ln -sf $(find /usr/local -name \"cuda-*\" -type d | head -n1) /usr/local/cuda || \\\n",
    "    ln -sf /usr/local/cuda /usr/local/cuda\n",
    "\n",
    "# Copy project files for dependency installation (from repo root)\n",
    "COPY pyproject.toml /workspace/\n",
    "COPY uv.lock* /workspace/\n",
    "\n",
    "# Install project dependencies with better error handling\n",
    "RUN --mount=type=cache,target=/root/.cache/uv \\\n",
    "    cd /workspace && \\\n",
    "    (uv sync --frozen --no-dev || (echo \"ðŸ” Lock out-of-date; refreshingâ€¦\" && uv sync --no-dev && uv lock))\n",
    "\n",
    "# Essential CUDA libraries path\n",
    "ENV LD_LIBRARY_PATH=\"/app/.venv/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n",
    "\n",
    "# Workspace setup\n",
    "WORKDIR /workspace\n",
    "RUN echo 'cd /workspace' > /etc/profile.d/99-workspace-cd.sh && \\\n",
    "    mkdir -p /root/.ipython/profile_default/startup && \\\n",
    "    printf \"import os, sys\\nos.chdir('/workspace')\\nsys.path.append('/workspace')\\n\" \\\n",
    "      > /root/.ipython/profile_default/startup/00-cd-workspace.py && \\\n",
    "    echo '. /app/.venv/bin/activate' > /etc/profile.d/10-uv-activate.sh\n",
    "\n",
    "CMD [\"bash\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/verify_env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/verify_env.py\n",
    "#!/usr/bin/env python3\n",
    "import sys, importlib, os, textwrap\n",
    "\n",
    "CRIT = []\n",
    "WARN = []\n",
    "\n",
    "def _have(mod: str) -> bool:\n",
    "    try:\n",
    "        importlib.import_module(mod); return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _msg_box(title: str, body: str) -> None:\n",
    "    line = \"=\" * 80\n",
    "    print(f\"\\n{line}\\n{title}\\n{line}\\n{body}\\n\")\n",
    "\n",
    "def _probe_jax() -> None:\n",
    "    \"\"\"\n",
    "    Probe JAX availability and devices; detect duplicate PJRT CUDA packages and\n",
    "    mixed CUDA stacks (system + pip nvidia-*), which commonly trigger allocator\n",
    "    double-frees inside XLA/PJRT.\n",
    "    \"\"\"\n",
    "    import importlib.util as u\n",
    "    import importlib, os, textwrap, subprocess\n",
    "\n",
    "    jp = os.environ.get(\"JAX_PLATFORM_NAME\", \"<unset>\")\n",
    "    print(f\"   JAX_PLATFORM_NAME: {jp}\")\n",
    "\n",
    "    try:\n",
    "        jax = importlib.import_module(\"jax\")\n",
    "    except Exception as e:\n",
    "        WARN.append(f\"jax not importable: {e!r}\")\n",
    "        _msg_box(\n",
    "            \"Action: JAX not importable\",\n",
    "            \"â€¢ Install JAX into /app/.venv:\\n\"\n",
    "            \"    uv pip install 'jax[cuda12-local]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\\n\"\n",
    "            \"â€¢ Avoid bare 'pip'; prefer 'uv pip' with UV_PROJECT_ENVIRONMENT=/app/.venv .\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        jaxlib = importlib.import_module(\"jaxlib\")\n",
    "        print(f\"   jax: {getattr(jax,'__version__','?')} @ {getattr(jax,'__file__','?')}\")\n",
    "        print(f\"   jaxlib @ {getattr(jaxlib,'__file__','?')}\")\n",
    "    except Exception as e:\n",
    "        WARN.append(f\"jaxlib import failed: {e!r}\")\n",
    "\n",
    "    # Detect PJRT packages\n",
    "    has_plugin = u.find_spec(\"jax_cuda12_plugin\") is not None\n",
    "    has_pjrt   = u.find_spec(\"jax_cuda12_pjrt\") is not None\n",
    "    if has_plugin and has_pjrt:\n",
    "        WARN.append(\"Both jax-cuda12-plugin and jax-cuda12-pjrt are installed (conflict).\")\n",
    "        _msg_box(\n",
    "            \"Conflict: Two JAX PJRT CUDA backends found\",\n",
    "            textwrap.dedent(\"\"\"\\\n",
    "                You must keep exactly one of these:\n",
    "                  â€¢ jax-cuda12-plugin  (for LOCAL /usr/local/cuda)\n",
    "                  â€¢ jax-cuda12-pjrt    (bundled runtime)\n",
    "                Recommended (in this image): keep the plugin and remove pjrt:\n",
    "                  uv pip uninstall -y jax-cuda12-pjrt\n",
    "            \"\"\"),\n",
    "        )\n",
    "\n",
    "    # Detect pip NVIDIA CUDA stacks if using local plugin\n",
    "    def _pip_freeze_contains(prefix: str) -> bool:\n",
    "        try:\n",
    "            out = subprocess.check_output([os.environ.get(\"UV_BIN\",\"uv\"), \"pip\", \"freeze\"], text=True)\n",
    "        except Exception:\n",
    "            try:\n",
    "                out = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "            except Exception:\n",
    "                return False\n",
    "        return any(line.strip().startswith(prefix) for line in out.splitlines())\n",
    "\n",
    "    if has_plugin:\n",
    "        offending = [p for p in (\n",
    "            \"nvidia-cuda-runtime-cu12\", \"nvidia-cudnn-cu12\", \"nvidia-cublas-cu12\",\n",
    "            \"nvidia-cusolver-cu12\", \"nvidia-cusparse-cu12\", \"nvidia-cufft-cu12\",\n",
    "            \"nvidia-curand-cu12\", \"nvidia-nvtx-cu12\", \"nvidia-nvjitlink-cu12\", \"nvidia-cuda-cupti-cu12\"\n",
    "        ) if _pip_freeze_contains(p)]\n",
    "        if offending:\n",
    "            WARN.append(f\"Local CUDA policy but pip NVIDIA libs present: {', '.join(offending)}\")\n",
    "            _msg_box(\n",
    "                \"Mixed CUDA stacks detected\",\n",
    "                \"You're using the local PJRT plugin, but pip-installed NVIDIA CUDA libs are present.\\n\"\n",
    "                \"Remove them to avoid duplicate allocators:\\n\"\n",
    "                \"  uv pip uninstall -y \" + \" \".join(offending)\n",
    "            )\n",
    "\n",
    "    # Device probe\n",
    "    try:\n",
    "        devs = jax.devices()\n",
    "        print(f\"   jax {getattr(jax,'__version__','?')} devices: {devs}\")\n",
    "    except Exception as e:\n",
    "        WARN.append(f\"jax.devices() raised: {e!r}\")\n",
    "        _msg_box(\n",
    "            \"Action: Fix JAX GPU backend\",\n",
    "            textwrap.dedent(\"\"\"\\\n",
    "                â€¢ Install the PJRT CUDA plugin (local CUDA policy):\n",
    "                  uv pip install \"jax[cuda12-local]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "                â€¢ Ensure only one backend (plugin XOR pjrt) is installed.\n",
    "                â€¢ Avoid mixing pip 'nvidia-*' CUDA stacks with the system CUDA.\n",
    "            \"\"\"),\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # CPU-only hints\n",
    "    gpu = [d for d in devs if \"gpu\" in str(d).lower() or \"cuda\" in str(d).lower()]\n",
    "    if not gpu:\n",
    "        WARN.append(\"JAX imported but reports CPU-only devices.\")\n",
    "        _msg_box(\n",
    "            \"Info: JAX is CPU-only right now\",\n",
    "            textwrap.dedent(\"\"\"\\\n",
    "                Likely causes (check in order):\n",
    "                1) CUDA plugin not installed in this venv:\n",
    "                   uv pip install \"jax[cuda12-local]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "                2) Backend conflict (both plugin and pjrt installed) â€” remove one.\n",
    "                3) Mixed CUDA stacks (remove pip 'nvidia-*' libs when using local CUDA).\n",
    "            \"\"\"),\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"## Python & library diagnostics ##\")\n",
    "    print(\"Python:\", sys.version.split()[0])\n",
    "    print(\"sys.executable:\", sys.executable)\n",
    "    print(\"sys.prefix:\", sys.prefix)\n",
    "    print(\"VIRTUAL_ENV:\", os.environ.get(\"VIRTUAL_ENV\",\"<unset>\"))\n",
    "    print(\"PATH head:\", os.environ.get(\"PATH\",\"\").split(\":\")[:3])\n",
    "\n",
    "    if not sys.executable.startswith(\"/app/.venv/\"):\n",
    "        CRIT.append(\"Interpreter is not /app/.venv â€” uv env not active for this process\")\n",
    "\n",
    "    jlab_ok = _have(\"jupyterlab\")\n",
    "    print(f\" - jupyterlab: {'OK' if jlab_ok else 'MISSING'}\")\n",
    "\n",
    "    torch_ok = _have(\"torch\")\n",
    "    print(f\" - torch: {'OK' if torch_ok else 'MISSING'}\")\n",
    "    if torch_ok:\n",
    "        try:\n",
    "            import torch\n",
    "            print(\"   torch\", torch.__version__, \"CUDA available:\", torch.cuda.is_available())\n",
    "        except Exception as e:\n",
    "            WARN.append(f\"torch import ok but CUDA probe errored: {e}\")\n",
    "\n",
    "    print(f\" - jax: {'OK' if _have('jax') else 'MISSING'}\")\n",
    "    _probe_jax()\n",
    "\n",
    "    if CRIT:\n",
    "        _msg_box(\"Critical failures\", \"\\n\".join(f\"â€¢ {m}\" for m in CRIT))\n",
    "        sys.exit(1)\n",
    "\n",
    "    if WARN:\n",
    "        _msg_box(\"Warnings (non-blocking)\", \"\\n\".join(f\"â€¢ {m}\" for m in WARN))\n",
    "\n",
    "    print(\"âœ… verify_env completed (warnings above are informational).\")\n",
    "    sys.exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../scripts/test-gpu.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../scripts/test-gpu.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "PROJECT=${1:-cancer_bayes_iris_env}\n",
    "SERVICE=${2:-datascience}\n",
    "CID=$(docker compose -p \"$PROJECT\" ps -q \"$SERVICE\")\n",
    "[ -z \"$CID\" ] && { echo \"no container\"; exit 1; }\n",
    "\n",
    "echo \"== nvidia-smi ==\" && docker exec \"$CID\" nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits\n",
    "\n",
    "echo \"== PyTorch ==\" && docker exec \"$CID\" python - <<'PY'\n",
    "import torch, gc\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"cuda.is_available:\", torch.cuda.is_available())\n",
    "assert torch.cuda.is_available()\n",
    "print(\"device:\", torch.cuda.get_device_name(0))\n",
    "x = torch.randn(1024,1024, device=\"cuda\")\n",
    "y = torch.randn(1024,1024, device=\"cuda\")\n",
    "_ = (x@y).sum().item()\n",
    "del x,y; gc.collect(); torch.cuda.empty_cache()\n",
    "print(\"PyTorch GPU: OK\")\n",
    "PY\n",
    "\n",
    "echo \"== JAX ==\" && docker exec \"$CID\" python - <<'PY'\n",
    "import jax, jax.numpy as jnp\n",
    "print(\"jax:\", jax.__version__)\n",
    "print(\"devices:\", jax.devices())\n",
    "assert any(\"gpu\" in str(d).lower() or \"cuda\" in str(d).lower() for d in jax.devices())\n",
    "x=jnp.ones((1024,1024)); y=(x@x.T).sum(); _=y.block_until_ready()\n",
    "print(\"JAX GPU: OK\")\n",
    "PY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/scripts/test-gpu.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/scripts/test-gpu.sh\n",
    "# PowerShell GPU Testing Script for Windows Docker Development\n",
    "# ==========================================================\n",
    "# This script provides Windows-native testing capabilities for GPU frameworks\n",
    "# in Docker containers, optimized for NVIDIA Blackwell RTX 5080 setups.\n",
    "#\n",
    "# Usage:\n",
    "#   .\\scripts\\test-gpu.ps1\n",
    "#   .\\scripts\\test-gpu.ps1 -Verbose\n",
    "#   .\\scripts\\test-gpu.ps1 -ContainerName \"custom_container\"\n",
    "\n",
    "param(\n",
    "  [string]$ComposeProject = \"cancer_bayes_iris_env\",\n",
    "  [string]$Service = \"datascience\",\n",
    "  [switch]$Verbose\n",
    ")\n",
    "\n",
    "function Out-Green($m){ Write-Host \"âœ… $m\" -ForegroundColor Green }\n",
    "function Out-Red($m){ Write-Host \"âŒ $m\" -ForegroundColor Red }\n",
    "function Out-Cyan($m){ Write-Host \"â„¹ï¸  $m\" -ForegroundColor Cyan }\n",
    "\n",
    "$cid = docker compose -p $ComposeProject ps -q $Service\n",
    "if (-not $cid) { Out-Red \"No container found\"; exit 1 }\n",
    "Out-Green \"Container: $cid\"\n",
    "\n",
    "Out-Cyan \"nvidia-smiâ€¦\"\n",
    "docker exec $cid nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits\n",
    "\n",
    "Out-Cyan \"PyTorch CUDAâ€¦\"\n",
    "$pt = @'\n",
    "import torch, gc\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"cuda.is_available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device:\", torch.cuda.get_device_name(0))\n",
    "    x = torch.randn(1024,1024, device=\"cuda\")\n",
    "    y = torch.randn(1024,1024, device=\"cuda\")\n",
    "    _ = (x@y).sum().item()\n",
    "    del x,y; gc.collect(); torch.cuda.empty_cache()\n",
    "    print(\"PyTorch GPU: OK\")\n",
    "else:\n",
    "    raise SystemExit(1)\n",
    "'@\n",
    "docker exec $cid python -c $pt; if ($LASTEXITCODE) { Out-Red \"PyTorch failed\"; exit 1 } else { Out-Green \"PyTorch OK\" }\n",
    "\n",
    "Out-Cyan \"JAX CUDAâ€¦\"\n",
    "$jx = @'\n",
    "import jax, importlib.util as u\n",
    "print(\"jax:\", jax.__version__)\n",
    "print(\"devices:\", jax.devices())\n",
    "g = [d for d in jax.devices() if \"gpu\" in str(d).lower() or \"cuda\" in str(d).lower()]\n",
    "if not g: raise SystemExit(1)\n",
    "import jax.numpy as jnp\n",
    "x=jnp.ones((1024,1024)); y=(x@x.T).sum(); _=y.block_until_ready()\n",
    "print(\"JAX GPU: OK\")\n",
    "'@\n",
    "docker exec $cid python -c $jx; if ($LASTEXITCODE) { Out-Red \"JAX failed\"; exit 1 } else { Out-Green \"JAX OK\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/opening.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/opening.sh\n",
    "#!/usr/bin/env bash\n",
    "set -Eeuo pipefail\n",
    "\n",
    "log() { printf \"\\n[%s] %s\\n\" \"$(date +'%F %T')\" \"$*\" >&2; }\n",
    "die() { printf \"\\n[ERROR] %s\\n\" \"$*\" >&2; exit 1; }\n",
    "\n",
    "# Parse boolean-ish env values: 1/true/yes/on â†’ 1, else 0\n",
    "parse_env_bool() {\n",
    "  case \"${1:-}\" in\n",
    "    1|true|TRUE|yes|YES|on|ON) echo \"1\" ;;\n",
    "    *) echo \"0\" ;;\n",
    "  esac\n",
    "}\n",
    "\n",
    "# Load /workspace/.env if present, exporting its variables\n",
    "read_dotenv_if_present() {\n",
    "  if [ -f \"/workspace/.env\" ]; then\n",
    "    log \"Sourcing /workspace/.env into the environment\"\n",
    "    sed -i 's/\\r$//' /workspace/.env || true\n",
    "    set -a; . /workspace/.env; set +a\n",
    "  fi\n",
    "}\n",
    "\n",
    "# Normalize line endings for shell scripts\n",
    "normalize_line_endings() {\n",
    "  log \"Normalizing CRLF -> LF for .devcontainer/*.sh\"\n",
    "  find .devcontainer -maxdepth 1 -type f -name \"*.sh\" -print0 | \\\n",
    "    xargs -0 -I{} bash -lc 'sed -i \"s/\\r$//\" \"{}\"'\n",
    "}\n",
    "\n",
    "# Ensure git safe.directory\n",
    "fix_git_safety() {\n",
    "  if git rev-parse --show-toplevel >/dev/null 2>&1; then\n",
    "    local root=\"$(git rev-parse --show-toplevel || echo /workspace)\"\n",
    "    log \"Marking ${root} as a safe.directory for git.\"\n",
    "    git config --global --add safe.directory \"${root}\" || true\n",
    "  fi\n",
    "}\n",
    "\n",
    "# Ensure uv targets the project venv\n",
    "ensure_uv_env() {\n",
    "  export UV_PROJECT_ENVIRONMENT=\"${UV_PROJECT_ENVIRONMENT:-/app/.venv}\"\n",
    "  log \"UV_PROJECT_ENVIRONMENT=${UV_PROJECT_ENVIRONMENT}\"\n",
    "  if [[ ! -d \"${UV_PROJECT_ENVIRONMENT}\" ]]; then\n",
    "    log \"Creating uv venv at ${UV_PROJECT_ENVIRONMENT}\"\n",
    "    uv venv \"${UV_PROJECT_ENVIRONMENT}\" --python \"3.10\" --prompt \"cancer_bayes_iris_env\"\n",
    "  fi\n",
    "  echo \". ${UV_PROJECT_ENVIRONMENT}/bin/activate\" > /etc/profile.d/10-uv-activate.sh\n",
    "}\n",
    "\n",
    "# Sync core project dependencies (prefer root pyproject.toml)\n",
    "sync_project_deps() {\n",
    "  # Prefer root pyproject for universal local+container dev\n",
    "  if [[ -f \"/workspace/pyproject.toml\" ]]; then\n",
    "    log \"Syncing project deps from /workspace (frozen)\"\n",
    "    (cd /workspace && uv sync --frozen --no-dev) || {\n",
    "      log \"Lock out-of-date; refreshing from /workspaceâ€¦\"\n",
    "      (cd /workspace && uv sync --no-dev && uv lock)\n",
    "    }\n",
    "  elif [[ -f \"/workspace/.devcontainer/pyproject.toml\" ]]; then\n",
    "    log \"Syncing project deps from .devcontainer (legacy fallback)\"\n",
    "    (cd /workspace/.devcontainer && uv sync --frozen --no-dev) || {\n",
    "      log \"Lock out-of-date; refreshing from .devcontainerâ€¦\"\n",
    "      (cd /workspace/.devcontainer && uv sync --no-dev && uv lock)\n",
    "    }\n",
    "  else\n",
    "    die \"No pyproject.toml found at /workspace or .devcontainer\"\n",
    "  fi\n",
    "}\n",
    "\n",
    "# Simplified memory management for RTX 5080\n",
    "setup_memory_management() {\n",
    "  log \"Setting up RTX 5080 memory management...\"\n",
    "\n",
    "  # Verify jemalloc\n",
    "  if python -c \"import ctypes; ctypes.CDLL('/usr/lib/x86_64-linux-gnu/libjemalloc.so.2')\" 2>/dev/null; then\n",
    "    log \"jemalloc memory allocator loaded successfully\"\n",
    "  else\n",
    "    log \"jemalloc not available - using system malloc\"\n",
    "    unset LD_PRELOAD\n",
    "  fi\n",
    "\n",
    "  # Essential memory settings only (removed complex variables)\n",
    "  export MALLOC_ARENA_MAX=1\n",
    "  export MALLOC_TCACHE_MAX=0\n",
    "  export PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "  export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True\n",
    "  export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "  export XLA_PYTHON_CLIENT_MEM_FRACTION=0.25\n",
    "\n",
    "  log \"Memory management configured\"\n",
    "}\n",
    "\n",
    "# GPU Framework Installation (JAX + PyTorch only, TensorFlow removed)\n",
    "install_gpu_frameworks() {\n",
    "  log \"Installing GPU frameworks (PyTorch + JAX only)...\"\n",
    "\n",
    "  read_dotenv_if_present\n",
    "\n",
    "  # Clear JAX platform forcing\n",
    "  unset JAX_PLATFORM_NAME || true\n",
    "  unset JAX_PLATFORMS || true\n",
    "\n",
    "  # Remove conflicting NVIDIA packages to prevent double loads\n",
    "  log \"Cleaning up conflicting NVIDIA packages...\"\n",
    "  uv pip uninstall -y \\\n",
    "    nvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 \\\n",
    "    nvidia-cuda-cupti-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 \\\n",
    "    nvidia-nvjitlink-cu12 nvidia-nvtx-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 \\\n",
    "    || true\n",
    "\n",
    "  # PyTorch with CUDA 12.8 support\n",
    "  if ! python -c \"import torch; assert torch.cuda.is_available()\" 2>/dev/null; then\n",
    "    log \"Installing PyTorch nightly cu128...\"\n",
    "    uv pip install --no-cache-dir --pre torch torchvision torchaudio \\\n",
    "      --index-url \"https://download.pytorch.org/whl/nightly/cu128\"\n",
    "  else\n",
    "    log \"PyTorch CUDA already available\"\n",
    "  fi\n",
    "\n",
    "  # JAX with CUDA support  \n",
    "  log \"Installing JAX with CUDA support...\"\n",
    "  uv pip uninstall -y jax jaxlib jax-cuda12-plugin jax-cuda12-pjrt || true\n",
    "  uv pip install --no-cache-dir \"jax[cuda12-local]\" \\\n",
    "    -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "}\n",
    "\n",
    "# Framework initialization (PyTorch + JAX only)\n",
    "initialize_frameworks() {\n",
    "  log \"Initializing PyTorch and JAX...\"\n",
    "\n",
    "  python3 -c \"\n",
    "import os, gc\n",
    "print('Initializing JAX...')\n",
    "try:\n",
    "    import jax, jax.numpy as jnp\n",
    "    devs = jax.devices()\n",
    "    print('   JAX devices:', devs)\n",
    "    x = jnp.ones((128,128)); y = (x @ x.T).sum(); _ = y.block_until_ready()\n",
    "    print('   JAX small matmul: OK')\n",
    "except Exception as e:\n",
    "    print('   JAX init failed:', e)\n",
    "\n",
    "print('Initializing PyTorch...')\n",
    "try:\n",
    "    import torch\n",
    "    print('   torch:', torch.__version__, 'CUDA avail:', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print('   device:', torch.cuda.get_device_name(0))\n",
    "        a = torch.randn(128,128, device='cuda')\n",
    "        b = torch.randn(128,128, device='cuda')\n",
    "        _ = (a@b).sum().item()\n",
    "        print('   PyTorch small matmul: OK')\n",
    "except Exception as e:\n",
    "    print('   PyTorch init failed:', e)\n",
    "\n",
    "gc.collect()\n",
    "\"\n",
    "}\n",
    "\n",
    "# Register Jupyter kernel\n",
    "register_kernel() {\n",
    "  log \"Registering Jupyter ipykernel...\"\n",
    "  python -c \"\n",
    "import json, sys, subprocess\n",
    "name = 'cancer_bayes_iris_env'\n",
    "display = 'Python (cancer_bayes_iris_env)'\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'ipykernel', 'install',\n",
    "                           '--sys-prefix', '--name', name, '--display-name', display])\n",
    "    print('Kernel registered:', display)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print('Kernel registration failed:', e)\n",
    "    sys.exit(1)\n",
    "\"\n",
    "}\n",
    "\n",
    "# Simplified verification (PyTorch + JAX only)\n",
    "verify_frameworks() {\n",
    "  log \"Verifying frameworks...\"\n",
    "\n",
    "  local failed=0\n",
    "\n",
    "  # PyTorch verification\n",
    "  if python -c \"\n",
    "import torch, gc\n",
    "assert torch.cuda.is_available(), 'CUDA not available'\n",
    "for _ in range(3):\n",
    "    x = torch.randn(1000,1000, device='cuda')\n",
    "    del x; torch.cuda.empty_cache(); gc.collect()\n",
    "print('PyTorch CUDA verification passed')\n",
    "\" 2>/dev/null; then\n",
    "    log \"PyTorch verification: PASSED\"\n",
    "  else\n",
    "    log \"PyTorch verification: FAILED\"\n",
    "    failed=$((failed+1))\n",
    "  fi\n",
    "\n",
    "  # JAX verification  \n",
    "  if python -c \"\n",
    "import jax, jax.numpy as jnp, gc\n",
    "devices = jax.devices()\n",
    "assert any('gpu' in str(d).lower() or 'cuda' in str(d).lower() for d in devices), 'No GPU devices'\n",
    "for _ in range(3):\n",
    "    x = jnp.ones((1000,1000)); del x; gc.collect()\n",
    "print('JAX CUDA verification passed')\n",
    "\" 2>/dev/null; then\n",
    "    log \"JAX verification: PASSED\"\n",
    "  else\n",
    "    log \"JAX verification: FAILED\"  \n",
    "    failed=$((failed+1))\n",
    "  fi\n",
    "\n",
    "  if [ \"${failed}\" -gt 0 ]; then\n",
    "    log \"Some frameworks failed verification but continuing...\"\n",
    "  else\n",
    "    log \"All frameworks verified successfully\"\n",
    "  fi\n",
    "}\n",
    "\n",
    "# MAIN EXECUTION\n",
    "main() {\n",
    "  normalize_line_endings\n",
    "  read_dotenv_if_present\n",
    "  fix_git_safety\n",
    "  ensure_uv_env\n",
    "  setup_memory_management\n",
    "\n",
    "  if [[ \"${1:-}\" == \"--verify-only\" ]]; then\n",
    "    verify_frameworks || true\n",
    "    return 0\n",
    "  fi\n",
    "\n",
    "  sync_project_deps\n",
    "  install_gpu_frameworks\n",
    "  initialize_frameworks  \n",
    "  register_kernel\n",
    "  verify_frameworks\n",
    "  log \"Setup completed successfully\"\n",
    "}\n",
    "\n",
    "main \"$@\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/rebuild.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/rebuild.sh\n",
    "#!/bin/bash\n",
    "set -euo pipefail\n",
    "\n",
    "# DevContainer GPU Setup Rebuild Script\n",
    "# This script helps rebuild the DevContainer with all the latest fixes\n",
    "\n",
    "echo \"ðŸ”§ DevContainer GPU Setup Rebuild Script\"\n",
    "echo \"========================================\"\n",
    "echo \"\"\n",
    "\n",
    "# Function for logging\n",
    "log() {\n",
    "    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n",
    "}\n",
    "\n",
    "# Function for status indicators\n",
    "status_ok() { echo \"âœ… $1\"; }\n",
    "status_warn() { echo \"âš ï¸  $1\"; }\n",
    "status_error() { echo \"âŒ $1\"; }\n",
    "\n",
    "# Check if we're in the right directory\n",
    "if [ ! -f \".devcontainer/devcontainer.json\" ]; then\n",
    "    status_error \"This script must be run from the project root directory\"\n",
    "    echo \"Please run: cd /path/to/your/project && bash .devcontainer/rebuild.sh\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "log \"Starting DevContainer rebuild process...\"\n",
    "\n",
    "# Step 1: Stop existing containers\n",
    "log \"Step 1: Stopping existing containers...\"\n",
    "if docker compose -p cancer_bayes_iris_env ps -q >/dev/null 2>&1; then\n",
    "    docker compose -p cancer_bayes_iris_env down\n",
    "    status_ok \"Existing containers stopped\"\n",
    "else\n",
    "    status_warn \"No existing containers found\"\n",
    "fi\n",
    "\n",
    "# Step 2: Clean up Docker resources\n",
    "log \"Step 2: Cleaning up Docker resources...\"\n",
    "docker system prune -f --volumes\n",
    "status_ok \"Docker resources cleaned\"\n",
    "\n",
    "# Step 3: Rebuild with no cache\n",
    "log \"Step 3: Rebuilding DevContainer with no cache...\"\n",
    "docker compose -p cancer_bayes_iris_env build --no-cache datascience\n",
    "status_ok \"DevContainer rebuilt successfully\"\n",
    "\n",
    "# Step 4: Start the container\n",
    "log \"Step 4: Starting DevContainer...\"\n",
    "docker compose -p cancer_bayes_iris_env up -d\n",
    "status_ok \"DevContainer started\"\n",
    "\n",
    "# Step 5: Wait for container to be ready\n",
    "log \"Step 5: Waiting for container to be ready...\"\n",
    "sleep 15\n",
    "\n",
    "# Step 6: Check container status\n",
    "log \"Step 6: Checking container status...\"\n",
    "if docker compose -p cancer_bayes_iris_env ps | grep -q \"Up\"; then\n",
    "    status_ok \"Container is running\"\n",
    "else\n",
    "    status_error \"Container failed to start\"\n",
    "    echo \"Check logs with: docker compose -p cancer_bayes_iris_env logs -f\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Step 7: Run setup script inside container\n",
    "log \"Step 7: Running setup script inside container...\"\n",
    "CONTAINER_NAME=$(docker compose -p cancer_bayes_iris_env ps -q datascience)\n",
    "if [ -n \"$CONTAINER_NAME\" ]; then\n",
    "    docker exec -it \"$CONTAINER_NAME\" bash -c \"cd /workspace && bash .devcontainer/setup.sh\"\n",
    "    status_ok \"Setup script completed\"\n",
    "else\n",
    "    status_error \"Could not find container\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Step 8: Verify GPU access\n",
    "log \"Step 8: Verifying GPU access...\"\n",
    "docker exec -it \"$CONTAINER_NAME\" bash -c \"nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits\" || {\n",
    "    status_warn \"GPU not accessible (this may be expected if no GPU is available)\"\n",
    "}\n",
    "\n",
    "# Step 9: Test Python imports\n",
    "log \"Step 9: Testing Python imports...\"\n",
    "docker exec -it \"$CONTAINER_NAME\" bash -c \"python - <<'PY'\n",
    "import importlib, os\n",
    "mods = ['pymc','torch','jax']\n",
    "ok = True\n",
    "for m in mods:\n",
    "    try: importlib.import_module(m); print('âœ…', m, 'import OK')\n",
    "    except Exception as e: print('âŒ', m, 'failed:', e); ok = False\n",
    "if not ok: raise SystemExit(1)\n",
    "print('âœ… All key packages imported successfully')\n",
    "PY\" || { status_error \"Some packages failed to import\"; }\n",
    "\n",
    "# Step 10: Run basic GPU verification\n",
    "log \"Step 10: Running basic GPU verification...\"\n",
    "docker exec -it \"$CONTAINER_NAME\" bash -c \"nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits\" || {\n",
    "    status_warn \"GPU verification failed - check the output above\"\n",
    "}\n",
    "\n",
    "# Step 11: Test framework imports (TensorFlow optional)\n",
    "log \"Step 11: Testing framework imports...\"\n",
    "docker exec -it \"$CONTAINER_NAME\" bash -c \"python - <<'PY'\n",
    "import importlib, os, sys\n",
    "install_tf = os.getenv('INSTALL_TF','0').lower() in ('1','true','yes','on')\n",
    "to_check = ['torch','jax'] + (['tensorflow'] if install_tf else [])\n",
    "ok = True\n",
    "for m in to_check:\n",
    "    try: importlib.import_module(m); print('âœ…', m, 'import OK')\n",
    "    except Exception as e: print('âŒ', m, 'failed:', e); ok = False\n",
    "if not ok: raise SystemExit(1)\n",
    "print('âœ… Requested frameworks imported successfully')\n",
    "PY\" || { status_warn \"A requested framework failed to import - check logs\"; }\n",
    "\n",
    "# Step 12: Check memory management\n",
    "log \"Step 12: Checking memory management setup...\"\n",
    "docker exec -it \"$CONTAINER_NAME\" bash -c \"\n",
    "echo 'ðŸ” Checking jemalloc installation...'\n",
    "if [ -f '/usr/lib/x86_64-linux-gnu/libjemalloc.so.2' ]; then\n",
    "    echo 'âœ… jemalloc library found'\n",
    "else\n",
    "    echo 'âŒ jemalloc library not found'\n",
    "fi\n",
    "\n",
    "echo 'ðŸ” Checking memory environment variables...'\n",
    "env | grep -E '(LD_PRELOAD|MALLOC|PYTORCH|XLA|TF_FORCE)' || echo 'No memory management variables found'\n",
    "\n",
    "echo 'ðŸ” Checking memory allocator in use...'\n",
    "python -c \\\"\n",
    "import ctypes\n",
    "try:\n",
    "    ctypes.CDLL('/usr/lib/x86_64-linux-gnu/libjemalloc.so.2')\n",
    "    print('âœ… jemalloc is loaded')\n",
    "except:\n",
    "    print('âŒ jemalloc not loaded')\n",
    "\\\"\n",
    "\" || {\n",
    "    status_warn \"Memory management check failed\"\n",
    "}\n",
    "\n",
    "echo \"\"\n",
    "echo \"ðŸŽ‰ DevContainer rebuild completed!\"\n",
    "echo \"\"\n",
    "echo \"Next steps:\"\n",
    "echo \"1. Open VS Code and press F1 â†’ 'Dev Containers: Reopen in Container'\"\n",
    "echo \"2. Or connect to Jupyter Lab at: http://localhost:8890 (token: jupyter)\"\n",
    "echo \"3. Run diagnostics: bash .devcontainer/troubleshoot.sh\"\n",
    "echo \"4. Test GPU from host: ./scripts/test-gpu.ps1 (Windows) or ./scripts/test-gpu.sh (Linux/macOS)\"\n",
    "echo \"\"\n",
    "echo \"Useful commands:\"\n",
    "echo \"  View logs: docker compose -p cancer_bayes_iris_env logs -f\"\n",
    "echo \"  Enter container: docker exec -it $CONTAINER_NAME bash\"\n",
    "echo \"  Stop container: docker compose -p cancer_bayes_iris_env down\"\n",
    "echo \"  Test GPU: ./scripts/test-gpu.ps1 (Windows) or ./scripts/test-gpu.sh (Linux/macOS)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/setup.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/setup.sh\n",
    "#!/bin/bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Error handling\n",
    "handle_error() {\n",
    "    echo \"âŒ Error at line: ${1}\"\n",
    "    exit 1\n",
    "}\n",
    "trap 'handle_error ${LINENO}' ERR\n",
    "\n",
    "log() {\n",
    "    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n",
    "}\n",
    "\n",
    "log \"ðŸš€ Starting DevContainer setup...\"\n",
    "\n",
    "# Verify environment\n",
    "log \"ðŸ“‹ Environment check...\"\n",
    "echo \"Python: $(python --version)\"\n",
    "echo \"uv: $(uv --version)\"\n",
    "echo \"Working directory: $(pwd)\"\n",
    "echo \"VIRTUAL_ENV: ${VIRTUAL_ENV:-<unset>}\"\n",
    "\n",
    "# Ensure we're in workspace\n",
    "cd /workspace\n",
    "\n",
    "# Check if dependencies need to be installed\n",
    "if ! python -c \"import pymc\" 2>/dev/null; then\n",
    "    log \"ðŸ“¦ Installing project dependencies...\"\n",
    "    \n",
    "    # Prefer root pyproject.toml for universal local+container dev\n",
    "    if [ -f \"pyproject.toml\" ]; then\n",
    "        log \"Using workspace pyproject.toml\"\n",
    "        uv sync --frozen || uv sync\n",
    "    elif [ -f \".devcontainer/pyproject.toml\" ]; then\n",
    "        log \"Using .devcontainer/pyproject.toml (fallback)\"\n",
    "        cd .devcontainer\n",
    "        uv sync --frozen || uv sync\n",
    "        cd ..\n",
    "    else\n",
    "        log \"âŒ No pyproject.toml found in /workspace or .devcontainer\"\n",
    "        exit 1\n",
    "    fi\n",
    "else\n",
    "    log \"âœ… Dependencies already installed\"\n",
    "fi\n",
    "\n",
    "# Ensure ipykernel is installed and registered\n",
    "log \"ðŸ”§ Setting up Jupyter kernel...\"\n",
    "python -m ipykernel install --name uv-app-venv --display-name \"Python (uv /app/.venv)\" --user || true\n",
    "\n",
    "# Install PyJAGS (optional)\n",
    "log \"ðŸ“Š Installing PyJAGS (optional)...\"\n",
    "CPPFLAGS='-include cstdint' uv pip install --no-build-isolation pyjags==1.3.8 || {\n",
    "    log \"âš ï¸  PyJAGS installation failed (optional)\"\n",
    "}\n",
    "\n",
    "# Verify GPU access\n",
    "log \"ðŸ” Checking GPU...\"\n",
    "if command -v nvidia-smi >/dev/null 2>&1; then\n",
    "    nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits || true\n",
    "fi\n",
    "\n",
    "# Run verification\n",
    "log \"ðŸ”¬ Running environment verification...\"\n",
    "python .devcontainer/verify_env.py || true\n",
    "\n",
    "log \"âœ… Setup completed\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/postcreate.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/postcreate.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "echo \"## postCreate (idempotent) ##\"\n",
    "\n",
    "PROJECT_DIR=${PROJECT_DIR:-/workspace}\n",
    "DEV_DIR=\"${PROJECT_DIR}/.devcontainer\"\n",
    "TARGET_DIR=\"${PROJECT_DIR}\"\n",
    "if [ ! -f \"${TARGET_DIR}/pyproject.toml\" ]; then\n",
    "  TARGET_DIR=\"${DEV_DIR}\"\n",
    "fi\n",
    "\n",
    "# Ensure .env exists first\n",
    "bash \"${DEV_DIR}/setup_env.sh\"\n",
    "\n",
    "export UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "# Dependency hash sentinel\n",
    "mkdir -p \"${DEV_DIR}\"\n",
    "dep_hash=\"$(\n",
    "  { [ -f \"${TARGET_DIR}/pyproject.toml\" ] && cat \"${TARGET_DIR}/pyproject.toml\"; } 2>/dev/null\n",
    "  { [ -f \"${TARGET_DIR}/uv.lock\" ] && cat \"${TARGET_DIR}/uv.lock\"; } 2>/dev/null\n",
    "  ) | sha256sum | awk '{print $1}'\n",
    "\"\n",
    "sentinel=\"${DEV_DIR}/.postcreate.hash\"\n",
    "prev_hash=\"$( [ -f \"${sentinel}\" ] && cat \"${sentinel}\" || echo \"\" )\"\n",
    "\n",
    "uv --version || true\n",
    "\n",
    "if [ \"${dep_hash}\" != \"${prev_hash}\" ]; then\n",
    "  echo \"[postCreate] Dependency hash changed or first run â†’ uv sync\"\n",
    "  (uv sync --frozen || uv sync)\n",
    "  echo \"${dep_hash}\" > \"${sentinel}\"\n",
    "else\n",
    "  echo \"[postCreate] Dependencies up-to-date; skipping uv sync\"\n",
    "fi\n",
    "\n",
    "python - <<'PY'\n",
    "import sys, subprocess\n",
    "print('[postCreate] Ensuring ipykernel present for', sys.executable)\n",
    "try:\n",
    "    import ipykernel  # noqa\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ipykernel'])\n",
    "subprocess.check_call([\n",
    "    sys.executable, '-m', 'ipykernel', 'install',\n",
    "    '--name', 'uv-app-venv',\n",
    "    '--display-name', 'Python (uv /app/.venv)',\n",
    "    '--user'\n",
    "])\n",
    "print('[postCreate] ipykernel installed/updated as uv-app-venv')\n",
    "PY\n",
    "\n",
    "echo \"[postCreate] Optional PyJAGS\"\n",
    "CPPFLAGS='-include cstdint' uv pip install --no-build-isolation pyjags==1.3.8 || true\n",
    "\n",
    "python \"${DEV_DIR}/verify_env.py\" || true\n",
    "echo \"## postCreate DONE ##\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/postCreate_rtx5080.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/postCreate_rtx5080.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"## postCreate RTX 5080 Optimized (idempotent) ##\"\n",
    "\n",
    "PROJECT_DIR=${PROJECT_DIR:-/workspace}\n",
    "DEV_DIR=\"${PROJECT_DIR}/.devcontainer\"\n",
    "TARGET_DIR=\"${PROJECT_DIR}\"\n",
    "if [ ! -f \"${TARGET_DIR}/pyproject.toml\" ]; then\n",
    "  TARGET_DIR=\"${DEV_DIR}\"\n",
    "fi\n",
    "\n",
    "# Ensure .env exists first\n",
    "bash \"${DEV_DIR}/setup_env.sh\"\n",
    "\n",
    "export UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "# --- CRITICAL: Environment hygiene for JAX ---\n",
    "echo \"[postCreate] Clearing JAX platform forcing for RTX 5080 compatibility...\"\n",
    "unset JAX_PLATFORM_NAME || true\n",
    "unset JAX_PLATFORMS || true\n",
    "\n",
    "# --- Memory management for RTX 5080 ---\n",
    "export TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "export TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "export XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "export XLA_PYTHON_CLIENT_MEM_FRACTION=0.25\n",
    "\n",
    "echo \"[postCreate] Using uv=$(uv --version || true)\"\n",
    "\n",
    "# Upgrade pip tooling inside uv venv for reliability\n",
    "uv pip install --upgrade pip wheel setuptools\n",
    "\n",
    "# Dependency hash sentinel\n",
    "mkdir -p \"${DEV_DIR}\"\n",
    "dep_hash=\"$(\n",
    "  { [ -f \"${TARGET_DIR}/pyproject.toml\" ] && cat \"${TARGET_DIR}/pyproject.toml\"; } 2>/dev/null\n",
    "  { [ -f \"${TARGET_DIR}/uv.lock\" ] && cat \"${TARGET_DIR}/uv.lock\"; } 2>/dev/null\n",
    "  ) | sha256sum | awk '{print $1}'\n",
    "\"\n",
    "sentinel=\"${DEV_DIR}/.postcreate.hash\"\n",
    "prev_hash=\"$( [ -f \"${sentinel}\" ] && cat \"${sentinel}\" || echo \"\" )\"\n",
    "\n",
    "if [ \"${dep_hash}\" != \"${prev_hash}\" ]; then\n",
    "  echo \"[postCreate] Dependency hash changed or first run â†’ uv sync\"\n",
    "  (uv sync --frozen || uv sync)\n",
    "  echo \"${dep_hash}\" > \"${sentinel}\"\n",
    "else\n",
    "  echo \"[postCreate] Dependencies up-to-date; skipping uv sync\"\n",
    "fi\n",
    "\n",
    "# ---- PyTorch (CUDA 12.8) ----\n",
    "echo \"[postCreate] Installing PyTorch nightly for RTX 5080 (CUDA 12.8)...\"\n",
    "uv pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "# ---- JAX (CUDA 12) ----\n",
    "echo \"[postCreate] Installing JAX with CUDA 12 support for RTX 5080...\"\n",
    "# Remove any existing JAX installations to avoid conflicts\n",
    "uv pip uninstall -y jax jaxlib jax-cuda12-plugin jax-cuda12-pjrt || true\n",
    "# Install JAX with proper CUDA 12 support\n",
    "uv pip install -U \"jax[cuda12]\"\n",
    "\n",
    "# ---- TensorFlow removed for efficiency ----\n",
    "echo \"[postCreate] TensorFlow installation skipped (removed for efficiency)\"\n",
    "\n",
    "# ---- Quick version check ----\n",
    "python - <<'PY'\n",
    "import os, importlib.util\n",
    "print(\"=== RTX 5080 Framework Versions ===\")\n",
    "try:\n",
    "    import torch; print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "except Exception as e: print(\"torch import failed:\", e)\n",
    "try:\n",
    "    import jax, jaxlib; print(\"jax:\", jax.__version__, \"jaxlib:\", jaxlib.__version__)\n",
    "except Exception as e: print(\"jax import failed:\", e)\n",
    "# TensorFlow removed for efficiency\n",
    "print(\"tf: removed for efficiency\")\n",
    "print(\"JAX_PLATFORM_NAME =\", os.getenv(\"JAX_PLATFORM_NAME\"))\n",
    "print(\"===================================\")\n",
    "PY\n",
    "\n",
    "# ---- Ipykernel setup ----\n",
    "python - <<'PY'\n",
    "import sys, subprocess\n",
    "print('[postCreate] Ensuring ipykernel present for', sys.executable)\n",
    "try:\n",
    "    import ipykernel  # noqa\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ipykernel'])\n",
    "subprocess.check_call([\n",
    "    sys.executable, '-m', 'ipykernel', 'install',\n",
    "    '--name', 'uv-app-venv',\n",
    "    '--display-name', 'Python (uv /app/.venv)',\n",
    "    '--user'\n",
    "])\n",
    "print('[postCreate] ipykernel installed/updated as uv-app-venv')\n",
    "PY\n",
    "\n",
    "echo \"[postCreate] Optional PyJAGS\"\n",
    "CPPFLAGS='-include cstdint' uv pip install --no-build-isolation pyjags==1.3.8 || true\n",
    "\n",
    "# ---- Framework verification (non-fatal) ----\n",
    "echo \"[postCreate] Running framework verification...\"\n",
    "python \"${DEV_DIR}/verify_env.py\" || {\n",
    "    echo \"[postCreate] Framework verification had issues - check logs above\"\n",
    "    echo \"[postCreate] This is non-fatal - container will continue\"\n",
    "}\n",
    "\n",
    "echo \"## postCreate RTX 5080 DONE ##\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/rtx5080_repair.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/rtx5080_repair.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"ðŸ”§ RTX 5080 GPU Framework Repair Script\"\n",
    "echo \"=======================================\"\n",
    "echo \"This script will fix JAX and TensorFlow issues for RTX 5080\"\n",
    "echo \"\"\n",
    "\n",
    "# Check if we're in the right environment\n",
    "if [[ ! -f \"/app/.venv/bin/activate\" ]]; then\n",
    "    echo \"âŒ Error: This script must be run inside the container with uv environment\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Activate the environment\n",
    "source /app/.venv/bin/activate\n",
    "\n",
    "echo \"ðŸ“‹ Current environment:\"\n",
    "echo \"   UV_PROJECT_ENVIRONMENT: ${UV_PROJECT_ENVIRONMENT:-<unset>}\"\n",
    "echo \"   Python: $(python --version)\"\n",
    "echo \"   uv: $(uv --version)\"\n",
    "echo \"\"\n",
    "\n",
    "# --- Step 1: Environment Cleanup ---\n",
    "echo \"ðŸ§¹ Step 1: Cleaning up environment variables...\"\n",
    "unset JAX_PLATFORM_NAME || true\n",
    "unset JAX_PLATFORMS || true\n",
    "\n",
    "# Set optimal environment variables\n",
    "export TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "export TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "export XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "export XLA_PYTHON_CLIENT_MEM_FRACTION=0.25\n",
    "\n",
    "echo \"   âœ… Environment variables configured\"\n",
    "echo \"\"\n",
    "\n",
    "# --- Step 2: JAX Repair ---\n",
    "echo \"ðŸ§  Step 2: Repairing JAX...\"\n",
    "echo \"   Removing existing JAX installations...\"\n",
    "\n",
    "# Remove all JAX-related packages\n",
    "uv pip uninstall -y jax jaxlib jax-cuda12-plugin jax-cuda12-pjrt || true\n",
    "\n",
    "echo \"   Installing JAX with CUDA 12 support...\"\n",
    "uv pip install -U \"jax[cuda12]\"\n",
    "\n",
    "echo \"   âœ… JAX installation completed\"\n",
    "echo \"\"\n",
    "\n",
    "# --- Step 3: TensorFlow removed for efficiency ---\n",
    "echo \"ðŸ”¢ Step 3: TensorFlow removed for efficiency...\"\n",
    "echo \"   TensorFlow installation skipped to improve build speed and reduce conflicts\"\n",
    "echo \"\"\n",
    "\n",
    "# --- Step 4: Verification ---\n",
    "echo \"ðŸ” Step 4: Verifying installations...\"\n",
    "\n",
    "python - <<'PY'\n",
    "import os, importlib.util\n",
    "print(\"=== Installation Verification ===\")\n",
    "\n",
    "# Check JAX\n",
    "try:\n",
    "    import jax, jaxlib\n",
    "    print(f\"âœ… JAX: {jax.__version__}\")\n",
    "    print(f\"âœ… JAXlib: {jaxlib.__version__}\")\n",
    "\n",
    "    # Check for CUDA plugin\n",
    "    has_cuda = importlib.util.find_spec(\"jax_cuda12_plugin\") or importlib.util.find_spec(\"jax_cuda12_pjrt\")\n",
    "    print(f\"âœ… JAX CUDA plugin: {'Yes' if has_cuda else 'No'}\")\n",
    "\n",
    "    # Check devices\n",
    "    try:\n",
    "        devices = jax.devices()\n",
    "        gpu_devices = [d for d in devices if 'gpu' in str(d).lower() or 'cuda' in str(d).lower()]\n",
    "        print(f\"âœ… JAX GPU devices: {len(gpu_devices)} found\")\n",
    "        for d in gpu_devices:\n",
    "            print(f\"   - {d}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JAX device check failed: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ JAX verification failed: {e}\")\n",
    "\n",
    "# TensorFlow removed for efficiency\n",
    "print(\"âœ… TensorFlow: removed for efficiency\")\n",
    "\n",
    "# Check PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "    print(f\"âœ… PyTorch CUDA: {torch.version.cuda}\")\n",
    "    print(f\"âœ… PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… PyTorch device: {torch.cuda.get_device_name(0)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ PyTorch verification failed: {e}\")\n",
    "\n",
    "print(\"=== Environment Variables ===\")\n",
    "print(f\"JAX_PLATFORM_NAME: {os.environ.get('JAX_PLATFORM_NAME', '<unset>')}\")\n",
    "print(f\"TF_FORCE_GPU_ALLOW_GROWTH: {os.environ.get('TF_FORCE_GPU_ALLOW_GROWTH', '<unset>')}\")\n",
    "print(\"=============================\")\n",
    "PY\n",
    "\n",
    "echo \"\"\n",
    "echo \"ðŸŽ‰ Repair completed!\"\n",
    "echo \"\"\n",
    "echo \"ðŸ’¡ Next steps:\"\n",
    "echo \"   1. Test the frameworks with: python .devcontainer/enhanced_gpu_test_functions.py\"\n",
    "echo \"   2. If issues persist, check the diagnostic output above\"\n",
    "echo \"   3. For TensorFlow PTX errors, consider building from source with CUDA 12.8+\"\n",
    "echo \"\"\n",
    "echo \"ðŸ”— Useful commands:\"\n",
    "echo \"   # Test all frameworks\"\n",
    "echo \"   python .devcontainer/enhanced_gpu_test_functions.py\"\n",
    "echo \"\"\n",
    "echo \"   # Quick JAX test\"\n",
    "echo \"   python -c \\\"import jax; print('JAX devices:', jax.devices())\\\"\"\n",
    "echo \"\"\n",
    "echo \"   # Quick TensorFlow test\"\n",
    "echo \"   python -c \\\"import tensorflow as tf; print('TF GPUs:', tf.config.list_physical_devices('GPU'))\\\"\"\n",
    "echo \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/enhanced_gpu_test_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/enhanced_gpu_test_functions.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enhanced GPU Test Functions for RTX 5080 (PyTorch + JAX Only)\n",
    "============================================================\n",
    "\n",
    "Drop-in replacements for the test functions in your notebook.\n",
    "These provide comprehensive diagnostics and actionable guidance for RTX 5080 issues.\n",
    "\n",
    "Usage:\n",
    "    # Import and use in your notebook\n",
    "    from .devcontainer.enhanced_gpu_test_functions import check_jax, check_pytorch, comprehensive_gpu_test\n",
    "\n",
    "    # Or run standalone\n",
    "    python .devcontainer/enhanced_gpu_test_functions.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import textwrap\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "def _run(cmd):\n",
    "    \"\"\"\n",
    "    Run a command and return (success, output).\n",
    "\n",
    "    Args:\n",
    "        cmd: Command to run\n",
    "\n",
    "    Returns:\n",
    "        tuple: (success: bool, output: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)\n",
    "        return True, out.strip()\n",
    "    except Exception as e:\n",
    "        return False, f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "\n",
    "def check_jax():\n",
    "    \"\"\"\n",
    "    Enhanced JAX GPU test with backend detection and Blackwell compatibility.\n",
    "\n",
    "    This function provides:\n",
    "    - Detection of CUDA backend plugins vs PJRT runtime\n",
    "    - Identification of mixed backend conflicts\n",
    "    - GPU device enumeration and computation testing\n",
    "    - Clear error messages for common configuration issues\n",
    "    - Actionable hints for RTX 5080 compatibility\n",
    "    \"\"\"\n",
    "    import os, importlib.util, textwrap\n",
    "    print(\"=== JAX ===\")\n",
    "    try:\n",
    "        import jax\n",
    "        print(f\"jax.__version__={jax.__version__}\")\n",
    "        try:\n",
    "            import jaxlib\n",
    "            print(f\"jaxlib.__version__={jaxlib.__version__}\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not import jaxlib:\", repr(e))\n",
    "\n",
    "        forced = os.environ.get(\"JAX_PLATFORM_NAME\"), os.environ.get(\"JAX_PLATFORMS\")\n",
    "        print(f\"JAX_PLATFORM_NAME={forced[0]!r} JAX_PLATFORMS={forced[1]!r}\")\n",
    "\n",
    "        # Is a CUDA PJRT plugin present?\n",
    "        has_cuda_extra = importlib.util.find_spec(\"jax_cuda12_plugin\") or importlib.util.find_spec(\"jax_cuda12_pjrt\")\n",
    "        print(f\"JAX CUDA plugin present? {bool(has_cuda_extra)}\")\n",
    "\n",
    "        try:\n",
    "            devs = jax.devices()\n",
    "            if not devs:\n",
    "                print(\"No JAX devices found.\")\n",
    "            else:\n",
    "                for d in devs:\n",
    "                    print(f\"Device: kind={getattr(d,'device_kind',None)}, platform={d.platform}, id={d.id}\")\n",
    "            # Smoke test\n",
    "            from jax import numpy as jnp\n",
    "            x = jnp.ones((1024,1024), dtype=jnp.float32)\n",
    "            y = (x @ x.T).sum()\n",
    "            _ = y.block_until_ready()\n",
    "            print(\"small matmul check (JAX default backend): OK\")\n",
    "        except RuntimeError as re:\n",
    "            msg = str(re)\n",
    "            print(\"jax.devices() raised RuntimeError:\", msg)\n",
    "            if \"Unknown backend: 'gpu'\" in msg:\n",
    "                print(textwrap.dedent(\"\"\"\n",
    "                    HINT: GPU was forced but CUDA-enabled jaxlib is missing.\n",
    "                    Fix:\n",
    "                      1) unset JAX_PLATFORM_NAME / JAX_PLATFORMS\n",
    "                      2) install GPU wheels (CUDA12):  uv pip install -U \"jax[cuda12]\"\n",
    "                \"\"\").strip())\n",
    "        except Exception as e:\n",
    "            print(\"jax.devices() failed:\", repr(e))\n",
    "    except Exception as e:\n",
    "        print(\"JAX import failed:\", repr(e))\n",
    "    print(\"===========\\n\")\n",
    "\n",
    "\n",
    "def check_pytorch():\n",
    "    \"\"\"\n",
    "    Enhanced PyTorch GPU test with memory management.\n",
    "\n",
    "    This function provides:\n",
    "    - CUDA availability and device information\n",
    "    - Memory cleanup after computation\n",
    "    - Version and capability reporting\n",
    "    - RTX 5080 specific compatibility checks\n",
    "    \"\"\"\n",
    "    print(\"=== PyTorch ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"torch.__version__={torch.__version__}\")\n",
    "        print(f\"torch.version.cuda={torch.version.cuda}\")\n",
    "        print(f\"torch.cuda.is_available()={torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"torch.cuda.device_count()={torch.cuda.device_count()}\")\n",
    "            print(f\"current device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "            # Test computation with memory cleanup\n",
    "            x = torch.randn(1024, 1024, device=\"cuda\")\n",
    "            y = torch.randn(1024, 1024, device=\"cuda\")\n",
    "            z = (x @ y).sum().item()\n",
    "            del x, y\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"small matmul check (PyTorch CUDA): OK\")\n",
    "        else:\n",
    "            print(\"CUDA not available in PyTorch.\")\n",
    "    except Exception as e:\n",
    "        print(\"PyTorch check failed:\", repr(e))\n",
    "    print(\"==============\\n\")\n",
    "\n",
    "\n",
    "def comprehensive_gpu_test():\n",
    "    \"\"\"\n",
    "    Run comprehensive GPU framework testing (PyTorch + JAX only).\n",
    "\n",
    "    This function tests PyTorch and JAX frameworks\n",
    "    and provides a summary of results with specific guidance for failures.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all frameworks passed, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª COMPREHENSIVE GPU FRAMEWORK TEST (PyTorch + JAX Only)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Testing GPU support for PyTorch and JAX...\")\n",
    "    print(\"Optimized for NVIDIA Blackwell RTX 5080 / CUDA 12.8\")\n",
    "    print(\"TensorFlow removed for faster builds and cleaner environment\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test frameworks\n",
    "    check_pytorch()\n",
    "    check_jax()  \n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… Comprehensive GPU test completed!\")\n",
    "    print(\"Check output above for any framework-specific issues.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for standalone execution.\"\"\"\n",
    "    import os\n",
    "\n",
    "    # Show environment snapshot\n",
    "    print(\"ðŸ”§ GPU ENVIRONMENT SNAPSHOT\")\n",
    "    print(\"=\" * 30)\n",
    "    env_vars = [\n",
    "        \"JAX_PLATFORM_NAME\", \"JAX_PLATFORMS\", \"CUDA_VISIBLE_DEVICES\",\n",
    "        \"XLA_FLAGS\", \"NVIDIA_VISIBLE_DEVICES\", \"NVIDIA_DRIVER_CAPABILITIES\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\"\n",
    "    ]\n",
    "    for var in env_vars:\n",
    "        value = os.environ.get(var, \"<unset>\")\n",
    "        print(f\"{var}={value}\")\n",
    "    print(\"=\" * 30)\n",
    "    print()\n",
    "\n",
    "    # Run comprehensive test\n",
    "    comprehensive_gpu_test()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/gpu_bootstrap.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/gpu_bootstrap.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euxo pipefail\n",
    "\n",
    "echo \"[gpu-bootstrap] BEGIN\"\n",
    "\n",
    "PY=\"/app/.venv/bin/python\"\n",
    "export UV_PROJECT_ENVIRONMENT=\"/app/.venv\"\n",
    "\n",
    "handle_error() {\n",
    "  echo \"âŒ GPU bootstrap error at line: ${1}\"\n",
    "  exit 1\n",
    "}\n",
    "trap 'handle_error ${LINENO}' ERR\n",
    "\n",
    "log() { echo \"[$(date '+%Y-%m-%d %H:%M:%S')] [gpu-bootstrap] $1\"; }\n",
    "_have_cmd() { command -v \"$1\" >/dev/null 2>&1; }\n",
    "\n",
    "seed_pip() {\n",
    "  if ! \"$PY\" -m pip --version >/dev/null 2>&1; then\n",
    "    log \"Seeding pip (ensurepip)\"\n",
    "    \"$PY\" -m ensurepip --upgrade || true\n",
    "  fi\n",
    "}\n",
    "\n",
    "PIP() {\n",
    "  if _have_cmd uv; then uv pip \"$@\"; else seed_pip; \"$PY\" -m pip \"$@\"; fi\n",
    "}\n",
    "PIP_SHOW() {\n",
    "  if _have_cmd uv; then uv pip show \"$@\" || true; else seed_pip; \"$PY\" -m pip show \"$@\" || true; fi\n",
    "}\n",
    "\n",
    "pick_torch_index() {\n",
    "  local tag=\"${CUDA_TAG:-12.8.0}\"\n",
    "  case \"$tag\" in\n",
    "    12.1* ) echo \"cu121\" ;;\n",
    "    12.4* ) echo \"cu124\" ;;\n",
    "    12.5*|12.6*|12.7*|12.8*|12.9* ) echo \"cu128\" ;;\n",
    "    * ) echo \"cu128\" ;;\n",
    "  esac\n",
    "}\n",
    "\n",
    "log \"Env:\"\n",
    "echo \"  whoami=$(whoami)\"\n",
    "echo \"  PY=$PY\"\n",
    "echo \"  UV_PROJECT_ENVIRONMENT=${UV_PROJECT_ENVIRONMENT}\"\n",
    "echo \"  CUDA_TAG=${CUDA_TAG:-12.8.0}\"\n",
    "\n",
    "$PY - <<'PY'\n",
    "import sys, os\n",
    "print(\"[gpu-bootstrap] sys.executable:\", sys.executable)\n",
    "print(\"[gpu-bootstrap] VIRTUAL_ENV:\", os.environ.get(\"VIRTUAL_ENV\",\"<unset>\"))\n",
    "for m in (\"torch\",\"jax\",\"jaxlib\"):\n",
    "    try:\n",
    "        mod = __import__(m)\n",
    "        print(f\"[gpu-bootstrap] pre: import {m}: OK from\", getattr(mod,\"__file__\",\"?\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[gpu-bootstrap] pre: import {m}: FAIL -> {e.__class__.__name__}: {e}\")\n",
    "PY\n",
    "\n",
    "# GPU availability\n",
    "if _have_cmd nvidia-smi; then\n",
    "  log \"nvidia-smi present\"\n",
    "  nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader,nounits || true\n",
    "else\n",
    "  log \"No nvidia-smi; GPU likely not available\"\n",
    "fi\n",
    "\n",
    "unset JAX_PLATFORM_NAME || true\n",
    "\n",
    "# --- 1) Ensure PyTorch (GPU) -------------------------------------------------\n",
    "log \"Checking PyTorch CUDA availability...\"\n",
    "if $PY -c \"import torch, sys; sys.exit(0 if torch.cuda.is_available() else 1)\" 2>/dev/null; then\n",
    "  log \"âœ… PyTorch CUDA available\"\n",
    "else\n",
    "  if _have_cmd nvidia-smi; then\n",
    "    IDX=\"$(pick_torch_index)\"\n",
    "    log \"Installing PyTorch (${IDX})\"\n",
    "    PIP install --no-cache-dir torch torchvision torchaudio --index-url \"https://download.pytorch.org/whl/${IDX}\"\n",
    "  else\n",
    "    log \"âš ï¸  Skipping PyTorch GPU install (no nvidia-smi)\"\n",
    "  fi\n",
    "fi\n",
    "\n",
    "# --- 2) Enforce single JAX PJRT backend (prefer local plugin) ---------------\n",
    "log \"Reconciling JAX backend packages...\"\n",
    "HAS_PLUGIN=0\n",
    "HAS_PJRT=0\n",
    "$PY - <<'PY'\n",
    "import importlib.util, sys\n",
    "def have(pkg):\n",
    "    return importlib.util.find_spec(pkg) is not None\n",
    "print(\"has_plugin\", have(\"jax_cuda12_plugin\"))\n",
    "print(\"has_pjrt\", have(\"jax_cuda12_pjrt\"))\n",
    "PY | tee /tmp/jax_backends.txt >/dev/null\n",
    "\n",
    "if grep -q \"has_plugin True\" /tmp/jax_backends.txt; then HAS_PLUGIN=1; fi\n",
    "if grep -q \"has_pjrt True\" /tmp/jax_backends.txt; then HAS_PJRT=1; fi\n",
    "\n",
    "if [ \"$HAS_PLUGIN\" -eq 1 ] && [ \"$HAS_PJRT\" -eq 1 ]; then\n",
    "  log \"Both jax-cuda backends present â†’ uninstall pjrt\"\n",
    "  PIP uninstall -y jax-cuda12-pjrt || true\n",
    "  HAS_PJRT=0\n",
    "fi\n",
    "\n",
    "# Policy: keep LOCAL plugin (system CUDA) and remove pip NVIDIA runtime libs\n",
    "if [ \"$HAS_PLUGIN\" -eq 0 ]; then\n",
    "  log \"Installing jax[cuda12-local] (PJRT CUDA plugin)\"\n",
    "  PIP install --no-cache-dir \"jax[cuda12-local]>=0.4.26\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "fi\n",
    "\n",
    "# Remove pip-provided NVIDIA CUDA stacks to avoid mixing with system CUDA\n",
    "log \"Removing pip NVIDIA CUDA stacks (if any) to prevent double-lib loads\"\n",
    "PIP uninstall -y \\\n",
    "  nvidia-cublas-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 \\\n",
    "  nvidia-cuda-cupti-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 \\\n",
    "  nvidia-nvjitlink-cu12 nvidia-nvtx-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 \\\n",
    "  || true\n",
    "\n",
    "# Re-probe JAX devices\n",
    "log \"Probing JAX devices...\"\n",
    "$PY - <<'PY'\n",
    "import sys\n",
    "try:\n",
    "    import jax\n",
    "    print(\"[gpu-bootstrap] JAX\", jax.__version__, \"devices:\", jax.devices())\n",
    "    # sanity: ensure at most one CUDA PJRT present\n",
    "    import importlib.util as u\n",
    "    print(\"[gpu-bootstrap] has plugin:\", u.find_spec(\"jax_cuda12_plugin\") is not None)\n",
    "    print(\"[gpu-bootstrap] has pjrt:\", u.find_spec(\"jax_cuda12_pjrt\") is not None)\n",
    "except Exception as e:\n",
    "    print(\"[gpu-bootstrap] JAX import/probe error:\", e)\n",
    "    sys.exit(1)\n",
    "PY\n",
    "\n",
    "log \"Package snapshot:\"\n",
    "PIP_SHOW jax jaxlib jax-cuda12-plugin jax-cuda12-pjrt torch torchvision torchaudio || true\n",
    "\n",
    "log \"END\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/troubleshoot.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/troubleshoot.sh\n",
    "#!/bin/bash\n",
    "set -euo pipefail\n",
    "\n",
    "# DevContainer GPU Troubleshooting Script\n",
    "# This script provides comprehensive diagnostics for GPU-enabled ML development environments\n",
    "\n",
    "echo \"ðŸ” DevContainer GPU Troubleshooting Report\"\n",
    "echo \"==========================================\"\n",
    "echo \"Generated: $(date)\"\n",
    "echo \"\"\n",
    "\n",
    "# Function for logging\n",
    "log() {\n",
    "    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\"\n",
    "}\n",
    "\n",
    "# Function for section headers\n",
    "section() {\n",
    "    echo \"\"\n",
    "    echo \"ðŸ“‹ $1\"\n",
    "    echo \"$(printf '%.0s-' {1..50})\"\n",
    "}\n",
    "\n",
    "# Function for status indicators\n",
    "status_ok() { echo \"âœ… $1\"; }\n",
    "status_warn() { echo \"âš ï¸  $1\"; }\n",
    "status_error() { echo \"âŒ $1\"; }\n",
    "status_info() { echo \"â„¹ï¸  $1\"; }\n",
    "\n",
    "# Phase 1: System Prerequisites\n",
    "section \"System Prerequisites\"\n",
    "\n",
    "# Check if we're in a container\n",
    "if [ -f /.dockerenv ]; then\n",
    "    status_ok \"Running in Docker container\"\n",
    "else\n",
    "    status_warn \"Not running in Docker container\"\n",
    "fi\n",
    "\n",
    "# Check OS\n",
    "if [ -f /etc/os-release ]; then\n",
    "    . /etc/os-release\n",
    "    status_info \"OS: $PRETTY_NAME\"\n",
    "else\n",
    "    status_warn \"Could not determine OS\"\n",
    "fi\n",
    "\n",
    "# Check Python environment\n",
    "section \"Python Environment\"\n",
    "\n",
    "PYTHON_VERSION=$(python --version 2>&1 || echo \"Python not found\")\n",
    "status_info \"Python: $PYTHON_VERSION\"\n",
    "\n",
    "if [ -n \"${VIRTUAL_ENV:-}\" ]; then\n",
    "    status_ok \"Virtual environment active: $VIRTUAL_ENV\"\n",
    "else\n",
    "    status_warn \"No virtual environment detected\"\n",
    "fi\n",
    "\n",
    "# Check uv\n",
    "if command -v uv >/dev/null 2>&1; then\n",
    "    UV_VERSION=$(uv --version)\n",
    "    status_ok \"uv available: $UV_VERSION\"\n",
    "else\n",
    "    status_error \"uv not found\"\n",
    "fi\n",
    "\n",
    "# Phase 2: GPU Hardware & Drivers\n",
    "section \"GPU Hardware & Drivers\"\n",
    "\n",
    "# Check for NVIDIA drivers\n",
    "if command -v nvidia-smi >/dev/null 2>&1; then\n",
    "    status_ok \"nvidia-smi available\"\n",
    "\n",
    "    # Try to run nvidia-smi\n",
    "    if nvidia-smi >/dev/null 2>&1; then\n",
    "        status_ok \"GPU accessible\"\n",
    "\n",
    "        # Get GPU info\n",
    "        echo \"\"\n",
    "        echo \"GPU Information:\"\n",
    "        nvidia-smi --query-gpu=name,memory.total,driver_version,compute_cap --format=csv,noheader,nounits | while IFS=, read -r name memory driver compute; do\n",
    "            echo \"  GPU: $name\"\n",
    "            echo \"  Memory: $memory MB\"\n",
    "            echo \"  Driver: $driver\"\n",
    "            echo \"  Compute Capability: $compute\"\n",
    "        done\n",
    "    else\n",
    "        status_error \"nvidia-smi failed to run\"\n",
    "    fi\n",
    "else\n",
    "    status_error \"nvidia-smi not available\"\n",
    "fi\n",
    "\n",
    "# Check CUDA installation\n",
    "section \"CUDA Installation\"\n",
    "\n",
    "if [ -d \"/usr/local/cuda\" ]; then\n",
    "    CUDA_VERSION=$(cat /usr/local/cuda/version.txt 2>/dev/null | grep \"CUDA Version\" | cut -d' ' -f3 || echo \"Unknown\")\n",
    "    status_ok \"CUDA installed: $CUDA_VERSION\"\n",
    "\n",
    "    # Check CUDA libraries\n",
    "    if [ -f \"/usr/local/cuda/lib64/libcudart.so\" ]; then\n",
    "        status_ok \"CUDA runtime libraries found\"\n",
    "    else\n",
    "        status_warn \"CUDA runtime libraries not found\"\n",
    "    fi\n",
    "\n",
    "    if [ -f \"/usr/local/cuda/lib64/libcudnn.so\" ]; then\n",
    "        status_ok \"cuDNN libraries found\"\n",
    "    else\n",
    "        status_warn \"cuDNN libraries not found\"\n",
    "    fi\n",
    "else\n",
    "    status_error \"CUDA not installed\"\n",
    "fi\n",
    "\n",
    "# Check environment variables\n",
    "section \"GPU Environment Variables\"\n",
    "\n",
    "ENV_VARS=(\n",
    "    \"NVIDIA_VISIBLE_DEVICES\"\n",
    "    \"CUDA_VISIBLE_DEVICES\"\n",
    "    \"LD_LIBRARY_PATH\"\n",
    "    \"PATH\"\n",
    "    \"XLA_PYTHON_CLIENT_PREALLOCATE\"\n",
    "    \"XLA_PYTHON_CLIENT_MEM_FRACTION\"\n",
    "    \"PYTORCH_CUDA_ALLOC_CONF\"\n",
    "    \"TF_FORCE_GPU_ALLOW_GROWTH\"\n",
    ")\n",
    "\n",
    "for var in \"${ENV_VARS[@]}\"; do\n",
    "    if [ -n \"${!var:-}\" ]; then\n",
    "        status_ok \"$var: ${!var}\"\n",
    "    else\n",
    "        status_warn \"$var: not set\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "# Phase 3: ML Framework Installation\n",
    "section \"ML Framework Installation\"\n",
    "\n",
    "# Check PyTorch\n",
    "if python -c \"import torch; print(f'PyTorch {torch.__version__}')\" 2>/dev/null; then\n",
    "    PYTORCH_VERSION=$(python -c \"import torch; print(torch.__version__)\" 2>/dev/null)\n",
    "    status_ok \"PyTorch installed: $PYTORCH_VERSION\"\n",
    "\n",
    "    # Check CUDA support\n",
    "    if python -c \"import torch; print('CUDA available:', torch.cuda.is_available())\" 2>/dev/null; then\n",
    "        CUDA_AVAILABLE=$(python -c \"import torch; print(torch.cuda.is_available())\" 2>/dev/null)\n",
    "        if [ \"$CUDA_AVAILABLE\" = \"True\" ]; then\n",
    "            status_ok \"PyTorch CUDA support: Available\"\n",
    "            CUDA_COUNT=$(python -c \"import torch; print(torch.cuda.device_count())\" 2>/dev/null)\n",
    "            status_info \"CUDA devices: $CUDA_COUNT\"\n",
    "        else\n",
    "            status_warn \"PyTorch CUDA support: Not available\"\n",
    "        fi\n",
    "    fi\n",
    "else\n",
    "    status_error \"PyTorch not installed\"\n",
    "fi\n",
    "\n",
    "# Check JAX\n",
    "if python -c \"import jax; print(f'JAX {jax.__version__}')\" 2>/dev/null; then\n",
    "    JAX_VERSION=$(python -c \"import jax; print(jax.__version__)\" 2>/dev/null)\n",
    "    status_ok \"JAX installed: $JAX_VERSION\"\n",
    "\n",
    "    # Check JAX devices\n",
    "    if python -c \"import jax; print('JAX devices:', jax.devices())\" 2>/dev/null; then\n",
    "        JAX_DEVICES=$(python -c \"import jax; print(jax.devices())\" 2>/dev/null)\n",
    "        status_info \"JAX devices: $JAX_DEVICES\"\n",
    "\n",
    "        # Check for GPU devices\n",
    "        if echo \"$JAX_DEVICES\" | grep -q \"gpu\\|cuda\"; then\n",
    "            status_ok \"JAX GPU support: Available\"\n",
    "        else\n",
    "            status_warn \"JAX GPU support: CPU only\"\n",
    "        fi\n",
    "    fi\n",
    "else\n",
    "    status_error \"JAX not installed\"\n",
    "fi\n",
    "\n",
    "# TensorFlow removed for efficiency\n",
    "status_info \"TensorFlow: Removed for efficiency (PyTorch + JAX only)\"\n",
    "\n",
    "# Phase 4: Package Manager Status\n",
    "section \"Package Manager Status\"\n",
    "\n",
    "# Check uv sync status\n",
    "if [ -f \"pyproject.toml\" ]; then\n",
    "    status_ok \"pyproject.toml found\"\n",
    "\n",
    "    if [ -f \"uv.lock\" ]; then\n",
    "        status_ok \"uv.lock found\"\n",
    "    else\n",
    "        status_warn \"uv.lock not found (run 'uv sync' to generate)\"\n",
    "    fi\n",
    "\n",
    "    # Check if dependencies are installed\n",
    "    if python -c \"import pandas, numpy, matplotlib\" 2>/dev/null; then\n",
    "        status_ok \"Core dependencies installed\"\n",
    "    else\n",
    "        status_error \"Core dependencies missing\"\n",
    "    fi\n",
    "else\n",
    "    status_warn \"pyproject.toml not found in current directory\"\n",
    "fi\n",
    "\n",
    "# Phase 5: Network & Connectivity\n",
    "section \"Network & Connectivity\"\n",
    "\n",
    "# Check internet connectivity\n",
    "if ping -c 1 8.8.8.8 >/dev/null 2>&1; then\n",
    "    status_ok \"Internet connectivity available\"\n",
    "else\n",
    "    status_error \"No internet connectivity\"\n",
    "fi\n",
    "\n",
    "# Check PyPI connectivity\n",
    "if curl -s https://pypi.org/simple/ >/dev/null 2>&1; then\n",
    "    status_ok \"PyPI accessible\"\n",
    "else\n",
    "    status_error \"PyPI not accessible\"\n",
    "fi\n",
    "\n",
    "# Phase 6: Recommendations\n",
    "section \"Recommendations\"\n",
    "\n",
    "echo \"Based on the diagnostics above, here are some recommendations:\"\n",
    "echo \"\"\n",
    "\n",
    "if ! command -v nvidia-smi >/dev/null 2>&1; then\n",
    "    echo \"âŒ Install NVIDIA drivers and NVIDIA Container Toolkit\"\n",
    "    echo \"   - For Ubuntu: sudo apt-get install nvidia-container-toolkit\"\n",
    "    echo \"   - Restart Docker daemon after installation\"\n",
    "fi\n",
    "\n",
    "if ! python -c \"import torch; torch.cuda.is_available()\" 2>/dev/null | grep -q \"True\"; then\n",
    "    echo \"âš ï¸  PyTorch CUDA support not working\"\n",
    "    echo \"   - Check CUDA version compatibility\"\n",
    "    echo \"   - Reinstall PyTorch with: uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\"\n",
    "fi\n",
    "\n",
    "if ! python -c \"import jax; 'gpu' in str(jax.devices()).lower()\" 2>/dev/null; then\n",
    "    echo \"âš ï¸  JAX GPU support not working\"\n",
    "    echo \"   - Install JAX with CUDA: uv pip install 'jax[cuda12-local]' -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\"\n",
    "fi\n",
    "\n",
    "if [ ! -f \"uv.lock\" ]; then\n",
    "    echo \"âš ï¸  Run 'uv sync' to install dependencies\"\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"ðŸ” Troubleshooting completed at $(date)\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/setup_env.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/setup_env.sh\n",
    "#!/usr/bin/env sh\n",
    "set -eu\n",
    "if [ ! -f /workspace/.env ]; then\n",
    "  echo \"ðŸ“  Generating default .env from template\"\n",
    "  cp /workspace/.devcontainer/.env.template /workspace/.env\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/docker-compose.yml\n",
    "# .devcontainer/docker-compose.yml - TensorFlow removed, optimized for faster builds\n",
    "name: ${ENV_NAME:-cancer_bayes_iris_env}\n",
    "\n",
    "services:\n",
    "  datascience:\n",
    "    build:\n",
    "      context: ..\n",
    "      dockerfile: .devcontainer/Dockerfile\n",
    "      args:\n",
    "        CUDA_TAG: ${CUDA_TAG:-12.8.0}\n",
    "        PYTHON_VER: ${PYTHON_VER:-3.10}\n",
    "        ENV_NAME: ${ENV_NAME:-cancer_bayes_iris_env}\n",
    "      # Enable BuildKit for better caching\n",
    "      target: \"\"\n",
    "      cache_from:\n",
    "        - nvidia/cuda:${CUDA_TAG:-12.8.0}-runtime-ubuntu22.04\n",
    "\n",
    "    restart: unless-stopped\n",
    "    depends_on:\n",
    "      mlflow:\n",
    "        condition: service_healthy\n",
    "\n",
    "    # GPU configuration - simplified\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: all\n",
    "              capabilities: [gpu]\n",
    "\n",
    "    init: true\n",
    "    gpus: all\n",
    "    shm_size: 4g  # Reduced from 8g since no TensorFlow\n",
    "    ulimits:\n",
    "      memlock: -1\n",
    "      stack: 67108864\n",
    "\n",
    "    environment:\n",
    "      # Core configuration\n",
    "      - PYTHON_VER=${PYTHON_VER:-3.10}\n",
    "      - UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "      # GPU Environment  \n",
    "      - NVIDIA_VISIBLE_DEVICES=all\n",
    "      - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
    "      - CUDA_VISIBLE_DEVICES=0\n",
    "      - LD_LIBRARY_PATH=/app/.venv/lib:/usr/local/cuda/lib64\n",
    "\n",
    "      # Simplified memory management (jemalloc)\n",
    "      - LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2\n",
    "      - MALLOC_ARENA_MAX=1\n",
    "      - MALLOC_TCACHE_MAX=0\n",
    "      - PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
    "\n",
    "      # JAX Configuration (RTX 5080 optimized)\n",
    "      - XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "      - XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "      - XLA_PYTHON_CLIENT_MEM_FRACTION=0.25\n",
    "      - XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda\n",
    "      - JAX_PREALLOCATION_SIZE_LIMIT_BYTES=8589934592\n",
    "\n",
    "      # PyTorch Configuration (RTX 5080 optimized)\n",
    "      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512,expandable_segments:True\n",
    "\n",
    "      # Jupyter\n",
    "      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-jupyter}\n",
    "\n",
    "    volumes:\n",
    "      - ..:/workspace:delegated\n",
    "      - ../mlruns:/workspace/mlruns\n",
    "      # Cache mounts for faster rebuilds\n",
    "      - uv-cache:/root/.cache/uv\n",
    "\n",
    "    ports:\n",
    "      - \"${HOST_JUPYTER_PORT:-8890}:8888\"\n",
    "      - \"${HOST_TENSORBOARD_PORT:-6008}:6008\"\n",
    "      - \"${HOST_EXPLAINER_PORT:-8050}:8050\"\n",
    "      - \"${HOST_STREAMLIT_PORT:-8501}:8501\"\n",
    "\n",
    "    command: >\n",
    "      bash -lc '\n",
    "        echo \"[boot] Activating environment...\";\n",
    "        source /app/.venv/bin/activate;\n",
    "        echo \"[boot] Starting Jupyter Lab...\";\n",
    "        jupyter lab --ip=0.0.0.0 --port=8888 --allow-root \n",
    "        --NotebookApp.token=\"${JUPYTER_TOKEN}\" \n",
    "        --NotebookApp.allow_origin=\"*\" \n",
    "        --NotebookApp.open_browser=false\n",
    "      '\n",
    "\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"python -c 'import jupyterlab' 2>/dev/null || exit 1\"]\n",
    "      interval: 30s\n",
    "      timeout: 5s\n",
    "      retries: 3\n",
    "      start_period: 30s  # Reduced from 60s\n",
    "\n",
    "    labels:\n",
    "      - \"com.docker.compose.project=${ENV_NAME:-cancer_bayes_iris_env}\"\n",
    "      - \"com.docker.compose.service=datascience\"\n",
    "      - \"description=AI/ML Dev Env (PyTorch+JAX GPU)\"\n",
    "\n",
    "  mlflow:\n",
    "    image: ghcr.io/mlflow/mlflow:latest\n",
    "    command: >\n",
    "      mlflow server\n",
    "      --host 0.0.0.0\n",
    "      --port 5000\n",
    "      --backend-store-uri sqlite:///mlflow.db\n",
    "      --default-artifact-root /mlflow_artifacts\n",
    "    environment:\n",
    "      MLFLOW_EXPERIMENTS_DEFAULT_ARTIFACT_LOCATION: /mlflow_artifacts\n",
    "    volumes:\n",
    "      - ../mlruns:/mlflow_artifacts\n",
    "      - ../mlflow_db:/mlflow_db\n",
    "    ports:\n",
    "      - \"${HOST_MLFLOW_PORT:-5000}:5000\"\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"python\", \"-c\", \"import requests; requests.get('http://localhost:5000/health').raise_for_status()\"]\n",
    "      interval: 10s\n",
    "      timeout: 3s\n",
    "      retries: 5\n",
    "      start_period: 15s  # Reduced from 30s\n",
    "\n",
    "# Named volume for uv cache persistence\n",
    "volumes:\n",
    "  uv-cache:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/pyproject.toml\n",
    "[project]\n",
    "name = \"cancer_bayes_iris_env\"\n",
    "version = \"0.1.0\"\n",
    "description = \"PyTorch and JAX GPU docker container (TensorFlow removed)\"\n",
    "authors = [{ name = \"Geoffrey Hadfield\" }]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  \"pandas>=1.2.0\",\n",
    "  \"numpy>=1.20.0\",\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"mlflow>=2.10.2\",\n",
    "  \"mlflow-skinny>=2.10.2\",\n",
    "  \"scikit-learn>=1.4.2\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.2.8,<1.3.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard==0.5.1\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",\n",
    "  \"numpyro>=0.18.0,<1.0.0\",\n",
    "  \"pytensor>=2.18.3\",\n",
    "  \"aesara>=2.9.4\",\n",
    "  \"tqdm>=4.66.5\",\n",
    "  \"pyarrow>=12.0.0\",\n",
    "  \"optuna>=3.0.0\",\n",
    "  \"optuna-integration[mlflow]>=0.2.0\",\n",
    "  \"omegaconf>=2.3.0,<2.4.0\",\n",
    "  \"hydra-core>=1.3.2,<1.4.0\",\n",
    "  \"fastapi>=0.104.0\",\n",
    "  \"uvicorn[standard]>=0.24.0\",\n",
    "  \"pydantic>=2.0.0\",\n",
    "  \"pydantic-settings\",\n",
    "  \"kagglehub[pandas-datasets]>=0.3.12,<0.4.0\",\n",
    "  \"duckdb~=1.3.2\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"invoke>=2.2\",\n",
    "]\n",
    "\n",
    "# GPU frameworks for Linux only (TensorFlow completely removed)\n",
    "linux-gpu = [\n",
    "  \"torch      ; platform_system == 'Linux'\",\n",
    "  \"torchvision; platform_system == 'Linux'\",\n",
    "  \"torchaudio ; platform_system == 'Linux'\",\n",
    "  \"jax[cuda12-local]>=0.4.26 ; platform_system == 'Linux'\",\n",
    "]\n",
    "\n",
    "memory-management = [\n",
    "  \"psutil>=5.9.0\",\n",
    "  \"memory-profiler>=0.61.0\",\n",
    "]\n",
    "\n",
    "[tool.pytensor]\n",
    "device = \"cuda\"\n",
    "floatX = \"float32\"\n",
    "allow_gc = true\n",
    "optimizer = \"fast_run\"\n",
    "\n",
    "[tool.uv]\n",
    "compile-bytecode = true\n",
    "link-mode = \"copy\"\n",
    "python-downloads = \"never\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/tasks.py\n",
    "# tasks.py  â”€â”€ invoke â‰¥2.2\n",
    "from invoke import task, Context  # type: ignore\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import tempfile\n",
    "import datetime as _dt\n",
    "import atexit\n",
    "import socket\n",
    "import contextlib\n",
    "import errno\n",
    "\n",
    "\n",
    "BASE_ENV = pathlib.Path(__file__).parent\n",
    "\n",
    "\n",
    "# Track temporary env files for cleanup\n",
    "_saved_env_files: List[str] = []\n",
    "\n",
    "\n",
    "def _parse_port(port: Union[str, int, None]) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Parse and validate a port number.\n",
    "    \n",
    "    Args:\n",
    "        port: Port number as string or int, or None\n",
    "        \n",
    "    Returns:\n",
    "        Validated port number as int, or None if input was None\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If port is invalid or out of range\n",
    "    \"\"\"\n",
    "    if port is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        port_int = int(port)\n",
    "        if not (0 < port_int < 65536):\n",
    "            raise ValueError(f\"Port {port_int} out of valid range (1-65535)\")\n",
    "        return port_int\n",
    "    except (TypeError, ValueError) as e:\n",
    "        raise ValueError(f\"Invalid port value: {port}\") from e\n",
    "\n",
    "\n",
    "def _first_free_port(start: int = 5200) -> int:\n",
    "    \"\"\"Return the first TCP port >= *start* that is unused on localhost.\"\"\"\n",
    "    print(f\"DEBUG: Searching for free port starting at {start}\")  # Debug\n",
    "    import socket\n",
    "    import contextlib\n",
    "    for port in range(start, 65535):\n",
    "        with contextlib.closing(socket.socket()) as s:\n",
    "            if s.connect_ex((\"127.0.0.1\", port)):\n",
    "                print(f\"DEBUG: Found free port {port}\")  # Debug\n",
    "                return port\n",
    "    raise RuntimeError(\"No free port found\")\n",
    "\n",
    "\n",
    "def _free_port(start=5200) -> int:\n",
    "    \"\"\"Find a free port by letting the OS assign one.\"\"\"\n",
    "    print(f\"DEBUG: Finding free port starting at {start}\")  # Debug\n",
    "    import socket\n",
    "    import contextlib\n",
    "    with contextlib.closing(\n",
    "        socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    ) as s:\n",
    "        s.bind(('', 0))\n",
    "        port = s.getsockname()[1]\n",
    "        print(f\"DEBUG: Found free port {port}\")  # Debug\n",
    "        return port\n",
    "\n",
    "\n",
    "def _port_free(host: str, port: int, timeout: float = 0.1) -> bool:\n",
    "    \"\"\"\n",
    "    Return True iff *host:port* is NOT in use.\n",
    "\n",
    "    Uses a non-blocking TCP connect â€“ works on Linux, macOS, Windows,\n",
    "    inside or outside WSL â€“ and does **not** rely on lsof / netstat.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Checking if port {port} is free on {host}\")  # Debug\n",
    "    try:\n",
    "        with contextlib.closing(\n",
    "            socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        ) as s:\n",
    "            s.settimeout(timeout)\n",
    "            s.connect((host, port))\n",
    "            print(f\"DEBUG: Port {port} is in use\")  # Debug\n",
    "            return False      # connection succeeded â‡’ something listening\n",
    "    except (OSError, socket.timeout):\n",
    "        print(f\"DEBUG: Port {port} is free\")  # Debug\n",
    "        return True           # connection failed â‡’ port is free\n",
    "\n",
    "\n",
    "def _find_port(preferred: int, start: int = 5200) -> int:\n",
    "    \"\"\"\n",
    "    Try to use preferred port, fall back to finding first available port.\n",
    "    \n",
    "    Args:\n",
    "        preferred: The preferred port number to try first\n",
    "        start: Where to start searching if preferred port is taken\n",
    "        \n",
    "    Returns:\n",
    "        An available port number\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Trying preferred port {preferred}\")  # Debug\n",
    "    if _port_free(\"127.0.0.1\", preferred):\n",
    "        return preferred\n",
    "    return _first_free_port(start)\n",
    "\n",
    "\n",
    "def _write_envfile(name: str, \n",
    "                   ports: Optional[dict[str, int]] = None) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Create a throw-away .env file for the current `invoke up` run.\n",
    "    \n",
    "    Docker-compose will use this to see the chosen host-ports. We include all\n",
    "    services we know about; anything unset falls back to .env.template defaults.\n",
    "    \"\"\"\n",
    "    env_lines = [f\"ENV_NAME={name}\"]\n",
    "    mapping = {\n",
    "        \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "        \"tensorboard\": \"HOST_TENSORBOARD_PORT\",\n",
    "        \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "        \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "        \"mlflow\": \"HOST_MLFLOW_PORT\",      # NEW\n",
    "    }\n",
    "    for svc, var in mapping.items():\n",
    "        if ports and svc in ports:\n",
    "            env_lines.append(f\"{var}={ports[svc]}\")\n",
    "    env_lines.append(f\"# generated {_dt.datetime.now().isoformat()}\")\n",
    "    tmp = tempfile.NamedTemporaryFile(\n",
    "        \"w\", \n",
    "        delete=False, \n",
    "        prefix=\".env.\",\n",
    "        dir=BASE_ENV\n",
    "    )\n",
    "    tmp.write(\"\\n\".join(env_lines))\n",
    "    tmp.close()\n",
    "    _saved_env_files.append(tmp.name)\n",
    "    return pathlib.Path(tmp.name)\n",
    "\n",
    "\n",
    "# Register cleanup function\n",
    "def _cleanup_env_files() -> None:\n",
    "    \"\"\"Remove all temporary env files.\"\"\"\n",
    "    for path in _saved_env_files:\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "atexit.register(_cleanup_env_files)\n",
    "\n",
    "\n",
    "def _compose(\n",
    "    c: Context,\n",
    "    cmd: str,\n",
    "    name: str,\n",
    "    rebuild: bool = False,\n",
    "    force_pty: bool = False,\n",
    "    ports: Optional[dict[str, int]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Wrapper around `docker compose` that also sanity-checks host ports.\n",
    "    \"\"\"\n",
    "    # ---------- NEW pre-flight check --------------------------------------\n",
    "    if ports:\n",
    "        for svc, port in ports.items():\n",
    "            if port is None:\n",
    "                continue\n",
    "            if not _port_free(\"127.0.0.1\", int(port)):\n",
    "                print(f\"âŒ  Host port {port} already bound â€“ \"\n",
    "                      f\"{svc} cannot start. Choose another port (invoke up \"\n",
    "                      f\"--{svc}-port XXXXX) or free it first.\")\n",
    "                sys.exit(1)\n",
    "\n",
    "    env = {**os.environ, \"ENV_NAME\": name, \"COMPOSE_PROJECT_NAME\": name}\n",
    "    \n",
    "    # Add port overrides if provided\n",
    "    if ports:\n",
    "        port_mapping = {\n",
    "            \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "            \"tensorboard\": \"HOST_TENSORBOARD_PORT\", \n",
    "            \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "            \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "        }\n",
    "        for service, port in ports.items():\n",
    "            if service in port_mapping:\n",
    "                env[port_mapping[service]] = str(port)\n",
    "    \n",
    "    use_pty = force_pty or (os.name != \"nt\" and sys.stdin.isatty())\n",
    "\n",
    "    if not use_pty and not getattr(_compose, \"_warned\", False):\n",
    "        print(\"â„¹ï¸  PTY not supported â€“ running without TTY.\")\n",
    "        _compose._warned = True  # type: ignore[attr-defined]\n",
    "\n",
    "    if rebuild:\n",
    "        full_cmd = f\"docker compose -p {name} {cmd} --build\"\n",
    "    else:\n",
    "        full_cmd = f\"docker compose -p {name} {cmd}\"\n",
    "    c.run(full_cmd, env=env, pty=use_pty)\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "        \"use_pty\": \"Force PTY even on non-POSIX hosts\",\n",
    "        \"jupyter_port\": \"Jupyter Lab port (default: 8890)\",\n",
    "        \"tensorboard_port\": \"TensorBoard port (default: auto-assigned)\",\n",
    "        \"explainer_port\": \"Explainer Dashboard port (default: auto-assigned)\", \n",
    "        \"streamlit_port\": \"Streamlit port (default: auto-assigned)\",\n",
    "        \"mlflow_port\": \"MLflow UI port (default: 5000, auto-assigns if busy)\",\n",
    "    }\n",
    ")\n",
    "def up(\n",
    "    c,\n",
    "    name: Optional[str] = None,\n",
    "    rebuild: bool = False,\n",
    "    detach: bool = True,\n",
    "    use_pty: bool = False,\n",
    "    jupyter_port: Union[str, int, None] = None,\n",
    "    tensorboard_port: Union[str, int, None] = None,\n",
    "    explainer_port: Union[str, int, None] = None,\n",
    "    streamlit_port: Union[str, int, None] = None,\n",
    "    mlflow_port: Union[str, int, None] = None,\n",
    ") -> None:\n",
    "    \"\"\"Build (optionally --rebuild) & start the container with custom ports.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "\n",
    "    # ---------- Parse and validate all ports -----------------\n",
    "    try:\n",
    "        jupyter_port = _parse_port(jupyter_port)\n",
    "        tensorboard_port = _parse_port(tensorboard_port)\n",
    "        explainer_port = _parse_port(explainer_port)\n",
    "        streamlit_port = _parse_port(streamlit_port)\n",
    "        mlflow_port = _parse_port(mlflow_port)\n",
    "    except ValueError as e:\n",
    "        print(f\"âŒ Port validation failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # ---------- build dynamic port map -----------------\n",
    "    ports = {}\n",
    "    if jupyter_port is not None:\n",
    "        ports[\"jupyter\"] = jupyter_port\n",
    "    if tensorboard_port is not None:\n",
    "        ports[\"tensorboard\"] = tensorboard_port\n",
    "    if explainer_port is not None:\n",
    "        ports[\"explainer\"] = explainer_port\n",
    "    if streamlit_port is not None:\n",
    "        ports[\"streamlit\"] = streamlit_port\n",
    "\n",
    "    # ---------- Explainer auto-assign (NEW) ------------\n",
    "    print(\"DEBUG: Starting explainer port assignment\")  # Debug\n",
    "    try:\n",
    "        # Try to use the explainer's version first\n",
    "        from src.mlops.explainer import _first_free_port  # type: ignore\n",
    "        print(\"DEBUG: Successfully imported _first_free_port from explainer\")  # Debug\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"DEBUG: Failed to import _first_free_port, using local implementation\")  # Debug\n",
    "        # We'll use our local _first_free_port implementation\n",
    "        pass\n",
    "\n",
    "    if explainer_port is None:\n",
    "        print(\"DEBUG: No explainer port specified, finding one\")  # Debug\n",
    "        explainer_port = _find_port(8050, 5200)\n",
    "    elif not _port_free(\"127.0.0.1\", explainer_port):\n",
    "        print(f\"DEBUG: Specified explainer port {explainer_port} is in use\")  # Debug\n",
    "        sys.exit(1)\n",
    "    ports[\"explainer\"] = explainer_port\n",
    "    print(f\"ðŸ”Œ Explainer host-port â†’ {explainer_port}\")\n",
    "\n",
    "    # ----- MLflow auto-assign (default 5000) -----------\n",
    "    print(\"DEBUG: Starting MLflow port assignment\")  # Debug\n",
    "    if mlflow_port is None:\n",
    "        print(\"DEBUG: No MLflow port specified, finding one\")  # Debug\n",
    "        mlflow_port = _find_port(5000, 5200)\n",
    "    elif not _port_free(\"127.0.0.1\", mlflow_port):\n",
    "        print(f\"DEBUG: Specified MLflow port {mlflow_port} is in use\")  # Debug\n",
    "        sys.exit(1)\n",
    "    ports[\"mlflow\"] = mlflow_port\n",
    "    print(f\"ðŸ”Œ MLflow host-port â†’ {mlflow_port}\")\n",
    "\n",
    "    # Generate environment file\n",
    "    env_path = _write_envfile(name, ports)\n",
    "    compose_cmd = \"up -d\" if detach else \"up\"\n",
    "\n",
    "    _compose(\n",
    "        c,\n",
    "        f\"--env-file {env_path} {compose_cmd}\",\n",
    "        name,\n",
    "        rebuild=rebuild,\n",
    "        force_pty=use_pty,\n",
    "        ports=ports,\n",
    "    )\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "    }\n",
    ")\n",
    "def stop(c, name: Optional[str] = None) -> None:\n",
    "    \"\"\"Stop and remove dev container (keeps volumes).\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} down\"\n",
    "    try:\n",
    "        c.run(cmd)\n",
    "        print(f\"\\nðŸ›‘ Stopped and removed project '{name}'\")\n",
    "    except Exception:\n",
    "        print(f\"âŒ No running containers found for project '{name}'\")\n",
    "\n",
    "\n",
    "@task\n",
    "def shell(c, name: str | None = None) -> None:\n",
    "    \"\"\"Open an interactive shell inside the running container.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps -q datascience\"\n",
    "    cid = c.run(cmd, hide=True).stdout.strip()\n",
    "    c.run(f\"docker exec -it {cid} bash\", env={\"ENV_NAME\": name}, pty=False)\n",
    "\n",
    "\n",
    "@task\n",
    "def clean(c) -> None:\n",
    "    \"\"\"Prune stopped containers + dangling images.\"\"\"\n",
    "    c.run(\"docker system prune -f\")\n",
    "\n",
    "\n",
    "@task\n",
    "def ports(c, name: str | None = None) -> None:\n",
    "    \"\"\"Show current port mappings for the named project.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps --format table\"\n",
    "    try:\n",
    "        c.run(cmd, hide=False)\n",
    "        print(f\"\\nðŸ“Š Port mappings for project '{name}':\")\n",
    "        print(\"=\" * 50)\n",
    "    except Exception:\n",
    "        print(f\"âŒ No running containers found for project '{name}'\")\n",
    "        print(\"\\nðŸ’¡ Usage examples:\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8891\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8892 \\\\\")\n",
    "        print(\"    --tensorboard-port 6009\")\n",
    "\n",
    "\n",
    "# --- utilities ---------------------------------------------------------------\n",
    "def _norm(path: str | pathlib.Path) -> str:\n",
    "    \"\"\"Return a lower-case, forward-slash, no-trailing-slash version of *path*.\"\"\"\n",
    "    p = str(path).replace(\"\\\\\", \"/\").rstrip(\"/\").lower()\n",
    "    return p\n",
    "\n",
    "def _docker_projects_from_this_repo() -> set[str]:\n",
    "    \"\"\"\n",
    "    Discover every Compose *project name* whose working_dir label ends with\n",
    "    the current repo path.\n",
    "\n",
    "    Works across Windows â†” WSL â†” macOS because we do suffix-match on a\n",
    "    normalised path.\n",
    "    \"\"\"\n",
    "    here_tail = _norm(pathlib.Path(__file__).parent.resolve())\n",
    "    cmd = (\n",
    "        \"docker container ls -a \"\n",
    "        \"--format '{{.Label \\\"com.docker.compose.project\\\"}} \"\n",
    "        \"{{.Label \\\"com.docker.compose.project.working_dir\\\"}}' \"\n",
    "        \"--filter label=com.docker.compose.project\"\n",
    "    )\n",
    "    projects: set[str] = set()\n",
    "    for line in os.popen(cmd).read().strip().splitlines():\n",
    "        try:\n",
    "            proj, wd = line.split(maxsplit=1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if _norm(wd).endswith(here_tail):\n",
    "            projects.add(proj)\n",
    "    return projects\n",
    "\n",
    "# --- task --------------------------------------------------------------------\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project name (defaults to folder). Ignored with --all.\",\n",
    "        \"all\":  \"Remove *all* projects launched from this repo.\",\n",
    "        \"rmi\":  \"Image-removal policy: all | local | none (default: local).\",\n",
    "    }\n",
    ")\n",
    "def down(c, name: str | None = None, all: bool = False, rmi: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Stop containers **and** fully delete every artefact so next `invoke up`\n",
    "    starts from a clean slate.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    invoke down                  # nuke current-folder project\n",
    "    invoke down --name ml_project --rmi all   # wipe everything for ml_project\n",
    "    invoke down --all            # tear down every project from this repo\n",
    "    \"\"\"\n",
    "    if rmi not in {\"all\", \"local\", \"none\"}:\n",
    "        raise ValueError(\"--rmi must be all | local | none\")\n",
    "\n",
    "    targets = _docker_projects_from_this_repo() if all else {name or BASE_ENV.name}\n",
    "    flags = \"-v --remove-orphans\"\n",
    "    if rmi != \"none\":\n",
    "        flags += f\" --rmi {rmi}\"\n",
    "\n",
    "    for proj in targets:\n",
    "        try:\n",
    "            c.run(f\"docker compose -p {proj} down {flags}\")\n",
    "            print(f\"ðŸ—‘ï¸  Removed project '{proj}'\")\n",
    "        except Exception:\n",
    "            print(f\"âš ï¸  Nothing to remove for '{proj}'\")\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"yaml\": \"Path to dashboard.yaml file\",\n",
    "        \"port\": \"Port to serve on (default: 8150)\",\n",
    "        \"host\": \"Host to bind to (default: 0.0.0.0)\",\n",
    "    }\n",
    ")\n",
    "def dashboard(c, yaml: str, port: int = 8150, host: str = \"0.0.0.0\") -> None:\n",
    "    \"\"\"\n",
    "    Serve a saved ExplainerDashboard from a YAML configuration file.\n",
    "    \n",
    "    This task allows you to re-serve dashboards that were previously saved\n",
    "    with build_and_log_dashboard(save_yaml=True).\n",
    "    \n",
    "    Examples:\n",
    "        invoke dashboard --yaml dashboard.yaml\n",
    "        invoke dashboard --yaml dashboard.yaml --port 8200\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    from src.mlops.explainer import load_dashboard_yaml\n",
    "    \n",
    "    yaml_path = Path(yaml)\n",
    "    if not yaml_path.exists():\n",
    "        print(f\"âŒ Dashboard YAML file not found: {yaml_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Check if port is available\n",
    "    if not _port_free(host, port):\n",
    "        print(f\"âŒ Port {port} is already in use on {host}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        print(f\"ðŸ”„ Loading dashboard from {yaml_path}\")\n",
    "        dashboard_obj = load_dashboard_yaml(yaml_path)\n",
    "        \n",
    "        print(f\"ðŸŒ Serving ExplainerDashboard on {host}:{port}\")\n",
    "        dashboard_obj.run(port=port, host=host, use_waitress=True, open_browser=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load or serve dashboard: {e}\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../tests/diagnose_devcontainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../tests/diagnose_devcontainer.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive diagnostic script for dev container issues.\n",
    "Run this inside the container to diagnose Python environment and remote extension problems.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    \"\"\"Run a command and return its output.\"\"\"\n",
    "    print(f\"\\nðŸ” {description}\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"âŒ Error (code {result.returncode}): {result.stderr.strip()}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_paths_and_environment():\n",
    "    \"\"\"Check Python paths and environment variables.\"\"\"\n",
    "    print(\"\\nðŸ PYTHON ENVIRONMENT DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Python executable and version\n",
    "    print(f\"Python executable: {sys.executable}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Python path: {sys.path[:3]}...\")  # First few paths\n",
    "    \n",
    "    # Environment variables\n",
    "    print(f\"\\nVIRTUAL_ENV: {os.environ.get('VIRTUAL_ENV', 'Not set')}\")\n",
    "    print(f\"PATH (first 3): {':'.join(os.environ.get('PATH', '').split(':')[:3])}\")\n",
    "    \n",
    "    # Virtual environment validation\n",
    "    venv_path = Path('/app/.venv')\n",
    "    if venv_path.exists():\n",
    "        print(f\"âœ… Virtual environment exists at {venv_path}\")\n",
    "        print(f\"   - bin directory: {list(venv_path.glob('bin/python*'))}\")\n",
    "        print(f\"   - site-packages: {(venv_path / 'lib/python3.10/site-packages').exists()}\")\n",
    "    else:\n",
    "        print(f\"âŒ Virtual environment NOT found at {venv_path}\")\n",
    "\n",
    "\n",
    "def check_key_packages():\n",
    "    \"\"\"Check if key packages are importable.\"\"\"\n",
    "    print(\"\\nðŸ“¦ PACKAGE IMPORT TESTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    packages = [\n",
    "        'jax', 'torch', 'numpy', 'pandas', 'matplotlib', \n",
    "        'jupyterlab', 'streamlit', 'sklearn'\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            if package == 'sklearn':\n",
    "                import sklearn\n",
    "                version = sklearn.__version__\n",
    "            else:\n",
    "                module = __import__(package)\n",
    "                version = getattr(module, '__version__', 'unknown')\n",
    "            print(f\"âœ… {package}: {version}\")\n",
    "        except ImportError as e:\n",
    "            print(f\"âŒ {package}: Import failed - {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  {package}: {e}\")\n",
    "\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"Check GPU-related environment variables.\"\"\"\n",
    "    print(\"\\nðŸŽ® GPU ENVIRONMENT VARIABLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gpu_env_vars = [\n",
    "        'XLA_PYTHON_CLIENT_PREALLOCATE',\n",
    "        'XLA_PYTHON_CLIENT_ALLOCATOR', \n",
    "        'XLA_PYTHON_CLIENT_MEM_FRACTION',\n",
    "        'JAX_PLATFORM_NAME',\n",
    "        'XLA_FLAGS',\n",
    "        'JAX_DISABLE_JIT',\n",
    "        'JAX_ENABLE_X64',\n",
    "        'JAX_PREALLOCATION_SIZE_LIMIT_BYTES',\n",
    "        'TF_FORCE_GPU_ALLOW_GROWTH',\n",
    "        'NVIDIA_VISIBLE_DEVICES',\n",
    "        'NVIDIA_DRIVER_CAPABILITIES'\n",
    "    ]\n",
    "    \n",
    "    for var in gpu_env_vars:\n",
    "        value = os.environ.get(var, 'Not set')\n",
    "        print(f\"   {var}: {value}\")\n",
    "\n",
    "\n",
    "def check_gpu_support():\n",
    "    \"\"\"Check GPU support for JAX and PyTorch with enhanced diagnostics.\"\"\"\n",
    "    print(\"\\nðŸŽ® ENHANCED GPU SUPPORT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # JAX GPU check with detailed info\n",
    "    try:\n",
    "        import jax\n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "        \n",
    "        devices = jax.devices()\n",
    "        print(f\"JAX devices: {devices}\")\n",
    "        \n",
    "        if devices:\n",
    "            for i, device in enumerate(devices):\n",
    "                print(f\"   Device {i}: {device}\")\n",
    "                \n",
    "        if any('gpu' in str(device).lower() or 'cuda' in str(device).lower() for device in devices):\n",
    "            print(\"âœ… JAX GPU/CUDA support detected!\")\n",
    "            \n",
    "            # Test a simple computation\n",
    "            try:\n",
    "                import jax.numpy as jnp\n",
    "                x = jnp.ones((1000, 1000))\n",
    "                result = jnp.sum(x)\n",
    "                print(f\"   âœ… JAX GPU computation test passed: sum = {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  JAX GPU computation test failed: {e}\")\n",
    "        else:\n",
    "            print(\"âš ï¸  JAX GPU support not detected\")\n",
    "            print(\"   This might be due to GPU architecture compatibility\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ JAX GPU check failed: {e}\")\n",
    "    \n",
    "    # PyTorch GPU check with enhanced info\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "        print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device_count = torch.cuda.device_count()\n",
    "            print(f\"âœ… PyTorch CUDA device count: {device_count}\")\n",
    "            \n",
    "            for i in range(device_count):\n",
    "                try:\n",
    "                    device_name = torch.cuda.get_device_name(i)\n",
    "                    memory_total = torch.cuda.get_device_properties(i).total_memory\n",
    "                    print(f\"   Device {i}: {device_name}\")\n",
    "                    print(f\"     Total memory: {memory_total / (1024**3):.1f} GB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Device {i}: Error getting info - {e}\")\n",
    "            \n",
    "            # Test a simple computation\n",
    "            try:\n",
    "                device = torch.device('cuda:0')\n",
    "                x = torch.ones(1000, 1000, device=device)\n",
    "                result = torch.sum(x)\n",
    "                print(f\"   âœ… PyTorch GPU computation test passed: sum = {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸  PyTorch GPU computation test failed: {e}\")\n",
    "        else:\n",
    "            print(\"âš ï¸  PyTorch CUDA not available\")\n",
    "            print(\"   Check CUDA installation and GPU compatibility\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PyTorch GPU check failed: {e}\")\n",
    "\n",
    "\n",
    "def check_workspace_mount():\n",
    "    \"\"\"Check if workspace is properly mounted.\"\"\"\n",
    "    print(\"\\nðŸ“ WORKSPACE MOUNT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workspace_path = Path('/workspace')\n",
    "    if workspace_path.exists():\n",
    "        print(f\"âœ… /workspace directory exists\")\n",
    "        try:\n",
    "            contents = list(workspace_path.iterdir())[:10]  # First 10 items\n",
    "            print(f\"   Contents (first 10): {[p.name for p in contents]}\")\n",
    "            \n",
    "            # Check for specific expected files\n",
    "            expected_files = ['.devcontainer', 'pyproject.toml', 'docker-compose.yml']\n",
    "            for file in expected_files:\n",
    "                if (workspace_path / file).exists():\n",
    "                    print(f\"   âœ… Found: {file}\")\n",
    "                else:\n",
    "                    print(f\"   âŒ Missing: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error reading workspace: {e}\")\n",
    "    else:\n",
    "        print(f\"âŒ /workspace directory does not exist\")\n",
    "\n",
    "\n",
    "def check_dev_container_config():\n",
    "    \"\"\"Check dev container configuration.\"\"\"\n",
    "    print(\"\\nâš™ï¸  DEV CONTAINER CONFIG CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config_path = Path('/workspace/.devcontainer/devcontainer.json')\n",
    "    if config_path.exists():\n",
    "        print(\"âœ… devcontainer.json found\")\n",
    "        try:\n",
    "            with open(config_path) as f:\n",
    "                config = json.load(f)\n",
    "            print(f\"   Name: {config.get('name', 'Not specified')}\")\n",
    "            print(f\"   Python path: {config.get('customizations', {}).get('vscode', {}).get('settings', {}).get('python.defaultInterpreterPath', 'Not specified')}\")\n",
    "            print(f\"   Workspace folder: {config.get('workspaceFolder', 'Not specified')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error reading config: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ devcontainer.json not found\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all diagnostic checks.\"\"\"\n",
    "    print(\"ðŸ” DEV CONTAINER COMPREHENSIVE DIAGNOSTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Running from: {os.getcwd()}\")\n",
    "    print(f\"User: {os.getenv('USER', 'unknown')}\")\n",
    "    print(f\"Container hostname: {os.getenv('HOSTNAME', 'unknown')}\")\n",
    "    \n",
    "    # System commands\n",
    "    run_command(\"uv --version\", \"UV Version\")\n",
    "    run_command(\"which python\", \"Python Location\")\n",
    "    run_command(\"ls -la /app/.venv/\", \"Virtual Environment Contents\")\n",
    "    run_command(\"mount | grep workspace\", \"Workspace Mount Status\")\n",
    "    run_command(\"nvidia-smi\", \"NVIDIA GPU Status\")\n",
    "    \n",
    "    # Python-based checks\n",
    "    check_paths_and_environment()\n",
    "    check_gpu_environment()\n",
    "    check_key_packages()\n",
    "    check_gpu_support()\n",
    "    check_workspace_mount()\n",
    "    check_dev_container_config()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸŽ¯ SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"If you see issues:\")\n",
    "    print(\"1. âŒ Virtual env missing â†’ Check Dockerfile uv sync step\")\n",
    "    print(\"2. âŒ Workspace not mounted â†’ Check devcontainer.json mounts config\")\n",
    "    print(\"3. âŒ Packages missing â†’ Check uv.lock and pip install steps\")\n",
    "    print(\"4. âš ï¸  GPU not detected â†’ Check docker-compose.yml gpu settings\")\n",
    "    print(\"5. ðŸ”§ For VS Code issues â†’ Check python.defaultInterpreterPath setting\")\n",
    "    print(\"6. ðŸŽ® For GPU issues â†’ Check NVIDIA drivers and CUDA compatibility\")\n",
    "    print(\"\\nâœ… All checks passed = ready for development!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../tests/test_pytorch_jax_gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../tests/test_pytorch_jax_gpu.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script to verify that PyTorch and JAX can access the GPU,\n",
    "and that PyJAGS is working correctly.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def test_pytorch_gpu():\n",
    "    \"\"\"Test PyTorch GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing PyTorch GPU ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"âŒ PyTorch CUDA not available!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Run a simple test computation\n",
    "        x = torch.rand(1000, 1000).cuda()\n",
    "        y = torch.rand(1000, 1000).cuda()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        z = torch.matmul(x, y)\n",
    "        end.record()\n",
    "        \n",
    "        # Wait for GPU computation to finish\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Matrix multiplication time: {start.elapsed_time(end):.2f} ms\")\n",
    "        print(f\"Result shape: {z.shape}\")\n",
    "        print(\"âœ… PyTorch GPU test passed!\")\n",
    "        return True\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"âŒ PyTorch not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during PyTorch GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_jax_gpu():\n",
    "    \"\"\"Test JAX GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing JAX GPU ===\")\n",
    "    try:\n",
    "        import jax, jax.numpy as jnp\n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "\n",
    "        # Probe both: this won't crash if GPU plugin isn't present\n",
    "        gpu_devs = []\n",
    "        try:\n",
    "            gpu_devs = jax.devices(\"gpu\")\n",
    "        except Exception:\n",
    "            # Some builds don't register 'gpu' backend explicitly\n",
    "            pass\n",
    "\n",
    "        if not gpu_devs:\n",
    "            # Fallback: inspect all devices for cuda/gpu strings\n",
    "            devs = jax.devices()\n",
    "            gpu_devs = [d for d in devs if \"gpu\" in str(d).lower() or \"cuda\" in str(d).lower()]\n",
    "\n",
    "        if not gpu_devs:\n",
    "            print(\"âŒ No GPU devices found by JAX!\")\n",
    "            return False\n",
    "\n",
    "        print(f\"Available JAX GPU devices: {gpu_devs}\")\n",
    "        @jax.jit\n",
    "        def matmul(a, b): return jnp.matmul(a, b)\n",
    "        x = jnp.ones((1024, 1024))\n",
    "        y = jnp.ones((1024, 1024))\n",
    "        result = matmul(x, y)\n",
    "        print(f\"Result shape: {result.shape}\")\n",
    "        print(\"âœ… JAX GPU test passed!\")\n",
    "        return True\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"âŒ JAX not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during JAX GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def test_pyjags():\n",
    "    \"\"\"Test PyJAGS installation and basic functionality.\"\"\"\n",
    "    print(\"\\n=== Testing PyJAGS ===\")\n",
    "    try:\n",
    "        import pyjags\n",
    "        print(f\"PyJAGS version: {pyjags.__version__}\")\n",
    "        \n",
    "        # Create a simple model to verify that PyJAGS works\n",
    "        code = \"\"\"\n",
    "        model {\n",
    "            # Likelihood\n",
    "            y ~ dnorm(mu, 1/sigma^2)\n",
    "            \n",
    "            # Priors\n",
    "            mu ~ dnorm(0, 0.001)\n",
    "            sigma ~ dunif(0, 100)\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample data\n",
    "        data = {'y': 0.5}\n",
    "        \n",
    "        # Initialize model with data\n",
    "        model = pyjags.Model(code, data=data, chains=1, adapt=100)\n",
    "        print(\"JAGS model initialized successfully!\")\n",
    "        \n",
    "        # Sample from the model\n",
    "        samples = model.sample(200, vars=['mu', 'sigma'])\n",
    "        print(\"JAGS sampling completed successfully!\")\n",
    "        \n",
    "        # Verify the samples\n",
    "        mu_samples = samples['mu']\n",
    "        sigma_samples = samples['sigma']\n",
    "        print(f\"mu mean: {mu_samples.mean():.4f}\")\n",
    "        print(f\"sigma mean: {sigma_samples.mean():.4f}\")\n",
    "        \n",
    "        print(\"âœ… PyJAGS test passed!\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"âŒ PyJAGS not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during PyJAGS test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running GPU and PyJAGS verification tests...\")\n",
    "    \n",
    "    pytorch_success = test_pytorch_gpu()\n",
    "    jax_success = test_jax_gpu()\n",
    "    pyjags_success = test_pyjags()\n",
    "    \n",
    "    print(\"\\n=== Test Summary ===\")\n",
    "    print(f\"PyTorch GPU: {'âœ… PASS' if pytorch_success else 'âŒ FAIL'}\")\n",
    "    print(f\"JAX GPU: {'âœ… PASS' if jax_success else 'âŒ FAIL'}\")\n",
    "    print(f\"PyJAGS: {'âœ… PASS' if pyjags_success else 'âŒ FAIL'}\")\n",
    "    \n",
    "    if pytorch_success and jax_success and pyjags_success:\n",
    "        print(\"\\nðŸŽ‰ All tests passed! The container is working correctly.\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(\"\\nâŒ Some tests failed. Please check the output for details.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../tests/enhanced_gpu_test_harness.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../tests/enhanced_gpu_test_harness.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enhanced GPU Test Harness with Blackwell/SM_120 Support\n",
    "======================================================\n",
    "\n",
    "This module provides enhanced test functions for GPU frameworks with specific\n",
    "support for NVIDIA Blackwell RTX 5080 and CUDA 12.8 compatibility.\n",
    "\n",
    "Features:\n",
    "- Enhanced JAX CUDA backend detection and conflict resolution\n",
    "- TensorFlow INVALID_PTX error detection with diagnostic messages\n",
    "- Comprehensive error reporting for Blackwell-specific issues\n",
    "\n",
    "Usage:\n",
    "    python tests/enhanced_gpu_test_harness.py\n",
    "    \n",
    "Or import individual functions:\n",
    "    from tests.enhanced_gpu_test_harness import check_jax, check_tensorflow\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import gc\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def check_jax():\n",
    "    \"\"\"\n",
    "    Enhanced JAX GPU test with backend detection and Blackwell compatibility.\n",
    "    \n",
    "    This function provides:\n",
    "    - Detection of CUDA backend plugins vs PJRT runtime\n",
    "    - Identification of mixed backend conflicts\n",
    "    - GPU device enumeration and computation testing\n",
    "    - Clear error messages for common configuration issues\n",
    "    \"\"\"\n",
    "    print(\"=== JAX ===\")\n",
    "    try:\n",
    "        import jax, importlib.util as u\n",
    "        print(f\"jax.__version__={jax.__version__}\")\n",
    "        try:\n",
    "            import jaxlib\n",
    "            print(f\"jaxlib.__version__={jaxlib.__version__}\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not import jaxlib:\", repr(e))\n",
    "\n",
    "        print(\"Backends: plugin?\", u.find_spec(\"jax_cuda12_plugin\") is not None,\n",
    "              \" pjrt?\", u.find_spec(\"jax_cuda12_pjrt\") is not None)\n",
    "\n",
    "        try:\n",
    "            devs = jax.devices()\n",
    "            print(f\"devices: {devs}\")\n",
    "            gpu = [d for d in devs if 'gpu' in str(d).lower() or 'cuda' in str(d).lower()]\n",
    "            if not gpu:\n",
    "                print(\"No JAX GPU devices found.\")\n",
    "            else:\n",
    "                from jax import numpy as jnp\n",
    "                x = jnp.ones((1024,1024), dtype=jnp.float32)\n",
    "                y = (x @ x.T).sum()\n",
    "                _ = y.block_until_ready()\n",
    "                print(\"small matmul check (JAX GPU): OK\")\n",
    "        except RuntimeError as re:\n",
    "            print(\"jax.devices() raised RuntimeError:\", str(re))\n",
    "        except Exception as e:\n",
    "            print(\"jax.devices() failed:\", repr(e))\n",
    "    except Exception as e:\n",
    "        print(\"JAX import failed:\", repr(e))\n",
    "    print(\"===========\\n\")\n",
    "\n",
    "\n",
    "def check_tensorflow():\n",
    "    \"\"\"\n",
    "    Enhanced TensorFlow GPU test with Blackwell INVALID_PTX detection.\n",
    "    \n",
    "    This function provides:\n",
    "    - GPU device enumeration and memory growth configuration  \n",
    "    - Explicit CUDA_ERROR_INVALID_PTX detection for Blackwell GPUs\n",
    "    - Diagnostic messages for SM_120/CUDA 12.8 compatibility issues\n",
    "    - Clear guidance for resolving PTX compilation failures\n",
    "    \"\"\"\n",
    "    print(\"=== TensorFlow ===\")\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        print(f\"tf.__version__={tf.__version__}\")\n",
    "        try:\n",
    "            gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "            print(f\"tf GPUs: {gpus}\")\n",
    "            if gpus:\n",
    "                # Memory growth first\n",
    "                for g in gpus:\n",
    "                    try:\n",
    "                        tf.config.experimental.set_memory_growth(g, True)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                # Minimal compute test with explicit error surfacing\n",
    "                try:\n",
    "                    with tf.device(\"/GPU:0\"):\n",
    "                        a = tf.random.normal((1024,1024))\n",
    "                        b = tf.random.normal((1024,1024))\n",
    "                        c = tf.reduce_sum(tf.matmul(a, b))\n",
    "                    _ = c.numpy()\n",
    "                    print(\"small matmul check (TensorFlow GPU): OK\")\n",
    "                except Exception as e:\n",
    "                    msg = str(e)\n",
    "                    print(\"TensorFlow GPU op failed:\", type(e).__name__, msg)\n",
    "                    if \"CUDA_ERROR_INVALID_PTX\" in msg or \"INVALID_PTX\" in msg:\n",
    "                        print(textwrap.dedent(\"\"\"\n",
    ">>> DIAG: INVALID_PTX on Blackwell usually means the TF wheel lacks SM_120 SASS and the PTX\n",
    "          embedded in the wheel predates CUDA 12.8 support for Blackwell. Options:\n",
    "          â€¢ install tf-nightly (post-CUDA-12.8 builds), or\n",
    "          â€¢ use NVIDIA's TensorFlow NGC container built for CUDA 12.8+.\n",
    "                        \"\"\").strip())\n",
    "                    raise\n",
    "            else:\n",
    "                print(\"No TensorFlow GPU devices found.\")\n",
    "        except Exception as e:\n",
    "            print(\"TensorFlow device query failed:\", repr(e))\n",
    "    except Exception as e:\n",
    "        print(\"TensorFlow import failed:\", repr(e))\n",
    "    print(\"==================\\n\")\n",
    "\n",
    "\n",
    "def check_pytorch():\n",
    "    \"\"\"\n",
    "    Enhanced PyTorch GPU test with memory management.\n",
    "    \n",
    "    This function provides:\n",
    "    - CUDA availability and device information\n",
    "    - Memory cleanup after computation\n",
    "    - Version and capability reporting\n",
    "    \"\"\"\n",
    "    print(\"=== PyTorch ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"torch.__version__={torch.__version__}\")\n",
    "        print(f\"torch.version.cuda={torch.version.cuda}\")\n",
    "        print(f\"torch.cuda.is_available()={torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"torch.cuda.device_count()={torch.cuda.device_count()}\")\n",
    "            print(f\"current device name: {torch.cuda.get_device_name(0)}\")\n",
    "            \n",
    "            # Test computation with memory cleanup\n",
    "            x = torch.randn(1024, 1024, device=\"cuda\")\n",
    "            y = torch.randn(1024, 1024, device=\"cuda\")\n",
    "            z = (x @ y).sum().item()\n",
    "            del x, y\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"small matmul check (PyTorch CUDA): OK\")\n",
    "        else:\n",
    "            print(\"CUDA not available in PyTorch.\")\n",
    "    except Exception as e:\n",
    "        print(\"PyTorch check failed:\", repr(e))\n",
    "    print(\"==============\\n\")\n",
    "\n",
    "\n",
    "def comprehensive_gpu_test():\n",
    "    \"\"\"\n",
    "    Run comprehensive GPU framework testing.\n",
    "    \n",
    "    This function tests all three major frameworks (PyTorch, JAX, TensorFlow)\n",
    "    and provides a summary of results with specific guidance for failures.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all frameworks passed, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª COMPREHENSIVE GPU FRAMEWORK TEST\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Testing GPU support for PyTorch, JAX, and TensorFlow...\")\n",
    "    print(\"Optimized for NVIDIA Blackwell RTX 5080 / CUDA 12.8\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test all frameworks\n",
    "    check_pytorch()\n",
    "    check_jax()  \n",
    "    check_tensorflow()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"âœ… Comprehensive GPU test completed!\")\n",
    "    print(\"Check output above for any framework-specific issues.\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for standalone execution.\"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Show environment snapshot\n",
    "    print(\"ðŸ”§ GPU ENVIRONMENT SNAPSHOT\")\n",
    "    print(\"=\" * 30)\n",
    "    env_vars = [\n",
    "        \"JAX_PLATFORM_NAME\", \"JAX_PLATFORMS\", \"CUDA_VISIBLE_DEVICES\",\n",
    "        \"XLA_FLAGS\", \"NVIDIA_VISIBLE_DEVICES\", \"NVIDIA_DRIVER_CAPABILITIES\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\", \"TF_FORCE_GPU_ALLOW_GROWTH\"\n",
    "    ]\n",
    "    for var in env_vars:\n",
    "        value = os.environ.get(var, \"<unset>\")\n",
    "        print(f\"{var}={value}\")\n",
    "    print(\"=\" * 30)\n",
    "    print()\n",
    "    \n",
    "    # Run comprehensive test\n",
    "    comprehensive_gpu_test()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
