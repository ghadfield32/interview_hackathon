{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fb6a9b",
   "metadata": {},
   "source": [
    "# NBA Play‑by‑Play → Turn‑In Package (Lineups & Rim Defense)\n",
    "\n",
    "This document is the **turn‑in** for the single‑game pipeline. It includes:\n",
    "\n",
    "1. A table of **every unique 5‑man lineup** per team with possessions and ratings.\n",
    "2. A table of **every player** with rim defense on/off metrics.\n",
    "3. A concise, submission‑focused description of the two lineup engines (**Traditional** vs **Lineup Automation**) and how they relate to the deliverables.\n",
    "4. A clear **process tree** you can follow to reproduce without SQL.\n",
    "\n",
    "---\n",
    "\n",
    "## Projects:\n",
    "Using the data provided (a play-by-play csv file, a box score csv file, and 3 csvs that map columns in the pbp), please transform the data to return two tables:\n",
    "\n",
    " \n",
    "\n",
    "    A table containing every unique 5-man lineup for each team with columns for the following:\n",
    "\n",
    "    The 5 players in the lineup\n",
    "    Team\n",
    "    Offensive possessions played\n",
    "    Defensive possessions played\n",
    "    Offensive rating\n",
    "    Defensive rating\n",
    "    Net rating\n",
    "\n",
    "    A table containing every player who played in the game with columns for the following:\n",
    "\n",
    "    Player ID\n",
    "    Player Name\n",
    "    Team\n",
    "    Offensive possessions played\n",
    "    Defensive possessions played\n",
    "    Opponent rim field goal percentage when player in on the court\n",
    "    Opponent rim field goal percentage when player is off the court\n",
    "    Opponent rim field goal percentage on/off difference (on-off)\n",
    "\n",
    " \n",
    "\n",
    "We will define a rim shot as any shot that occurred within 4 feet of the basket.\n",
    "\n",
    " \n",
    "\n",
    "You may return the tables in any format you see fit. Please provide the code you used to generate the tables. \n",
    "\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "### Project 1 — Unique 5‑Man Lineups (per team)\n",
    "\n",
    "# Final Turn ins: project1_lineups_FINAL.csv (lineup automation) and project2_players_FINAL.csv (player rim defense), with project1_lineups_traditional_with_6th_player.csv (traditional) for reference \n",
    "* usually use parquet but used csv for ease of use\n",
    "\n",
    "Columns: *Team, Player 1…Player 5, Offensive possessions played, Defensive possessions played, Offensive rating, Defensive rating, Net rating*\n",
    "\n",
    "> **Two views are shown below** for transparency:\n",
    ">\n",
    "> * **Traditional (data‑driven)** results **may include 4–6 player lineups** when the raw feed is inconsistent (we still list them in this section for auditability).\n",
    "> * **Lineup Automation** is the submission‑ready **always‑five** view used for final grading/comparison.\n",
    "\n",
    "#### Project 1 — Traditional (data‑driven) results *(includes 4–6 player lineups)*: project1_lineups_traditional_with_6th_player.csv\n",
    "\n",
    "#### Head of substitution‑violation flags (Traditional):\n",
    "\n",
    "#### Project 1 — Lineup Automation (always‑five; submission‑ready): project1_lineups_FINAL.csv\n",
    "\n",
    "#### Project 2 — Player Rim Defense (On/Off): project2_players_FINAL.csv\n",
    "\n",
    "Columns: Player ID, Player Name, Team, Offensive possessions played, Defensive possessions played, Opponent rim FG% (on), Opponent rim FG% (off), On–off difference\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How We Built It (No SQL)\n",
    "\n",
    "### Two Complementary Lineup Engines (used for turn‑in)\n",
    "\n",
    "**Traditional (data‑driven) — `run_traditional_data_driven_lineups`**\n",
    "\n",
    "* Strictly follows raw PBP substitutions (`msgType=8`: `playerId1=IN`, `playerId2=OUT`).\n",
    "* Lineups may be size **≠ 5** when the feed is inconsistent.\n",
    "* Period handling: **Q1 & Q3 reset to starters**; **Q2/Q4/OT carry forward** the current five.\n",
    "* Flags anomalies instead of fixing them: `lineup_size_deviation`, `sub_out_player_not_in_lineup`, `sub_in_player_already_in_lineup`, `action_by_non_lineup_player`.\n",
    "* **Purpose:** auditing/data‑forensics; exposes feed gaps that can distort analytics.\n",
    "\n",
    "**Lineup Automation (enhanced) — `run_enhanced_substitution_tracking_with_flags`**\n",
    "\n",
    "* Adds **intelligent inference** to maintain **exactly five** on court per team.\n",
    "* Period handling: **Q1 & Q3 reset to starters**; **Q2/Q4/OT carry forward**.\n",
    "* **First‑Action Auto‑IN:** if a player acts with no prior sub‑in, we inject them (and flag).\n",
    "* **Inactivity Auto‑OUT:** idle **>120s** → candidate removal (flag and record).\n",
    "* Emits flags: `missing_sub_in`, `inactivity_periods`, `first_action_events`, `auto_out_events`, `lineup_violations`.\n",
    "* **Purpose:** submission‑ready analytics (clean, always‑five lineups).\n",
    "\n",
    "> **Turn‑in rule:** Project 1’s submitted table uses the **Lineup Automation** results (always‑five). The Traditional table is included for transparency and QA.\n",
    "\n",
    "### End‑to‑End Process Tree\n",
    "\n",
    "```\n",
    "Inputs\n",
    "├─ Play‑by‑Play CSV\n",
    "├─ Box Score CSV\n",
    "└─ PBP Lookup CSVs (event, action, option)\n",
    "\n",
    "Process\n",
    "├─ Load & Validate\n",
    "│  ├─ Load box (ACTIVE only) & pbp; drop admin rows\n",
    "│  ├─ Distance sanity & rim tagging rule (≤4 ft)\n",
    "│  └─ Join lookup CSVs to enrich event families/types\n",
    "├─ Dimensions & Maps\n",
    "│  ├─ Team & player dims (filter out officials; infer team where needed)\n",
    "│  └─ Enriched PBP view (names, team abbrevs, labeled events)\n",
    "├─ Lineup Engines (two parallel passes)\n",
    "│  ├─ Traditional (data‑driven): allow non‑5; generate diagnostic flags\n",
    "│  └─ Lineup Automation: always‑five; infer missing sub‑ins/auto‑outs + flags\n",
    "├─ Possession Builder\n",
    "│  ├─ Start/extend/end per rules (made FG, TO, DREB, FT sequencing, period end)\n",
    "│  └─ Attribute points to offense lineups on floor\n",
    "├─ Rim Tagging\n",
    "│  ├─ Compute shot distance; mark rim attempts/makes (≤4 ft)\n",
    "│  └─ Accumulate “on‑court” vs “off‑court” opponent rim tallies per player\n",
    "├─ Aggregations\n",
    "│  ├─ Project 1: unique 5‑man lineups → off/def possessions & ratings\n",
    "│  └─ Project 2: player rim on/off → opponent rim FG% on/off & on–off\n",
    "├─ Validation (no SQL surfaced)\n",
    "│  ├─ Minutes parity vs box (tolerance window)\n",
    "│  └─ Consistency checks on lineup sizes & distance outliers\n",
    "└─ Turn‑In Artifacts\n",
    "   ├─ **project1_lineups_FINAL.csv** (Lineup Automation, always‑five)\n",
    "   └─ **project2_players_FINAL.csv** (player rim on/off)\n",
    "\n",
    "\n",
    "structure:\n",
    "# NBA Data Engineering Notebook - Process Overview\n",
    "\n",
    "## Project Overview\n",
    "NBA data engineering pipeline for analyzing 5-man lineups and player rim defense statistics from play-by-play and box score data.\n",
    "\n",
    "## Main Processes\n",
    "\n",
    "### 1. Configuration & Setup\n",
    "```\n",
    "├── Project Dependencies\n",
    "│   ├── Data processing: pandas, numpy, duckdb\n",
    "│   ├── Orchestration: apache-airflow\n",
    "│   └── Web framework: FastAPI\n",
    "│\n",
    "├── Airflow Configuration\n",
    "│   ├── Connections, pools, variables setup\n",
    "│   └── DAG scheduling and dependencies\n",
    "│\n",
    "└── Data Schema Definition\n",
    "    ├── Box score column mappings\n",
    "    ├── Play-by-play column mappings\n",
    "    └── Output table specifications\n",
    "```\n",
    "\n",
    "### 2. Data Validation Framework\n",
    "```\n",
    "└── NBADataValidator\n",
    "    ├── File structure validation\n",
    "    ├── Box score data quality checks\n",
    "    ├── Play-by-play data validation\n",
    "    └── Coordinate system validation\n",
    "```\n",
    "\n",
    "### 3. Data Loading & Processing\n",
    "```\n",
    "└── EnhancedNBADataLoader\n",
    "    ├── Load and validate box score data\n",
    "    ├── Load and validate play-by-play data\n",
    "    ├── Create lookup tables and dimensions\n",
    "    ├── Build enriched data views\n",
    "    └── Generate data quality reports\n",
    "```\n",
    "\n",
    "### 4. Lineup Tracking System\n",
    "```\n",
    "├── Traditional Method\n",
    "│   ├── Standard substitution-based tracking\n",
    "│   ├── 5-player lineup enforcement\n",
    "│   └── Basic possession attribution\n",
    "│\n",
    "├── Enhanced Method\n",
    "│   ├── Intelligent inference for missing substitutions\n",
    "│   ├── First-action event detection\n",
    "│   ├── Auto-out player identification\n",
    "│   └── Advanced lineup state management\n",
    "│\n",
    "└── PBPProcessor\n",
    "    ├── Process play-by-play events\n",
    "    ├── Track lineup changes in real-time\n",
    "    └── Maintain dual-method state\n",
    "```\n",
    "\n",
    "### 5. Possession Analysis Engine\n",
    "```\n",
    "└── DualMethodPossessionEngine\n",
    "    ├── Identify possession boundaries\n",
    "    ├── Attribute possessions to lineups\n",
    "    ├── Calculate offensive/defensive ratings\n",
    "    ├── Track rim defense statistics\n",
    "    └── Generate comparative analysis\n",
    "```\n",
    "\n",
    "### 6. Output Generation\n",
    "```\n",
    "├── Lineup Analysis\n",
    "│   ├── 5-man lineup statistics\n",
    "│   ├── Offensive/defensive ratings\n",
    "│   └── Net rating calculations\n",
    "│\n",
    "├── Player Statistics\n",
    "│   ├── Rim defense metrics (on/off court)\n",
    "│   ├── Possession counts\n",
    "│   └── Performance validation\n",
    "│\n",
    "└── Quality Reports\n",
    "    ├── Data validation summaries\n",
    "    ├── Processing statistics\n",
    "    └── Error logs and diagnostics\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Dual-Method Processing**: Traditional + enhanced lineup tracking\n",
    "- **Rim Defense Analysis**: 4-foot rim shot tracking and on/off court analysis\n",
    "- **Possession Attribution**: Accurate possession counting and rating calculations\n",
    "- **Data Quality Assurance**: Multi-layer validation and error handling\n",
    "- **Performance Optimization**: DuckDB integration with memory management\n",
    "- **Production Ready**: Airflow orchestration with proper dependency management\n",
    "\n",
    "```\n",
    "\n",
    "### Reproduce (no SQL required, final cell with full pipeline output saved, search run_complete_pipeline.py for more details)\n",
    "\n",
    "1. Place the provided CSVs in the `data/.../` folder.\n",
    "2. run this notebook (make folder structure as needed), the notebook will set up codes in order needed\n",
    "3. us uv sync at repo root\n",
    "4. cd api/src/airflow_project; astro dev init; astro dev start; \n",
    "6. Run the pipeline (run_complete_pipeline.py) (e.g., via Python entrypoint or notebook cells that call the two engines, possession builder, rim tagging, and aggregations).\n",
    "3. The pipeline writes two CSVs used for turn‑in under an exports/output folder as listed above.\n",
    "\n",
    "> Notes:\n",
    ">\n",
    "> * Rim attempt definition is **fixed at 4 ft**.\n",
    "> * Traditional results are kept for QA; the submitted `Project 1` table uses the **Lineup Automation** results to ensure 5‑man compliance.\n",
    "\n",
    "---\n",
    "\n",
    "## DAG (automation overview)\n",
    "\n",
    "* Backfilling supported\n",
    "* Scheduling supported\n",
    "\n",
    "![Pipeline DAG](api/src/airflow_project/data/mavs_data_engineer_2025/exports/final_submission/dag.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix (Quick Reference)\n",
    "\n",
    "* **Project 1 columns:** Team, Player 1–5, Off/Def possessions, OffRtg, DefRtg, Net.\n",
    "* **Project 2 columns:** Player ID, Player Name, Team, Off/Def possessions, Opponent rim FG% (on/off), On–off.\n",
    "* **Lineup engines:** Traditional (audit) vs Lineup Automation (submission).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f70ca6",
   "metadata": {},
   "source": [
    "# Root level Data Engineering needs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f483826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyproject.toml\n",
    "[project]\n",
    "name = \"nba_data_engineering\"\n",
    "version = \"0.2.0\"\n",
    "description = \"NBA lineup analysis and player rim defense data pipeline\"\n",
    "authors = [\n",
    "  { name = \"Data Engineering Team\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  # Core web framework and API\n",
    "  \"fastapi>=0.104.0\",\n",
    "  \"uvicorn[standard]>=0.24.0\",\n",
    "  \"python-dotenv>=1.0.0\",\n",
    "  \n",
    "  # Settings and validation\n",
    "  \"pydantic>=2.0.0\",\n",
    "  \"pydantic-settings>=2.0.0\",\n",
    "  \n",
    "  # Data processing core\n",
    "  \"pandas>=2.1.0,<2.3.0\",\n",
    "  \"numpy>=1.24.0,<1.27.0\",\n",
    "  \"pyarrow>=12.0.0,<16.0.0\",\n",
    "  \n",
    "  # High-performance analytics\n",
    "  \"duckdb>=0.10.0,<0.11.0\",\n",
    "  \n",
    "  # Airflow and orchestration\n",
    "  \"apache-airflow==2.10.4\",\n",
    "  \"apache-airflow-providers-postgres>=6.2.1,<7.0.0\",\n",
    "  \"apache-airflow-providers-standard>=1.4.1,<2.0.0\",\n",
    "  \"apache-airflow-providers-http>=4.5.0,<5.0.0\",\n",
    "  \n",
    "  # Database connectivity\n",
    "  \"sqlalchemy>=1.4.49,<2.0.0\",\n",
    "  \"psycopg2-binary>=2.9.9\",\n",
    "  \n",
    "  # Data quality and validation\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"scipy>=1.7.0,<1.12.0\",\n",
    "  \n",
    "  # Development and analysis\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"ipykernel>=6.25.0\",\n",
    "  \n",
    "  # Utilities\n",
    "  \"tqdm>=4.67.0\",\n",
    "  \"psutil>=5.0.0,<8.0.0\",\n",
    "  \"python-jose[cryptography]>=3.3.0\",\n",
    "  \"passlib[bcrypt]>=1.7.4\",\n",
    "  \"bcrypt==4.0.1\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"pytest-cov>=4.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"pre-commit>=3.0.0\",\n",
    "]\n",
    "\n",
    "test = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"pytest-cov>=4.0.0\",\n",
    "  \"pytest-mock>=3.10.0\",\n",
    "  \"pytest-xdist>=3.0.0\",\n",
    "]\n",
    "\n",
    "performance = [\n",
    "  \"memory-profiler>=0.60.0\",\n",
    "  \"py-spy>=0.3.14\",\n",
    "]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"setuptools>=64\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[tool.black]\n",
    "line-length = 100\n",
    "target-version = ['py310', 'py311', 'py312']\n",
    "include = '\\.pyi?$'\n",
    "extend-exclude = '''\n",
    "/(\n",
    "  \\.git\n",
    "  | \\.hg\n",
    "  | \\.mypy_cache\n",
    "  | \\.tox\n",
    "  | \\.venv\n",
    "  | _build\n",
    "  | buck-out\n",
    "  | build\n",
    "  | dist\n",
    ")/\n",
    "'''\n",
    "\n",
    "[tool.isort]\n",
    "profile = \"black\"\n",
    "line_length = 100\n",
    "multi_line_output = 3\n",
    "\n",
    "[tool.mypy]\n",
    "python_version = \"3.10\"\n",
    "warn_return_any = true\n",
    "warn_unused_configs = true\n",
    "disallow_untyped_defs = true\n",
    "ignore_missing_imports = true\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "testpaths = [\"tests\"]\n",
    "python_files = [\"test_*.py\", \"*_test.py\"]\n",
    "python_classes = [\"Test*\"]\n",
    "python_functions = [\"test_*\"]\n",
    "addopts = [\n",
    "    \"-v\",\n",
    "    \"--strict-markers\",\n",
    "    \"--tb=short\",\n",
    "    \"--cov=eda\",\n",
    "    \"--cov=utils\",\n",
    "    \"--cov-report=html\",\n",
    "    \"--cov-report=term-missing\"\n",
    "]\n",
    "markers = [\n",
    "    \"slow: marks tests as slow\",\n",
    "    \"integration: marks tests as integration tests\",\n",
    "    \"unit: marks tests as unit tests\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db7b13",
   "metadata": {},
   "source": [
    "# Astro/Airflow Local Dev Quickstart\n",
    "1. Initialize & Start the Project\n",
    "\n",
    "# From the directory where you want the project to live\n",
    "cd api/src\n",
    "mkdir -p airflow_project && cd airflow_project\n",
    "\n",
    "astro dev init            # Scaffold Airflow project (dags/, Dockerfile, etc.)\n",
    "astro dev start           # Build image & start all Airflow services (webserver, scheduler, DB)\n",
    "\n",
    "    If ports (8080/5432) are busy, set alternates before start:\n",
    "    astro config set webserver.port 8081 / astro config set postgres.port 5433.\n",
    "    Astronomer\n",
    "    Astronomer\n",
    "\n",
    "2. Everyday Lifecycle Commands\n",
    "\n",
    "Use these while iterating on DAGs and dependencies:\n",
    "\n",
    "astro dev stop            # Stop containers, keep project state\n",
    "astro dev restart         # Stop → rebuild image → start (after reqs/Dockerfile changes)\n",
    "\n",
    "    These two are your main “apply changes” loop during development.\n",
    "    Astronomer\n",
    "    Astronomer\n",
    "\n",
    "3. Inspect, Logs, & Diagnostics\n",
    "\n",
    "astro dev status          # Health & ports of services\n",
    "astro dev logs            # Combined logs (Ctrl+C to quit)\n",
    "astro dev tail scheduler  # Live-tail a specific service (scheduler/webserver/triggerer)\n",
    "astro dev ps              # Show running containers for this project\n",
    "astro dev top             # Process table inside a service container\n",
    "astro dev stats           # CPU/mem stats per container\n",
    "\n",
    "    Helpful when debugging start-up issues, stuck tasks, or resource pressure.\n",
    "    Astronomer\n",
    "    GitHub\n",
    "\n",
    "4. Force a DAG Reparse (No Waiting)\n",
    "\n",
    "astro dev run dags reserialize\n",
    "\n",
    "    Airflow auto-parses: new files ~5 min, edits ~30 s. This command forces an immediate parse.\n",
    "    Astronomer\n",
    "    Astronomer\n",
    "\n",
    "5. One-off DAG Test Runs\n",
    "\n",
    "astro run <dag-id>\n",
    "\n",
    "    Compiles and executes a single DAG in a throwaway worker container—fast feedback without touching the scheduler.\n",
    "    Astronomer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e36bc",
   "metadata": {},
   "source": [
    "# Within Airflow project, created files from astro dev init that we adjusted to fit our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "542e30e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/requirements.txt\n",
    "# path: api/src/airflow_project/requirements.txt\n",
    "# Minimal requirements for Airflow worker containers\n",
    "# Core data processing\n",
    "pandas==2.2.2\n",
    "numpy==1.26.4\n",
    "\n",
    "# High-performance analytics\n",
    "duckdb==0.10.3\n",
    "\n",
    "# Data formats and I/O\n",
    "pyarrow==15.0.2\n",
    "\n",
    "# Configuration management  \n",
    "python-dotenv==1.0.1\n",
    "\n",
    "# Data quality and validation\n",
    "tabulate==0.9.0\n",
    "scipy==1.11.4\n",
    "\n",
    "# Development utilities\n",
    "tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49361241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/airflow_settings.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/airflow_settings.yaml\n",
    "# This file allows you to configure Airflow Connections, Pools, and Variables in a single place for local development only.\n",
    "# NOTE: json dicts can be added to the conn_extra field as yaml key value pairs. See the example below.\n",
    "\n",
    "# For more information, refer to our docs: https://www.astronomer.io/docs/astro/cli/develop-project#configure-airflow_settingsyaml-local-development-only\n",
    "# For questions, reach out to: https://support.astronomer.io\n",
    "# For issues create an issue ticket here: https://github.com/astronomer/astro-cli/issues\n",
    "\n",
    "airflow:\n",
    "  connections:\n",
    "    - conn_id:\n",
    "      conn_type:\n",
    "      conn_host:\n",
    "      conn_schema:\n",
    "      conn_login:\n",
    "      conn_password:\n",
    "      conn_port:\n",
    "      conn_extra:\n",
    "        example_extra_field: example-value\n",
    "  pools:\n",
    "    - pool_name:\n",
    "      pool_slot:\n",
    "      pool_description:\n",
    "  variables:\n",
    "    - variable_name:\n",
    "      variable_value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36477788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/.dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/.dockerignore\n",
    "astro\n",
    ".git\n",
    ".env\n",
    "airflow_settings.yaml\n",
    "logs/\n",
    ".venv\n",
    "airflow.db\n",
    "airflow.cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0f733f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/.env\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/.env\n",
    "#not empty\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e922b",
   "metadata": {},
   "source": [
    "# Utils + Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9561890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/utils/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/utils/__init__.py\n",
    "\"\"\"\n",
    "Utils package for the NBA Player Valuation project.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8065cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/utils/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/utils/config.py\n",
    "# path: api/src/airflow_project/utils/config.py\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def _find_project_root(anchor: str = \"airflow_project\") -> Path:\n",
    "    \"\"\"Return the project root by locating the given anchor directory.\"\"\"\n",
    "    p = Path(__file__).resolve()\n",
    "    for parent in (p, *p.parents):\n",
    "        if parent.name == anchor:\n",
    "            return parent\n",
    "    return Path.cwd()\n",
    "\n",
    "\n",
    "# Core project paths\n",
    "PROJECT_ROOT: Path = _find_project_root()\n",
    "DATA_DIR: Path = PROJECT_ROOT / \"data\"\n",
    "MAVS_DATA_DIR: Path = DATA_DIR / \"mavs_data_engineer_2025\"\n",
    "PROCESSED_DIR: Path = MAVS_DATA_DIR / \"processed\"\n",
    "EXPORTS_DIR: Path = MAVS_DATA_DIR / \"exports\"\n",
    "DUCKDB_DIR: Path = MAVS_DATA_DIR / \"duckdb\"\n",
    "LOGS_DIR: Path = PROJECT_ROOT / \"logs\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for directory in (MAVS_DATA_DIR, PROCESSED_DIR, EXPORTS_DIR, DUCKDB_DIR, LOGS_DIR):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Database configuration\n",
    "DUCKDB_PATH: Path = DUCKDB_DIR / \"mavs_enhanced.duckdb\"\n",
    "DUCKDB_CONFIG: Dict[str, str] = {\n",
    "    \"threads\": str(min(8, os.cpu_count() or 1)),\n",
    "    \"memory_limit\": \"6GB\",\n",
    "    \"preserve_insertion_order\": \"false\",\n",
    "    \"max_memory\": \"6GB\",\n",
    "    \"temp_directory\": str(DUCKDB_DIR / \"temp\"),\n",
    "    \"checkpoint_threshold\": \"1GB\",\n",
    "}\n",
    "\n",
    "# Input data files\n",
    "BOX_SCORE_FILE: Path = MAVS_DATA_DIR / \"box_HOU-DAL.csv\"\n",
    "PBP_FILE: Path = MAVS_DATA_DIR / \"pbp_HOU-DAL.csv\"\n",
    "PBP_ACTION_TYPES_FILE: Path = MAVS_DATA_DIR / \"pbp_action_types.csv\"\n",
    "PBP_EVENT_MSG_TYPES_FILE: Path = MAVS_DATA_DIR / \"pbp_event_msg_types.csv\"\n",
    "PBP_OPTION_TYPES_FILE: Path = MAVS_DATA_DIR / \"pbp_option_types.csv\"\n",
    "\n",
    "# === COLUMN SPECIFICATIONS ===\n",
    "\n",
    "# Box Score Table - Columns used\n",
    "BOX_SCORE_COLUMNS = {\n",
    "    \"core\": [\n",
    "        \"gameId\",\n",
    "        \"nbaId\",\n",
    "        \"name\",\n",
    "        \"nbaTeamId\",\n",
    "        \"team\",\n",
    "    ],\n",
    "    \"lineup_tracking\": [\n",
    "        \"gs\",            # starter flag\n",
    "        \"boxScoreOrder\", # sort key for lineup ordering\n",
    "    ],\n",
    "    \"performance\": [\n",
    "        \"secPlayed\",\n",
    "        \"pts\",\n",
    "        \"reb\",\n",
    "        \"ast\",\n",
    "    ],\n",
    "    \"optional\": [\n",
    "        \"minDisplay\",\n",
    "        \"jerseyNum\",\n",
    "        \"startPos\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Play-by-Play Table - Columns used\n",
    "PBP_COLUMNS = {\n",
    "    \"core\": [\n",
    "        \"gameId\",\n",
    "        \"pbpId\",\n",
    "        \"period\",\n",
    "        \"msgType\",\n",
    "    ],\n",
    "    \"timing\": [\n",
    "        \"gameClock\",\n",
    "        \"wallClock\",\n",
    "        \"wallClockInt\",\n",
    "    ],\n",
    "    \"team_context\": [\n",
    "        \"offTeamId\",\n",
    "        \"defTeamId\",\n",
    "    ],\n",
    "    \"player_context\": [\n",
    "        \"playerId1\",\n",
    "        \"playerId2\",\n",
    "        \"playerId3\",\n",
    "    ],\n",
    "    \"shot_data\": [\n",
    "        \"locX\",\n",
    "        \"locY\",\n",
    "        \"pts\",\n",
    "    ],\n",
    "    \"event_details\": [\n",
    "        \"actionType\",\n",
    "        \"option1\",\n",
    "        \"option2\",\n",
    "        \"option3\",\n",
    "        \"description\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Reference Tables - Full columns needed\n",
    "PBP_EVENT_TYPES_COLUMNS = {\"all\": [\"EventType\", \"Description\"]}\n",
    "PBP_ACTION_TYPES_COLUMNS = {\"all\": [\"EventType\", \"ActionType\", \"Event\", \"Description\"]}\n",
    "PBP_OPTION_TYPES_COLUMNS = {\n",
    "    \"all\": [\"Event\", \"EventType\", \"Option1\", \"Option2\", \"Option3\", \"Option4\", \"Description\"]\n",
    "}\n",
    "\n",
    "# === OUTPUT TABLE SPECIFICATIONS ===\n",
    "\n",
    "LINEUPS_OUTPUT_COLUMNS = [\n",
    "    \"Team\",\n",
    "    \"Player 1\",\n",
    "    \"Player 2\",\n",
    "    \"Player 3\",\n",
    "    \"Player 4\",\n",
    "    \"Player 5\",\n",
    "    \"Offensive possessions played\",\n",
    "    \"Defensive possessions played\",\n",
    "    \"Offensive rating\",\n",
    "    \"Defensive rating\",\n",
    "    \"Net rating\",\n",
    "]\n",
    "\n",
    "PLAYERS_OUTPUT_COLUMNS = [\n",
    "    \"Player ID\",\n",
    "    \"Player Name\",\n",
    "    \"Team\",\n",
    "    \"Offensive possessions played\",\n",
    "    \"Defensive possessions played\",\n",
    "    \"Opponent rim field goal percentage when player is on the court\",\n",
    "    \"Opponent rim field goal percentage when player is off the court\",\n",
    "    \"Opponent rim field goal percentage on/off difference (on-off)\",\n",
    "]\n",
    "\n",
    "# === BUSINESS RULES ===\n",
    "\n",
    "RIM_DISTANCE_FEET: float = 4.0\n",
    "HOOP_CENTER_X: float = 0.0\n",
    "HOOP_CENTER_Y: float = 0.0\n",
    "COORDINATE_SCALE: float = 10.0\n",
    "\n",
    "MINIMUM_POSSESSIONS_FOR_LINEUP: int = 2\n",
    "MINIMUM_ATTEMPTS_FOR_RIM_STATS: int = 1\n",
    "MINIMUM_SECONDS_PLAYED: int = 30\n",
    "\n",
    "PERFORMANCE_MONITORING: bool = True\n",
    "MAX_PIPELINE_RUNTIME_SECONDS: int = 120\n",
    "WARN_IF_SLOWER_THAN_SECONDS: int = 30\n",
    "\n",
    "EXTREME_DISTANCE_FEET: float = 35.0\n",
    "TREAT_ZERO_TEAM_AS_ADMIN: bool = True\n",
    "MAX_CONSECUTIVE_SUBSTITUTION_FAILURES: int = 5\n",
    "\n",
    "# === NBA SUBSTITUTION CONFIGURATION ===\n",
    "\n",
    "NBA_SUBSTITUTION_CONFIG = {\n",
    "    \"starter_reset_periods\": [1, 3],     # reset to starters at Q1 and Q3\n",
    "    \"lineup_continuity_periods\": [2, 4], # continue prior lineups at Q2 and Q4\n",
    "\n",
    "    \"msg_types\": {\n",
    "        \"shot_made\": 1,\n",
    "        \"shot_missed\": 2,\n",
    "        \"rebound\": 4,\n",
    "        \"turnover\": 5,\n",
    "        \"foul\": 6,\n",
    "        \"substitution\": 8,\n",
    "        \"start_period\": 12,\n",
    "        \"end_period\": 13,\n",
    "    },\n",
    "\n",
    "    \"one_direction\": {\n",
    "        \"enabled\": True,\n",
    "        \"remove_out_if_present\": True,\n",
    "        \"appearance_via_last_name\": True,\n",
    "        \"allow_temp_sixth\": True,\n",
    "        \"max_lineup_size\": 6,\n",
    "    },\n",
    "\n",
    "    \"validation\": {\n",
    "        \"validate_team_membership\": True,\n",
    "        \"validate_pre_sub_state\": True,\n",
    "        \"min_lineup_size\": 5,\n",
    "        \"hard_max_lineup_size\": 6,\n",
    "    },\n",
    "\n",
    "    \"minutes_validation\": {\n",
    "        \"enabled\": True,\n",
    "        \"tolerance_seconds\": 60,\n",
    "    },\n",
    "\n",
    "    \"recovery\": {\n",
    "        \"enable_intelligent_recovery\": True,\n",
    "        \"log_recovery_details\": True,\n",
    "        \"max_recovery_attempts\": 3,\n",
    "        \"prefer_conservative_approach\": True,\n",
    "        \"validate_post_recovery\": True,\n",
    "    },\n",
    "\n",
    "    \"debug\": {\n",
    "        \"log_all_substitutions\": True,\n",
    "        \"log_lineup_state_changes\": True,\n",
    "        \"log_period_transitions\": True,\n",
    "        \"include_player_context\": True,\n",
    "        \"track_recovery_statistics\": True,\n",
    "    },\n",
    "\n",
    "    \"performance\": {\n",
    "        \"batch_substitution_processing\": False,\n",
    "        \"cache_player_lookups\": True,\n",
    "        \"optimize_lineup_comparisons\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "# === AIRFLOW SCHEDULING & SENSOR CONFIG ===\n",
    "\n",
    "AIRFLOW_OWNER = \"nba-analytics\"\n",
    "AIRFLOW_TIMEZONE = \"America/New_York\"\n",
    "SCHEDULE_CRON = \"0 6 * * *\"          # daily at 06:00 NY time\n",
    "AIRFLOW_RETRIES = 1\n",
    "AIRFLOW_RETRY_DELAY_MIN = 2\n",
    "\n",
    "AIRFLOW_FS_CONN_ID = \"fs_default\"\n",
    "FILE_SENSOR_POKE_SEC = 30\n",
    "FILE_SENSOR_TIMEOUT_SEC = 60 * 60     # 1 hour\n",
    "\n",
    "\n",
    "def airflow_default_args() -> dict:\n",
    "    \"\"\"Build default_args with a timezone-aware start_date (uses pendulum if available).\"\"\"\n",
    "    try:\n",
    "        import pendulum\n",
    "        start = pendulum.datetime(2025, 1, 1, tz=AIRFLOW_TIMEZONE)\n",
    "    except Exception:\n",
    "        from datetime import datetime\n",
    "        start = datetime(2025, 1, 1)\n",
    "    return {\n",
    "        \"owner\": AIRFLOW_OWNER,\n",
    "        \"depends_on_past\": False,\n",
    "        \"start_date\": start,\n",
    "        \"email_on_failure\": False,\n",
    "        \"email_on_retry\": False,\n",
    "        \"retries\": AIRFLOW_RETRIES,\n",
    "        \"retry_delay\": timedelta(minutes=AIRFLOW_RETRY_DELAY_MIN),\n",
    "    }\n",
    "\n",
    "\n",
    "def required_input_files():\n",
    "    \"\"\"Absolute paths the DAG must see before running the pipeline.\"\"\"\n",
    "    return [\n",
    "        BOX_SCORE_FILE,\n",
    "        PBP_FILE,\n",
    "        PBP_ACTION_TYPES_FILE,\n",
    "        PBP_EVENT_MSG_TYPES_FILE,\n",
    "        PBP_OPTION_TYPES_FILE,\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_input_assets_or_datasets():\n",
    "    \"\"\"\n",
    "    Prefer Assets (Airflow 3.0+) else Datasets (Airflow 2.9/2.10).\n",
    "    Returns (objects, kind) where kind is 'asset', 'dataset', or None.\n",
    "    \"\"\"\n",
    "    files = [str(p) for p in required_input_files()]\n",
    "\n",
    "    # Airflow 3.x Asset API (Task SDK)\n",
    "    try:\n",
    "        from airflow.sdk import Asset  # Airflow 3.0+\n",
    "        return [Asset(f) for f in files], \"asset\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Airflow 2.9/2.10 Dataset API\n",
    "    try:\n",
    "        from airflow.datasets import Dataset\n",
    "        return [Dataset(f) for f in files], \"dataset\"\n",
    "    except Exception:\n",
    "        return [], None\n",
    "\n",
    "\n",
    "def build_combined_schedule():\n",
    "    \"\"\"\n",
    "    Return a schedule that works across Airflow versions:\n",
    "      - Prefer AssetOrTimeSchedule (AF >= 3.0) or DatasetOrTimeSchedule (AF 2.9/2.10)\n",
    "      - Otherwise, return a plain cron string which all versions accept via 'schedule='\n",
    "    \"\"\"\n",
    "    cron_expr = SCHEDULE_CRON\n",
    "    objects, kind = get_input_assets_or_datasets()\n",
    "\n",
    "    # AF 3.x: Assets + cron\n",
    "    if objects and kind == \"asset\":\n",
    "        try:\n",
    "            from airflow.timetables.assets import AssetOrTimeSchedule\n",
    "            from airflow.timetables.trigger import CronTriggerTimetable\n",
    "            return AssetOrTimeSchedule(\n",
    "                timetable=CronTriggerTimetable(cron_expr, timezone=AIRFLOW_TIMEZONE),\n",
    "                assets=tuple(objects),\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # AF 2.9/2.10: Datasets + cron\n",
    "    if objects and kind == \"dataset\":\n",
    "        try:\n",
    "            from airflow.timetables.datasets import DatasetOrTimeSchedule\n",
    "            from airflow.timetables.trigger import CronTriggerTimetable\n",
    "            return DatasetOrTimeSchedule(\n",
    "                timetable=CronTriggerTimetable(cron_expr, timezone=AIRFLOW_TIMEZONE),\n",
    "                datasets=tuple(objects),\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback that is universally valid with DAG(schedule=...)\n",
    "    return cron_expr\n",
    "\n",
    "\n",
    "\n",
    "def validate_data_files() -> bool:\n",
    "    \"\"\"Validate required data files exist and are non-empty.\"\"\"\n",
    "    required_files = [\n",
    "        BOX_SCORE_FILE,\n",
    "        PBP_FILE,\n",
    "        PBP_ACTION_TYPES_FILE,\n",
    "        PBP_EVENT_MSG_TYPES_FILE,\n",
    "        PBP_OPTION_TYPES_FILE,\n",
    "    ]\n",
    "    missing_files, empty_files = [], []\n",
    "\n",
    "    for file_path in required_files:\n",
    "        if not file_path.exists():\n",
    "            missing_files.append(file_path)\n",
    "        elif file_path.stat().st_size == 0:\n",
    "            empty_files.append(file_path)\n",
    "\n",
    "    if missing_files:\n",
    "        print(\"Missing required data files:\", [str(f.name) for f in missing_files])\n",
    "        return False\n",
    "\n",
    "    if empty_files:\n",
    "        print(\"Warning: empty data files found:\", [str(f.name) for f in empty_files])\n",
    "    return True\n",
    "\n",
    "\n",
    "def validate_performance_config() -> bool:\n",
    "    \"\"\"Validate DuckDB performance settings.\"\"\"\n",
    "    cpu_count = os.cpu_count() or 1\n",
    "    configured_threads = int(DUCKDB_CONFIG.get(\"threads\", \"1\"))\n",
    "    if configured_threads > cpu_count:\n",
    "        print(f\"Warning: configured threads ({configured_threads}) > CPU count ({cpu_count})\")\n",
    "\n",
    "    try:\n",
    "        import psutil\n",
    "        available_gb = psutil.virtual_memory().total / (1024**3)\n",
    "        configured_gb = int(DUCKDB_CONFIG.get(\"memory_limit\", \"4GB\").rstrip(\"GB\"))\n",
    "        if configured_gb > available_gb * 0.8:\n",
    "            print(f\"Warning: configured memory ({configured_gb}GB) > 80% of available ({available_gb:.1f}GB)\")\n",
    "    except ImportError:\n",
    "        print(\"Note: psutil not available; skipping memory validation\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_column_usage_report() -> str:\n",
    "    \"\"\"Generate a column usage report for documentation.\"\"\"\n",
    "    report_lines = [\n",
    "        \"# NBA Pipeline Column Usage Report\",\n",
    "        \"\",\n",
    "        \"## Box Score Table\",\n",
    "        f\"- Core: {BOX_SCORE_COLUMNS['core']}\",\n",
    "        f\"- Lineup Tracking: {BOX_SCORE_COLUMNS['lineup_tracking']}\",\n",
    "        f\"- Performance: {BOX_SCORE_COLUMNS['performance']}\",\n",
    "        f\"- Optional: {BOX_SCORE_COLUMNS['optional']}\",\n",
    "        \"\",\n",
    "        \"## Play-by-Play Table\",\n",
    "        f\"- Core: {PBP_COLUMNS['core']}\",\n",
    "        f\"- Timing: {PBP_COLUMNS['timing']}\",\n",
    "        f\"- Team Context: {PBP_COLUMNS['team_context']}\",\n",
    "        f\"- Player Context: {PBP_COLUMNS['player_context']}\",\n",
    "        f\"- Shot Data: {PBP_COLUMNS['shot_data']}\",\n",
    "        f\"- Event Details: {PBP_COLUMNS['event_details']}\",\n",
    "        \"\",\n",
    "        \"## Output Tables\",\n",
    "        f\"- Lineups Columns: {len(LINEUPS_OUTPUT_COLUMNS)} columns\",\n",
    "        f\"- Players Columns: {len(PLAYERS_OUTPUT_COLUMNS)} columns\",\n",
    "        \"\",\n",
    "        \"## Business Rules\",\n",
    "        f\"- Rim Distance: {RIM_DISTANCE_FEET} feet\",\n",
    "        f\"- Min Possessions: {MINIMUM_POSSESSIONS_FOR_LINEUP}\",\n",
    "        f\"- Min Rim Attempts: {MINIMUM_ATTEMPTS_FOR_RIM_STATS}\",\n",
    "        f\"- Performance Target: {MAX_PIPELINE_RUNTIME_SECONDS}s\",\n",
    "    ]\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "\n",
    "def print_configuration_summary() -> None:\n",
    "    \"\"\"Print a concise configuration summary and write the column usage report.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"NBA PIPELINE - CONFIGURATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "    print(f\"Database: {DUCKDB_PATH}\")\n",
    "    print(f\"Memory Limit: {DUCKDB_CONFIG.get('memory_limit')}\")\n",
    "    print(f\"Threads: {DUCKDB_CONFIG.get('threads')}\")\n",
    "\n",
    "    total_box_cols = sum(len(cols) for cols in BOX_SCORE_COLUMNS.values())\n",
    "    print(f\"\\nBox Score Columns: {total_box_cols} total\")\n",
    "    for category, cols in BOX_SCORE_COLUMNS.items():\n",
    "        print(f\"   - {category}: {len(cols)} columns\")\n",
    "\n",
    "    total_pbp_cols = sum(len(cols) for cols in PBP_COLUMNS.values())\n",
    "    print(f\"\\nPBP Columns: {total_pbp_cols} total\")\n",
    "    for category, cols in PBP_COLUMNS.items():\n",
    "        print(f\"   - {category}: {len(cols)} columns\")\n",
    "\n",
    "    print(\"\\nBusiness Rules:\")\n",
    "    print(f\"   - Rim Distance: {RIM_DISTANCE_FEET} feet\")\n",
    "    print(f\"   - Min Lineup Possessions: {MINIMUM_POSSESSIONS_FOR_LINEUP}\")\n",
    "    print(f\"   - Performance Target: {MAX_PIPELINE_RUNTIME_SECONDS}s\")\n",
    "\n",
    "    print(\"\\nOutput Tables:\")\n",
    "    print(f\"   - Lineups: {len(LINEUPS_OUTPUT_COLUMNS)} columns\")\n",
    "    print(f\"   - Players: {len(PLAYERS_OUTPUT_COLUMNS)} columns\")\n",
    "    print(\"=\" * 80)\n",
    "    report_path = LOGS_DIR / \"column_usage_report.md\"\n",
    "    report_path.write_text(get_column_usage_report(), encoding=\"utf-8\")\n",
    "    print(f\"Column usage report: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "386e45e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/utils/nba_pipeline_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/utils/nba_pipeline_analysis.py\n",
    "# api/src/airflow_project/eda/utils/nba_pipeline_analysis.py\n",
    "# Step 1: NBA Pipeline Analysis & Data Structure Validation\n",
    "\"\"\"\n",
    "NBA Play-by-Play Data Pipeline - Step 1: Analysis & Setup\n",
    "---------------------------------------------------------\n",
    "\n",
    "This step analyzes the required inputs and sets up basic validations for:\n",
    "\n",
    "1. Box score data (player info, starters, team mapping)\n",
    "2. Play-by-play data (events, shots, substitutions, possessions)\n",
    "3. Lookup tables (event types, action types, option types)\n",
    "\n",
    "Key requirements:\n",
    "- Track 5-man lineups\n",
    "- Count offensive/defensive possessions per lineup\n",
    "- Track rim attempts (≤ 4 feet from basket)\n",
    "- Compute on/off rim defense stats\n",
    "- Handle substitutions and lineup changes\n",
    "- Validate data integrity at each step\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import duckdb  # kept intentionally; may be imported transitively elsewhere\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def configure_logging(level: int = logging.INFO) -> None:\n",
    "    \"\"\"\n",
    "    Configure plain logging to stderr with timestamps.\n",
    "    No custom formatters; avoids encoding issues by keeping ASCII-only output.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        stream=sys.stderr,\n",
    "        level=level,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        force=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Activate plain logging for the module execution context\n",
    "configure_logging(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Structure to track validation results at each step.\"\"\"\n",
    "    step_name: str\n",
    "    passed: bool\n",
    "    details: str\n",
    "    data_count: int = 0\n",
    "    processing_time: float = 0.0\n",
    "    warnings: List[str] | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.warnings is None:\n",
    "            self.warnings = []\n",
    "\n",
    "\n",
    "class NBADataValidator:\n",
    "    \"\"\"Data validation routines for the NBA pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.validation_results: List[ValidationResult] = []\n",
    "        self.rim_distance_threshold: float = 4.0  # feet\n",
    "        self.coordinate_scale: float = 10.0       # coordinates are in tenths of feet\n",
    "\n",
    "    def log_validation(self, result: ValidationResult) -> None:\n",
    "        \"\"\"Log and store validation results.\"\"\"\n",
    "        self.validation_results.append(result)\n",
    "        status = \"[PASS]\" if result.passed else \"[FAIL]\"\n",
    "        logger.info(f\"{status} {result.step_name}: {result.details}\")\n",
    "        for warning in result.warnings:\n",
    "            logger.warning(f\"[WARN] {result.step_name}: {warning}\")\n",
    "\n",
    "    def validate_file_structure(self, file_paths: Dict[str, Path]) -> ValidationResult:\n",
    "        \"\"\"Validate that all required files exist and are non-empty.\"\"\"\n",
    "        start_time = time.time()\n",
    "        missing_files: List[str] = []\n",
    "        empty_files: List[str] = []\n",
    "        total_files = len(file_paths)\n",
    "\n",
    "        for name, path in file_paths.items():\n",
    "            if not path.exists():\n",
    "                missing_files.append(f\"{name}: {path}\")\n",
    "            elif path.stat().st_size == 0:\n",
    "                empty_files.append(f\"{name}: {path}\")\n",
    "\n",
    "        warnings: List[str] = []\n",
    "        if empty_files:\n",
    "            warnings.extend([f\"Empty file: {f}\" for f in empty_files])\n",
    "\n",
    "        passed = len(missing_files) == 0\n",
    "        details = f\"Checked {total_files} files. Missing: {len(missing_files)}, Empty: {len(empty_files)}\"\n",
    "\n",
    "        return ValidationResult(\n",
    "            step_name=\"File Structure Validation\",\n",
    "            passed=passed,\n",
    "            details=details,\n",
    "            processing_time=time.time() - start_time,\n",
    "            warnings=warnings,\n",
    "        )\n",
    "\n",
    "    def validate_box_score_structure(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        \"\"\"Validate box score has expected structure and data quality.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        required_columns = ['nbaId', 'name', 'nbaTeamId', 'team', 'isHome', 'gs', 'status']\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_cols:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Box Score Structure\",\n",
    "                passed=False,\n",
    "                details=f\"Missing columns: {missing_cols}\",\n",
    "                data_count=len(df),\n",
    "                processing_time=time.time() - start_time,\n",
    "            )\n",
    "\n",
    "        active_players = df[df['status'] == 'ACTIVE']\n",
    "        teams = active_players['team'].unique()\n",
    "        starters_per_team: Dict[str, int] = {}\n",
    "        warnings: List[str] = []\n",
    "\n",
    "        for team in teams:\n",
    "            team_players = active_players[active_players['team'] == team]\n",
    "            starters = team_players[team_players['gs'] == 1]\n",
    "            starters_per_team[team] = len(starters)\n",
    "            if len(starters) != 5:\n",
    "                warnings.append(f\"Team {team} has {len(starters)} starters (expected 5)\")\n",
    "\n",
    "        details = (\n",
    "            f\"Active players: {len(active_players)}, \"\n",
    "            f\"Teams: {len(teams)}, \"\n",
    "            f\"Starters per team: {starters_per_team}\"\n",
    "        )\n",
    "\n",
    "        return ValidationResult(\n",
    "            step_name=\"Box Score Structure\",\n",
    "            passed=True,\n",
    "            details=details,\n",
    "            data_count=len(df),\n",
    "            processing_time=time.time() - start_time,\n",
    "            warnings=warnings,\n",
    "        )\n",
    "\n",
    "    def validate_pbp_structure(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        \"\"\"Validate play-by-play structure and event types.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        required_columns = [\n",
    "            'period', 'pbpOrder', 'msgType', 'offTeamId', 'defTeamId',\n",
    "            'playerId1', 'locX', 'locY', 'pts'\n",
    "        ]\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "        if missing_cols:\n",
    "            return ValidationResult(\n",
    "                step_name=\"PBP Structure\",\n",
    "                passed=False,\n",
    "                details=f\"Missing columns: {missing_cols}\",\n",
    "                data_count=len(df),\n",
    "                processing_time=time.time() - start_time,\n",
    "            )\n",
    "\n",
    "        valid_events = df[df['offTeamId'].notna() & df['defTeamId'].notna()]\n",
    "        event_types = df['msgType'].value_counts()\n",
    "\n",
    "        shots = df[df['msgType'].isin([1, 2])]\n",
    "        shots_with_coords = shots[(shots['locX'].notna()) & (shots['locY'].notna())]\n",
    "\n",
    "        warnings: List[str] = []\n",
    "        if len(shots) > 0 and len(shots_with_coords) < len(shots) * 0.8:\n",
    "            warnings.append(f\"Only {len(shots_with_coords)}/{len(shots)} shots have coordinates\")\n",
    "\n",
    "        details = (\n",
    "            f\"Total events: {len(df)}, \"\n",
    "            f\"Valid events: {len(valid_events)}, \"\n",
    "            f\"Event types: {len(event_types)}, \"\n",
    "            f\"Shots with coords: {len(shots_with_coords)}\"\n",
    "        )\n",
    "\n",
    "        return ValidationResult(\n",
    "            step_name=\"PBP Structure\",\n",
    "            passed=True,\n",
    "            details=details,\n",
    "            data_count=len(df),\n",
    "            processing_time=time.time() - start_time,\n",
    "            warnings=warnings,\n",
    "        )\n",
    "\n",
    "    def validate_coordinate_system(self, df: pd.DataFrame) -> ValidationResult:\n",
    "        \"\"\"Validate coordinate scaling and rim detection logic.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        shots = df[\n",
    "            (df['msgType'].isin([1, 2])) &\n",
    "            (df['locX'].notna()) &\n",
    "            (df['locY'].notna())\n",
    "        ].copy()\n",
    "\n",
    "        if len(shots) == 0:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Coordinate System\",\n",
    "                passed=False,\n",
    "                details=\"No shots with coordinates found\",\n",
    "                processing_time=time.time() - start_time,\n",
    "            )\n",
    "\n",
    "        # distance in feet (coords are tenths of feet)\n",
    "        shots['distance_ft'] = np.sqrt(shots['locX'] ** 2 + shots['locY'] ** 2) / self.coordinate_scale\n",
    "        shots['is_rim_attempt'] = shots['distance_ft'] <= self.rim_distance_threshold\n",
    "\n",
    "        rim_attempts = shots[shots['is_rim_attempt']]\n",
    "        rim_makes = rim_attempts[rim_attempts['msgType'] == 1]\n",
    "\n",
    "        warnings: List[str] = []\n",
    "        max_distance = float(shots['distance_ft'].max())\n",
    "        if max_distance > 35:\n",
    "            warnings.append(f\"Suspiciously long shot distance: {max_distance:.1f} feet\")\n",
    "\n",
    "        details = (\n",
    "            f\"Total shots: {len(shots)}, \"\n",
    "            f\"Rim attempts: {len(rim_attempts)}, \"\n",
    "            f\"Rim makes: {len(rim_makes)}, \"\n",
    "            f\"Max distance: {max_distance:.1f}ft\"\n",
    "        )\n",
    "\n",
    "        return ValidationResult(\n",
    "            step_name=\"Coordinate System\",\n",
    "            passed=True,\n",
    "            details=details,\n",
    "            data_count=len(shots),\n",
    "            processing_time=time.time() - start_time,\n",
    "            warnings=warnings,\n",
    "        )\n",
    "\n",
    "    def print_validation_summary(self) -> bool:\n",
    "        \"\"\"Print a concise validation summary. Returns True if all tests passed.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"NBA PIPELINE VALIDATION SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        total_passed = sum(1 for r in self.validation_results if r.passed)\n",
    "        total_tests = len(self.validation_results)\n",
    "        total_time = sum(r.processing_time for r in self.validation_results)\n",
    "        total_warnings = sum(len(r.warnings) for r in self.validation_results)\n",
    "\n",
    "        print(f\"OVERALL STATUS: {total_passed}/{total_tests} tests passed\")\n",
    "        print(f\"TOTAL VALIDATION TIME: {total_time:.2f} seconds\")\n",
    "        print(f\"TOTAL WARNINGS: {total_warnings}\\n\")\n",
    "\n",
    "        for result in self.validation_results:\n",
    "            status = \"[PASS]\" if result.passed else \"[FAIL]\"\n",
    "            print(f\"{status} {result.step_name}\")\n",
    "            print(f\"   Details: {result.details}\")\n",
    "            print(f\"   Data Count: {result.data_count:,}\")\n",
    "            print(f\"   Time: {result.processing_time:.3f}s\")\n",
    "            for warning in result.warnings:\n",
    "                print(f\"   [WARN] {warning}\")\n",
    "            print()\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        return total_passed == total_tests\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"NBA Pipeline - Step 1: Analysis & Validation Setup\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    validator = NBADataValidator()\n",
    "\n",
    "    expected_files = {\n",
    "        'box_score': Path('api/src/airflow_project/data/mavs_data_engineer_2025/box_HOU-DAL.csv'),\n",
    "        'pbp': Path('api/src/airflow_project/data/mavs_data_engineer_2025/pbp_HOU-DAL.csv'),\n",
    "        'event_types': Path('api/src/airflow_project/data/mavs_data_engineer_2025/pbp_event_msg_types.csv'),\n",
    "        'action_types': Path('api/src/airflow_project/data/mavs_data_engineer_2025/pbp_action_types.csv'),\n",
    "        'option_types': Path('api/src/airflow_project/data/mavs_data_engineer_2025/pbp_option_types.csv'),\n",
    "    }\n",
    "\n",
    "    # Step 1A: Validate file structure\n",
    "    file_validation = validator.validate_file_structure(expected_files)\n",
    "    validator.log_validation(file_validation)\n",
    "\n",
    "    print(\"\\nStep 1 Complete: Foundation analysis ready\")\n",
    "    print(\"Next Step: Load and validate data content\")\n",
    "    print(\"The pipeline will process data with validations at each stage\")\n",
    "\n",
    "    print(\"\\nDATA REQUIREMENTS SUMMARY:\")\n",
    "    print(\"- Box Score: Player info, starters (gs=1), team mapping, active status\")\n",
    "    print(\"- Play-by-Play: Events in chronological order, coordinates for shots\")\n",
    "    print(\"- Rim Attempts: Shots ≤ 4 feet from basket (coordinates/10 = feet)\")\n",
    "    print(\"- Lineup Tracking: 5 players per team; handle substitutions\")\n",
    "    print(\"- Possession Counting: Offensive/defensive possessions per lineup\")\n",
    "    print(\"- Outputs: 5-man lineups and individual player rim defense stats\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da412aa",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bfad122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/__init__.py\n",
    "# EDA module for data exploration and analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "004df514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/data/nba_data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/data/nba_data_loader.py\n",
    "# Enhanced NBA Data Loader - Step 2 Improvements\n",
    "\"\"\"\n",
    "NBA Pipeline - Enhanced Data Loader with Proper Validation\n",
    "==========================================================\n",
    "\n",
    "Keys:\n",
    "1. Robust DuckDB object type handling (table vs view conflicts)\n",
    "2. Improved error handling and validation\n",
    "3. Better resource management and cleanup\n",
    "4. Enhanced logging and debugging information\n",
    "\n",
    "Lineup Estimation Methodology (Updated Implementation)\n",
    "-----------------------------------------------------\n",
    "This module implements two parallel lineup estimation methods:\n",
    "\n",
    "TRADITIONAL DATA-DRIVEN METHOD (run_traditional_data_driven_lineups):\n",
    "- Strictly follows raw data without automation or inference\n",
    "- msgType=8: playerId1 = player subbed IN, playerId2 = player subbed OUT\n",
    "- Lineups can have any size (not forced to 5 players)\n",
    "- Comprehensive flagging for lineup size deviations and substitution issues\n",
    "- Detailed explanations for why lineups aren't size 5\n",
    "- Flags: lineup_size_deviation, sub_out_player_not_in_lineup, \n",
    "  sub_in_player_already_in_lineup, action_by_non_lineup_player\n",
    "\n",
    "ENHANCED METHOD (run_enhanced_substitution_tracking_with_flags):\n",
    "- Uses intelligent inference to maintain 5-player lineups\n",
    "- Period resets: Q1 and Q3 reset to starters, Q2/Q4/OT carry forward\n",
    "- First-Action Auto-IN: Players with actions but no sub-in are auto-added\n",
    "- Inactivity Auto-OUT: Players idle >120s are candidates for removal\n",
    "- Always-Five Enforcement: Maintains exactly 5 players per team\n",
    "- Flags: missing_sub_in, inactivity_periods, first_action_events, \n",
    "  auto_out_events, lineup_violations\n",
    "\n",
    "Both methods provide comprehensive validation and comparison capabilities.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure we're in the right directory\n",
    "cwd = os.getcwd()\n",
    "if not cwd.endswith(\"airflow_project\"):\n",
    "    os.chdir('api/src/airflow_project')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from eda.utils.nba_pipeline_analysis import NBADataValidator, ValidationResult\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedNBADataLoader:\n",
    "    \"\"\"Enhanced data loader with transparent validation and cleaning\"\"\"\n",
    "\n",
    "    def __init__(self, db_path: str = \"mavs_enhanced.duckdb\", export_dir: str = None):\n",
    "        \"\"\"\n",
    "        Enhanced NBA Data Loader with transparent validation and cleaning\n",
    "        \n",
    "        FIXED: Added export_dir initialization to prevent missing attribute errors\n",
    "        \n",
    "        Args:\n",
    "            db_path: Path to DuckDB database file\n",
    "            export_dir: Path to export directory (optional, auto-detected if None)\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.validator = NBADataValidator()\n",
    "        self.data_summary = {}\n",
    "\n",
    "        # FIXED: Initialize export_dir attribute to prevent missing attribute error\n",
    "        if export_dir is None:\n",
    "            # Try to use config-managed export directory\n",
    "            try:\n",
    "                from utils.config import EXPORTS_DIR\n",
    "                self.export_dir = EXPORTS_DIR\n",
    "            except ImportError:\n",
    "                # Fallback to default location relative to working directory\n",
    "                from pathlib import Path\n",
    "                self.export_dir = Path.cwd() / \"exports\"\n",
    "        else:\n",
    "            from pathlib import Path\n",
    "            self.export_dir = Path(export_dir)\n",
    "        \n",
    "        # Ensure export directory exists\n",
    "        self.export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Configuration for data quality\n",
    "        self.rim_distance_threshold = 4.0  # feet\n",
    "        self.coordinate_scale = 10.0  # NBA coordinates in tenths of feet\n",
    "        self.max_reasonable_distance = 35.0  # feet (beyond court boundaries)\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Prefer central config for path + engine settings\n",
    "        try:\n",
    "            from utils.config import DUCKDB_PATH, DUCKDB_CONFIG\n",
    "            db_path = str(DUCKDB_PATH) if self.db_path in (None, \"\", \"mavs_enhanced.duckdb\") else self.db_path\n",
    "            self.conn = duckdb.connect(db_path)\n",
    "            # Apply engine config once, centrally\n",
    "            if \"memory_limit\" in DUCKDB_CONFIG:\n",
    "                self.conn.execute(f\"SET memory_limit = '{DUCKDB_CONFIG['memory_limit']}'\")\n",
    "            if \"threads\" in DUCKDB_CONFIG:\n",
    "                self.conn.execute(f\"SET threads = {int(DUCKDB_CONFIG['threads'])}\")\n",
    "            if \"temp_directory\" in DUCKDB_CONFIG:\n",
    "                self.conn.execute(f\"SET temp_directory = '{DUCKDB_CONFIG['temp_directory']}'\")\n",
    "            if \"preserve_insertion_order\" in DUCKDB_CONFIG:\n",
    "                self.conn.execute(f\"SET preserve_insertion_order = {DUCKDB_CONFIG['preserve_insertion_order']}\")\n",
    "            if \"checkpoint_threshold\" in DUCKDB_CONFIG:\n",
    "                self.conn.execute(f\"SET checkpoint_threshold = '{DUCKDB_CONFIG['checkpoint_threshold']}'\")\n",
    "        except Exception:\n",
    "            # Fallback to original behavior if config import fails\n",
    "            self.conn = duckdb.connect(self.db_path or \"mavs_enhanced.duckdb\")\n",
    "            self.conn.execute(\"SET memory_limit = '4GB'\")\n",
    "            self.conn.execute(\"SET threads = 4\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "    def _robust_drop_object(self, object_name: str) -> None:\n",
    "        \"\"\"Robustly drop any DuckDB object regardless of type\"\"\"\n",
    "        try:\n",
    "            # Try dropping as table first\n",
    "            self.conn.execute(f\"DROP TABLE IF EXISTS {object_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Try dropping as view\n",
    "            self.conn.execute(f\"DROP VIEW IF EXISTS {object_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Try dropping as sequence\n",
    "            self.conn.execute(f\"DROP SEQUENCE IF EXISTS {object_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _to_native(self, obj):\n",
    "        \"\"\"\n",
    "        Recursively convert objects to JSON-serializable Python builtins.\n",
    "        - np.integer -> int\n",
    "        - np.floating -> float\n",
    "        - np.bool_ -> bool\n",
    "        - pd.Timestamp/Timedelta -> str (ISO)\n",
    "        - set/tuple -> list\n",
    "        - dict -> dict with native keys/values\n",
    "        - DataFrame/Series -> list/dict where sensible\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        if obj is None:\n",
    "            return None\n",
    "\n",
    "        # Basic scalars\n",
    "        if isinstance(obj, (bool, int, float, str)):\n",
    "            return obj\n",
    "\n",
    "        # NumPy scalars and arrays\n",
    "        if isinstance(obj, (np.integer,)):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, (np.floating,)):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, (np.bool_,)):\n",
    "            return bool(obj)\n",
    "        # Handle numpy arrays and other array-like objects first\n",
    "        if hasattr(obj, \"tolist\"):  # catches numpy arrays, pd.Series\n",
    "            return self._to_native(obj.tolist())\n",
    "        # Handle remaining numpy scalars that might have slipped through\n",
    "        if hasattr(obj, 'dtype') and hasattr(obj, 'item'):\n",
    "            if 'float' in str(obj.dtype):\n",
    "                return float(obj.item())\n",
    "            elif 'int' in str(obj.dtype):\n",
    "                return int(obj.item())\n",
    "            elif 'bool' in str(obj.dtype):\n",
    "                return bool(obj.item())\n",
    "\n",
    "        # Pandas-specific\n",
    "        if isinstance(obj, (pd.Timestamp, pd.Timedelta)):\n",
    "            return str(obj)\n",
    "\n",
    "        # Mappings\n",
    "        if isinstance(obj, dict):\n",
    "            return {str(self._to_native(k)): self._to_native(v) for k, v in obj.items()}\n",
    "\n",
    "        # Iterables (lists, tuples, sets)\n",
    "        if isinstance(obj, (list, tuple, set)):\n",
    "            return [self._to_native(v) for v in obj]\n",
    "\n",
    "        # Fallback\n",
    "        return str(obj)\n",
    "\n",
    "    def _get_object_type(self, object_name: str) -> Optional[str]:\n",
    "        \"\"\"Get the type of a DuckDB object if it exists\"\"\"\n",
    "        try:\n",
    "            result = self.conn.execute(f\"\"\"\n",
    "                SELECT table_type \n",
    "                FROM information_schema.tables \n",
    "                WHERE table_name = '{object_name}'\n",
    "            \"\"\").fetchall()\n",
    "\n",
    "            if result:\n",
    "                return result[0][0]\n",
    "\n",
    "            # Check views separately\n",
    "            result = self.conn.execute(f\"\"\"\n",
    "                SELECT 'VIEW' as table_type\n",
    "                FROM information_schema.views \n",
    "                WHERE table_name = '{object_name}'\n",
    "            \"\"\").fetchall()\n",
    "\n",
    "            if result:\n",
    "                return 'VIEW'\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Could not check object type for {object_name}: {e}\")\n",
    "\n",
    "        return None\n",
    "\n",
    "    def load_and_validate_box_score(self, file_path: Path) -> ValidationResult:\n",
    "        \"\"\"Load box score with enhanced validation and transparent reporting\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Loading box score from {file_path}\")\n",
    "\n",
    "            # Load raw data\n",
    "            df_raw = pd.read_csv(file_path)\n",
    "            original_count = len(df_raw)\n",
    "\n",
    "            logger.info(f\"Raw box score: {original_count} rows\")\n",
    "\n",
    "            # Filter to active players only (as specified in requirements)\n",
    "            df_active = df_raw[df_raw['status'] == 'ACTIVE'].copy()\n",
    "            active_count = len(df_active)\n",
    "\n",
    "            logger.info(f\"Active players: {active_count} rows\")\n",
    "\n",
    "            # Validate we have the minimum required data\n",
    "            if active_count < 10:  # Need at least 5 per team\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Load Box Score\",\n",
    "                    passed=False,\n",
    "                    details=f\"Insufficient active players: {active_count} (minimum 10 required)\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Check for required columns\n",
    "            required_cols = ['nbaId', 'name', 'nbaTeamId', 'team', 'isHome', 'gs', 'secPlayed']\n",
    "            missing_cols = [col for col in required_cols if col not in df_active.columns]\n",
    "\n",
    "            if missing_cols:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Load Box Score\",\n",
    "                    passed=False,\n",
    "                    details=f\"Missing required columns: {missing_cols}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Clean and validate data\n",
    "            warnings = []\n",
    "\n",
    "            # Remove players with no playing time (they shouldn't affect lineup analysis)\n",
    "            df_played = df_active[df_active['secPlayed'] > 0].copy()\n",
    "            no_time_removed = active_count - len(df_played)\n",
    "\n",
    "            if no_time_removed > 0:\n",
    "                warnings.append(f\"Removed {no_time_removed} players with no playing time\")\n",
    "\n",
    "            # Validate team structure\n",
    "            team_analysis = self._analyze_team_structure(df_played)\n",
    "            warnings.extend(team_analysis['warnings'])\n",
    "\n",
    "            # Create optimized table with robust object handling\n",
    "            self._robust_drop_object(\"box_score\")\n",
    "            self.conn.register(\"box_temp\", df_played)\n",
    "\n",
    "            create_sql = \"\"\"\n",
    "            CREATE TABLE box_score AS\n",
    "            SELECT \n",
    "                nbaId as player_id,\n",
    "                name as player_name,\n",
    "                nbaTeamId as team_id,\n",
    "                team as team_abbrev,\n",
    "                CAST(isHome as BOOLEAN) as is_home,\n",
    "                CAST(gs as BOOLEAN) as is_starter,\n",
    "                status,\n",
    "                secPlayed as seconds_played,\n",
    "                COALESCE(pts, 0) as points,\n",
    "                COALESCE(reb, 0) as rebounds,\n",
    "                COALESCE(ast, 0) as assists,\n",
    "                COALESCE(jerseyNum, 99) as jersey_number\n",
    "            FROM box_temp\n",
    "            WHERE player_id IS NOT NULL \n",
    "            AND player_name IS NOT NULL\n",
    "            AND team_id IS NOT NULL\n",
    "            ORDER BY team_id, seconds_played DESC\n",
    "            \"\"\"\n",
    "\n",
    "            self.conn.execute(create_sql)\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS box_temp\")\n",
    "\n",
    "            # Create indexes for performance with error handling\n",
    "            try:\n",
    "                self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_box_player ON box_score(player_id)\")\n",
    "                self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_box_team ON box_score(team_id)\")\n",
    "                self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_box_starter ON box_score(is_starter)\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not create indexes: {e}\")\n",
    "\n",
    "            # Get final count and store summary\n",
    "            final_count = self.conn.execute(\"SELECT COUNT(*) FROM box_score\").fetchone()[0]\n",
    "\n",
    "            self.data_summary['box_score'] = {\n",
    "                'original_rows': original_count,\n",
    "                'active_rows': active_count,\n",
    "                'final_rows': final_count,\n",
    "                'teams': team_analysis['teams'],\n",
    "                'starters_per_team': team_analysis['starters_per_team']\n",
    "            }\n",
    "\n",
    "            details = f\"Processed box score: {original_count} → {active_count} active → {final_count} final rows\"\n",
    "            details += f\". Teams: {team_analysis['teams']}, Starters: {team_analysis['starters_per_team']}\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load Box Score\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=final_count,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load Box Score\",\n",
    "                passed=False,\n",
    "                details=f\"Error loading box score: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _analyze_team_structure(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze team structure and identify issues transparently\"\"\"\n",
    "        analysis = {\n",
    "            'teams': [],\n",
    "            'starters_per_team': {},\n",
    "            'players_per_team': {},\n",
    "            'warnings': []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Analyze each team\n",
    "            for team_abbrev in df['team'].unique():\n",
    "                if pd.isna(team_abbrev):\n",
    "                    continue\n",
    "\n",
    "                team_data = df[df['team'] == team_abbrev]\n",
    "                analysis['teams'].append(team_abbrev)\n",
    "                analysis['players_per_team'][team_abbrev] = len(team_data)\n",
    "\n",
    "                # Count starters\n",
    "                starters = team_data[team_data['gs'] == 1]\n",
    "                analysis['starters_per_team'][team_abbrev] = len(starters)\n",
    "\n",
    "                # Validate starter count (should be exactly 5)\n",
    "                if len(starters) != 5:\n",
    "                    analysis['warnings'].append(\n",
    "                        f\"Team {team_abbrev} has {len(starters)} starters (expected 5)\"\n",
    "                    )\n",
    "\n",
    "                # Validate minimum roster size\n",
    "                if len(team_data) < 8:\n",
    "                    analysis['warnings'].append(\n",
    "                        f\"Team {team_abbrev} has only {len(team_data)} players (minimum 8 expected)\"\n",
    "                    )\n",
    "\n",
    "            # Validate exactly 2 teams\n",
    "            if len(analysis['teams']) != 2:\n",
    "                analysis['warnings'].append(\n",
    "                    f\"Found {len(analysis['teams'])} teams (expected 2): {analysis['teams']}\"\n",
    "                )\n",
    "\n",
    "            # Validate home/away designation\n",
    "            home_teams = df[df['isHome'] == 1]['team'].unique()\n",
    "            away_teams = df[df['isHome'] == 0]['team'].unique()\n",
    "\n",
    "            if len(home_teams) != 1 or len(away_teams) != 1:\n",
    "                analysis['warnings'].append(\n",
    "                    f\"Invalid home/away setup: home={list(home_teams)}, away={list(away_teams)}\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            analysis['warnings'].append(f\"Team analysis error: {str(e)}\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def load_and_validate_pbp(self, file_path: Path) -> ValidationResult:\n",
    "        \"\"\"Load PBP with enhanced validation and coordinate analysis\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(f\"Loading PBP from {file_path}\")\n",
    "            df_raw = pd.read_csv(file_path)\n",
    "            original_count = len(df_raw)\n",
    "\n",
    "            # Identify admin rows\n",
    "            admin_mask = (\n",
    "                df_raw['offTeamId'].isna() | df_raw['defTeamId'].isna() |\n",
    "                (df_raw['offTeamId'] == 0) | (df_raw['defTeamId'] == 0)\n",
    "            )\n",
    "            admin_rows = df_raw[admin_mask]\n",
    "            game_events = df_raw[~admin_mask].copy()\n",
    "\n",
    "            admin_count = len(admin_rows)\n",
    "            game_count = len(game_events)\n",
    "            logger.info(f\"Admin rows: {admin_count}, Game events: {game_count}\")\n",
    "\n",
    "            warnings = []\n",
    "\n",
    "            # Coordinate system analysis\n",
    "            shots_mask = game_events['msgType'].isin([1, 2])\n",
    "            shots = game_events[shots_mask].copy()\n",
    "            coordinate_analysis = self._analyze_coordinate_system(shots)\n",
    "            warnings.extend(coordinate_analysis['warnings'])\n",
    "\n",
    "            # Create PBP table with robust object handling\n",
    "            self._robust_drop_object(\"pbp\")\n",
    "            self.conn.register(\"pbp_temp\", game_events)\n",
    "\n",
    "            dist_expr = \"(sqrt((loc_x::DOUBLE * loc_x::DOUBLE) + (loc_y::DOUBLE * loc_y::DOUBLE)) / 10.0)\"\n",
    "\n",
    "            create_sql = f\"\"\"\n",
    "            CREATE TABLE pbp AS\n",
    "            SELECT \n",
    "                pbpId AS pbp_id,\n",
    "                period,\n",
    "                pbpOrder AS pbp_order,\n",
    "                wallClockInt AS wall_clock_int,\n",
    "                COALESCE(gameClock, '') AS game_clock,\n",
    "                COALESCE(description, '') AS description,\n",
    "                msgType AS msg_type,\n",
    "                COALESCE(actionType, 0) AS action_type,\n",
    "                offTeamId AS team_id_off,\n",
    "                defTeamId AS team_id_def,\n",
    "                playerId1 AS player_id_1,\n",
    "                playerId2 AS player_id_2,\n",
    "                playerId3 AS player_id_3,\n",
    "                -- keep last names from the raw file so we can label unknowns\n",
    "                COALESCE(lastName1, '') AS last_name_1,\n",
    "                COALESCE(lastName2, '') AS last_name_2,\n",
    "                COALESCE(lastName3, '') AS last_name_3,\n",
    "                locX AS loc_x,\n",
    "                locY AS loc_y,\n",
    "                COALESCE(pts, 0) AS points,\n",
    "\n",
    "                CASE \n",
    "                WHEN msgType IN (1,2) AND locX IS NOT NULL AND locY IS NOT NULL \n",
    "                THEN {dist_expr} \n",
    "                END AS shot_distance_ft,\n",
    "\n",
    "                CASE \n",
    "                WHEN msgType IN (1,2) AND locX IS NOT NULL AND locY IS NOT NULL \n",
    "                    AND {dist_expr} <= {self.rim_distance_threshold}\n",
    "                THEN 1 ELSE 0 \n",
    "                END::TINYINT AS is_rim_attempt,\n",
    "\n",
    "                CASE \n",
    "                WHEN msgType IN (1,2) AND locX IS NOT NULL AND locY IS NOT NULL \n",
    "                    AND {dist_expr} > {self.max_reasonable_distance}\n",
    "                THEN 1 ELSE 0 \n",
    "                END::TINYINT AS is_extreme_distance,\n",
    "                CASE \n",
    "                WHEN msgType IN (1,2) AND locX IS NOT NULL AND locY IS NOT NULL \n",
    "                    AND {dist_expr} > {self.max_reasonable_distance}\n",
    "                THEN {dist_expr}\n",
    "                END AS extreme_distance_ft\n",
    "\n",
    "            FROM pbp_temp\n",
    "            ORDER BY period, pbp_order, wall_clock_int\n",
    "            \"\"\"\n",
    "            self.conn.execute(create_sql)\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS pbp_temp\")\n",
    "\n",
    "            # Create indexes with error handling\n",
    "            try:\n",
    "                self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_pbp_chronological ON pbp(period, pbp_order)\")\n",
    "                self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_pbp_msg_type ON pbp(msg_type)\")\n",
    "                self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_pbp_teams ON pbp(team_id_off, team_id_def)\")\n",
    "                self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_pbp_rim ON pbp(is_rim_attempt)\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not create PBP indexes: {e}\")\n",
    "\n",
    "            final_count = self.conn.execute(\"SELECT COUNT(*) FROM pbp\").fetchone()[0]\n",
    "\n",
    "            flagged_extremes = self.conn.execute(\n",
    "                \"SELECT COUNT(*) FROM pbp WHERE is_extreme_distance = 1\"\n",
    "            ).fetchone()[0]\n",
    "            if flagged_extremes > 0:\n",
    "                warnings.append(f\"Flagged {flagged_extremes} extreme-distance shots (> {self.max_reasonable_distance} ft)\")\n",
    "\n",
    "            self.data_summary['pbp'] = {\n",
    "                'original_rows': original_count,\n",
    "                'admin_rows': admin_count,\n",
    "                'game_events': game_count,\n",
    "                'final_rows': final_count,\n",
    "                'coordinate_analysis': coordinate_analysis\n",
    "            }\n",
    "\n",
    "            details = (f\"Processed PBP: {original_count} → {game_count} game events → {final_count} final rows. \"\n",
    "                    f\"Shots: {coordinate_analysis['total_shots']}, Rim attempts: {coordinate_analysis['rim_attempts']}\")\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load PBP\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=final_count,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load PBP\",\n",
    "                passed=False,\n",
    "                details=f\"Error loading PBP: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def _analyze_coordinate_system(self, shots_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze coordinate system and identify issues\"\"\"\n",
    "        analysis = {\n",
    "            'total_shots': len(shots_df),\n",
    "            'shots_with_coords': 0,\n",
    "            'rim_attempts': 0,\n",
    "            'rim_makes': 0,\n",
    "            'extreme_shots': 0,\n",
    "            'avg_distance': 0.0,\n",
    "            'max_distance': 0.0,\n",
    "            'warnings': []\n",
    "        }\n",
    "\n",
    "        if len(shots_df) == 0:\n",
    "            analysis['warnings'].append(\"No shots found for coordinate analysis\")\n",
    "            return analysis\n",
    "\n",
    "        # Filter to shots with coordinates\n",
    "        coord_mask = shots_df['locX'].notna() & shots_df['locY'].notna()\n",
    "        shots_with_coords = shots_df[coord_mask].copy()\n",
    "\n",
    "        analysis['shots_with_coords'] = len(shots_with_coords)\n",
    "\n",
    "        if len(shots_with_coords) == 0:\n",
    "            analysis['warnings'].append(\"No shots have coordinate data\")\n",
    "            return analysis\n",
    "\n",
    "        # Calculate distances\n",
    "        shots_with_coords['distance_ft'] = np.sqrt(\n",
    "            shots_with_coords['locX']**2 + shots_with_coords['locY']**2\n",
    "        ) / self.coordinate_scale\n",
    "\n",
    "        # Analyze distances\n",
    "        analysis['avg_distance'] = shots_with_coords['distance_ft'].mean()\n",
    "        analysis['max_distance'] = shots_with_coords['distance_ft'].max()\n",
    "\n",
    "        # Count rim attempts (≤ 4 feet as specified)\n",
    "        rim_mask = shots_with_coords['distance_ft'] <= self.rim_distance_threshold\n",
    "        rim_shots = shots_with_coords[rim_mask]\n",
    "\n",
    "        analysis['rim_attempts'] = len(rim_shots)\n",
    "        analysis['rim_makes'] = len(rim_shots[rim_shots['msgType'] == 1])\n",
    "\n",
    "        # Count extreme distances (likely data errors)\n",
    "        extreme_mask = shots_with_coords['distance_ft'] > self.max_reasonable_distance\n",
    "        analysis['extreme_shots'] = extreme_mask.sum()\n",
    "\n",
    "        # Generate warnings\n",
    "        if analysis['shots_with_coords'] < analysis['total_shots'] * 0.9:\n",
    "            analysis['warnings'].append(\n",
    "                f\"Only {analysis['shots_with_coords']}/{analysis['total_shots']} shots have coordinates\"\n",
    "            )\n",
    "\n",
    "        if analysis['extreme_shots'] > 0:\n",
    "            analysis['warnings'].append(\n",
    "                f\"{analysis['extreme_shots']} shots beyond {self.max_reasonable_distance} feet (max: {analysis['max_distance']:.1f}ft)\"\n",
    "            )\n",
    "\n",
    "        if analysis['rim_attempts'] == 0:\n",
    "            analysis['warnings'].append(\"No rim attempts detected - check coordinate system\")\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def validate_data_relationships(self) -> ValidationResult:\n",
    "        \"\"\"Validate box ↔ pbp team/player alignment. Recompute team set AFTER cleanup.\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Validating data relationships...\")\n",
    "            warnings = []\n",
    "\n",
    "            # Box teams\n",
    "            box_teams_df = self.conn.execute(\"\"\"\n",
    "                SELECT DISTINCT team_id, team_abbrev FROM box_score ORDER BY team_id\n",
    "            \"\"\").df()\n",
    "            box_team_ids = set(box_teams_df['team_id'])\n",
    "\n",
    "            # PBP teams\n",
    "            pbp_teams_df = self.conn.execute(\"\"\"\n",
    "                SELECT DISTINCT team_id FROM (\n",
    "                    SELECT team_id_off AS team_id FROM pbp\n",
    "                    UNION \n",
    "                    SELECT team_id_def AS team_id FROM pbp\n",
    "                ) ORDER BY team_id\n",
    "            \"\"\").df()\n",
    "            pbp_team_ids = set(pbp_teams_df['team_id'])\n",
    "\n",
    "            # If mismatch, remove unknowns with counted logging\n",
    "            extra_pbp = pbp_team_ids - box_team_ids\n",
    "            if extra_pbp:\n",
    "                warnings.append(f\"Team mismatch - Box: {sorted(box_team_ids)}, PBP: {sorted(pbp_team_ids)}\")\n",
    "                warnings.append(f\"Extra teams in PBP: {sorted(extra_pbp)}\")\n",
    "\n",
    "                to_delete = self.conn.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) FROM pbp \n",
    "                    WHERE team_id_off NOT IN ({\",\".join(map(str, box_team_ids))})\n",
    "                    OR team_id_def NOT IN ({\",\".join(map(str, box_team_ids))})\n",
    "                \"\"\").fetchone()[0]\n",
    "\n",
    "                self.conn.execute(f\"\"\"\n",
    "                    DELETE FROM pbp \n",
    "                    WHERE team_id_off NOT IN ({\",\".join(map(str, box_team_ids))})\n",
    "                    OR team_id_def NOT IN ({\",\".join(map(str, box_team_ids))})\n",
    "                \"\"\")\n",
    "                if to_delete > 0:\n",
    "                    warnings.append(f\"Removed {to_delete} PBP events from unknown teams\")\n",
    "\n",
    "                # Recompute\n",
    "                pbp_teams_df = self.conn.execute(\"\"\"\n",
    "                    SELECT DISTINCT team_id FROM (\n",
    "                        SELECT team_id_off AS team_id FROM pbp\n",
    "                        UNION \n",
    "                        SELECT team_id_def AS team_id FROM pbp\n",
    "                    ) ORDER BY team_id\n",
    "                \"\"\").df()\n",
    "                pbp_team_ids = set(pbp_teams_df['team_id'])\n",
    "\n",
    "            # Player consistency\n",
    "            box_players = set(self.conn.execute(\"SELECT DISTINCT player_id FROM box_score\").df()['player_id'])\n",
    "            pbp_players = set(self.conn.execute(\"\"\"\n",
    "                SELECT DISTINCT player_id FROM (\n",
    "                    SELECT player_id_1 AS player_id FROM pbp WHERE player_id_1 IS NOT NULL\n",
    "                    UNION SELECT player_id_2 FROM pbp WHERE player_id_2 IS NOT NULL\n",
    "                    UNION SELECT player_id_3 FROM pbp WHERE player_id_3 IS NOT NULL\n",
    "                )\n",
    "            \"\"\").df()['player_id'])\n",
    "\n",
    "            extra_pbp_players = pbp_players - box_players\n",
    "            missing_pbp_players = box_players - pbp_players\n",
    "            if extra_pbp_players:\n",
    "                warnings.append(f\"{len(extra_pbp_players)} players in PBP not in box score: players = {extra_pbp_players}\")\n",
    "                logger.info(f\"Extra PBP players: {sorted(list(extra_pbp_players))[:10]}\")\n",
    "            if missing_pbp_players:\n",
    "                warnings.append(f\"{len(missing_pbp_players)} players in box score not in PBP\")\n",
    "\n",
    "            final_pbp_count = self.conn.execute(\"SELECT COUNT(*) FROM pbp\").fetchone()[0]\n",
    "            passed = (box_team_ids == pbp_team_ids) and len(box_team_ids) == 2\n",
    "\n",
    "            details = (f\"Relationship validation: Box teams: {len(box_team_ids)}, \"\n",
    "                    f\"PBP teams: {len(pbp_team_ids)}, Final PBP events: {final_pbp_count}\")\n",
    "            return ValidationResult(\n",
    "                step_name=\"Data Relationships\",\n",
    "                passed=passed,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                    step_name=\"Data Relationships\",\n",
    "                    passed=False,\n",
    "                    details=f\"Error validating relationships: {str(e)}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "    def create_lookup_views(self) -> ValidationResult:\n",
    "        \"\"\"Load lookup CSVs into DuckDB tables with robust handling\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Creating lookup views...\")\n",
    "\n",
    "            # Use config-managed locations\n",
    "            try:\n",
    "                from utils.config import (\n",
    "                    MAVS_DATA_DIR,\n",
    "                    PBP_EVENT_MSG_TYPES_FILE,\n",
    "                    PBP_ACTION_TYPES_FILE,\n",
    "                    PBP_OPTION_TYPES_FILE,\n",
    "                )\n",
    "            except Exception as _e:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Create Lookup Views\",\n",
    "                    passed=False,\n",
    "                    details=f\"Config import failed: {_e}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            lookup_specs = [\n",
    "                (\"pbp_event_msg_types\", PBP_EVENT_MSG_TYPES_FILE),\n",
    "                (\"pbp_action_types\",    PBP_ACTION_TYPES_FILE),\n",
    "                (\"pbp_option_types\",    PBP_OPTION_TYPES_FILE),\n",
    "            ]\n",
    "\n",
    "            created = []\n",
    "            missing = []\n",
    "            for table_name, file_path in lookup_specs:\n",
    "                if not Path(file_path).exists():\n",
    "                    missing.append(str(file_path))\n",
    "                    continue\n",
    "\n",
    "                # Robust object handling\n",
    "                self._robust_drop_object(table_name)\n",
    "\n",
    "                df = pd.read_csv(file_path)\n",
    "                self.conn.register(f\"{table_name}_temp\", df)\n",
    "                self.conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM {table_name}_temp\")\n",
    "                self.conn.execute(f\"DROP VIEW IF EXISTS {table_name}_temp\")\n",
    "                created.append(table_name)\n",
    "\n",
    "            if missing:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Create Lookup Views\",\n",
    "                    passed=False,\n",
    "                    details=f\"Missing lookup files: {missing}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            details = f\"Created/Replaced {len(created)} lookup tables: {', '.join(created)}\"\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Lookup Views\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Lookup Views\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating lookup views: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def create_dimensions(self) -> ValidationResult:\n",
    "        \"\"\"Create dim_teams and dim_players with strict validation + provenance + confidence + referee filtering\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Robust object handling\n",
    "            self._robust_drop_object(\"dim_teams\")\n",
    "            self._robust_drop_object(\"dim_players\")\n",
    "            self._robust_drop_object(\"dim_officials\")\n",
    "\n",
    "            # STEP 1: Detect and filter referees/officials BEFORE creating dimensions\n",
    "            logger.info(\"🔍 Detecting referees/officials for filtering...\")\n",
    "            referee_ids = self.identify_referees_and_officials()\n",
    "\n",
    "            # STEP 2: Create officials table for transparency\n",
    "            self.create_officials_table(referee_ids)\n",
    "\n",
    "            # Create dim_teams\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE TABLE dim_teams AS\n",
    "                SELECT\n",
    "                    team_id,\n",
    "                    ANY_VALUE(team_abbrev) AS team_abbrev,\n",
    "                    ANY_VALUE(is_home) AS is_home\n",
    "                FROM box_score\n",
    "                GROUP BY team_id\n",
    "                ORDER BY team_id\n",
    "            \"\"\")\n",
    "\n",
    "            n_teams = self.conn.execute(\"SELECT COUNT(*) FROM dim_teams\").fetchone()[0]\n",
    "            if n_teams != 2:\n",
    "                raise AssertionError(f\"dim_teams must have 2 rows, found {n_teams}\")\n",
    "\n",
    "            null_abbrev = self.conn.execute(\n",
    "                \"SELECT COUNT(*) FROM dim_teams WHERE team_abbrev IS NULL\"\n",
    "            ).fetchone()[0]\n",
    "            if null_abbrev > 0:\n",
    "                raise AssertionError(\"dim_teams has NULL team_abbrev\")\n",
    "\n",
    "            dup_map = self.conn.execute(\"\"\"\n",
    "                WITH m AS (\n",
    "                    SELECT team_id, COUNT(DISTINCT team_abbrev) AS c\n",
    "                    FROM box_score\n",
    "                    GROUP BY team_id\n",
    "                )\n",
    "                SELECT COUNT(*) FROM m WHERE c <> 1\n",
    "            \"\"\").fetchone()[0]\n",
    "            if dup_map > 0:\n",
    "                raise AssertionError(\"box_score has multiple team_abbrev values for the same team_id\")\n",
    "\n",
    "            # STEP 3: Create referee filter clause\n",
    "            referee_filter = \"\"\n",
    "            if referee_ids:\n",
    "                referee_list = ','.join(map(str, referee_ids))\n",
    "                referee_filter = f\"AND player_id NOT IN ({referee_list})\"\n",
    "                logger.info(f\"🚫 Filtering out {len(referee_ids)} referee IDs: {sorted(referee_ids)}\")\n",
    "\n",
    "            # --- collect names from pbp slots (FILTERED) ---\n",
    "            self.conn.execute(f\"\"\"\n",
    "                CREATE OR REPLACE TEMP VIEW _pbp_names AS\n",
    "                WITH p1 AS (\n",
    "                    SELECT player_id_1 AS player_id, ANY_VALUE(NULLIF(last_name_1,'')) AS last_name\n",
    "                    FROM pbp\n",
    "                    WHERE player_id_1 IS NOT NULL {referee_filter.replace('player_id', 'player_id_1')}\n",
    "                    GROUP BY player_id_1\n",
    "                ),\n",
    "                p2 AS (\n",
    "                    SELECT player_id_2 AS player_id, ANY_VALUE(NULLIF(last_name_2,'')) AS last_name\n",
    "                    FROM pbp\n",
    "                    WHERE player_id_2 IS NOT NULL {referee_filter.replace('player_id', 'player_id_2')}\n",
    "                    GROUP BY player_id_2\n",
    "                ),\n",
    "                p3 AS (\n",
    "                    SELECT player_id_3 AS player_id, ANY_VALUE(NULLIF(last_name_3,'')) AS last_name\n",
    "                    FROM pbp\n",
    "                    WHERE player_id_3 IS NOT NULL {referee_filter.replace('player_id', 'player_id_3')}\n",
    "                    GROUP BY player_id_3\n",
    "                ),\n",
    "                unioned AS (\n",
    "                    SELECT * FROM p1\n",
    "                    UNION ALL\n",
    "                    SELECT * FROM p2\n",
    "                    UNION ALL\n",
    "                    SELECT * FROM p3\n",
    "                )\n",
    "                SELECT player_id, ANY_VALUE(last_name) AS last_name\n",
    "                FROM unioned\n",
    "                WHERE last_name IS NOT NULL\n",
    "                GROUP BY player_id\n",
    "            \"\"\")\n",
    "\n",
    "            # --- infer team_id for pbp-only players WITH CONFIDENCE (FILTERED) ---\n",
    "            self.conn.execute(f\"\"\"\n",
    "                CREATE OR REPLACE TEMP VIEW _player_team_guess AS\n",
    "                WITH occ AS (\n",
    "                    SELECT player_id_1 AS player_id, team_id_off AS team_id FROM pbp WHERE player_id_1 IS NOT NULL {referee_filter.replace('player_id', 'player_id_1')}\n",
    "                    UNION ALL SELECT player_id_2, team_id_off FROM pbp WHERE player_id_2 IS NOT NULL {referee_filter.replace('player_id', 'player_id_2')}\n",
    "                    UNION ALL SELECT player_id_3, team_id_off FROM pbp WHERE player_id_3 IS NOT NULL {referee_filter.replace('player_id', 'player_id_3')}\n",
    "                    UNION ALL SELECT player_id_1, team_id_def FROM pbp WHERE player_id_1 IS NOT NULL {referee_filter.replace('player_id', 'player_id_1')}\n",
    "                    UNION ALL SELECT player_id_2, team_id_def FROM pbp WHERE player_id_2 IS NOT NULL {referee_filter.replace('player_id', 'player_id_2')}\n",
    "                    UNION ALL SELECT player_id_3, team_id_def FROM pbp WHERE player_id_3 IS NOT NULL {referee_filter.replace('player_id', 'player_id_3')}\n",
    "                ),\n",
    "                agg AS (\n",
    "                    SELECT player_id, team_id, COUNT(*) AS c\n",
    "                    FROM occ\n",
    "                    GROUP BY player_id, team_id\n",
    "                ),\n",
    "                totals AS (\n",
    "                    SELECT player_id, SUM(c) AS tot\n",
    "                    FROM agg\n",
    "                    GROUP BY player_id\n",
    "                ),\n",
    "                ranked AS (\n",
    "                    SELECT\n",
    "                        a.player_id,\n",
    "                        a.team_id,\n",
    "                        a.c,\n",
    "                        t.tot,\n",
    "                        ROW_NUMBER() OVER (PARTITION BY a.player_id ORDER BY a.c DESC, a.team_id) AS rn\n",
    "                    FROM agg a\n",
    "                    JOIN totals t USING(player_id)\n",
    "                )\n",
    "                SELECT\n",
    "                    player_id,\n",
    "                    team_id,\n",
    "                    c,\n",
    "                    tot,\n",
    "                    (c::DOUBLE)/NULLIF(tot,0) AS confidence\n",
    "                FROM ranked\n",
    "                WHERE rn = 1\n",
    "            \"\"\")\n",
    "\n",
    "            # --- universe of pbp player_ids (FILTERED) ---\n",
    "            self.conn.execute(f\"\"\"\n",
    "                CREATE OR REPLACE TEMP VIEW _pbp_players AS\n",
    "                SELECT player_id FROM (\n",
    "                    SELECT DISTINCT player_id_1 AS player_id FROM pbp WHERE player_id_1 IS NOT NULL {referee_filter.replace('player_id', 'player_id_1')}\n",
    "                    UNION\n",
    "                    SELECT DISTINCT player_id_2 FROM pbp WHERE player_id_2 IS NOT NULL {referee_filter.replace('player_id', 'player_id_2')}\n",
    "                    UNION\n",
    "                    SELECT DISTINCT player_id_3 FROM pbp WHERE player_id_3 IS NOT NULL {referee_filter.replace('player_id', 'player_id_3')}\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "            # Create comprehensive dim_players (with provenance + confidence)\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE TABLE dim_players AS\n",
    "                SELECT\n",
    "                    COALESCE(b.player_id, p.player_id) AS player_id,\n",
    "                    COALESCE(b.player_name, n.last_name, CAST(COALESCE(b.player_id, p.player_id) AS VARCHAR)) AS player_name,\n",
    "                    COALESCE(b.team_id, tg.team_id) AS team_id,\n",
    "                    t.team_abbrev,\n",
    "                    COALESCE(b.is_starter, false) AS is_starter,\n",
    "                    COALESCE(b.seconds_played, 0) AS seconds_played,\n",
    "                    -- provenance\n",
    "                    CASE\n",
    "                        WHEN b.player_name IS NOT NULL THEN 'box'\n",
    "                        WHEN n.last_name IS NOT NULL THEN 'pbp_last_name'\n",
    "                        ELSE 'player_id'\n",
    "                    END AS name_source,\n",
    "                    CASE\n",
    "                        WHEN b.team_id IS NOT NULL THEN 'box'\n",
    "                        WHEN tg.team_id IS NOT NULL THEN 'pbp_team_guess'\n",
    "                        ELSE NULL\n",
    "                    END AS team_source,\n",
    "                    tg.confidence AS team_confidence\n",
    "                FROM _pbp_players p\n",
    "                FULL OUTER JOIN (\n",
    "                    SELECT DISTINCT player_id, player_name, team_id, is_starter, seconds_played\n",
    "                    FROM box_score\n",
    "                ) b ON p.player_id = b.player_id\n",
    "                LEFT JOIN _pbp_names n ON COALESCE(b.player_id, p.player_id) = n.player_id\n",
    "                LEFT JOIN _player_team_guess tg ON COALESCE(b.player_id, p.player_id) = tg.player_id\n",
    "                LEFT JOIN dim_teams t ON COALESCE(b.team_id, tg.team_id) = t.team_id\n",
    "            \"\"\")\n",
    "\n",
    "            # PBP-only players view (unchanged structure, still helpful)\n",
    "            self._robust_drop_object(\"pbp_only_players\")\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE VIEW pbp_only_players AS\n",
    "                WITH box_ids AS (SELECT DISTINCT player_id FROM box_score),\n",
    "                pbp_ids AS (SELECT DISTINCT player_id FROM dim_players),\n",
    "                only_ids AS (\n",
    "                    SELECT p.player_id\n",
    "                    FROM pbp_ids p\n",
    "                    LEFT JOIN box_ids b USING(player_id)\n",
    "                    WHERE b.player_id IS NULL\n",
    "                )\n",
    "                SELECT\n",
    "                    o.player_id,\n",
    "                    dp.player_name,\n",
    "                    dp.team_id,\n",
    "                    dp.team_abbrev,\n",
    "                    ANY_VALUE(CONCAT('Q', pbp.period, ' ', pbp.game_clock, ' | ', pbp.description)) AS sample_event\n",
    "                FROM only_ids o\n",
    "                JOIN dim_players dp USING(player_id)\n",
    "                LEFT JOIN pbp ON (o.player_id = pbp.player_id_1 OR o.player_id = pbp.player_id_2 OR o.player_id = pbp.player_id_3)\n",
    "                GROUP BY o.player_id, dp.player_name, dp.team_id, dp.team_abbrev\n",
    "                ORDER BY player_id\n",
    "            \"\"\")\n",
    "\n",
    "            cnt_players = self.conn.execute(\"SELECT COUNT(*) FROM dim_players\").fetchone()[0]\n",
    "            cnt_only = self.conn.execute(\"SELECT COUNT(*) FROM pbp_only_players\").fetchone()[0]\n",
    "            cnt_officials = self.conn.execute(\"SELECT COUNT(*) FROM dim_officials\").fetchone()[0]\n",
    "\n",
    "            logger.info(f\"✅ Created dim_players: {cnt_players} players\")\n",
    "            logger.info(f\"✅ Created pbp_only_players: {cnt_only} PBP-only players\")\n",
    "            logger.info(f\"✅ Created dim_officials: {cnt_officials} referees/officials\")\n",
    "            if referee_ids:\n",
    "                logger.info(f\"🚫 Filtered out referees: {sorted(referee_ids)}\")\n",
    "\n",
    "            # Ensure every active box player is present with a team\n",
    "            missing_active = self.conn.execute(\"\"\"\n",
    "                WITH active_box AS (SELECT DISTINCT player_id FROM box_score)\n",
    "                SELECT COUNT(*) FROM active_box a\n",
    "                LEFT JOIN dim_players d USING(player_id)\n",
    "                WHERE d.player_id IS NULL OR d.team_id IS NULL\n",
    "            \"\"\").fetchone()[0]\n",
    "            if missing_active > 0:\n",
    "                raise AssertionError(f\"{missing_active} active box players missing in dim_players or missing team_id\")\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Dimensions\",\n",
    "                passed=True,\n",
    "                details=f\"dim_players: {cnt_players} rows; pbp_only_players: {cnt_only} rows; dim_officials: {cnt_officials} rows\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Dimensions\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating dimensions: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def identify_referees_and_officials(self):\n",
    "        \"\"\"\n",
    "        Identify referee/official IDs based on event patterns.\n",
    "\n",
    "        Criteria for referee detection:\n",
    "        1. Appears in PBP events but NOT in active box score\n",
    "        2. Only appears in foul calls (msgType=6) or turnovers (msgType=5) \n",
    "        3. Never appears in shots (msgType=1,2), rebounds (msgType=4), or subs (msgType=8)\n",
    "\n",
    "        Returns:\n",
    "            set: Set of player IDs identified as referees/officials\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all player IDs that appear in PBP\n",
    "        pbp_players = self.conn.execute(\"\"\"\n",
    "            SELECT DISTINCT player_id FROM (\n",
    "                SELECT player_id_1 AS player_id FROM pbp WHERE player_id_1 IS NOT NULL\n",
    "                UNION\n",
    "                SELECT player_id_2 FROM pbp WHERE player_id_2 IS NOT NULL  \n",
    "                UNION\n",
    "                SELECT player_id_3 FROM pbp WHERE player_id_3 IS NOT NULL\n",
    "            )\n",
    "        \"\"\").df()['player_id'].tolist()\n",
    "\n",
    "        # Get active players from box score\n",
    "        box_players = self.conn.execute(\"\"\"\n",
    "            SELECT DISTINCT player_id FROM box_score WHERE status = 'ACTIVE'\n",
    "        \"\"\").df()['player_id'].tolist()\n",
    "\n",
    "        # Find players in PBP but not in box score (potential referees)\n",
    "        potential_refs = set(pbp_players) - set(box_players)\n",
    "\n",
    "        if not potential_refs:\n",
    "            return set()\n",
    "\n",
    "        logger.info(f\"🔍 Analyzing {len(potential_refs)} potential referee/official IDs...\")\n",
    "\n",
    "        confirmed_refs = set()\n",
    "\n",
    "        for player_id in potential_refs:\n",
    "            # Analyze event patterns for this player\n",
    "            events = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    msg_type,\n",
    "                    description,\n",
    "                    last_name_1,\n",
    "                    last_name_2, \n",
    "                    last_name_3\n",
    "                FROM pbp \n",
    "                WHERE player_id_1 = {player_id} \n",
    "                   OR player_id_2 = {player_id}\n",
    "                   OR player_id_3 = {player_id}\n",
    "            \"\"\").df()\n",
    "\n",
    "            if len(events) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get player name from events\n",
    "            names = set()\n",
    "            for _, row in events.iterrows():\n",
    "                for col in ['last_name_1', 'last_name_2', 'last_name_3']:\n",
    "                    if pd.notna(row[col]):\n",
    "                        names.add(row[col])\n",
    "\n",
    "            name = list(names)[0] if names else f\"ID_{player_id}\"\n",
    "\n",
    "            # Analyze event type patterns\n",
    "            msg_types = events['msg_type'].value_counts().to_dict()\n",
    "\n",
    "            # Referee criteria:\n",
    "            # 1. Only appears in fouls (6) or turnovers (5) or technical fouls (16,17,18)\n",
    "            # 2. Never in shots (1,2), rebounds (4), substitutions (8)\n",
    "\n",
    "            referee_event_types = {5, 6, 7, 16, 17, 18}  # turnovers, fouls, technicals\n",
    "            player_event_types = {1, 2, 4, 8}  # shots, rebounds, substitutions\n",
    "\n",
    "            has_referee_events = any(msg_type in referee_event_types for msg_type in msg_types.keys())\n",
    "            has_player_events = any(msg_type in player_event_types for msg_type in msg_types.keys())\n",
    "\n",
    "            if has_referee_events and not has_player_events:\n",
    "                confirmed_refs.add(player_id)\n",
    "                logger.info(f\"  ✅ {name} (ID: {player_id}) - REFEREE/OFFICIAL\")\n",
    "                logger.info(f\"      Events: {dict(msg_types)}\")\n",
    "\n",
    "        logger.info(f\"🎯 Identified {len(confirmed_refs)} confirmed referees/officials\")\n",
    "        return confirmed_refs\n",
    "\n",
    "    def create_officials_table(self, referee_ids):\n",
    "        \"\"\"Create a separate table for referees/officials for transparency\"\"\"\n",
    "\n",
    "        if not referee_ids:\n",
    "            # Create empty table\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE TABLE dim_officials AS\n",
    "                SELECT \n",
    "                    CAST(NULL AS INTEGER) AS official_id,\n",
    "                    CAST(NULL AS VARCHAR) AS official_name,\n",
    "                    CAST(NULL AS INTEGER) AS total_events,\n",
    "                    CAST(NULL AS VARCHAR) AS event_types,\n",
    "                    CAST(NULL AS VARCHAR) AS sample_description\n",
    "                WHERE FALSE\n",
    "            \"\"\")\n",
    "            return\n",
    "\n",
    "        # Build officials data\n",
    "        officials_data = []\n",
    "\n",
    "        for official_id in referee_ids:\n",
    "            # Get event details for this official\n",
    "            events = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    msg_type,\n",
    "                    description,\n",
    "                    last_name_1,\n",
    "                    last_name_2,\n",
    "                    last_name_3\n",
    "                FROM pbp \n",
    "                WHERE player_id_1 = {official_id}\n",
    "                   OR player_id_2 = {official_id}\n",
    "                   OR player_id_3 = {official_id}\n",
    "            \"\"\").df()\n",
    "\n",
    "            # Extract name\n",
    "            names = set()\n",
    "            for _, row in events.iterrows():\n",
    "                for col in ['last_name_1', 'last_name_2', 'last_name_3']:\n",
    "                    if pd.notna(row[col]):\n",
    "                        names.add(row[col])\n",
    "\n",
    "            official_name = list(names)[0] if names else f\"Official_{official_id}\"\n",
    "\n",
    "            # Event type summary\n",
    "            msg_types = events['msg_type'].value_counts().to_dict()\n",
    "            event_types_str = ', '.join([f\"msgType{k}:{v}\" for k, v in sorted(msg_types.items())])\n",
    "\n",
    "            # Sample description\n",
    "            sample_desc = events.iloc[0]['description'] if len(events) > 0 else \"No events\"\n",
    "\n",
    "            officials_data.append({\n",
    "                'official_id': official_id,\n",
    "                'official_name': official_name,\n",
    "                'total_events': len(events),\n",
    "                'event_types': event_types_str,\n",
    "                'sample_description': sample_desc\n",
    "            })\n",
    "\n",
    "        # Insert data\n",
    "        if officials_data:\n",
    "            officials_df = pd.DataFrame(officials_data)\n",
    "\n",
    "            # Create table\n",
    "            self.conn.execute(\"DROP TABLE IF EXISTS dim_officials\")\n",
    "            self.conn.register('officials_temp', officials_df)\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE TABLE dim_officials AS\n",
    "                SELECT * FROM officials_temp\n",
    "            \"\"\")\n",
    "            self.conn.unregister('officials_temp')\n",
    "\n",
    "            logger.info(f\"📋 Created dim_officials table with {len(officials_data)} officials\")\n",
    "\n",
    "    def create_pbp_enriched_view(self) -> ValidationResult:\n",
    "        \"\"\"Create enriched PBP view with robust object handling\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Check required tables exist\n",
    "            required = [\"pbp\", \"pbp_event_msg_types\", \"pbp_action_types\", \"pbp_option_types\", \"dim_teams\", \"dim_players\"]\n",
    "            for t in required:\n",
    "                exists = self.conn.execute(\n",
    "                    f\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '{t}'\"\n",
    "                ).fetchone()[0]\n",
    "                if exists == 0:\n",
    "                    return ValidationResult(\n",
    "                        step_name=\"Create PBP Enriched View\",\n",
    "                        passed=False,\n",
    "                        details=f\"Missing required table: {t}\",\n",
    "                        processing_time=time.time() - start_time\n",
    "                    )\n",
    "\n",
    "            # Robust cleanup\n",
    "            self._robust_drop_object(\"pbp_enriched\")\n",
    "\n",
    "            # Create enriched view with proper deduplication\n",
    "            self.conn.execute(\"\"\"\n",
    "            CREATE VIEW pbp_enriched AS\n",
    "            WITH team_map AS (\n",
    "                SELECT DISTINCT team_id, team_abbrev FROM dim_teams\n",
    "            ),\n",
    "            event_types AS (\n",
    "                SELECT DISTINCT EventType, Description FROM pbp_event_msg_types\n",
    "            ),\n",
    "            action_types AS (\n",
    "                SELECT DISTINCT EventType, ActionType, Event, Description \n",
    "                FROM pbp_action_types\n",
    "            ),\n",
    "            option_types AS (\n",
    "                SELECT EventType, \n",
    "                       ANY_VALUE(Option1) AS Option1,\n",
    "                       ANY_VALUE(Option2) AS Option2,\n",
    "                       ANY_VALUE(Option3) AS Option3,\n",
    "                       ANY_VALUE(Option4) AS Option4\n",
    "                FROM pbp_option_types\n",
    "                GROUP BY EventType\n",
    "            )\n",
    "            SELECT\n",
    "                p.*,\n",
    "                emt.Description AS event_family,\n",
    "                act.Event AS action_event,\n",
    "                act.Description AS action_desc,\n",
    "                toff.team_abbrev AS team_off_abbrev,\n",
    "                tdef.team_abbrev AS team_def_abbrev,\n",
    "                COALESCE(p1.player_name, NULLIF(p.last_name_1, '')) AS player1_name,\n",
    "                COALESCE(p2.player_name, NULLIF(p.last_name_2, '')) AS player2_name,\n",
    "                COALESCE(p3.player_name, NULLIF(p.last_name_3, '')) AS player3_name,\n",
    "                opt.Option1 AS option1_label,\n",
    "                opt.Option2 AS option2_label,\n",
    "                opt.Option3 AS option3_label,\n",
    "                opt.Option4 AS option4_label\n",
    "            FROM pbp p\n",
    "            LEFT JOIN event_types emt ON p.msg_type = emt.EventType\n",
    "            LEFT JOIN action_types act ON p.msg_type = act.EventType AND p.action_type = act.ActionType\n",
    "            LEFT JOIN option_types opt ON p.msg_type = opt.EventType\n",
    "            LEFT JOIN team_map toff ON p.team_id_off = toff.team_id\n",
    "            LEFT JOIN team_map tdef ON p.team_id_def = tdef.team_id\n",
    "            LEFT JOIN dim_players p1 ON p.player_id_1 = p1.player_id\n",
    "            LEFT JOIN dim_players p2 ON p.player_id_2 = p2.player_id\n",
    "            LEFT JOIN dim_players p3 ON p.player_id_3 = p3.player_id\n",
    "            ORDER BY p.period, p.pbp_order, p.wall_clock_int\n",
    "            \"\"\")\n",
    "\n",
    "            # Validate row count matches\n",
    "            n_pbp = self.conn.execute(\"SELECT COUNT(*) FROM pbp\").fetchone()[0]\n",
    "            n_enriched = self.conn.execute(\"SELECT COUNT(*) FROM pbp_enriched\").fetchone()[0]\n",
    "\n",
    "            if n_pbp != n_enriched:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Create PBP Enriched View\",\n",
    "                    passed=False,\n",
    "                    details=f\"Row count mismatch: pbp={n_pbp} vs enriched={n_enriched}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create PBP Enriched View\",\n",
    "                passed=True,\n",
    "                details=f\"Created view pbp_enriched with {n_enriched} rows (matches pbp)\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create PBP Enriched View\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating pbp_enriched view: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def print_enhanced_summary(self):\n",
    "        \"\"\"Print enhanced data loading summary (ASCII-only).\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ENHANCED NBA PIPELINE - DATA LOADING SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        if 'box_score' in self.data_summary:\n",
    "            box_data = self.data_summary['box_score']\n",
    "            print(\"BOX SCORE:\")\n",
    "            print(f\"   Original rows: {box_data['original_rows']:,}\")\n",
    "            print(f\"   Active players: {box_data['active_rows']:,}\")\n",
    "            print(f\"   Final rows: {box_data['final_rows']:,}\")\n",
    "            teams_str = \", \".join(box_data['teams'])\n",
    "            print(f\"   Teams: {teams_str}\")\n",
    "            print(f\"   Starters per team: {box_data['starters_per_team']}\")\n",
    "\n",
    "        if 'pbp' in self.data_summary:\n",
    "            pbp_data = self.data_summary['pbp']\n",
    "            coord_data = pbp_data['coordinate_analysis']\n",
    "            print(\"\\nPLAY-BY-PLAY:\")\n",
    "            print(f\"   Original rows: {pbp_data['original_rows']:,}\")\n",
    "            print(f\"   Game events: {pbp_data['game_events']:,}\")\n",
    "            print(f\"   Final rows: {pbp_data['final_rows']:,}\")\n",
    "            print(f\"   Total shots: {coord_data['total_shots']:,}\")\n",
    "            print(f\"   Shots with coordinates: {coord_data['shots_with_coords']:,}\")\n",
    "            print(f\"   Rim attempts: {coord_data['rim_attempts']:,}\")\n",
    "            print(f\"   Average distance: {coord_data['avg_distance']:.1f} ft\")\n",
    "\n",
    "        if 'enhanced_substitution_debug' in self.data_summary:\n",
    "            d = self.data_summary['enhanced_substitution_debug']\n",
    "            print(\"\\nLINEUP ENGINE:\")\n",
    "            print(f\"   Substitutions: {d.get('substitutions', 0)}\")\n",
    "            print(f\"   First-actions auto-IN: {d.get('first_actions', 0)}\")\n",
    "            print(f\"   Inactivity auto-OUTs: {d.get('auto_outs', 0)}\")\n",
    "            print(f\"   5-on-floor fixes: {d.get('always_five_fixes', 0)}\")\n",
    "            v = d.get('validation', {})\n",
    "            print(f\"   Minutes tolerance: ±{v.get('tolerance', 0)}s\")\n",
    "            print(f\"   Minutes offenders: {v.get('offenders', 0)}/{v.get('total_players', 0)}\")\n",
    "\n",
    "        if 'lineup_results' in self.data_summary:\n",
    "            print(\"\\nANALYTICS RESULTS:\")\n",
    "            print(f\"   Lineup combinations: {self.data_summary['lineup_results']['rows']:,}\")\n",
    "            print(f\"   Player rim stats: {self.data_summary['player_rim_results']['rows']:,}\")\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "\n",
    "    def write_final_report(self, reports_dir: Optional[Path] = None) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Emit an end-of-run report:\n",
    "        - minutes_validation_full / minutes_offenders (enhanced pass)\n",
    "        - basic_lineup_state.csv / basic_lineup_flags.csv / minutes_basic.csv\n",
    "        - enhanced_lineup_state.csv / enhanced_lineup_flags.csv / minutes_enhanced.csv\n",
    "        - minutes_compare.csv\n",
    "        - traditional_vs_enhanced_comparison.csv\n",
    "        - comprehensive_flags_analysis.csv (when available)\n",
    "        - run_summary.json (JSON-safe)\n",
    "        - UPDATED:\n",
    "            * unique_lineups_traditional_5.csv        (5-man only)\n",
    "            * unique_lineups_traditional_all.csv      (all sizes)\n",
    "            * unique_lineups_enhanced_5.csv           (5-man only)\n",
    "            * Logs show traditional 5-man AND all-sizes\n",
    "        \"\"\"\n",
    "        import time, json as _json\n",
    "        from pathlib import Path\n",
    "        import pandas as pd\n",
    "\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            mv_df = self.data_summary.get('minutes_validation_full')\n",
    "            offenders = self.data_summary.get('minutes_offenders')\n",
    "            debug = self.data_summary.get('enhanced_substitution_debug', {})\n",
    "\n",
    "            if reports_dir is None:\n",
    "                reports_dir = Path(\"reports\")\n",
    "            reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Persist enhanced minutes into DuckDB for reproducibility\n",
    "            try:\n",
    "                self._robust_drop_object(\"minutes_validation_full\")\n",
    "                self._robust_drop_object(\"minutes_offenders\")\n",
    "                if mv_df is not None and len(mv_df) > 0:\n",
    "                    self.conn.register(\"mv_temp\", mv_df)\n",
    "                    self.conn.execute(\"CREATE TABLE minutes_validation_full AS SELECT * FROM mv_temp\")\n",
    "                    self.conn.execute(\"DROP VIEW IF EXISTS mv_temp\")\n",
    "                else:\n",
    "                    self.conn.execute(\"CREATE TABLE minutes_validation_full AS SELECT 1 WHERE FALSE\")\n",
    "                if offenders is not None and len(offenders) > 0:\n",
    "                    self.conn.register(\"off_temp\", offenders)\n",
    "                    self.conn.execute(\"CREATE TABLE minutes_offenders AS SELECT * FROM off_temp\")\n",
    "                    self.conn.execute(\"DROP VIEW IF EXISTS off_temp\")\n",
    "                else:\n",
    "                    self.conn.execute(\"CREATE TABLE minutes_offenders AS SELECT 1 WHERE FALSE\")\n",
    "            except Exception as e:\n",
    "                logger.warning(\"[Report] Could not create DuckDB tables: %s\", e)\n",
    "\n",
    "            # CSVs for enhanced minutes tables\n",
    "            if mv_df is not None:\n",
    "                mv_df.to_csv(reports_dir / \"minutes_validation_full.csv\", index=False)\n",
    "            if offenders is not None:\n",
    "                offenders.to_csv(reports_dir / \"minutes_offenders.csv\", index=False)\n",
    "\n",
    "            # Export basic / enhanced state snapshots if present\n",
    "            try:\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='basic_lineup_state'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM basic_lineup_state ORDER BY period, pbp_order, team_id) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"basic_lineup_state.csv\").as_posix())))\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='basic_lineup_flags'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM basic_lineup_flags ORDER BY abs_time, team_id) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"basic_lineup_flags.csv\").as_posix())))\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='minutes_basic'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM minutes_basic ORDER BY team_abbrev, player_name) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"minutes_basic.csv\").as_posix())))\n",
    "\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='enhanced_lineup_state'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM enhanced_lineup_state ORDER BY period, pbp_order, team_id) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"enhanced_lineup_state.csv\").as_posix())))\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='enhanced_lineup_flags'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM enhanced_lineup_flags ORDER BY abs_time, team_id) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"enhanced_lineup_flags.csv\").as_posix())))\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='minutes_enhanced'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM minutes_enhanced ORDER BY team_abbrev, player_name) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"minutes_enhanced.csv\").as_posix())))\n",
    "            except Exception as e:\n",
    "                logger.warning(\"[Report] Could not export basic/enhanced CSVs: %s\", e)\n",
    "\n",
    "            # Comparison exports (minutes and flags)\n",
    "            try:\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='minutes_compare'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM minutes_compare ORDER BY team_abbrev, player_name) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"minutes_compare.csv\").as_posix())))\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='traditional_vs_enhanced_comparison'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM traditional_vs_enhanced_comparison ORDER BY team_abbrev, player_name) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"traditional_vs_enhanced_comparison.csv\").as_posix())))\n",
    "                if self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='comprehensive_flags_analysis'\").fetchone()[0]:\n",
    "                    self.conn.execute(\"COPY (SELECT * FROM comprehensive_flags_analysis ORDER BY time, team) TO '{}' (HEADER, DELIMITER ',')\"\n",
    "                                    .format(str((reports_dir / \"comprehensive_flags_analysis.csv\").as_posix())))\n",
    "            except Exception as e:\n",
    "                logger.warning(\"[Report] Could not export comparison/flags CSVs: %s\", e)\n",
    "\n",
    "            # -----------------------------\n",
    "            # UPDATED: UNIQUE LINEUPS COMPUTE\n",
    "            # -----------------------------\n",
    "            def _table_exists(name: str) -> bool:\n",
    "                return bool(self.conn.execute(\n",
    "                    \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = ?\", [name]\n",
    "                ).fetchone()[0])\n",
    "\n",
    "            # Name map for pretty printing\n",
    "            try:\n",
    "                if _table_exists(\"dim_players\"):\n",
    "                    nm_df = self.conn.execute(\"SELECT DISTINCT player_id, player_name FROM dim_players\").df()\n",
    "                else:\n",
    "                    nm_df = self.conn.execute(\"SELECT DISTINCT player_id, player_name FROM box_score\").df()\n",
    "            except Exception:\n",
    "                nm_df = pd.DataFrame(columns=[\"player_id\",\"player_name\"])\n",
    "            name_map = dict(zip(nm_df.get(\"player_id\", []), nm_df.get(\"player_name\", [])))\n",
    "\n",
    "            def _ids_to_names(ids):\n",
    "                return [str(name_map.get(int(pid), str(int(pid)))) for pid in ids]\n",
    "\n",
    "            def _parse_ids_json(s) -> list:\n",
    "                # Robust JSON or bracketed string parsing\n",
    "                try:\n",
    "                    v = _json.loads(s)\n",
    "                    if isinstance(v, list):\n",
    "                        return [int(x) for x in v]\n",
    "                except Exception:\n",
    "                    pass\n",
    "                # fallback \"1,2,3\" or \"[1, 2, 3]\"\n",
    "                s2 = str(s).strip().strip(\"[]\")\n",
    "                if not s2:\n",
    "                    return []\n",
    "                return [int(x.strip()) for x in s2.split(\",\") if x.strip()]\n",
    "\n",
    "            def _compute_unique_lineups(table_name: str, label: str, size_filter: Optional[int]) -> tuple[pd.DataFrame, dict]:\n",
    "                \"\"\"\n",
    "                Returns df with columns:\n",
    "                [method, team_id, team_abbrev, lineup_size, occurrences,\n",
    "                lineup_player_ids_json, lineup_player_names_json]\n",
    "                'counts' includes: total_unique, by_team, and by_size.\n",
    "                \"\"\"\n",
    "                cols = [\"method\",\"team_id\",\"team_abbrev\",\"lineup_size\",\"occurrences\",\n",
    "                        \"lineup_player_ids_json\",\"lineup_player_names_json\"]\n",
    "                if not _table_exists(table_name):\n",
    "                    return pd.DataFrame(columns=cols), {\"total_unique\": 0, \"by_team\": {}, \"by_size\": {}}\n",
    "\n",
    "                df = self.conn.execute(f\"\"\"\n",
    "                    SELECT team_id, team_abbrev, lineup_player_ids_json, lineup_size\n",
    "                    FROM {table_name}\n",
    "                    WHERE lineup_player_ids_json IS NOT NULL\n",
    "                \"\"\").df()\n",
    "\n",
    "                if df.empty:\n",
    "                    return pd.DataFrame(columns=cols), {\"total_unique\": 0, \"by_team\": {}, \"by_size\": {}}\n",
    "\n",
    "                if size_filter is not None:\n",
    "                    df = df[df[\"lineup_size\"] == size_filter].copy()\n",
    "\n",
    "                # Group by team, size, and the player-ids JSON\n",
    "                grp = df.groupby([\"team_id\",\"team_abbrev\",\"lineup_size\",\"lineup_player_ids_json\"], as_index=False)\\\n",
    "                        .size().rename(columns={\"size\":\"occurrences\"})\n",
    "\n",
    "                # Convert ids json to names json (canonical sort for stability)\n",
    "                names_json = []\n",
    "                for s in grp[\"lineup_player_ids_json\"]:\n",
    "                    ids = sorted(_parse_ids_json(s))\n",
    "                    names = _ids_to_names(ids)\n",
    "                    names_json.append(_json.dumps(names))\n",
    "                grp[\"lineup_player_names_json\"] = names_json\n",
    "                grp.insert(0, \"method\", label)\n",
    "\n",
    "                # Counts (overall unique, by team, by size)\n",
    "                total_unique = int(grp[[\"team_id\",\"lineup_player_ids_json\"]].drop_duplicates().shape[0])\n",
    "                by_team = grp.groupby(\"team_abbrev\")[\"lineup_player_ids_json\"].nunique().to_dict()\n",
    "                by_size = grp.groupby(\"lineup_size\")[\"lineup_player_ids_json\"].nunique().to_dict()\n",
    "\n",
    "                grp = grp.sort_values([\"team_abbrev\",\"lineup_size\",\"occurrences\"], ascending=[True, True, False]).reset_index(drop=True)\n",
    "                counts = {\"total_unique\": total_unique,\n",
    "                        \"by_team\": {str(k): int(v) for k, v in by_team.items()},\n",
    "                        \"by_size\": {int(k): int(v) for k, v in by_size.items()}}\n",
    "                return grp, counts\n",
    "\n",
    "            # Compute: TRADITIONAL (5-man), TRADITIONAL (all sizes), ENHANCED (5-man)\n",
    "            trad_5_df,   trad_5_counts   = _compute_unique_lineups(\"traditional_lineup_state\", \"traditional\", size_filter=5)\n",
    "            trad_all_df, trad_all_counts = _compute_unique_lineups(\"traditional_lineup_state\", \"traditional\", size_filter=None)\n",
    "            enh_5_df,    enh_5_counts    = _compute_unique_lineups(\"enhanced_lineup_state\",    \"enhanced\",   size_filter=5)\n",
    "\n",
    "            # Write CSVs\n",
    "            if not trad_5_df.empty:\n",
    "                trad_5_df.to_csv(reports_dir / \"unique_lineups_traditional_5.csv\", index=False)\n",
    "            if not trad_all_df.empty:\n",
    "                trad_all_df.to_csv(reports_dir / \"unique_lineups_traditional_all.csv\", index=False)\n",
    "            if not enh_5_df.empty:\n",
    "                enh_5_df.to_csv(reports_dir / \"unique_lineups_enhanced_5.csv\", index=False)\n",
    "\n",
    "            # Helper: ASCII logging of unique lineups\n",
    "            def _log_unique_list(df: pd.DataFrame, title: str):\n",
    "                if df.empty:\n",
    "                    logger.info(\"[%s] No unique lineups found.\", title)\n",
    "                    return\n",
    "                logger.info(\"=\" * 78)\n",
    "                logger.info(\"UNIQUE LINEUPS — %s\", title)\n",
    "                logger.info(\"=\" * 78)\n",
    "                for team_abbrev, sub in df.groupby(\"team_abbrev\"):\n",
    "                    # Include lineup_size in the heading when mixed\n",
    "                    sizes = sorted(sub[\"lineup_size\"].unique().tolist())\n",
    "                    size_tag = \"\" if sizes == [5] else f\" (sizes: {sizes})\"\n",
    "                    logger.info(\"%s: %d unique lineups%s\", team_abbrev, len(sub), size_tag)\n",
    "                    sub = sub.sort_values([\"lineup_size\",\"occurrences\"], ascending=[True, False]).reset_index(drop=True)\n",
    "                    for i, row in sub.iterrows():\n",
    "                        try:\n",
    "                            names = _json.loads(row[\"lineup_player_names_json\"])\n",
    "                        except Exception:\n",
    "                            names = row[\"lineup_player_names_json\"]\n",
    "                        names_str = \", \".join(names) if isinstance(names, list) else str(names)\n",
    "                        logger.info(\"  %2d. [%d] size=%d  %s\", i+1, int(row[\"occurrences\"]), int(row[\"lineup_size\"]), names_str)\n",
    "                logger.info(\"-\" * 78)\n",
    "\n",
    "            # Log both traditional views and enhanced 5-man\n",
    "            _log_unique_list(trad_5_df,   \"TRADITIONAL (5-man)\")\n",
    "            _log_unique_list(trad_all_df, \"TRADITIONAL (ALL sizes)\")\n",
    "            _log_unique_list(enh_5_df,    \"ENHANCED (5-man)\")\n",
    "\n",
    "            # Build JSON summary (JSON-safe)\n",
    "            comparison_data = self.data_summary.get('traditional_vs_enhanced_comparison', {})\n",
    "            comparison_summary = comparison_data.get('summary', {})\n",
    "\n",
    "            # Traditional/enhanced state counts for summary\n",
    "            traditional_states = int(self.data_summary.get(\"traditional_data_driven\", {}).get(\"state_rows\", 0))\n",
    "            enhanced_states    = int(self.data_summary.get(\"enhanced_substitution_tracking\", {}).get(\"state_rows\", 0))\n",
    "\n",
    "            # Unique counts via SQL (secondary check)\n",
    "            def _unique_count(table_name: str, size_filter: Optional[int]) -> int:\n",
    "                try:\n",
    "                    exists = self.conn.execute(\n",
    "                        \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = ?\",\n",
    "                        [table_name]\n",
    "                    ).fetchone()[0]\n",
    "                    if not exists:\n",
    "                        return 0\n",
    "                    if size_filter is None:\n",
    "                        q = f\"\"\"\n",
    "                            SELECT COUNT(*) FROM (\n",
    "                                SELECT team_id, lineup_player_ids_json\n",
    "                                FROM {table_name}\n",
    "                                GROUP BY team_id, lineup_player_ids_json\n",
    "                            )\n",
    "                        \"\"\"\n",
    "                    else:\n",
    "                        q = f\"\"\"\n",
    "                            SELECT COUNT(*) FROM (\n",
    "                                SELECT team_id, lineup_player_ids_json\n",
    "                                FROM {table_name}\n",
    "                                WHERE lineup_size = {int(size_filter)}\n",
    "                                GROUP BY team_id, lineup_player_ids_json\n",
    "                            )\n",
    "                        \"\"\"\n",
    "                    return int(self.conn.execute(q).fetchone()[0])\n",
    "                except Exception:\n",
    "                    return 0\n",
    "\n",
    "            traditional_unique_5   = _unique_count(\"traditional_lineup_state\", 5)\n",
    "            traditional_unique_all = _unique_count(\"traditional_lineup_state\", None)\n",
    "            enhanced_unique_5      = _unique_count(\"enhanced_lineup_state\", 5)\n",
    "\n",
    "            # Build summary safely\n",
    "            summary = {\n",
    "                \"substitutions\": int(debug.get(\"substitutions\", 0)),\n",
    "                \"first_actions\": int(debug.get(\"first_actions\", 0)),\n",
    "                \"auto_outs\": int(debug.get(\"auto_outs\", 0)),\n",
    "                \"always_five_fixes\": int(debug.get(\"always_five_fixes\", 0)),\n",
    "                \"total_players\": int(debug.get(\"validation\", {}).get(\"total_players\", 0)),\n",
    "                \"offenders\": int(debug.get(\"validation\", {}).get(\"offenders\", 0)),\n",
    "                \"tolerance_seconds\": int(debug.get(\"validation\", {}).get(\"tolerance\", 120)),\n",
    "                \"traditional\": {\n",
    "                    \"lineup_states\": traditional_states,\n",
    "                    \"unique_lineups_5\": traditional_unique_5,\n",
    "                    \"unique_lineups_all\": traditional_unique_all,\n",
    "                    \"flag_types\": self._to_native(self.data_summary.get(\"traditional_data_driven\", {}).get(\"flag_summary\", {})),\n",
    "                    \"by_team_5\": self._to_native(trad_5_counts.get(\"by_team\", {})),\n",
    "                    \"by_team_all\": self._to_native(trad_all_counts.get(\"by_team\", {})),\n",
    "                    \"by_size_all\": self._to_native(trad_all_counts.get(\"by_size\", {}))\n",
    "                },\n",
    "                \"enhanced\": {\n",
    "                    \"lineup_states\": enhanced_states,\n",
    "                    \"unique_lineups_5\": enhanced_unique_5,\n",
    "                    \"flag_totals\": self._to_native(self.data_summary.get(\"enhanced_substitution_tracking\", {}).get(\"flag_totals\", {})),\n",
    "                    \"by_team_5\": self._to_native(enh_5_counts.get(\"by_team\", {}))\n",
    "                },\n",
    "                \"minutes_compare_rows\": int(self.data_summary.get(\"minutes_compare\", {}).get(\"rows\", 0)),\n",
    "                \"minutes_compare_basic_within10\": int(self.data_summary.get(\"minutes_compare\", {}).get(\"within10_basic\", 0)),\n",
    "                \"minutes_compare_total_with_box\": int(self.data_summary.get(\"minutes_compare\", {}).get(\"total_with_box\", 0)),\n",
    "                \"traditional_vs_enhanced\": self._to_native(comparison_summary.get(\"method_comparison\", {})),\n",
    "                \"enhanced_flags_summary\": self._to_native(comparison_summary.get(\"flag_analysis\", {})),\n",
    "                \"accuracy_metrics\": self._to_native(comparison_summary.get(\"accuracy_metrics\", {}))\n",
    "            }\n",
    "\n",
    "            with open(reports_dir / \"run_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                import json as __json\n",
    "                __json.dump(self._to_native(summary), f, indent=2)\n",
    "\n",
    "            details = (\n",
    "                \"Report written: minutes_validation_full.csv, minutes_offenders.csv, \"\n",
    "                \"basic_lineup_state.csv, basic_lineup_flags.csv, minutes_basic.csv, \"\n",
    "                \"enhanced_lineup_state.csv, enhanced_lineup_flags.csv, minutes_enhanced.csv, \"\n",
    "                \"minutes_compare.csv, traditional_vs_enhanced_comparison.csv, comprehensive_flags_analysis.csv, \"\n",
    "                \"unique_lineups_traditional_5.csv, unique_lineups_traditional_all.csv, unique_lineups_enhanced_5.csv, \"\n",
    "                \"run_summary.json\"\n",
    "            )\n",
    "            return ValidationResult(\"Write Final Report\", True, details, processing_time=time.time()-start_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\"Write Final Report\", False, f\"Error writing report: {e}\", processing_time=time.time()-start_time)\n",
    "\n",
    "\n",
    "\n",
    "    def run_traditional_data_driven_lineups(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        TRADITIONAL DATA-DRIVEN SUBSTITUTION TRACKING (Updated Implementation):\n",
    "\n",
    "        This method strictly follows the raw data without any automation or inference:\n",
    "        - msgType=8: playerId1 = player subbed IN, playerId2 = player subbed OUT\n",
    "        - Lineups can have any size (not forced to 5)\n",
    "        - Comprehensive flagging for lineup size deviations and substitution issues\n",
    "        - Detailed explanations for why lineups aren't size 5\n",
    "\n",
    "        Key Changes from Original:\n",
    "        1. Removed automatic lineup size enforcement\n",
    "        2. Added detailed flagging for substitution anomalies\n",
    "        3. Enhanced validation of player states\n",
    "        4. Better tracking of player availability vs. actual lineup membership\n",
    "\n",
    "        Outputs written to DuckDB tables:\n",
    "            * traditional_lineup_state\n",
    "            * traditional_lineup_flags\n",
    "            * minutes_traditional\n",
    "        \"\"\"\n",
    "        from collections import defaultdict, deque\n",
    "        import json\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # ---- Configuration ----\n",
    "            CFG = {\n",
    "                \"starter_reset_periods\": [1, 3],\n",
    "                \"sub_msg_type\": 8,\n",
    "                \"action_msg_types\": {1, 2, 4, 5, 6},  # FG made/miss, rebound, turnover, foul\n",
    "                \"allow_variable_lineup_sizes\": True,  # NEW: Allow non-5 player lineups\n",
    "                \"detailed_flagging\": True  # NEW: Enhanced flagging system\n",
    "            }\n",
    "\n",
    "            # ---- Helper Functions ----\n",
    "            def _period_len(p: int) -> float:\n",
    "                return 720.0 if p <= 4 else 300.0\n",
    "\n",
    "            def _parse_gc(gc: str | None) -> float | None:\n",
    "                if not gc or not isinstance(gc, str) or \":\" not in gc:\n",
    "                    return None\n",
    "                try:\n",
    "                    mm, ss = gc.split(\":\")\n",
    "                    return float(mm) * 60.0 + float(ss)\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "            def _abs_t(period: int, rem: float | None) -> float:\n",
    "                total = 0.0\n",
    "                for pi in range(1, period):\n",
    "                    total += _period_len(pi)\n",
    "                pl = _period_len(period)\n",
    "                if rem is None:\n",
    "                    return total + pl\n",
    "                return total + (pl - rem)\n",
    "\n",
    "            # ---- Load Data ----\n",
    "            box_df = self.conn.execute(\"\"\"\n",
    "                SELECT player_id, player_name, team_id, team_abbrev, is_starter, seconds_played\n",
    "                FROM box_score\n",
    "                WHERE seconds_played > 0\n",
    "                ORDER BY team_id, seconds_played DESC\n",
    "            \"\"\").df()\n",
    "\n",
    "            if box_df.empty:\n",
    "                return ValidationResult(\"Traditional Data-Driven Lineups\", False, \n",
    "                                      \"No active players in box_score\", processing_time=time.time()-start_time)\n",
    "\n",
    "            teams = sorted(box_df.team_id.unique().tolist())\n",
    "            if len(teams) != 2:\n",
    "                return ValidationResult(\"Traditional Data-Driven Lineups\", False, \n",
    "                                      f\"Expected 2 teams, found {teams}\", processing_time=time.time()-start_time)\n",
    "\n",
    "            # Build player mappings\n",
    "            team_abbrev = {int(t): box_df[box_df.team_id == t].team_abbrev.iloc[0] for t in teams}\n",
    "            starters = {int(t): set(box_df[(box_df.team_id==t)&(box_df.is_starter==True)].player_id.tolist()) for t in teams}\n",
    "            name_map = dict(zip(box_df.player_id, box_df.player_name))\n",
    "            pteam_map = dict(zip(box_df.player_id, box_df.team_id))\n",
    "\n",
    "            # Validate starters\n",
    "            for t in teams:\n",
    "                if len(starters[int(t)]) != 5:\n",
    "                    return ValidationResult(\"Traditional Data-Driven Lineups\", False, \n",
    "                                          f\"Team {team_abbrev[int(t)]} does not have 5 starters\", \n",
    "                                          processing_time=time.time()-start_time)\n",
    "\n",
    "            # Load events\n",
    "            events = self.conn.execute(\"\"\"\n",
    "                SELECT period, pbp_order, wall_clock_int,\n",
    "                       COALESCE(game_clock,'') AS game_clock,\n",
    "                       COALESCE(description,'') AS description,\n",
    "                       team_id_off, team_id_def, msg_type, action_type,\n",
    "                       player_id_1, player_id_2, player_id_3,\n",
    "                       NULLIF(last_name_1,'') AS last_name_1,\n",
    "                       NULLIF(last_name_2,'') AS last_name_2,\n",
    "                       NULLIF(last_name_3,'') AS last_name_3,\n",
    "                       COALESCE(points,0) AS points\n",
    "                FROM pbp\n",
    "                ORDER BY period, pbp_order, wall_clock_int\n",
    "            \"\"\").df()\n",
    "\n",
    "            if events.empty:\n",
    "                return ValidationResult(\"Traditional Data-Driven Lineups\", False, \n",
    "                                      \"No PBP events\", processing_time=time.time()-start_time)\n",
    "\n",
    "            # ---- State Tracking ----\n",
    "            on_court = {int(t): set(starters[int(t)]) for t in teams}\n",
    "            last_action_time = defaultdict(lambda: 0.0)\n",
    "            player_last_seen = {}  # Track when each player was last active\n",
    "            seconds_traditional = defaultdict(float)  # Minutes tracking\n",
    "\n",
    "            # NEW: Enhanced tracking for flagging\n",
    "            substitution_history = []  # Track all substitution attempts\n",
    "            lineup_size_history = []   # Track lineup size changes\n",
    "            player_status_tracking = {  # Track detailed player states\n",
    "                tid: {\n",
    "                    'current_lineup': set(starters[int(tid)]),\n",
    "                    'last_sub_in': {},    # player_id -> timestamp\n",
    "                    'last_sub_out': {},   # player_id -> timestamp\n",
    "                    'action_without_sub': set()  # players who had actions but no sub-in\n",
    "                } for tid in teams\n",
    "            }\n",
    "\n",
    "            prev_abs_time = 0.0\n",
    "            prev_period = None\n",
    "\n",
    "            # Results tracking\n",
    "            state_rows = []\n",
    "            flag_rows = []\n",
    "\n",
    "            def snapshot_lineups(ev_time: float, period: int, pbp_order: int, desc: str, event_type: str = \"NORMAL\"):\n",
    "                \"\"\"Snapshot current lineups with enhanced metadata\"\"\"\n",
    "                for tid in teams:\n",
    "                    lineup = list(on_court[int(tid)])\n",
    "                    lineup_names = [name_map.get(p, str(p)) for p in lineup]\n",
    "                    lineup_size = len(lineup)\n",
    "\n",
    "                    # NEW: Flag lineup size deviations\n",
    "                    if lineup_size != 5:\n",
    "                        flag_lineup_size_deviation(ev_time, period, pbp_order, int(tid), lineup_size, desc)\n",
    "\n",
    "                    state_rows.append({\n",
    "                        \"period\": period,\n",
    "                        \"pbp_order\": pbp_order,\n",
    "                        \"abs_time\": round(ev_time, 3),\n",
    "                        \"team_id\": int(tid),\n",
    "                        \"team_abbrev\": team_abbrev[int(tid)],\n",
    "                        \"lineup_size\": lineup_size,\n",
    "                        \"lineup_player_ids_json\": json.dumps(sorted([int(p) for p in lineup])),\n",
    "                        \"lineup_player_names_json\": json.dumps(sorted(lineup_names)),\n",
    "                        \"event_desc\": desc,\n",
    "                        \"event_type\": event_type\n",
    "                    })\n",
    "\n",
    "            def flag_lineup_size_deviation(ev_time: float, period: int, pbp_order: int, team_id: int, \n",
    "                                         actual_size: int, desc: str):\n",
    "                \"\"\"NEW: Flag and analyze lineup size deviations\"\"\"\n",
    "                team_abbr = team_abbrev[team_id]\n",
    "\n",
    "                # Analyze why lineup isn't size 5\n",
    "                reasons = []\n",
    "                team_status = player_status_tracking[team_id]\n",
    "\n",
    "                if actual_size < 5:\n",
    "                    missing_count = 5 - actual_size\n",
    "                    reasons.append(f\"Missing {missing_count} player(s) for full lineup\")\n",
    "\n",
    "                    # Check if any players had recent actions but aren't in lineup\n",
    "                    if team_status['action_without_sub']:\n",
    "                        reasons.append(f\"Players with actions but no sub-in: {[name_map.get(p, str(p)) for p in team_status['action_without_sub']]}\")\n",
    "\n",
    "                elif actual_size > 5:\n",
    "                    excess_count = actual_size - 5\n",
    "                    reasons.append(f\"Excess {excess_count} player(s) beyond normal lineup\")\n",
    "\n",
    "                # Check recent substitution activity\n",
    "                recent_subs = [sub for sub in substitution_history[-10:] if sub['team_id'] == team_id]\n",
    "                if recent_subs:\n",
    "                    last_sub = recent_subs[-1]\n",
    "                    reasons.append(f\"Last substitution: {last_sub['description']}\")\n",
    "\n",
    "                flag_rows.append({\n",
    "                    \"period\": period,\n",
    "                    \"pbp_order\": pbp_order,\n",
    "                    \"abs_time\": round(ev_time, 3),\n",
    "                    \"team_id\": team_id,\n",
    "                    \"team_abbrev\": team_abbr,\n",
    "                    \"flag_type\": \"lineup_size_deviation\",\n",
    "                    \"player_id\": None,\n",
    "                    \"player_name\": None,\n",
    "                    \"idle_seconds\": None,\n",
    "                    \"description\": desc,\n",
    "                    \"flag_details\": f\"Lineup size {actual_size}/5. \" + \"; \".join(reasons),\n",
    "                    \"resolved_via_last_name\": False,\n",
    "                    \"sub_direction_inverted\": False,\n",
    "                    \"lineup_json\": json.dumps(sorted([int(p) for p in on_court[team_id]]))\n",
    "                })\n",
    "\n",
    "            def flag_substitution_issue(ev_time: float, period: int, pbp_order: int, issue_type: str, \n",
    "                                      team_id: int, player_id: int = None, details: str = \"\"):\n",
    "                \"\"\"NEW: Flag various substitution issues\"\"\"\n",
    "                flag_rows.append({\n",
    "                    \"period\": period,\n",
    "                    \"pbp_order\": pbp_order,\n",
    "                    \"abs_time\": round(ev_time, 3),\n",
    "                    \"team_id\": team_id,\n",
    "                    \"team_abbrev\": team_abbrev[team_id],\n",
    "                    \"flag_type\": issue_type,\n",
    "                    \"player_id\": int(player_id) if player_id else None,\n",
    "                    \"player_name\": name_map.get(player_id, str(player_id)) if player_id else None,\n",
    "                    \"idle_seconds\": None,\n",
    "                    \"description\": details,\n",
    "                    \"flag_details\": details,\n",
    "                    \"resolved_via_last_name\": False,\n",
    "                    \"sub_direction_inverted\": False,\n",
    "                    \"lineup_json\": json.dumps(sorted([int(p) for p in on_court[team_id]]))\n",
    "                })\n",
    "\n",
    "            def validate_substitution(in_pid: int, out_pid: int, sub_tid: int, desc: str, \n",
    "                                    ev_time: float, period: int, pbp_order: int) -> dict:\n",
    "                \"\"\"NEW: Comprehensive substitution validation\"\"\"\n",
    "                validation_result = {\n",
    "                    \"valid\": True,\n",
    "                    \"issues\": [],\n",
    "                    \"can_proceed\": True\n",
    "                }\n",
    "\n",
    "                team_status = player_status_tracking[sub_tid]\n",
    "                current_lineup = team_status['current_lineup']\n",
    "\n",
    "                # Check OUT player\n",
    "                if out_pid:\n",
    "                    if out_pid not in current_lineup:\n",
    "                        validation_result[\"issues\"].append(f\"OUT player {name_map.get(out_pid)} not in current lineup\")\n",
    "                        flag_substitution_issue(ev_time, period, pbp_order, \"sub_out_player_not_in_lineup\", \n",
    "                                              sub_tid, out_pid, f\"Attempted to sub out {name_map.get(out_pid)} who is not in lineup\")\n",
    "                        validation_result[\"valid\"] = False\n",
    "                    else:\n",
    "                        # Check when player was last in\n",
    "                        last_in = team_status['last_sub_in'].get(out_pid, \"Game start\")\n",
    "                        validation_result[\"issues\"].append(f\"OUT: {name_map.get(out_pid)} (last in: {last_in})\")\n",
    "\n",
    "                # Check IN player  \n",
    "                if in_pid:\n",
    "                    if in_pid in current_lineup:\n",
    "                        validation_result[\"issues\"].append(f\"IN player {name_map.get(in_pid)} already in lineup\")\n",
    "                        flag_substitution_issue(ev_time, period, pbp_order, \"sub_in_player_already_in_lineup\", \n",
    "                                              sub_tid, in_pid, f\"Attempted to sub in {name_map.get(in_pid)} who is already in lineup\")\n",
    "                        validation_result[\"valid\"] = False\n",
    "                    else:\n",
    "                        # Check when player was last out\n",
    "                        last_out = team_status['last_sub_out'].get(in_pid, \"Never subbed out\")\n",
    "                        validation_result[\"issues\"].append(f\"IN: {name_map.get(in_pid)} (last out: {last_out})\")\n",
    "\n",
    "                return validation_result\n",
    "\n",
    "            # ---- Main Processing Loop ----\n",
    "            logger.info(f\"Processing {len(events)} events with TRADITIONAL DATA-DRIVEN approach...\")\n",
    "\n",
    "            for _, ev in events.iterrows():\n",
    "                period = int(ev.period)\n",
    "                rem = _parse_gc(ev.game_clock)\n",
    "                cur_t = _abs_t(period, rem)\n",
    "\n",
    "                # Credit time between events to current lineups\n",
    "                if cur_t > prev_abs_time:\n",
    "                    delta = cur_t - prev_abs_time\n",
    "                    for tid in teams:\n",
    "                        for pid in on_court[int(tid)]:\n",
    "                            seconds_traditional[pid] += delta\n",
    "\n",
    "                # Handle period transitions\n",
    "                if period != prev_period:\n",
    "                    if period in CFG[\"starter_reset_periods\"]:\n",
    "                        # Reset to starters\n",
    "                        for tid in teams:\n",
    "                            on_court[int(tid)] = set(starters[int(tid)])\n",
    "                            player_status_tracking[tid]['current_lineup'] = set(starters[int(tid)])\n",
    "\n",
    "                        snapshot_lineups(cur_t, period, int(ev.pbp_order), f\"Period {period} start - reset to starters\", \"PERIOD_START\")\n",
    "\n",
    "                    prev_period = period\n",
    "\n",
    "                # TRADITIONAL SUBSTITUTION PROCESSING - STRICT DATA ADHERENCE\n",
    "                if int(ev.msg_type) == CFG[\"sub_msg_type\"]:\n",
    "                    # Extract players - STRICT: playerId1=IN, playerId2=OUT\n",
    "                    in_pid = int(ev.player_id_1) if pd.notna(ev.player_id_1) else None\n",
    "                    out_pid = int(ev.player_id_2) if pd.notna(ev.player_id_2) else None\n",
    "\n",
    "                    # Determine team\n",
    "                    sub_tid = None\n",
    "                    if in_pid and (in_pid in pteam_map):\n",
    "                        sub_tid = int(pteam_map[in_pid])\n",
    "                    elif out_pid and (out_pid in pteam_map):\n",
    "                        sub_tid = int(pteam_map[out_pid])\n",
    "                    elif pd.notna(ev.team_id_off) and int(ev.team_id_off) in teams:\n",
    "                        sub_tid = int(ev.team_id_off)\n",
    "\n",
    "                    if sub_tid is not None:\n",
    "                        # NEW: Validate substitution before applying\n",
    "                        validation = validate_substitution(in_pid, out_pid, sub_tid, str(ev.description), \n",
    "                                                         cur_t, period, int(ev.pbp_order))\n",
    "\n",
    "                        # Record substitution attempt\n",
    "                        sub_record = {\n",
    "                            \"time\": cur_t,\n",
    "                            \"period\": period,\n",
    "                            \"team_id\": sub_tid,\n",
    "                            \"in_player\": in_pid,\n",
    "                            \"out_player\": out_pid,\n",
    "                            \"description\": str(ev.description),\n",
    "                            \"validation\": validation\n",
    "                        }\n",
    "                        substitution_history.append(sub_record)\n",
    "\n",
    "                        # Apply substitution (even if flagged - we follow the data)\n",
    "                        team_status = player_status_tracking[sub_tid]\n",
    "\n",
    "                        if out_pid and out_pid in on_court[sub_tid]:\n",
    "                            on_court[sub_tid].remove(out_pid)\n",
    "                            team_status['current_lineup'].remove(out_pid)\n",
    "                            team_status['last_sub_out'][out_pid] = cur_t\n",
    "                            logger.info(f\"[TRADITIONAL SUB-OUT] {name_map.get(out_pid)} from {team_abbrev[sub_tid]}\")\n",
    "\n",
    "                        if in_pid:\n",
    "                            on_court[sub_tid].add(in_pid)\n",
    "                            team_status['current_lineup'].add(in_pid)\n",
    "                            team_status['last_sub_in'][in_pid] = cur_t\n",
    "                            # Remove from action_without_sub if present\n",
    "                            team_status['action_without_sub'].discard(in_pid)\n",
    "                            logger.info(f\"[TRADITIONAL SUB-IN] {name_map.get(in_pid)} to {team_abbrev[sub_tid]}\")\n",
    "\n",
    "                        # Snapshot after substitution\n",
    "                        snapshot_lineups(cur_t, period, int(ev.pbp_order), str(ev.description), \"SUBSTITUTION\")\n",
    "\n",
    "                # Check for actions by players not in lineup\n",
    "                elif int(ev.msg_type) in CFG[\"action_msg_types\"]:\n",
    "                    action_pid = int(ev.player_id_1) if pd.notna(ev.player_id_1) else None\n",
    "                    action_tid = None\n",
    "\n",
    "                    if action_pid and (action_pid in pteam_map):\n",
    "                        action_tid = int(pteam_map[action_pid])\n",
    "                    elif pd.notna(ev.team_id_off) and int(ev.team_id_off) in teams:\n",
    "                        action_tid = int(ev.team_id_off)\n",
    "\n",
    "                    if action_tid in teams and action_pid is not None:\n",
    "                        # Update last action time\n",
    "                        last_action_time[action_pid] = cur_t\n",
    "                        player_last_seen[action_pid] = cur_t\n",
    "\n",
    "                        # Check if player is in lineup\n",
    "                        if action_pid not in on_court[action_tid]:\n",
    "                            # Flag action by player not in lineup\n",
    "                            flag_substitution_issue(cur_t, period, int(ev.pbp_order), \"action_by_non_lineup_player\", \n",
    "                                                  action_tid, action_pid, \n",
    "                                                  f\"Player {name_map.get(action_pid)} had action but not in lineup: {ev.description}\")\n",
    "\n",
    "                            # Track for analysis\n",
    "                            player_status_tracking[action_tid]['action_without_sub'].add(action_pid)\n",
    "\n",
    "                    # Regular lineup snapshot for non-substitution events\n",
    "                    snapshot_lineups(cur_t, period, int(ev.pbp_order), str(ev.description), \"ACTION\")\n",
    "\n",
    "                else:\n",
    "                    # Other events - just snapshot\n",
    "                    snapshot_lineups(cur_t, period, int(ev.pbp_order), str(ev.description), \"OTHER\")\n",
    "\n",
    "                prev_abs_time = cur_t\n",
    "\n",
    "            # ---- Build Traditional Minutes ----\n",
    "            traditional_minutes_rows = []\n",
    "            for pid, secs in seconds_traditional.items():\n",
    "                if pid in pteam_map:\n",
    "                    traditional_minutes_rows.append({\n",
    "                        \"player_id\": int(pid),\n",
    "                        \"player_name\": name_map.get(pid, str(pid)),\n",
    "                        \"team_id\": int(pteam_map[pid]),\n",
    "                        \"team_abbrev\": team_abbrev[int(pteam_map[pid])],\n",
    "                        \"seconds_traditional\": round(float(secs), 3)\n",
    "                    })\n",
    "\n",
    "            traditional_minutes_df = pd.DataFrame(traditional_minutes_rows).sort_values([\"team_abbrev\", \"player_name\"]).reset_index(drop=True)\n",
    "\n",
    "            # ---- Persist to DuckDB ----\n",
    "            # Replace the basic tables with traditional data-driven versions\n",
    "            self._robust_drop_object(\"traditional_lineup_state\")\n",
    "            self.conn.register(\"traditional_lineup_state_temp\", pd.DataFrame(state_rows))\n",
    "            self.conn.execute(\"CREATE TABLE traditional_lineup_state AS SELECT * FROM traditional_lineup_state_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS traditional_lineup_state_temp\")\n",
    "\n",
    "            self._robust_drop_object(\"traditional_lineup_flags\")\n",
    "            self.conn.register(\"traditional_lineup_flags_temp\", pd.DataFrame(flag_rows))\n",
    "            self.conn.execute(\"CREATE TABLE traditional_lineup_flags AS SELECT * FROM traditional_lineup_flags_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS traditional_lineup_flags_temp\")\n",
    "\n",
    "            self._robust_drop_object(\"minutes_traditional\")\n",
    "            self.conn.register(\"minutes_traditional_temp\", traditional_minutes_df)\n",
    "            self.conn.execute(\"CREATE TABLE minutes_traditional AS SELECT * FROM minutes_traditional_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS minutes_traditional_temp\")\n",
    "\n",
    "            # ---- Summary Statistics ----\n",
    "            flag_summary = {}\n",
    "            if flag_rows:\n",
    "                flag_df = pd.DataFrame(flag_rows)\n",
    "                flag_summary = dict(flag_df[\"flag_type\"].value_counts())\n",
    "\n",
    "            lineup_size_analysis = {}\n",
    "            if state_rows:\n",
    "                state_df = pd.DataFrame(state_rows)\n",
    "                size_counts = state_df[\"lineup_size\"].value_counts().to_dict()\n",
    "                lineup_size_analysis = {\n",
    "                    \"total_states\": len(state_rows),\n",
    "                    \"size_distribution\": {str(k): int(v) for k, v in size_counts.items()},\n",
    "                    \"non_5_player_states\": int(sum(v for k, v in size_counts.items() if k != 5)),\n",
    "                    \"percentage_correct_size\": round(size_counts.get(5, 0) / len(state_rows) * 100, 1) if state_rows else 0\n",
    "                }\n",
    "\n",
    "            substitution_analysis = {\n",
    "                \"total_substitutions\": len(substitution_history),\n",
    "                \"valid_substitutions\": len([s for s in substitution_history if s[\"validation\"][\"valid\"]]),\n",
    "                \"flagged_substitutions\": len([s for s in substitution_history if not s[\"validation\"][\"valid\"]])\n",
    "            }\n",
    "\n",
    "            self.data_summary[\"traditional_data_driven\"] = {\n",
    "                \"state_rows\": len(state_rows),\n",
    "                \"flag_rows\": len(flag_rows),\n",
    "                \"minutes_rows\": len(traditional_minutes_df),\n",
    "                \"flag_summary\": flag_summary,\n",
    "                \"lineup_size_analysis\": lineup_size_analysis,\n",
    "                \"substitution_analysis\": substitution_analysis,\n",
    "                \"substitution_history\": substitution_history[-20:]  # Last 20 for debugging\n",
    "            }\n",
    "\n",
    "            logger.info(f\"[TRADITIONAL DATA-DRIVEN] {len(substitution_history)} total substitutions\")\n",
    "            logger.info(f\"[TRADITIONAL DATA-DRIVEN] {len(flag_rows)} flags generated\")\n",
    "            logger.info(f\"[TRADITIONAL DATA-DRIVEN] Lineup size distribution: {lineup_size_analysis['size_distribution']}\")\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Traditional Data-Driven Lineups\",\n",
    "                passed=True,\n",
    "                details=(f\"Traditional data-driven tracking: {len(state_rows)} states, \"\n",
    "                        f\"{len(flag_rows)} flags, {lineup_size_analysis['percentage_correct_size']}% correct lineup size\"),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Traditional Data-Driven Lineups\",\n",
    "                passed=False,\n",
    "                details=f\"Error in traditional data-driven tracking: {e}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def compare_traditional_vs_enhanced_lineups(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        UPDATED COMPREHENSIVE COMPARISON: Traditional Data-Driven vs Enhanced Methods\n",
    "\n",
    "        Key Changes:\n",
    "        1. Uses traditional_lineup_state instead of basic_lineup_state\n",
    "        2. Enhanced analysis of lineup size variations\n",
    "        3. Detailed flagging comparison between methods\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Check if both methods have been run\n",
    "            traditional_data = self.data_summary.get('traditional_data_driven', {})\n",
    "            enhanced_data = self.data_summary.get('enhanced_substitution_tracking', {})\n",
    "\n",
    "            if not traditional_data or not enhanced_data:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Compare Traditional vs Enhanced (Updated)\",\n",
    "                    passed=False,\n",
    "                    details=\"Both traditional data-driven and enhanced methods must be run first\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Get validation data from both methods\n",
    "            traditional_validation = self.conn.execute(\"SELECT * FROM minutes_traditional\").df()\n",
    "            enhanced_validation = enhanced_data.get('validation_data', pd.DataFrame())\n",
    "            box_validation = self.conn.execute(\"SELECT player_id, player_name, team_id, team_abbrev, seconds_played FROM box_score WHERE seconds_played > 0\").df()\n",
    "\n",
    "            # Merge all data for comparison\n",
    "            comparison_df = box_validation.merge(\n",
    "                traditional_validation[['player_id', 'seconds_traditional']], \n",
    "                on='player_id', \n",
    "                how='left'\n",
    "            ).merge(\n",
    "                enhanced_validation[['player_id', 'calc_seconds']].rename(columns={'calc_seconds': 'seconds_enhanced'}),\n",
    "                on='player_id',\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # Fill NaN values\n",
    "            comparison_df['seconds_traditional'] = comparison_df['seconds_traditional'].fillna(0.0)\n",
    "            comparison_df['seconds_enhanced'] = comparison_df['seconds_enhanced'].fillna(0.0)\n",
    "\n",
    "            # Calculate differences vs box score\n",
    "            comparison_df['traditional_vs_box_diff'] = comparison_df['seconds_traditional'] - comparison_df['seconds_played']\n",
    "            comparison_df['enhanced_vs_box_diff'] = comparison_df['seconds_enhanced'] - comparison_df['seconds_played']\n",
    "            comparison_df['traditional_vs_box_abs_diff'] = comparison_df['traditional_vs_box_diff'].abs()\n",
    "            comparison_df['enhanced_vs_box_abs_diff'] = comparison_df['enhanced_vs_box_diff'].abs()\n",
    "\n",
    "            # Calculate percentage differences\n",
    "            def safe_pct_diff(calc, box):\n",
    "                return (calc - box) / box if box > 0 else 0.0\n",
    "\n",
    "            comparison_df['traditional_vs_box_pct_diff'] = comparison_df.apply(\n",
    "                lambda row: safe_pct_diff(row['seconds_traditional'], row['seconds_played']), axis=1\n",
    "            )\n",
    "            comparison_df['enhanced_vs_box_pct_diff'] = comparison_df.apply(\n",
    "                lambda row: safe_pct_diff(row['seconds_enhanced'], row['seconds_played']), axis=1\n",
    "            )\n",
    "\n",
    "            # Determine which method is better for each player\n",
    "            comparison_df['method_improvement'] = comparison_df['traditional_vs_box_abs_diff'] - comparison_df['enhanced_vs_box_abs_diff']\n",
    "            comparison_df['better_method'] = comparison_df['method_improvement'].apply(\n",
    "                lambda x: 'Enhanced' if x > 0 else 'Traditional' if x < 0 else 'Tie'\n",
    "            )\n",
    "\n",
    "            # Calculate summary statistics\n",
    "            tolerance_seconds = 120\n",
    "\n",
    "            traditional_offenders = len(comparison_df[comparison_df['traditional_vs_box_abs_diff'] > tolerance_seconds])\n",
    "            enhanced_offenders = len(comparison_df[comparison_df['enhanced_vs_box_abs_diff'] > tolerance_seconds])\n",
    "\n",
    "            improved_players = len(comparison_df[comparison_df['method_improvement'] > 0])\n",
    "            worsened_players = len(comparison_df[comparison_df['method_improvement'] < 0])\n",
    "            tied_players = len(comparison_df[comparison_df['method_improvement'] == 0])\n",
    "\n",
    "            # UPDATED: Get flag statistics for traditional method\n",
    "            traditional_flags = traditional_data.get('flag_summary', {})\n",
    "            enhanced_flags = enhanced_data.get('flags', {})\n",
    "\n",
    "            flag_comparison = {\n",
    "                'traditional_total_flags': sum(traditional_flags.values()),\n",
    "                'enhanced_total_flags': sum(len(flag_list) for flag_list in enhanced_flags.values()),\n",
    "                'traditional_flag_types': traditional_flags,\n",
    "                'enhanced_flag_types': {k: len(v) for k, v in enhanced_flags.items()}\n",
    "            }\n",
    "\n",
    "            # UPDATED: Get lineup size analysis\n",
    "            traditional_lineup_analysis = traditional_data.get('lineup_size_analysis', {})\n",
    "            enhanced_lineup_analysis = self._get_enhanced_lineup_analysis()\n",
    "\n",
    "            lineup_comparison = {\n",
    "                'traditional': {\n",
    "                    'total_states': traditional_lineup_analysis.get('total_states', 0),\n",
    "                    'size_distribution': traditional_lineup_analysis.get('size_distribution', {}),\n",
    "                    'correct_size_percentage': traditional_lineup_analysis.get('percentage_correct_size', 0),\n",
    "                    'non_5_player_states': traditional_lineup_analysis.get('non_5_player_states', 0)\n",
    "                },\n",
    "                'enhanced': {\n",
    "                    'total_states': enhanced_lineup_analysis.get('total_states', 0),\n",
    "                    'size_distribution': enhanced_lineup_analysis.get('size_distribution', {}),\n",
    "                    'correct_size_percentage': enhanced_lineup_analysis.get('percentage_correct_size', 0),\n",
    "                    'non_5_player_states': enhanced_lineup_analysis.get('non_5_player_states', 0)\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Create comprehensive comparison summary\n",
    "            comparison_summary = {\n",
    "                'method_comparison': {\n",
    "                    'traditional_offenders': int(traditional_offenders),\n",
    "                    'enhanced_offenders': int(enhanced_offenders),\n",
    "                    'improvement': int(traditional_offenders - enhanced_offenders),\n",
    "                    'players_improved': int(improved_players),\n",
    "                    'players_worsened': int(worsened_players),\n",
    "                    'players_tied': int(tied_players),\n",
    "                    'total_players': int(len(comparison_df))\n",
    "                },\n",
    "                'accuracy_metrics': {\n",
    "                    'traditional_avg_abs_diff': float(comparison_df['traditional_vs_box_abs_diff'].mean()),\n",
    "                    'enhanced_avg_abs_diff': float(comparison_df['enhanced_vs_box_abs_diff'].mean()),\n",
    "                    'traditional_max_diff': float(comparison_df['traditional_vs_box_abs_diff'].max()),\n",
    "                    'enhanced_max_diff': float(comparison_df['enhanced_vs_box_abs_diff'].max()),\n",
    "                    'traditional_within_10pct': int(len(comparison_df[comparison_df['traditional_vs_box_pct_diff'].abs() <= 0.10])),\n",
    "                    'enhanced_within_10pct': int(len(comparison_df[comparison_df['enhanced_vs_box_pct_diff'].abs() <= 0.10]))\n",
    "                },\n",
    "                'flag_analysis': flag_comparison,\n",
    "                'lineup_analysis': lineup_comparison\n",
    "            }\n",
    "\n",
    "            # Store results\n",
    "            self.data_summary['traditional_vs_enhanced_comparison_updated'] = {\n",
    "                'comparison_data': comparison_df,\n",
    "                'summary': comparison_summary,\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "\n",
    "            # Create database tables\n",
    "            self._robust_drop_object(\"traditional_vs_enhanced_comparison_updated\")\n",
    "            self.conn.register('comparison_updated_temp', comparison_df)\n",
    "            self.conn.execute(\"CREATE TABLE traditional_vs_enhanced_comparison_updated AS SELECT * FROM comparison_updated_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS comparison_updated_temp\")\n",
    "\n",
    "            # Log results\n",
    "            logger.info(f\"[UPDATED COMPARISON] Traditional Data-Driven: {traditional_offenders} offenders, Enhanced: {enhanced_offenders} offenders\")\n",
    "            logger.info(f\"[UPDATED COMPARISON] Improvement: {traditional_offenders - enhanced_offenders} fewer offenders\")\n",
    "            logger.info(f\"[UPDATED COMPARISON] Traditional flags: {sum(traditional_flags.values())}, Enhanced flags: {sum(len(flag_list) for flag_list in enhanced_flags.values())}\")\n",
    "            logger.info(f\"[UPDATED COMPARISON] Traditional lineup size distribution: {traditional_lineup_analysis.get('size_distribution', {})}\")\n",
    "\n",
    "            details = (f\"Updated comparison complete: Traditional Data-Driven ({traditional_offenders} offenders) vs Enhanced ({enhanced_offenders} offenders). \"\n",
    "                      f\"Improvement: {traditional_offenders - enhanced_offenders} fewer offenders. \"\n",
    "                      f\"Traditional flagged {sum(traditional_flags.values())} issues, Enhanced flagged {sum(len(flag_list) for flag_list in enhanced_flags.values())} issues.\")\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Compare Traditional vs Enhanced (Updated)\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in updated traditional vs enhanced comparison: {e}\")\n",
    "            return ValidationResult(\n",
    "                step_name=\"Compare Traditional vs Enhanced (Updated)\",\n",
    "                passed=False,\n",
    "                details=f\"Error in updated comparison: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _get_enhanced_lineup_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"Helper method to get enhanced lineup size analysis\"\"\"\n",
    "        try:\n",
    "            enhanced_states = self.conn.execute(\"SELECT lineup_size FROM enhanced_lineup_state\").df()\n",
    "            if enhanced_states.empty:\n",
    "                return {}\n",
    "\n",
    "            size_counts = enhanced_states['lineup_size'].value_counts().to_dict()\n",
    "            return {\n",
    "                'total_states': len(enhanced_states),\n",
    "                'size_distribution': {str(k): int(v) for k, v in size_counts.items()},\n",
    "                'non_5_player_states': int(sum(v for k, v in size_counts.items() if k != 5)),\n",
    "                'percentage_correct_size': round(size_counts.get(5, 0) / len(enhanced_states) * 100, 1)\n",
    "            }\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    def _create_comprehensive_flags_table(self, flags_data: Dict[str, List]) -> None:\n",
    "        \"\"\"Create comprehensive flags analysis table\"\"\"\n",
    "        try:\n",
    "            all_flags = []\n",
    "\n",
    "            for flag_type, flag_list in flags_data.items():\n",
    "                for flag in flag_list:\n",
    "                    flag_record = {\n",
    "                        'flag_type': flag_type,\n",
    "                        'time': flag.get('time', 0),\n",
    "                        'player_id': flag.get('player_id'),\n",
    "                        'player_name': flag.get('player_name'),\n",
    "                        'team': flag.get('team'),\n",
    "                        'action_type': flag.get('action_type'),\n",
    "                        'idle_seconds': flag.get('idle_seconds'),\n",
    "                        'description': flag.get('description', ''),\n",
    "                        'resolution': flag.get('resolution', ''),\n",
    "                        'full_details': str(flag)\n",
    "                    }\n",
    "                    all_flags.append(flag_record)\n",
    "\n",
    "            if all_flags:\n",
    "                flags_df = pd.DataFrame(all_flags)\n",
    "                self._robust_drop_object(\"comprehensive_flags_analysis\")\n",
    "                self.conn.register('flags_analysis_temp', flags_df)\n",
    "                self.conn.execute(\"CREATE TABLE comprehensive_flags_analysis AS SELECT * FROM flags_analysis_temp\")\n",
    "                self.conn.execute(\"DROP VIEW IF EXISTS flags_analysis_temp\")\n",
    "\n",
    "                logger.info(f\"Created comprehensive_flags_analysis table with {len(all_flags)} flag records\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create comprehensive flags table: {e}\")\n",
    "\n",
    "    def compare_basic_vs_estimated_lineups(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Compare minutes from: basic pass vs enhanced estimator vs box score.\n",
    "        FIXED: Creates DuckDB table from available sources before comparison.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # FIXED: Check and create missing tables\n",
    "            have_basic = self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='minutes_basic'\").fetchone()[0]\n",
    "            have_box = self.conn.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='box_score'\").fetchone()[0]\n",
    "\n",
    "            # Solution 1: Create minutes_basic from CSV if it exists\n",
    "            if have_basic == 0:\n",
    "                try:\n",
    "                    # Check multiple possible locations for CSV\n",
    "                    csv_candidates = [\n",
    "                        \"minutes_basic.csv\",\n",
    "                        \"reports/minutes_basic.csv\", \n",
    "                        \"exports/minutes_basic.csv\",\n",
    "                        str(PROCESSED_DIR / \"minutes_basic.csv\") if 'PROCESSED_DIR' in globals() else None\n",
    "                    ]\n",
    "\n",
    "                    csv_found = False\n",
    "                    for csv_path in csv_candidates:\n",
    "                        if csv_path and Path(csv_path).exists():\n",
    "                            self.conn.execute(f\"CREATE TABLE minutes_basic AS SELECT * FROM read_csv_auto('{csv_path}')\")\n",
    "                            have_basic = 1\n",
    "                            csv_found = True\n",
    "                            logger.info(f\"Created minutes_basic table from {csv_path}\")\n",
    "                            break\n",
    "\n",
    "                    # Solution 2: Use alternative tables as fallback\n",
    "                    if not csv_found:\n",
    "                        alt_tables = ['minutes_traditional', 'minutes_enhanced']\n",
    "                        for alt_table in alt_tables:\n",
    "                            alt_exists = self.conn.execute(f\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='{alt_table}'\").fetchone()[0]\n",
    "                            if alt_exists:\n",
    "                                # Map the alternative table columns to minutes_basic format\n",
    "                                if alt_table == 'minutes_traditional':\n",
    "                                    self.conn.execute(\"\"\"\n",
    "                                        CREATE TABLE minutes_basic AS \n",
    "                                        SELECT \n",
    "                                            player_id, \n",
    "                                            player_name, \n",
    "                                            team_id, \n",
    "                                            team_abbrev,\n",
    "                                            seconds_traditional as seconds_basic\n",
    "                                        FROM minutes_traditional\n",
    "                                    \"\"\")\n",
    "                                elif alt_table == 'minutes_enhanced':\n",
    "                                    self.conn.execute(\"\"\"\n",
    "                                        CREATE TABLE minutes_basic AS \n",
    "                                        SELECT \n",
    "                                            player_id, \n",
    "                                            player_name, \n",
    "                                            team_id, \n",
    "                                            team_abbrev,\n",
    "                                            seconds_enhanced as seconds_basic\n",
    "                                        FROM minutes_enhanced\n",
    "                                    \"\"\")\n",
    "                                have_basic = 1\n",
    "                                logger.info(f\"Created minutes_basic table from {alt_table}\")\n",
    "                                break\n",
    "\n",
    "                    # Solution 3: Create synthetic minutes_basic from box_score if needed\n",
    "                    if have_basic == 0 and have_box > 0:\n",
    "                        self.conn.execute(\"\"\"\n",
    "                            CREATE TABLE minutes_basic AS \n",
    "                            SELECT \n",
    "                                nbaId as player_id, \n",
    "                                name as player_name, \n",
    "                                nbaTeamId as team_id, \n",
    "                                team as team_abbrev,\n",
    "                                secPlayed as seconds_basic\n",
    "                            FROM box_score \n",
    "                            WHERE secPlayed > 0\n",
    "                        \"\"\")\n",
    "                        have_basic = 1\n",
    "                        logger.info(\"Created synthetic minutes_basic from box_score\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not create minutes_basic table: {e}\")\n",
    "\n",
    "            if have_basic == 0 or have_box == 0:\n",
    "                missing_items = []\n",
    "                if have_basic == 0:\n",
    "                    missing_items.append(\"minutes_basic\")\n",
    "                if have_box == 0:\n",
    "                    missing_items.append(\"box_score\")\n",
    "                return ValidationResult(\"Compare Minutes\", False, f\"Missing required tables: {missing_items}\")\n",
    "\n",
    "            # Enhanced minutes are in self.data_summary['minutes_validation_full'] if run_lineups_and_rim_analytics() already executed.\n",
    "            # But we want the compare to be callable before/after. We'll use DuckDB if present; otherwise fall back to data_summary.\n",
    "            enhanced_df = self.data_summary.get(\"minutes_validation_full\")\n",
    "            if enhanced_df is None:\n",
    "                # Create a minimal enhanced view if needed to keep pipeline flowing\n",
    "                enhanced_df = pd.DataFrame(columns=[\n",
    "                    \"player_id\",\"player_name\",\"team\",\"calc_seconds\",\"box_seconds\",\"abs_diff_seconds\",\"segments_count\"\n",
    "                ])\n",
    "\n",
    "            minutes_basic = self.conn.execute(\"\"\"\n",
    "                SELECT b.player_id, b.player_name, b.team_id, b.team_abbrev, b.seconds_basic\n",
    "                FROM minutes_basic b\n",
    "            \"\"\").df()\n",
    "\n",
    "            box = self.conn.execute(\"\"\"\n",
    "                SELECT player_id, player_name, team_id, team_abbrev, seconds_played\n",
    "                FROM box_score\n",
    "            \"\"\").df()\n",
    "\n",
    "            # merge frames\n",
    "            cmp_df = minutes_basic.merge(\n",
    "                box.rename(columns={\"seconds_played\":\"box_seconds\"}),\n",
    "                on=[\"player_id\",\"player_name\",\"team_id\",\"team_abbrev\"],\n",
    "                how=\"outer\"\n",
    "            )\n",
    "\n",
    "            # attach enhanced if available\n",
    "            if not enhanced_df.empty:\n",
    "                e_small = enhanced_df[[\"player_id\",\"calc_seconds\"]].rename(columns={\"calc_seconds\":\"enhanced_seconds\"})\n",
    "                cmp_df = cmp_df.merge(e_small, on=\"player_id\", how=\"left\")\n",
    "\n",
    "            # fill NaNs with 0 where appropriate (for diffs only; we do not alter raw tables)\n",
    "            for col in [\"seconds_basic\",\"box_seconds\",\"enhanced_seconds\"]:\n",
    "                if col in cmp_df.columns:\n",
    "                    cmp_df[col] = cmp_df[col].fillna(0.0)\n",
    "\n",
    "            # percentage diffs vs box\n",
    "            def pct_diff(a, b):\n",
    "                return None if b == 0 else (a - b) / b\n",
    "\n",
    "            cmp_df[\"basic_vs_box_sec_diff\"] = cmp_df[\"seconds_basic\"] - cmp_df[\"box_seconds\"]\n",
    "            cmp_df[\"basic_vs_box_pct_diff\"] = cmp_df.apply(lambda r: pct_diff(r[\"seconds_basic\"], r[\"box_seconds\"]), axis=1)\n",
    "\n",
    "            if \"enhanced_seconds\" in cmp_df.columns:\n",
    "                cmp_df[\"enhanced_vs_box_sec_diff\"] = cmp_df[\"enhanced_seconds\"] - cmp_df[\"box_seconds\"]\n",
    "                cmp_df[\"enhanced_vs_box_pct_diff\"] = cmp_df.apply(lambda r: pct_diff(r.get(\"enhanced_seconds\",0.0), r[\"box_seconds\"]), axis=1)\n",
    "            else:\n",
    "                cmp_df[\"enhanced_vs_box_sec_diff\"] = 0.0\n",
    "                cmp_df[\"enhanced_vs_box_pct_diff\"] = None\n",
    "\n",
    "            cmp_df = cmp_df.sort_values([\"team_abbrev\",\"player_name\"]).reset_index(drop=True)\n",
    "\n",
    "            # persist\n",
    "            self._robust_drop_object(\"minutes_compare\")\n",
    "            self.conn.register(\"minutes_compare_temp\", cmp_df)\n",
    "            self.conn.execute(\"CREATE TABLE minutes_compare AS SELECT * FROM minutes_compare_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS minutes_compare_temp\")\n",
    "\n",
    "            # summary: how many within 10% of box?\n",
    "            within10_basic = int((cmp_df[\"basic_vs_box_pct_diff\"].abs() <= 0.10).sum())\n",
    "            total_w_box    = int((cmp_df[\"box_seconds\"] > 0).sum())\n",
    "\n",
    "            self.data_summary[\"minutes_compare\"] = {\n",
    "                \"rows\": len(cmp_df),\n",
    "                \"within10_basic\": within10_basic,\n",
    "                \"total_with_box\": total_w_box\n",
    "            }\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Compare Minutes\",\n",
    "                passed=True,\n",
    "                details=f\"minutes_compare built ({len(cmp_df)} rows). Basic within 10%: {within10_basic}/{total_w_box}.\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Compare Minutes\",\n",
    "                passed=False,\n",
    "                details=f\"Error building minutes_compare: {e}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def validate_dataset_compliance(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Validate that generated datasets meet project requirements.\n",
    "        Critical validation to ensure deliverables are usable.\n",
    "        \n",
    "        FIXED: Corrected SQL column references to match actual schema\n",
    "        - Changed 'secPlayed' to 'seconds_played' to match box_score table schema\n",
    "        - Added better error handling for missing tables\n",
    "        - Enhanced validation reporting with specific compliance metrics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            validation_issues = []\n",
    "            \n",
    "            # Project 1: Validate 5-man lineup requirement\n",
    "            lineup_table_exists = self.conn.execute(\n",
    "                \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='final_dual_lineups'\"\n",
    "            ).fetchone()[0]\n",
    "            \n",
    "            if lineup_table_exists:\n",
    "                lineup_compliance = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        method,\n",
    "                        COUNT(*) as total_lineups,\n",
    "                        SUM(CASE WHEN lineup_size = 5 THEN 1 ELSE 0 END) as five_man_lineups,\n",
    "                        (SUM(CASE WHEN lineup_size = 5 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) as compliance_pct\n",
    "                    FROM final_dual_lineups\n",
    "                    GROUP BY method\n",
    "                \"\"\").df()\n",
    "                \n",
    "                for _, row in lineup_compliance.iterrows():\n",
    "                    method = row['method']\n",
    "                    compliance = row['compliance_pct']\n",
    "                    if compliance < 100.0:\n",
    "                        validation_issues.append(\n",
    "                            f\"{method.title()} method: {compliance:.1f}% compliance with 5-man requirement (FAILS)\"\n",
    "                        )\n",
    "            else:\n",
    "                validation_issues.append(\"final_dual_lineups table not found - cannot validate lineup compliance\")\n",
    "                        \n",
    "            # Project 2: Validate rim defense coverage\n",
    "            players_table_exists = self.conn.execute(\n",
    "                \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='final_dual_players'\"\n",
    "            ).fetchone()[0]\n",
    "            \n",
    "            if players_table_exists:\n",
    "                rim_coverage = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        method,\n",
    "                        COUNT(*) as total_players,\n",
    "                        SUM(CASE WHEN opp_rim_attempts_on > 0 OR opp_rim_attempts_off > 0 THEN 1 ELSE 0 END) as players_with_rim_data,\n",
    "                        (SUM(CASE WHEN opp_rim_attempts_on > 0 OR opp_rim_attempts_off > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) as coverage_pct\n",
    "                    FROM final_dual_players\n",
    "                    GROUP BY method\n",
    "                \"\"\").df()\n",
    "                \n",
    "                # FIXED: Use correct column name 'seconds_played' instead of 'secPlayed'\n",
    "                box_table_exists = self.conn.execute(\n",
    "                    \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='box_score'\"\n",
    "                ).fetchone()[0]\n",
    "                \n",
    "                if box_table_exists:\n",
    "                    expected_players = self.conn.execute(\n",
    "                        \"SELECT COUNT(*) FROM box_score WHERE seconds_played > 0\"\n",
    "                    ).fetchone()[0]\n",
    "                    \n",
    "                    for _, row in rim_coverage.iterrows():\n",
    "                        method = row['method']\n",
    "                        coverage = row['coverage_pct']\n",
    "                        players_covered = int(row['players_with_rim_data'])\n",
    "                        missing_players = expected_players - players_covered\n",
    "                        \n",
    "                        if missing_players > 0:\n",
    "                            validation_issues.append(\n",
    "                                f\"{method.title()} method: Missing rim data for {missing_players}/{expected_players} players\"\n",
    "                            )\n",
    "                else:\n",
    "                    validation_issues.append(\"box_score table not found - cannot validate expected player count\")\n",
    "            else:\n",
    "                validation_issues.append(\"final_dual_players table not found - cannot validate rim coverage\")\n",
    "            \n",
    "            # Minutes validation tolerance check\n",
    "            enhanced_validation = self.data_summary.get('minutes_validation_full')\n",
    "            if enhanced_validation is not None:\n",
    "                tolerance_violations = len(enhanced_validation[enhanced_validation['abs_diff_seconds'] > 120])\n",
    "                if tolerance_violations > 0:\n",
    "                    validation_issues.append(\n",
    "                        f\"Minutes validation: {tolerance_violations} players exceed 120s tolerance\"\n",
    "                    )\n",
    "            \n",
    "            passed = len(validation_issues) == 0\n",
    "            details = f\"Dataset compliance check: {len(validation_issues)} issues found\"\n",
    "            \n",
    "            # Add success details if validation passed\n",
    "            if passed:\n",
    "                details += \" - All compliance requirements met\"\n",
    "            \n",
    "            return ValidationResult(\n",
    "                step_name=\"Dataset Compliance Validation\",\n",
    "                passed=passed,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=validation_issues\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Dataset Compliance Validation\", \n",
    "                passed=False,\n",
    "                details=f\"Error validating dataset compliance: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def create_project_submission_artifacts(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Create final artifacts specifically for project submission using only compliant methods.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # Use enhanced method only for final submission due to 100% 5-man compliance\n",
    "            logger.info(\"Creating project submission artifacts using enhanced method...\")\n",
    "\n",
    "            # Project 1: Final lineup submission\n",
    "            enhanced_lineups = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    team_abbrev as \"Team\",\n",
    "                    player_1_name as \"Player 1\",\n",
    "                    player_2_name as \"Player 2\", \n",
    "                    player_3_name as \"Player 3\",\n",
    "                    player_4_name as \"Player 4\",\n",
    "                    player_5_name as \"Player 5\",\n",
    "                    off_possessions as \"Offensive possessions played\",\n",
    "                    def_possessions as \"Defensive possessions played\",\n",
    "                    off_rating as \"Offensive rating\", \n",
    "                    def_rating as \"Defensive rating\",\n",
    "                    net_rating as \"Net rating\"\n",
    "                FROM final_dual_lineups\n",
    "                WHERE method = 'enhanced'\n",
    "                AND lineup_size = 5  -- Ensure 5-man compliance\n",
    "                AND (off_possessions > 0 OR def_possessions > 0)\n",
    "                ORDER BY team_abbrev, off_possessions DESC\n",
    "            \"\"\").df()\n",
    "\n",
    "            # Project 2: Final player submission\n",
    "            enhanced_players = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    player_id as \"Player ID\",\n",
    "                    player_name as \"Player Name\", \n",
    "                    team_abbrev as \"Team\",\n",
    "                    off_possessions as \"Offensive possessions played\",\n",
    "                    def_possessions as \"Defensive possessions played\",\n",
    "                    ROUND(COALESCE(opp_rim_fg_pct_on, 0), 4) as \"Opponent rim field goal percentage when player is on the court\",\n",
    "                    ROUND(COALESCE(opp_rim_fg_pct_off, 0), 4) as \"Opponent rim field goal percentage when player is off the court\", \n",
    "                    ROUND(COALESCE(rim_defense_on_off, 0), 4) as \"Opponent rim field goal percentage on/off difference (on-off)\"\n",
    "                FROM final_dual_players\n",
    "                WHERE method = 'enhanced'\n",
    "                AND (off_possessions > 0 OR def_possessions > 0)\n",
    "                ORDER BY team_abbrev, player_name\n",
    "            \"\"\").df()\n",
    "\n",
    "            # Export final submission files\n",
    "            submission_dir = self.export_dir / \"final_submission\"\n",
    "            submission_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            enhanced_lineups.to_csv(submission_dir / \"project1_lineups_FINAL.csv\", index=False)\n",
    "            enhanced_players.to_csv(submission_dir / \"project2_players_FINAL.csv\", index=False)\n",
    "\n",
    "            # Create submission validation report\n",
    "            validation_report = {\n",
    "                \"project1_lineups\": {\n",
    "                    \"total_lineups\": len(enhanced_lineups),\n",
    "                    \"teams_covered\": enhanced_lineups['Team'].nunique(),\n",
    "                    \"five_man_compliance\": \"100%\",\n",
    "                    \"file_size_kb\": round((submission_dir / \"project1_lineups_FINAL.csv\").stat().st_size / 1024, 1)\n",
    "                },\n",
    "                \"project2_players\": {\n",
    "                    \"total_players\": len(enhanced_players), \n",
    "                    \"teams_covered\": enhanced_players['Team'].nunique(),\n",
    "                    \"rim_data_coverage\": f\"{len(enhanced_players[enhanced_players['Opponent rim field goal percentage when player is on the court'] > 0])}/{len(enhanced_players)} players\",\n",
    "                    \"file_size_kb\": round((submission_dir / \"project2_players_FINAL.csv\").stat().st_size / 1024, 1)\n",
    "                }\n",
    "            }\n",
    "\n",
    "            with open(submission_dir / \"submission_validation_report.json\", 'w') as f:\n",
    "                import json\n",
    "                json.dump(validation_report, f, indent=2)\n",
    "\n",
    "            details = f\"Created final submission artifacts: {len(enhanced_lineups)} lineups, {len(enhanced_players)} players\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Submission Artifacts\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(enhanced_lineups) + len(enhanced_players),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Submission Artifacts\",\n",
    "                passed=False, \n",
    "                details=f\"Error creating submission artifacts: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def run_enhanced_substitution_tracking_with_flags(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        ENHANCED SUBSTITUTION TRACKING WITH COMPREHENSIVE FLAGGING\n",
    "\n",
    "        This method provides the enhanced substitution logic with detailed flagging:\n",
    "        - First-action rules (Reed Sheppard case)\n",
    "        - Auto-out for inactivity periods\n",
    "        - Comprehensive flagging system\n",
    "        - Lineup size enforcement\n",
    "\n",
    "        Flags captured:\n",
    "        - missing_sub_in: Players with actions but no substitution in\n",
    "        - inactivity_periods: Players on court >2 minutes without action\n",
    "        - first_action_events: Reed Sheppard case injections\n",
    "        - auto_out_events: Automatic removals due to inactivity\n",
    "        - lineup_violations: Any time lineup != 5 players\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        CFG = {\n",
    "            \"starter_reset_periods\": [1, 3],\n",
    "            \"one_direction\": {\n",
    "                \"appearance_via_last_name\": True,\n",
    "                \"remove_out_if_present\": True\n",
    "            },\n",
    "            \"msg_types\": {\n",
    "                \"shot_made\": 1, \"shot_missed\": 2, \"rebound\": 4,\n",
    "                \"turnover\": 5, \"foul\": 6, \"substitution\": 8\n",
    "            },\n",
    "            \"minutes_validation\": {\"tolerance_seconds\": 120},\n",
    "            \"inactivity_rule\": {\"idle_seconds_threshold\": 120}\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Helper functions\n",
    "            def _period_length_seconds(p: int) -> float:\n",
    "                return 720.0 if p <= 4 else 300.0\n",
    "\n",
    "            def _parse_game_clock(gc: str) -> float | None:\n",
    "                if not gc or not isinstance(gc, str):\n",
    "                    return None\n",
    "                s = gc.strip()\n",
    "                if s.count(\":\") != 1:\n",
    "                    return None\n",
    "                try:\n",
    "                    mm, ss = s.split(\":\")\n",
    "                    return float(mm) * 60.0 + float(ss)\n",
    "                except (ValueError, IndexError):\n",
    "                    return None\n",
    "\n",
    "            def _abs_time(period: int, rem_sec: float | None) -> float:\n",
    "                total = 0.0\n",
    "                for pi in range(1, period):\n",
    "                    total += _period_length_seconds(pi)\n",
    "                pl = _period_length_seconds(period)\n",
    "                if rem_sec is None:\n",
    "                    return total + pl\n",
    "                return total + (pl - rem_sec)\n",
    "\n",
    "            # Load core data\n",
    "            box_df = self.conn.execute(\"\"\"\n",
    "                SELECT player_id, player_name, team_id, team_abbrev, is_starter, seconds_played\n",
    "                FROM box_score\n",
    "                WHERE seconds_played > 0\n",
    "                ORDER BY team_id, seconds_played DESC\n",
    "            \"\"\").df()\n",
    "\n",
    "            if box_df.empty:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Enhanced Substitution Tracking with Flags\",\n",
    "                    passed=False,\n",
    "                    details=\"No players found in box_score with playing time\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            teams = sorted(box_df['team_id'].unique().tolist())\n",
    "            if len(teams) != 2:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Enhanced Substitution Tracking with Flags\",\n",
    "                    passed=False,\n",
    "                    details=f\"Expected exactly 2 teams, found {teams}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Build comprehensive player mappings\n",
    "            roster = {int(tid): set() for tid in teams}\n",
    "            starters = {int(tid): set() for tid in teams}\n",
    "            name_map, pteam_map = {}, {}\n",
    "            last_name_index = {}\n",
    "\n",
    "            for _, r in box_df.iterrows():\n",
    "                pid = int(r.player_id)\n",
    "                tid = int(r.team_id)\n",
    "                roster[tid].add(pid)\n",
    "                if bool(r.is_starter):\n",
    "                    starters[tid].add(pid)\n",
    "                name_map[pid] = str(r.player_name)\n",
    "                pteam_map[pid] = tid\n",
    "\n",
    "                # Enhanced last name indexing for first-action resolution\n",
    "                full_name = str(r.player_name).strip()\n",
    "                last_name = full_name.split()[-1].lower()\n",
    "                first_name = full_name.split()[0].lower() if len(full_name.split()) > 1 else \"\"\n",
    "\n",
    "                last_name_index.setdefault(last_name, []).append(pid)\n",
    "                if first_name:\n",
    "                    last_name_index.setdefault(first_name, []).append(pid)\n",
    "\n",
    "            team_abbrev_map = {int(tid): box_df[box_df.team_id == tid]['team_abbrev'].iloc[0] for tid in teams}\n",
    "\n",
    "            # Load events with last names for resolution\n",
    "            events = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    period, pbp_order, wall_clock_int,\n",
    "                    COALESCE(game_clock,'') AS game_clock,\n",
    "                    COALESCE(description,'') AS description,\n",
    "                    team_id_off, team_id_def, msg_type, action_type,\n",
    "                    player_id_1, player_id_2, player_id_3,\n",
    "                    NULLIF(last_name_1,'') AS last_name_1,\n",
    "                    NULLIF(last_name_2,'') AS last_name_2,\n",
    "                    NULLIF(last_name_3,'') AS last_name_3,\n",
    "                    COALESCE(points, 0) AS points\n",
    "                FROM pbp\n",
    "                ORDER BY period, pbp_order, wall_clock_int\n",
    "            \"\"\").df()\n",
    "\n",
    "            if events.empty:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Enhanced Substitution Tracking with Flags\",\n",
    "                    passed=False,\n",
    "                    details=\"No PBP events found\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Initialize enhanced tracking\n",
    "            on_court = {tid: set(starters[tid]) for tid in teams}\n",
    "            last_action_time = defaultdict(lambda: 0.0)\n",
    "            recent_out = {tid: deque(maxlen=10) for tid in teams}\n",
    "\n",
    "            active_segments = {}\n",
    "            completed_segments = defaultdict(list)\n",
    "\n",
    "            # Enhanced tracking variables with FLAGS\n",
    "            enhanced_stats = {\n",
    "                'total_substitutions': 0,\n",
    "                'successful_substitutions': 0,\n",
    "                'first_action_injections': 0,\n",
    "                'auto_outs_inactivity': 0,\n",
    "                'lineup_size_corrections': 0,\n",
    "                'flags': {\n",
    "                    'missing_sub_ins': [],      # Players with actions but no sub-in\n",
    "                    'inactivity_periods': [],   # Players inactive >2min while on court\n",
    "                    'lineup_violations': [],    # Times when lineup != 5 players\n",
    "                    'first_action_events': [],  # First-action injection events\n",
    "                    'auto_out_events': []       # Auto-out events due to inactivity\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Lineup state tracking (similar to basic method)\n",
    "            state_rows = []\n",
    "            flag_rows = []\n",
    "\n",
    "            def snapshot_lineups(ev_time: float, period: int, pbp_order: int, desc: str):\n",
    "                import json\n",
    "                for tid in teams:\n",
    "                    lineup = list(on_court[tid])\n",
    "                    lineup_names = [name_map.get(p, str(p)) for p in lineup]\n",
    "                    state_rows.append({\n",
    "                        \"period\": period,\n",
    "                        \"pbp_order\": pbp_order,\n",
    "                        \"abs_time\": round(ev_time, 3),\n",
    "                        \"team_id\": int(tid),\n",
    "                        \"team_abbrev\": team_abbrev_map[tid],\n",
    "                        \"lineup_size\": len(lineup),\n",
    "                        \"lineup_player_ids_json\": json.dumps(sorted([int(p) for p in lineup])),\n",
    "                        \"lineup_player_names_json\": json.dumps(sorted(lineup_names)),\n",
    "                        \"event_desc\": desc\n",
    "                    })\n",
    "\n",
    "            # Initialize segments for starters\n",
    "            for tid in teams:\n",
    "                for pid in on_court[tid]:\n",
    "                    active_segments[pid] = {'start': 0.0, 'reason': 'GAME_START'}\n",
    "                    last_action_time[pid] = 0.0\n",
    "\n",
    "            def enhanced_name_resolution(ln: str | None, tid_hint: int | None) -> int | None:\n",
    "                \"\"\"Enhanced name resolution with fuzzy matching\"\"\"\n",
    "                if not ln:\n",
    "                    return None\n",
    "\n",
    "                ln_clean = str(ln).strip().lower()\n",
    "                candidates = last_name_index.get(ln_clean, [])\n",
    "\n",
    "                if not candidates:\n",
    "                    # Try partial matching\n",
    "                    for key, pids in last_name_index.items():\n",
    "                        if ln_clean in key or key in ln_clean:\n",
    "                            candidates.extend(pids)\n",
    "\n",
    "                if not candidates:\n",
    "                    return None\n",
    "\n",
    "                # Prefer team hint if available\n",
    "                if tid_hint is not None:\n",
    "                    for cand in candidates:\n",
    "                        if pteam_map.get(cand) == tid_hint:\n",
    "                            return cand\n",
    "\n",
    "                # Return first valid candidate\n",
    "                for cand in candidates:\n",
    "                    if pteam_map.get(cand) in teams:\n",
    "                        return cand\n",
    "\n",
    "                return None\n",
    "\n",
    "            def end_player_segment(pid: int, end_time: float, reason: str) -> None:\n",
    "                \"\"\"End a player's active segment with validation\"\"\"\n",
    "                if pid not in active_segments:\n",
    "                    return\n",
    "\n",
    "                start_info = active_segments[pid]\n",
    "                start_time = start_info['start']\n",
    "\n",
    "                if end_time <= start_time:\n",
    "                    end_time = start_time + 1.0\n",
    "\n",
    "                duration = end_time - start_time\n",
    "                completed_segments[pid].append({\n",
    "                    'start': start_time,\n",
    "                    'end': end_time,\n",
    "                    'duration': duration,\n",
    "                    'reason': f\"{start_info['reason']} -> {reason}\"\n",
    "                })\n",
    "\n",
    "                del active_segments[pid]\n",
    "\n",
    "            def start_player_segment(pid: int, start_time: float, reason: str) -> None:\n",
    "                \"\"\"Start a new segment with overlap prevention\"\"\"\n",
    "                if pid in active_segments:\n",
    "                    end_player_segment(pid, start_time, f\"OVERLAP_{reason}\")\n",
    "\n",
    "                active_segments[pid] = {'start': start_time, 'reason': reason}\n",
    "\n",
    "            def flag_inactivity_check(current_time: float, period: int, pbp_order: int) -> None:\n",
    "                \"\"\"Check for players inactive > 2 minutes and flag them\"\"\"\n",
    "                for tid in teams:\n",
    "                    for pid in on_court[tid]:\n",
    "                        idle_time = current_time - last_action_time[pid]\n",
    "\n",
    "                        if idle_time > CFG[\"inactivity_rule\"][\"idle_seconds_threshold\"]:\n",
    "                            # Flag this as a potential missing sub-out\n",
    "                            enhanced_stats['flags']['inactivity_periods'].append({\n",
    "                                'time': current_time,\n",
    "                                'player_id': pid,\n",
    "                                'player_name': name_map.get(pid),\n",
    "                                'team': team_abbrev_map[tid],\n",
    "                                'idle_seconds': idle_time,\n",
    "                                'last_action_time': last_action_time[pid]\n",
    "                            })\n",
    "                            # Also add to flag_rows for CSV export\n",
    "                            flag_rows.append({\n",
    "                                \"period\": period,\n",
    "                                \"pbp_order\": pbp_order,\n",
    "                                \"abs_time\": round(current_time, 3),\n",
    "                                \"team_id\": int(tid),\n",
    "                                \"team_abbrev\": team_abbrev_map[tid],\n",
    "                                \"flag_type\": \"inactivity_periods\",\n",
    "                                \"player_id\": int(pid),\n",
    "                                \"player_name\": name_map.get(pid, str(pid)),\n",
    "                                \"idle_seconds\": round(idle_time, 3),\n",
    "                                \"description\": f\"Player inactive for {idle_time:.1f}s (threshold: {CFG['inactivity_rule']['idle_seconds_threshold']}s)\"\n",
    "                            })\n",
    "\n",
    "            def pick_auto_out_candidate(tid: int, current_time: float, exclude: set[int] = set()) -> int | None:\n",
    "                \"\"\"Enhanced auto-out selection based on activity patterns\"\"\"\n",
    "                if not on_court[tid]:\n",
    "                    return None\n",
    "\n",
    "                candidates = [p for p in on_court[tid] if p not in exclude]\n",
    "                if not candidates:\n",
    "                    return None\n",
    "\n",
    "                def activity_score(pid: int) -> tuple:\n",
    "                    idle_time = current_time - last_action_time[pid]\n",
    "                    is_starter = pid in starters[tid]\n",
    "                    recently_subbed = pid in recent_out[tid]\n",
    "\n",
    "                    # Score components (higher = more likely to be removed)\n",
    "                    idle_score = idle_time\n",
    "                    starter_penalty = -100 if is_starter else 0\n",
    "                    recent_sub_penalty = -50 if recently_subbed else 0\n",
    "\n",
    "                    total_score = idle_score + starter_penalty + recent_sub_penalty\n",
    "\n",
    "                    return (total_score, idle_time, pid)\n",
    "\n",
    "                candidates.sort(key=activity_score, reverse=True)\n",
    "                best_candidate = candidates[0]\n",
    "                idle_time = current_time - last_action_time[best_candidate]\n",
    "\n",
    "                if idle_time >= CFG[\"inactivity_rule\"][\"idle_seconds_threshold\"] or len(on_court[tid]) > 5:\n",
    "                    return best_candidate\n",
    "\n",
    "                return None\n",
    "\n",
    "            def ensure_valid_lineup(tid: int, current_time: float, prefer_keep: set[int] = set()) -> None:\n",
    "                \"\"\"Ensure team has exactly 5 players with enhanced logging\"\"\"\n",
    "                team_name = team_abbrev_map[tid]\n",
    "                changes_made = False\n",
    "\n",
    "                # Remove excess players\n",
    "                while len(on_court[tid]) > 5:\n",
    "                    auto_out = pick_auto_out_candidate(tid, current_time, exclude=prefer_keep)\n",
    "                    if auto_out is None:\n",
    "                        logger.error(f\"Cannot auto-remove from {team_name} - no valid candidates\")\n",
    "                        break\n",
    "\n",
    "                    on_court[tid].remove(auto_out)\n",
    "                    recent_out[tid].append(auto_out)\n",
    "                    changes_made = True\n",
    "\n",
    "                    idle_time = current_time - last_action_time[auto_out]\n",
    "                    enhanced_stats['auto_outs_inactivity'] += 1\n",
    "\n",
    "                    # Flag this auto-out event\n",
    "                    enhanced_stats['flags']['auto_out_events'].append({\n",
    "                        'time': current_time,\n",
    "                        'player_id': auto_out,\n",
    "                        'player_name': name_map.get(auto_out),\n",
    "                        'team': team_name,\n",
    "                        'idle_time': idle_time,\n",
    "                        'reason': 'INACTIVITY_AUTO_OUT'\n",
    "                    })\n",
    "\n",
    "                    logger.info(f\"[ENHANCED AUTO-OUT] {name_map.get(auto_out)} from {team_name} (idle: {idle_time:.1f}s)\")\n",
    "\n",
    "                # Add players if under 5\n",
    "                if len(on_court[tid]) < 5:\n",
    "                    available = [p for p in roster[tid] if p not in on_court[tid]]\n",
    "                    if available:\n",
    "                        def fill_priority(pid: int) -> tuple:\n",
    "                            recently_out_priority = 0 if pid in recent_out[tid] else 1\n",
    "                            starter_priority = 0 if pid in starters[tid] else 1\n",
    "                            activity_priority = -(current_time - last_action_time[pid])\n",
    "\n",
    "                            return (recently_out_priority, starter_priority, activity_priority)\n",
    "\n",
    "                        available.sort(key=fill_priority)\n",
    "\n",
    "                        needed = 5 - len(on_court[tid])\n",
    "                        for i in range(min(needed, len(available))):\n",
    "                            fill_player = available[i]\n",
    "                            on_court[tid].add(fill_player)\n",
    "                            changes_made = True\n",
    "                            logger.info(f\"[ENHANCED AUTO-IN] {name_map.get(fill_player)} to {team_name} (fill to 5)\")\n",
    "\n",
    "                # Update segments if changes were made\n",
    "                if changes_made:\n",
    "                    enhanced_stats['lineup_size_corrections'] += 1\n",
    "                    for pid in on_court[tid]:\n",
    "                        if pid not in active_segments:\n",
    "                            start_player_segment(pid, current_time, \"LINEUP_CORRECTION\")\n",
    "\n",
    "                    # Flag lineup correction\n",
    "                    enhanced_stats['flags']['lineup_violations'].append({\n",
    "                        'time': current_time,\n",
    "                        'team': team_name,\n",
    "                        'correction_type': 'AUTO_CORRECTION',\n",
    "                        'final_size': len(on_court[tid])\n",
    "                    })\n",
    "\n",
    "            # MAIN PROCESSING LOOP - ENHANCED APPROACH WITH FLAGS\n",
    "            prev_period = None\n",
    "\n",
    "            logger.info(f\"Processing {len(events)} events with ENHANCED substitution rules and flagging...\")\n",
    "\n",
    "            for idx, ev in events.iterrows():\n",
    "                period = int(ev.period)\n",
    "                clock_str = ev.game_clock\n",
    "                parsed_clock = _parse_game_clock(clock_str)\n",
    "                current_time = _abs_time(period, parsed_clock)\n",
    "                msg_type = int(ev.msg_type)\n",
    "\n",
    "                # Handle period transitions\n",
    "                if period != prev_period and prev_period is not None:\n",
    "                    period_end_time = _abs_time(prev_period, 0.0)\n",
    "\n",
    "                    for pid in list(active_segments.keys()):\n",
    "                        end_player_segment(pid, period_end_time, f\"PERIOD_{prev_period}_END\")\n",
    "\n",
    "                if period != prev_period:\n",
    "                    if period in CFG[\"starter_reset_periods\"]:\n",
    "                        on_court = {tid: set(starters[tid]) for tid in teams}\n",
    "                        logger.info(f\"[ENHANCED PERIOD {period}] Reset to starters\")\n",
    "                    else:\n",
    "                        logger.info(f\"[ENHANCED PERIOD {period}] Continue lineups\")\n",
    "\n",
    "                    for tid in teams:\n",
    "                        for pid in on_court[tid]:\n",
    "                            if pid not in active_segments:\n",
    "                                start_player_segment(pid, current_time, f\"PERIOD_{period}_START\")\n",
    "\n",
    "                    prev_period = period\n",
    "\n",
    "                # SUBSTITUTION PROCESSING\n",
    "                if msg_type == CFG[\"msg_types\"][\"substitution\"]:\n",
    "                    enhanced_stats['total_substitutions'] += 1\n",
    "\n",
    "                    out_pid = int(ev.player_id_1) if not pd.isna(ev.player_id_1) else None\n",
    "                    in_pid = int(ev.player_id_2) if not pd.isna(ev.player_id_2) else None\n",
    "                    out_ln, in_ln = ev.last_name_1, ev.last_name_2\n",
    "\n",
    "                    # Determine team\n",
    "                    sub_tid = None\n",
    "                    if in_pid and in_pid in pteam_map:\n",
    "                        sub_tid = pteam_map[in_pid]\n",
    "                    elif out_pid and out_pid in pteam_map:\n",
    "                        sub_tid = pteam_map[out_pid]\n",
    "                    elif pd.notna(ev.team_id_off) and int(ev.team_id_off) in teams:\n",
    "                        sub_tid = int(ev.team_id_off)\n",
    "\n",
    "                    # Enhanced name resolution\n",
    "                    if in_pid is None and CFG[\"one_direction\"][\"appearance_via_last_name\"] and in_ln:\n",
    "                        in_pid = enhanced_name_resolution(in_ln, sub_tid)\n",
    "                    if out_pid is None and out_ln:\n",
    "                        out_pid = enhanced_name_resolution(out_ln, sub_tid)\n",
    "\n",
    "                    if sub_tid is not None:\n",
    "                        team_name = team_abbrev_map[sub_tid]\n",
    "\n",
    "                        # Process OUT first\n",
    "                        if CFG[\"one_direction\"][\"remove_out_if_present\"] and out_pid and out_pid in on_court[sub_tid]:\n",
    "                            on_court[sub_tid].remove(out_pid)\n",
    "                            recent_out[sub_tid].append(out_pid)\n",
    "                            end_player_segment(out_pid, current_time, \"SUB_OUT\")\n",
    "                            logger.info(f\"[ENHANCED SUB-OUT] {name_map.get(out_pid)} from {team_name}\")\n",
    "\n",
    "                        # Process IN\n",
    "                        if in_pid and in_pid not in on_court[sub_tid]:\n",
    "                            # Make room if needed\n",
    "                            if len(on_court[sub_tid]) >= 5:\n",
    "                                auto_out = pick_auto_out_candidate(sub_tid, current_time, exclude={in_pid})\n",
    "                                if auto_out:\n",
    "                                    on_court[sub_tid].remove(auto_out)\n",
    "                                    recent_out[sub_tid].append(auto_out)\n",
    "                                    end_player_segment(auto_out, current_time, \"MAKE_ROOM\")\n",
    "                                    logger.info(f\"[ENHANCED MAKE-ROOM] {name_map.get(auto_out)} out for {name_map.get(in_pid)}\")\n",
    "\n",
    "                            on_court[sub_tid].add(in_pid)\n",
    "                            start_player_segment(in_pid, current_time, \"SUB_IN\")\n",
    "                            enhanced_stats['successful_substitutions'] += 1\n",
    "                            logger.info(f\"[ENHANCED SUB-IN] {name_map.get(in_pid)} to {team_name}\")\n",
    "\n",
    "                    # Update activity times\n",
    "                    if out_pid:\n",
    "                        last_action_time[out_pid] = current_time\n",
    "                    if in_pid:\n",
    "                        last_action_time[in_pid] = current_time\n",
    "\n",
    "                    # Snapshot lineup after substitution\n",
    "                    snapshot_lineups(current_time, period, int(ev.pbp_order), str(ev.description))\n",
    "\n",
    "                # FIRST ACTION PROCESSING (Reed Sheppard rule) WITH FLAGS\n",
    "                elif msg_type in [1, 2, 4, 5, 6] and CFG[\"one_direction\"][\"appearance_via_last_name\"]:\n",
    "                    action_pid = int(ev.player_id_1) if not pd.isna(ev.player_id_1) else None\n",
    "                    action_ln = ev.last_name_1\n",
    "                    action_tid = None\n",
    "\n",
    "                    if action_pid and action_pid in pteam_map:\n",
    "                        action_tid = pteam_map[action_pid]\n",
    "                    elif pd.notna(ev.team_id_off) and int(ev.team_id_off) in teams:\n",
    "                        action_tid = int(ev.team_id_off)\n",
    "\n",
    "                    # Resolve via last name if needed\n",
    "                    if action_pid is None and action_ln:\n",
    "                        action_pid = enhanced_name_resolution(action_ln, action_tid)\n",
    "                        if action_pid:\n",
    "                            action_tid = pteam_map[action_pid]\n",
    "\n",
    "                    # Apply Reed Sheppard rule with FLAG\n",
    "                    if action_tid in teams and action_pid and action_pid not in on_court[action_tid]:\n",
    "                        # FLAG: This is a missing sub-in scenario\n",
    "                        enhanced_stats['flags']['missing_sub_ins'].append({\n",
    "                            'time': current_time,\n",
    "                            'player_id': action_pid,\n",
    "                            'player_name': name_map.get(action_pid),\n",
    "                            'team': team_abbrev_map[action_tid],\n",
    "                            'action_type': msg_type,\n",
    "                            'description': ev.description,\n",
    "                            'resolution': 'FIRST_ACTION_INJECTION'\n",
    "                        })\n",
    "\n",
    "                        # Also add to flag_rows for CSV export\n",
    "                        flag_rows.append({\n",
    "                            \"period\": period,\n",
    "                            \"pbp_order\": int(ev.pbp_order),\n",
    "                            \"abs_time\": round(current_time, 3),\n",
    "                            \"team_id\": int(action_tid),\n",
    "                            \"team_abbrev\": team_abbrev_map[action_tid],\n",
    "                            \"flag_type\": \"missing_sub_in\",\n",
    "                            \"player_id\": int(action_pid),\n",
    "                            \"player_name\": name_map.get(action_pid, str(action_pid)),\n",
    "                            \"idle_seconds\": None,\n",
    "                            \"description\": str(ev.description or \"\")\n",
    "                        })\n",
    "\n",
    "                        # Inject player via first-action rule\n",
    "                        on_court[action_tid].add(action_pid)\n",
    "                        enhanced_stats['first_action_injections'] += 1\n",
    "\n",
    "                        logger.info(f\"[ENHANCED FIRST-ACTION] {name_map.get(action_pid)} -> {team_abbrev_map[action_tid]} (msg: {msg_type})\")\n",
    "\n",
    "                        # Flag the first-action event\n",
    "                        enhanced_stats['flags']['first_action_events'].append({\n",
    "                            'time': current_time,\n",
    "                            'player_id': action_pid,\n",
    "                            'player_name': name_map.get(action_pid),\n",
    "                            'team': team_abbrev_map[action_tid],\n",
    "                            'action': ev.description\n",
    "                        })\n",
    "\n",
    "                        start_player_segment(action_pid, current_time, \"FIRST_ACTION\")\n",
    "                        ensure_valid_lineup(action_tid, current_time, prefer_keep={action_pid})\n",
    "\n",
    "                        # Snapshot lineup after first-action injection\n",
    "                        snapshot_lineups(current_time, period, int(ev.pbp_order), f\"FIRST_ACTION: {ev.description}\")\n",
    "\n",
    "                    # Update activity time\n",
    "                    if action_pid:\n",
    "                        last_action_time[action_pid] = current_time\n",
    "\n",
    "                # Periodic inactivity checking\n",
    "                if idx % 20 == 0:  # Check every 20 events\n",
    "                    flag_inactivity_check(current_time, period, int(ev.pbp_order))\n",
    "\n",
    "                # Ensure lineup validity\n",
    "                for tid in teams:\n",
    "                    if len(on_court[tid]) != 5:\n",
    "                        ensure_valid_lineup(tid, current_time)\n",
    "\n",
    "            # Final processing\n",
    "            final_time = _abs_time(4, 0.0) if prev_period and prev_period <= 4 else 2880.0\n",
    "            for pid in list(active_segments.keys()):\n",
    "                end_player_segment(pid, final_time, \"GAME_END\")\n",
    "\n",
    "            # Calculate minutes\n",
    "            calculated_minutes = {}\n",
    "            for pid in set(list(completed_segments.keys()) + list(box_df['player_id'])):\n",
    "                segments = completed_segments[pid]\n",
    "                total_seconds = sum(seg['duration'] for seg in segments)\n",
    "                calculated_minutes[pid] = total_seconds\n",
    "\n",
    "            # Build validation results\n",
    "            enhanced_validation = []\n",
    "            for pid in set(list(calculated_minutes.keys()) + list(box_df['player_id'])):\n",
    "                calc_secs = calculated_minutes.get(pid, 0.0)\n",
    "                box_row = box_df[box_df['player_id'] == pid]\n",
    "                box_secs = float(box_row['seconds_played'].iloc[0]) if not box_row.empty else 0.0\n",
    "                diff = calc_secs - box_secs\n",
    "\n",
    "                enhanced_validation.append({\n",
    "                    \"player_id\": pid,\n",
    "                    \"player_name\": name_map.get(pid, f\"ID_{pid}\"),\n",
    "                    \"team\": team_abbrev_map.get(pteam_map.get(pid), \"UNK\"),\n",
    "                    \"calc_seconds\": round(calc_secs, 1),\n",
    "                    \"box_seconds\": round(box_secs, 1),\n",
    "                    \"abs_diff_seconds\": round(abs(diff), 1),\n",
    "                    \"segments_count\": len(completed_segments.get(pid, []))\n",
    "                })\n",
    "\n",
    "            enhanced_validation_df = pd.DataFrame(enhanced_validation).sort_values([\"team\", \"player_name\"]).reset_index(drop=True)\n",
    "            enhanced_offenders = enhanced_validation_df[enhanced_validation_df[\"abs_diff_seconds\"] > CFG[\"minutes_validation\"][\"tolerance_seconds\"]]\n",
    "\n",
    "            # Create lineup state and flag tables\n",
    "            state_df = pd.DataFrame(state_rows).sort_values([\"period\", \"pbp_order\", \"team_id\"]).reset_index(drop=True)\n",
    "            flag_df = pd.DataFrame(flag_rows).sort_values([\"abs_time\", \"team_id\"]).reset_index(drop=True)\n",
    "\n",
    "            # Enhanced minutes table (similar to basic_minutes)\n",
    "            enhanced_minutes_rows = []\n",
    "            for pid, secs in calculated_minutes.items():\n",
    "                if pid in pteam_map:\n",
    "                    enhanced_minutes_rows.append({\n",
    "                        \"player_id\": int(pid),\n",
    "                        \"player_name\": name_map.get(pid, str(pid)),\n",
    "                        \"team_id\": int(pteam_map[pid]),\n",
    "                        \"team_abbrev\": team_abbrev_map[int(pteam_map[pid])],\n",
    "                        \"seconds_enhanced\": round(float(secs), 3)\n",
    "                    })\n",
    "            enhanced_minutes_df = pd.DataFrame(enhanced_minutes_rows).sort_values([\"team_abbrev\", \"player_name\"]).reset_index(drop=True)\n",
    "\n",
    "            # Persist to DuckDB\n",
    "            self._robust_drop_object(\"enhanced_lineup_state\")\n",
    "            self.conn.register(\"enhanced_lineup_state_temp\", state_df)\n",
    "            self.conn.execute(\"CREATE TABLE enhanced_lineup_state AS SELECT * FROM enhanced_lineup_state_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS enhanced_lineup_state_temp\")\n",
    "\n",
    "            self._robust_drop_object(\"enhanced_lineup_flags\")\n",
    "            self.conn.register(\"enhanced_lineup_flags_temp\", flag_df)\n",
    "            self.conn.execute(\"CREATE TABLE enhanced_lineup_flags AS SELECT * FROM enhanced_lineup_flags_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS enhanced_lineup_flags_temp\")\n",
    "\n",
    "            self._robust_drop_object(\"minutes_enhanced\")\n",
    "            self.conn.register(\"minutes_enhanced_temp\", enhanced_minutes_df)\n",
    "            self.conn.execute(\"CREATE TABLE minutes_enhanced AS SELECT * FROM minutes_enhanced_temp\")\n",
    "            self.conn.execute(\"DROP VIEW IF EXISTS minutes_enhanced_temp\")\n",
    "\n",
    "            # Store results with enhanced data summary\n",
    "            flag_totals = {\n",
    "                \"missing_sub_in\": len([f for f in flag_rows if f.get(\"flag_type\") == \"missing_sub_in\"]),\n",
    "                \"inactivity_periods\": len([f for f in flag_rows if f.get(\"flag_type\") == \"inactivity_periods\"]),\n",
    "                \"first_action_events\": enhanced_stats['first_action_injections'],\n",
    "                \"auto_out_events\": enhanced_stats['auto_outs_inactivity'],\n",
    "                \"lineup_violations\": len([f for f in enhanced_stats['flags']['lineup_violations']])\n",
    "            }\n",
    "\n",
    "            self.data_summary['enhanced_substitution_tracking'] = {\n",
    "                'state_rows': len(state_rows),\n",
    "                'flag_rows': len(flag_rows),\n",
    "                'flag_totals': flag_totals,\n",
    "                'validation_data': enhanced_validation_df,\n",
    "                'offenders_data': enhanced_offenders,\n",
    "                'flags': enhanced_stats['flags'],\n",
    "                'statistics': enhanced_stats\n",
    "            }\n",
    "\n",
    "            # Store validation data for minutes report\n",
    "            self.data_summary['minutes_validation_full'] = enhanced_validation_df.copy()\n",
    "            self.data_summary['minutes_offenders'] = enhanced_offenders.copy()\n",
    "\n",
    "            # Store debug summary for final report\n",
    "            self.data_summary['enhanced_substitution_debug'] = {\n",
    "                \"substitutions\": enhanced_stats['total_substitutions'],\n",
    "                \"first_actions\": enhanced_stats['first_action_injections'],\n",
    "                \"auto_outs\": enhanced_stats['auto_outs_inactivity'],\n",
    "                \"always_five_fixes\": enhanced_stats['lineup_size_corrections'],\n",
    "                \"validation\": {\n",
    "                    \"tolerance\": CFG[\"minutes_validation\"][\"tolerance_seconds\"],\n",
    "                    \"offenders\": len(enhanced_offenders),\n",
    "                    \"total_players\": len(enhanced_validation_df)\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Create database tables for flags\n",
    "            self._create_enhanced_flags_tables(enhanced_stats['flags'])\n",
    "\n",
    "            # Summary statistics with FLAGS\n",
    "            summary = {\n",
    "                'method': 'ENHANCED_WITH_FLAGS',\n",
    "                'processing_time': time.time() - start_time,\n",
    "                'substitutions': enhanced_stats,\n",
    "                'validation': {\n",
    "                    'total_players': len(enhanced_validation_df),\n",
    "                    'offenders': len(enhanced_offenders),\n",
    "                    'tolerance_seconds': CFG[\"minutes_validation\"][\"tolerance_seconds\"],\n",
    "                    'max_difference': enhanced_validation_df['abs_diff_seconds'].max() if not enhanced_validation_df.empty else 0\n",
    "                },\n",
    "                'flags_summary': {\n",
    "                    'missing_sub_ins': len(enhanced_stats['flags']['missing_sub_ins']),\n",
    "                    'inactivity_periods': len(enhanced_stats['flags']['inactivity_periods']),\n",
    "                    'lineup_violations': len(enhanced_stats['flags']['lineup_violations']),\n",
    "                    'first_action_events': len(enhanced_stats['flags']['first_action_events']),\n",
    "                    'auto_out_events': len(enhanced_stats['flags']['auto_out_events'])\n",
    "                }\n",
    "            }\n",
    "\n",
    "            logger.info(f\"[ENHANCED COMPLETE] {enhanced_stats['successful_substitutions']} subs, {enhanced_stats['first_action_injections']} first-actions, {enhanced_stats['auto_outs_inactivity']} auto-outs\")\n",
    "            logger.info(f\"[ENHANCED FLAGS] Missing sub-ins: {len(enhanced_stats['flags']['missing_sub_ins'])}, Inactivity periods: {len(enhanced_stats['flags']['inactivity_periods'])}\")\n",
    "\n",
    "            total_flags = sum(len(flag_list) for flag_list in enhanced_stats['flags'].values())\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Enhanced Substitution Tracking with Flags\",\n",
    "                passed=True,\n",
    "                details=f\"Enhanced tracking complete: {enhanced_stats['successful_substitutions']} subs, {enhanced_stats['first_action_injections']} first-actions, {total_flags} total flags\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in enhanced substitution tracking: {e}\")\n",
    "            import traceback\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return ValidationResult(\n",
    "                step_name=\"Enhanced Substitution Tracking with Flags\",\n",
    "                passed=False,\n",
    "                details=f\"Error in enhanced tracking: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _create_enhanced_flags_tables(self, flags_data: Dict[str, List]) -> None:\n",
    "        \"\"\"Create database tables for enhanced flags\"\"\"\n",
    "        try:\n",
    "            # Create comprehensive flags table\n",
    "            all_flags = []\n",
    "\n",
    "            for flag_type, flag_list in flags_data.items():\n",
    "                for flag in flag_list:\n",
    "                    flag_record = {\n",
    "                        'flag_type': flag_type,\n",
    "                        'time': flag.get('time', 0),\n",
    "                        'player_id': flag.get('player_id'),\n",
    "                        'player_name': flag.get('player_name'),\n",
    "                        'team': flag.get('team'),\n",
    "                        'description': str(flag)\n",
    "                    }\n",
    "                    all_flags.append(flag_record)\n",
    "\n",
    "            if all_flags:\n",
    "                flags_df = pd.DataFrame(all_flags)\n",
    "                self._robust_drop_object(\"enhanced_flags\")\n",
    "                self.conn.register('enhanced_flags_temp', flags_df)\n",
    "                self.conn.execute(\"CREATE TABLE enhanced_flags AS SELECT * FROM enhanced_flags_temp\")\n",
    "                self.conn.execute(\"DROP VIEW IF EXISTS enhanced_flags_temp\")\n",
    "\n",
    "                logger.info(f\"Created enhanced_flags table with {len(all_flags)} flag records\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create enhanced flags tables: {e}\")\n",
    "\n",
    "    def run_lineups_and_rim_analytics(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        CORRECTED substitution engine that systematically fixes all identified issues:\n",
    "\n",
    "        1. Prevents double-crediting of period remainder time\n",
    "        2. Properly implements Reed Sheppard first-action rule  \n",
    "        3. Fixes time tracking between events\n",
    "        4. Implements intelligent 2-minute inactivity auto-out\n",
    "        5. Maintains strict 5-man lineups with gap-filling logic\n",
    "\n",
    "        Key Fixes:\n",
    "        - Single responsibility for period remainder crediting\n",
    "        - Enhanced last name resolution for first actions\n",
    "        - Proper time segment tracking without overlaps\n",
    "        - Comprehensive debugging and validation\n",
    "        - Activity-based auto-out selection to prevent inappropriate removals\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        CFG = {\n",
    "            \"starter_reset_periods\": [1, 3],\n",
    "            \"one_direction\": {\n",
    "                \"appearance_via_last_name\": True,\n",
    "                \"remove_out_if_present\": True\n",
    "            },\n",
    "            \"msg_types\": {\n",
    "                \"shot_made\": 1, \"shot_missed\": 2, \"rebound\": 4,\n",
    "                \"turnover\": 5, \"foul\": 6, \"substitution\": 8\n",
    "            },\n",
    "            \"minutes_validation\": {\"tolerance_seconds\": 120},  # Reasonable tolerance\n",
    "            \"inactivity_rule\": {\"idle_seconds_threshold\": 120}\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Helper functions with better error handling\n",
    "            def _period_length_seconds(p: int) -> float:\n",
    "                return 720.0 if p <= 4 else 300.0\n",
    "\n",
    "            def _parse_game_clock(gc: str) -> float | None:\n",
    "                if not gc or not isinstance(gc, str):\n",
    "                    return None\n",
    "                s = gc.strip()\n",
    "                if s.count(\":\") != 1:\n",
    "                    return None\n",
    "                try:\n",
    "                    mm, ss = s.split(\":\")\n",
    "                    return float(mm) * 60.0 + float(ss)\n",
    "                except (ValueError, IndexError):\n",
    "                    return None\n",
    "\n",
    "            def _abs_time(period: int, rem_sec: float | None) -> float:\n",
    "                \"\"\"Calculate absolute game time elapsed\"\"\"\n",
    "                total = 0.0\n",
    "                for pi in range(1, period):\n",
    "                    total += _period_length_seconds(pi)\n",
    "                pl = _period_length_seconds(period)\n",
    "                if rem_sec is None:\n",
    "                    return total + pl  # If no clock, assume period end\n",
    "                return total + (pl - rem_sec)\n",
    "\n",
    "            # Load and validate core data\n",
    "            box_df = self.conn.execute(\"\"\"\n",
    "                SELECT player_id, player_name, team_id, team_abbrev, is_starter, seconds_played\n",
    "                FROM box_score\n",
    "                WHERE seconds_played > 0\n",
    "                ORDER BY team_id, seconds_played DESC\n",
    "            \"\"\").df()\n",
    "\n",
    "            if box_df.empty:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Enhanced Lineups & Rim Analytics\",\n",
    "                    passed=False,\n",
    "                    details=\"No players found in box_score with playing time\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            teams = sorted(box_df['team_id'].unique().tolist())\n",
    "            if len(teams) != 2:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Enhanced Lineups & Rim Analytics\",\n",
    "                    passed=False,\n",
    "                    details=f\"Expected exactly 2 teams, found {teams}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Build comprehensive player mappings\n",
    "            roster = {int(tid): set() for tid in teams}\n",
    "            starters = {int(tid): set() for tid in teams}\n",
    "            name_map, pteam_map = {}, {}\n",
    "            last_name_index = {}\n",
    "\n",
    "            for _, r in box_df.iterrows():\n",
    "                pid = int(r.player_id)\n",
    "                tid = int(r.team_id)\n",
    "                roster[tid].add(pid)\n",
    "                if bool(r.is_starter):\n",
    "                    starters[tid].add(pid)\n",
    "                name_map[pid] = str(r.player_name)\n",
    "                pteam_map[pid] = tid\n",
    "\n",
    "                # Enhanced last name indexing\n",
    "                full_name = str(r.player_name).strip()\n",
    "                last_name = full_name.split()[-1].lower()\n",
    "                first_name = full_name.split()[0].lower() if len(full_name.split()) > 1 else \"\"\n",
    "\n",
    "                last_name_index.setdefault(last_name, []).append(pid)\n",
    "                if first_name:\n",
    "                    last_name_index.setdefault(first_name, []).append(pid)\n",
    "\n",
    "            # Validate team structure\n",
    "            if any(len(starters[tid]) != 5 for tid in teams):\n",
    "                detail = {tid: [name_map[p] for p in sorted(starters[tid])] for tid in teams}\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Enhanced Lineups & Rim Analytics\",\n",
    "                    passed=False,\n",
    "                    details=f\"Invalid starters: {detail}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            team_abbrev_map = {int(tid): box_df[box_df.team_id == tid]['team_abbrev'].iloc[0] for tid in teams}\n",
    "\n",
    "            # Load events with validation\n",
    "            events = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    period, pbp_order, wall_clock_int,\n",
    "                    COALESCE(game_clock,'') AS game_clock,\n",
    "                    COALESCE(description,'') AS description,\n",
    "                    team_id_off, team_id_def, msg_type, action_type,\n",
    "                    player_id_1, player_id_2, player_id_3,\n",
    "                    NULLIF(last_name_1,'') AS last_name_1,\n",
    "                    NULLIF(last_name_2,'') AS last_name_2,\n",
    "                    NULLIF(last_name_3,'') AS last_name_3,\n",
    "                    COALESCE(points, 0) AS points\n",
    "                FROM pbp\n",
    "                ORDER BY period, pbp_order, wall_clock_int\n",
    "            \"\"\").df()\n",
    "\n",
    "            if events.empty:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Enhanced Lineups & Rim Analytics\",\n",
    "                    passed=False,\n",
    "                    details=\"No PBP events found\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Initialize state tracking\n",
    "            from collections import defaultdict, deque\n",
    "\n",
    "            # CORRECTED: Enhanced state tracking with comprehensive segment management\n",
    "            on_court = {tid: set(starters[tid]) for tid in teams}\n",
    "            last_action_time = defaultdict(lambda: 0.0)\n",
    "            recent_out = {tid: deque(maxlen=10) for tid in teams}\n",
    "\n",
    "            # CORRECTED: Enhanced segment tracking with validation\n",
    "            active_segments = {}  # player_id -> {'start': time, 'reason': str}\n",
    "            completed_segments = defaultdict(list)  # player_id -> [{'start': time, 'end': time, 'duration': dur, 'reason': str}]\n",
    "            period_end_times = {}  # Track when we last ended a period to prevent double-crediting\n",
    "\n",
    "            # Initialize segments for starters\n",
    "            for tid in teams:\n",
    "                for pid in on_court[tid]:\n",
    "                    active_segments[pid] = {'start': 0.0, 'reason': 'GAME_START'}\n",
    "                    last_action_time[pid] = 0.0\n",
    "\n",
    "            # Enhanced debugging\n",
    "            debug_events = []\n",
    "            sub_count = 0\n",
    "            first_action_count = 0\n",
    "            auto_out_count = 0\n",
    "            lineup_violation_fixes = 0\n",
    "            validation_errors = []\n",
    "\n",
    "            idle_thresh = CFG.get(\"inactivity_rule\", {}).get(\"idle_seconds_threshold\", 120)\n",
    "            tol = CFG[\"minutes_validation\"][\"tolerance_seconds\"]\n",
    "\n",
    "            def enhanced_name_resolution(ln: str | None, tid_hint: int | None) -> int | None:\n",
    "                \"\"\"Enhanced name resolution with fuzzy matching\"\"\"\n",
    "                if not ln:\n",
    "                    return None\n",
    "\n",
    "                ln_clean = str(ln).strip().lower()\n",
    "                candidates = last_name_index.get(ln_clean, [])\n",
    "\n",
    "                if not candidates:\n",
    "                    # Try partial matching\n",
    "                    for key, pids in last_name_index.items():\n",
    "                        if ln_clean in key or key in ln_clean:\n",
    "                            candidates.extend(pids)\n",
    "\n",
    "                if not candidates:\n",
    "                    return None\n",
    "\n",
    "                # Prefer team hint if available\n",
    "                if tid_hint is not None:\n",
    "                    for cand in candidates:\n",
    "                        if pteam_map.get(cand) == tid_hint:\n",
    "                            return cand\n",
    "\n",
    "                # Return first valid candidate\n",
    "                for cand in candidates:\n",
    "                    if pteam_map.get(cand) in teams:\n",
    "                        return cand\n",
    "\n",
    "                return None\n",
    "\n",
    "            def end_player_segment(pid: int, end_time: float, reason: str) -> None:\n",
    "                \"\"\"CORRECTED: End a player's active segment with validation\"\"\"\n",
    "                if pid not in active_segments:\n",
    "                    return  # No active segment to end\n",
    "\n",
    "                start_info = active_segments[pid]\n",
    "                start_time = start_info['start']\n",
    "\n",
    "                if end_time <= start_time:\n",
    "                    # Safety fix: use a minimum duration of 1 second\n",
    "                    logger.warning(f\"Invalid segment timing fixed: end_time {end_time} <= start_time {start_time} for {name_map.get(pid)}\")\n",
    "                    end_time = start_time + 1.0\n",
    "                    validation_errors.append(f\"Fixed invalid segment timing for {name_map.get(pid)}\")\n",
    "\n",
    "                duration = end_time - start_time\n",
    "                completed_segments[pid].append({\n",
    "                    'start': start_time,\n",
    "                    'end': end_time,\n",
    "                    'duration': duration,\n",
    "                    'reason': f\"{start_info['reason']} -> {reason}\"\n",
    "                })\n",
    "\n",
    "                del active_segments[pid]\n",
    "                logger.debug(f\"Ended segment for {name_map.get(pid)}: {duration:.1f}s ({reason})\")\n",
    "\n",
    "            def start_player_segment(pid: int, start_time: float, reason: str) -> None:\n",
    "                \"\"\"CORRECTED: Start a new segment with overlap prevention\"\"\"\n",
    "                if pid in active_segments:\n",
    "                    # End existing segment first to prevent overlaps\n",
    "                    end_player_segment(pid, start_time, f\"OVERLAP_{reason}\")\n",
    "\n",
    "                active_segments[pid] = {'start': start_time, 'reason': reason}\n",
    "                logger.debug(f\"Started segment for {name_map.get(pid)}: {start_time:.1f}s ({reason})\")\n",
    "\n",
    "            def handle_lineup_change(tid: int, current_time: float, reason: str) -> None:\n",
    "                \"\"\"CORRECTED: Handle lineup changes with proper segment management\"\"\"\n",
    "                team_name = team_abbrev_map[tid]\n",
    "\n",
    "                # Get all players who should be tracked for this team\n",
    "                team_players = [pid for pid in pteam_map if pteam_map.get(pid) == tid]\n",
    "\n",
    "                # End segments for players no longer on court\n",
    "                for pid in team_players:\n",
    "                    if pid in active_segments and pid not in on_court[tid]:\n",
    "                        end_player_segment(pid, current_time, f\"{reason}_OUT\")\n",
    "\n",
    "                # Start segments for new players on court\n",
    "                for pid in on_court[tid]:\n",
    "                    if pid not in active_segments:\n",
    "                        start_player_segment(pid, current_time, f\"{reason}_IN\")\n",
    "\n",
    "            def pick_auto_out_candidate(tid: int, current_time: float, exclude: set[int] = set()) -> int | None:\n",
    "                \"\"\"CORRECTED: Enhanced auto-out selection based on activity patterns\"\"\"\n",
    "                if not on_court[tid]:\n",
    "                    return None\n",
    "\n",
    "                candidates = [p for p in on_court[tid] if p not in exclude]\n",
    "                if not candidates:\n",
    "                    return None\n",
    "\n",
    "                # CORRECTED: Enhanced scoring system for auto-out selection\n",
    "                def activity_score(pid: int) -> tuple:\n",
    "                    idle_time = current_time - last_action_time[pid]\n",
    "\n",
    "                    # Primary factors (lower is better for removal):\n",
    "                    # 1. Idle time (higher idle = more likely to remove)\n",
    "                    # 2. Starter status (prefer to keep starters)\n",
    "                    # 3. Recent sub activity (avoid ping-ponging)\n",
    "\n",
    "                    is_starter = pid in starters[tid]\n",
    "                    recently_subbed = pid in recent_out[tid]\n",
    "\n",
    "                    # Score components (higher = more likely to be removed)\n",
    "                    idle_score = idle_time\n",
    "                    starter_penalty = -100 if is_starter else 0  # Keep starters longer\n",
    "                    recent_sub_penalty = -50 if recently_subbed else 0  # Avoid ping-pong\n",
    "\n",
    "                    total_score = idle_score + starter_penalty + recent_sub_penalty\n",
    "\n",
    "                    return (total_score, idle_time, pid)  # Use pid for deterministic tiebreaking\n",
    "\n",
    "                # Sort by activity score (highest score = best candidate for removal)\n",
    "                candidates.sort(key=activity_score, reverse=True)\n",
    "\n",
    "                best_candidate = candidates[0]\n",
    "                idle_time = current_time - last_action_time[best_candidate]\n",
    "\n",
    "                # Only auto-remove if idle >= threshold OR we have >5 players\n",
    "                if idle_time >= idle_thresh or len(on_court[tid]) > 5:\n",
    "                    return best_candidate\n",
    "\n",
    "                return None\n",
    "\n",
    "            def ensure_valid_lineup(tid: int, current_time: float, prefer_keep: set[int] = set()) -> None:\n",
    "                \"\"\"CORRECTED: Ensure team has exactly 5 players with better logic\"\"\"\n",
    "                team_name = team_abbrev_map[tid]\n",
    "                changes_made = False\n",
    "\n",
    "                # Remove excess players\n",
    "                while len(on_court[tid]) > 5:\n",
    "                    auto_out = pick_auto_out_candidate(tid, current_time, exclude=prefer_keep)\n",
    "                    if auto_out is None:\n",
    "                        logger.error(f\"Cannot auto-remove from {team_name} - no valid candidates\")\n",
    "                        validation_errors.append(f\"Cannot auto-remove from {team_name}\")\n",
    "                        break\n",
    "\n",
    "                    on_court[tid].remove(auto_out)\n",
    "                    recent_out[tid].append(auto_out)\n",
    "                    changes_made = True\n",
    "\n",
    "                    idle_time = current_time - last_action_time[auto_out]\n",
    "                    logger.info(f\"[AUTO-OUT] {name_map.get(auto_out)} from {team_name} (idle: {idle_time:.1f}s)\")\n",
    "\n",
    "                    debug_events.append({\n",
    "                        'time': current_time,\n",
    "                        'type': 'AUTO_OUT',\n",
    "                        'player': name_map.get(auto_out),\n",
    "                        'team': team_name,\n",
    "                        'idle_time': idle_time\n",
    "                    })\n",
    "\n",
    "                    nonlocal auto_out_count\n",
    "                    auto_out_count += 1\n",
    "\n",
    "                # Add players if under 5\n",
    "                if len(on_court[tid]) < 5:\n",
    "                    available = [p for p in roster[tid] if p not in on_court[tid]]\n",
    "                    if available:\n",
    "                        # CORRECTED: Prioritize based on game context\n",
    "                        def fill_priority(pid: int) -> tuple:\n",
    "                            # Priority factors (lower is better):\n",
    "                            # 1. Recently out (prefer recent subs)\n",
    "                            # 2. Starter status (prefer starters)  \n",
    "                            # 3. Recent activity (prefer active players)\n",
    "\n",
    "                            recently_out_priority = 0 if pid in recent_out[tid] else 1\n",
    "                            starter_priority = 0 if pid in starters[tid] else 1\n",
    "                            activity_priority = -(current_time - last_action_time[pid])  # More recent = lower number\n",
    "\n",
    "                            return (recently_out_priority, starter_priority, activity_priority)\n",
    "\n",
    "                        available.sort(key=fill_priority)\n",
    "\n",
    "                        needed = 5 - len(on_court[tid])\n",
    "                        for i in range(min(needed, len(available))):\n",
    "                            fill_player = available[i]\n",
    "                            on_court[tid].add(fill_player)\n",
    "                            changes_made = True\n",
    "                            logger.info(f\"[AUTO-IN] {name_map.get(fill_player)} to {team_name} (fill to 5)\")\n",
    "\n",
    "                # Update segments if changes were made\n",
    "                if changes_made:\n",
    "                    handle_lineup_change(tid, current_time, \"LINEUP_CORRECTION\")\n",
    "\n",
    "            def guard_always_five(current_time: float):\n",
    "                \"\"\"Fix any deviation from 5 and count it.\"\"\"\n",
    "                nonlocal lineup_violation_fixes\n",
    "                for tid in teams:\n",
    "                    if len(on_court[tid]) != 5:\n",
    "                        lineup_violation_fixes += 1\n",
    "                        ensure_valid_lineup(tid, current_time)\n",
    "\n",
    "            # MAIN PROCESSING LOOP\n",
    "            prev_period = None\n",
    "            prev_time = None\n",
    "\n",
    "            logger.info(f\"Starting lineup processing: {team_abbrev_map[teams[0]]} vs {team_abbrev_map[teams[1]]}\")\n",
    "\n",
    "            for idx, ev in events.iterrows():\n",
    "                period = int(ev.period)\n",
    "                clock_str = ev.game_clock\n",
    "                parsed_clock = _parse_game_clock(clock_str)\n",
    "                current_time = _abs_time(period, parsed_clock)\n",
    "                msg_type = int(ev.msg_type)\n",
    "\n",
    "                # CORRECTED: Handle period transitions without double-crediting\n",
    "                if period != prev_period and prev_period is not None:\n",
    "                    # Calculate period end time\n",
    "                    period_end_time = _abs_time(prev_period, 0.0)\n",
    "\n",
    "                    # Only credit period end time if we haven't already done so\n",
    "                    if prev_period not in period_end_times:\n",
    "                        # End all active segments at period end\n",
    "                        for pid in list(active_segments.keys()):\n",
    "                            end_player_segment(pid, period_end_time, f\"PERIOD_{prev_period}_END\")\n",
    "\n",
    "                        period_end_times[prev_period] = period_end_time\n",
    "                        logger.debug(f\"Ended period {prev_period} at {period_end_time:.1f}s\")\n",
    "\n",
    "                # Initialize new period\n",
    "                if period != prev_period:\n",
    "                    if period in CFG[\"starter_reset_periods\"]:\n",
    "                        # Reset to starters\n",
    "                        on_court = {tid: set(starters[tid]) for tid in teams}\n",
    "                        logger.info(f\"[PERIOD {period}] Reset to starters\")\n",
    "                    else:\n",
    "                        # Continue previous lineups\n",
    "                        logger.info(f\"[PERIOD {period}] Continue lineups\")\n",
    "                        for tid in teams:\n",
    "                            ensure_valid_lineup(tid, current_time)\n",
    "\n",
    "                    # Start new segments for all on-court players\n",
    "                    for tid in teams:\n",
    "                        for pid in on_court[tid]:\n",
    "                            if pid not in active_segments:\n",
    "                                start_player_segment(pid, current_time, f\"PERIOD_{period}_START\")\n",
    "\n",
    "                    prev_period = period\n",
    "\n",
    "                # SUBSTITUTION PROCESSING\n",
    "                if msg_type == CFG[\"msg_types\"][\"substitution\"]:\n",
    "                    sub_count += 1\n",
    "\n",
    "                    out_pid = int(ev.player_id_1) if not pd.isna(ev.player_id_1) else None\n",
    "                    in_pid = int(ev.player_id_2) if not pd.isna(ev.player_id_2) else None\n",
    "                    out_ln, in_ln = ev.last_name_1, ev.last_name_2\n",
    "\n",
    "                    # Determine team\n",
    "                    sub_tid = None\n",
    "                    if in_pid and in_pid in pteam_map:\n",
    "                        sub_tid = pteam_map[in_pid]\n",
    "                    elif out_pid and out_pid in pteam_map:\n",
    "                        sub_tid = pteam_map[out_pid]\n",
    "                    elif pd.notna(ev.team_id_off) and int(ev.team_id_off) in teams:\n",
    "                        sub_tid = int(ev.team_id_off)\n",
    "\n",
    "                    # Enhanced name resolution\n",
    "                    if in_pid is None and CFG[\"one_direction\"][\"appearance_via_last_name\"] and in_ln:\n",
    "                        in_pid = enhanced_name_resolution(in_ln, sub_tid)\n",
    "                    if out_pid is None and out_ln:\n",
    "                        out_pid = enhanced_name_resolution(out_ln, sub_tid)\n",
    "\n",
    "                    if sub_tid is not None:\n",
    "                        team_name = team_abbrev_map[sub_tid]\n",
    "\n",
    "                        # Process OUT first\n",
    "                        if CFG[\"one_direction\"][\"remove_out_if_present\"] and out_pid and out_pid in on_court[sub_tid]:\n",
    "                            on_court[sub_tid].remove(out_pid)\n",
    "                            recent_out[sub_tid].append(out_pid)\n",
    "                            logger.info(f\"[SUB-OUT] {name_map.get(out_pid)} from {team_name}\")\n",
    "\n",
    "                        # Process IN\n",
    "                        if in_pid and in_pid not in on_court[sub_tid]:\n",
    "                            # Make room if needed\n",
    "                            if len(on_court[sub_tid]) >= 5:\n",
    "                                auto_out = pick_auto_out_candidate(sub_tid, current_time, exclude={in_pid})\n",
    "                                if auto_out:\n",
    "                                    on_court[sub_tid].remove(auto_out)\n",
    "                                    recent_out[sub_tid].append(auto_out)\n",
    "                                    logger.info(f\"[MAKE-ROOM] {name_map.get(auto_out)} out for {name_map.get(in_pid)}\")\n",
    "\n",
    "                            on_court[sub_tid].add(in_pid)\n",
    "                            logger.info(f\"[SUB-IN] {name_map.get(in_pid)} to {team_name}\")\n",
    "\n",
    "                        # Update segments for substitution\n",
    "                        handle_lineup_change(sub_tid, current_time, \"SUBSTITUTION\")\n",
    "\n",
    "                    # Update activity times\n",
    "                    if out_pid:\n",
    "                        last_action_time[out_pid] = current_time\n",
    "                    if in_pid:\n",
    "                        last_action_time[in_pid] = current_time\n",
    "\n",
    "                # FIRST ACTION PROCESSING (Reed Sheppard rule)\n",
    "                elif msg_type in [1, 2, 4, 5, 6] and CFG[\"one_direction\"][\"appearance_via_last_name\"]:\n",
    "                    action_pid = int(ev.player_id_1) if not pd.isna(ev.player_id_1) else None\n",
    "                    action_ln = ev.last_name_1\n",
    "                    action_tid = None\n",
    "\n",
    "                    if action_pid and action_pid in pteam_map:\n",
    "                        action_tid = pteam_map[action_pid]\n",
    "                    elif pd.notna(ev.team_id_off) and int(ev.team_id_off) in teams:\n",
    "                        action_tid = int(ev.team_id_off)\n",
    "\n",
    "                    # Resolve via last name if needed\n",
    "                    if action_pid is None and action_ln:\n",
    "                        action_pid = enhanced_name_resolution(action_ln, action_tid)\n",
    "                        if action_pid:\n",
    "                            action_tid = pteam_map[action_pid]\n",
    "\n",
    "                    # CORRECTED: Apply Reed Sheppard rule with proper time tracking\n",
    "                    if action_tid in teams and action_pid and action_pid not in on_court[action_tid]:\n",
    "                        # This is a first action - inject player\n",
    "                        on_court[action_tid].add(action_pid)\n",
    "                        first_action_count += 1\n",
    "\n",
    "                        logger.info(f\"[FIRST-ACTION] {name_map.get(action_pid)} -> {team_abbrev_map[action_tid]} (msg: {msg_type})\")\n",
    "\n",
    "                        debug_events.append({\n",
    "                            'time': current_time,\n",
    "                            'type': 'FIRST_ACTION',\n",
    "                            'player': name_map.get(action_pid),\n",
    "                            'team': team_abbrev_map[action_tid],\n",
    "                            'action': ev.description\n",
    "                        })\n",
    "\n",
    "                        # CORRECTED: Start segment for first-action player\n",
    "                        start_player_segment(action_pid, current_time, \"FIRST_ACTION\")\n",
    "\n",
    "                        # Ensure valid lineup after injection\n",
    "                        ensure_valid_lineup(action_tid, current_time, prefer_keep={action_pid})\n",
    "\n",
    "                    # Update activity time\n",
    "                    if action_pid:\n",
    "                        last_action_time[action_pid] = current_time\n",
    "\n",
    "                # after each event: enforce always-5\n",
    "                guard_always_five(current_time)\n",
    "\n",
    "                prev_time = current_time\n",
    "\n",
    "            # CORRECTED: Final processing - end all remaining segments\n",
    "            # Use the last actual event time or calculate proper game end time\n",
    "            if prev_period and current_time:\n",
    "                final_time = current_time\n",
    "            else:\n",
    "                final_time = 2880.0  # 48 minutes total\n",
    "\n",
    "            for pid in list(active_segments.keys()):\n",
    "                end_player_segment(pid, final_time, \"GAME_END\")\n",
    "\n",
    "            # CORRECTED: Calculate final minutes with comprehensive validation\n",
    "            calculated_minutes = {}\n",
    "            for pid in set(list(completed_segments.keys()) + list(box_df['player_id'])):\n",
    "                segments = completed_segments[pid]\n",
    "                total_seconds = sum(seg['duration'] for seg in segments)\n",
    "                calculated_minutes[pid] = total_seconds\n",
    "\n",
    "            # Build validation results\n",
    "            mv = []\n",
    "            for pid in set(list(calculated_minutes.keys()) + list(box_df['player_id'])):\n",
    "                calc_secs = calculated_minutes.get(pid, 0.0)\n",
    "                box_row = box_df[box_df['player_id'] == pid]\n",
    "                box_secs = float(box_row['seconds_played'].iloc[0]) if not box_row.empty else 0.0\n",
    "                diff = calc_secs - box_secs\n",
    "\n",
    "                mv.append({\n",
    "                    \"player_id\": pid,\n",
    "                    \"player_name\": name_map.get(pid, f\"ID_{pid}\"),\n",
    "                    \"team\": team_abbrev_map.get(pteam_map.get(pid), \"UNK\"),\n",
    "                    \"calc_seconds\": round(calc_secs, 1),\n",
    "                    \"box_seconds\": round(box_secs, 1),\n",
    "                    \"abs_diff_seconds\": round(abs(diff), 1),\n",
    "                    \"segments_count\": len(completed_segments.get(pid, []))\n",
    "                })\n",
    "\n",
    "            mv_df = pd.DataFrame(mv).sort_values([\"team\", \"player_name\"]).reset_index(drop=True)\n",
    "            offenders = mv_df[mv_df[\"abs_diff_seconds\"] > tol]\n",
    "\n",
    "            # Enhanced logging for validation\n",
    "            if len(offenders) > 0:\n",
    "                logger.warning(f\"CORRECTED: Minutes validation: {len(offenders)} players exceed {tol}s tolerance\")\n",
    "                for _, row in offenders.iterrows():\n",
    "                    logger.warning(f\"  {row.player_name} ({row.team}): calc={row.calc_seconds}s vs box={row.box_seconds}s (diff={row.abs_diff_seconds}s)\")\n",
    "\n",
    "                    # Debug segment details for offenders\n",
    "                    segments = completed_segments.get(row.player_id, [])\n",
    "                    logger.debug(f\"    Segments for {row.player_name}: {len(segments)} total\")\n",
    "                    for i, seg in enumerate(segments[:5]):  # Show first 5 segments\n",
    "                        logger.debug(f\"      {i+1}: {seg['duration']:.1f}s ({seg['reason']})\")\n",
    "\n",
    "            if validation_errors:\n",
    "                logger.warning(f\"Validation errors encountered: {len(validation_errors)}\")\n",
    "                for error in validation_errors[:5]:\n",
    "                    logger.warning(f\"  {error}\")\n",
    "\n",
    "            # Store enhanced debug data\n",
    "            self.data_summary['enhanced_substitution_debug'] = {\n",
    "                'total_events': len(events),\n",
    "                'substitutions': sub_count,\n",
    "                'first_actions': first_action_count,\n",
    "                'auto_outs': auto_out_count,\n",
    "                'always_five_fixes': lineup_violation_fixes,\n",
    "                'validation_errors': len(validation_errors),\n",
    "                'validation': {\n",
    "                    'total_players': len(mv_df),\n",
    "                    'offenders': len(offenders),\n",
    "                    'tolerance': tol,\n",
    "                    'max_difference': mv_df['abs_diff_seconds'].max() if not mv_df.empty else 0\n",
    "                }\n",
    "            }\n",
    "            # also stash full minutes table for the report writer\n",
    "            self.data_summary['minutes_validation_full'] = mv_df\n",
    "            self.data_summary['minutes_offenders'] = offenders\n",
    "\n",
    "            logger.info(f\"SUBSTITUTION SUMMARY: {sub_count} subs, {first_action_count} first-actions, {auto_out_count} auto-outs\")\n",
    "\n",
    "            # IMPORTANT: non-fatal -> passed = True (warnings carry the issues)\n",
    "            details = (f\"Enhanced engine: {len(events)} events, {sub_count} subs, {first_action_count} first-actions. \"\n",
    "                       f\"Validation: {len(offenders)}/{len(mv_df)} offenders; 5-on-floor fixes: {lineup_violation_fixes}\")\n",
    "            return ValidationResult(\n",
    "                step_name=\"Enhanced Lineups & Rim Analytics\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=[] if len(offenders)==0 else [f\"{len(offenders)} players exceed {tol}s tolerance\"]\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            logger.error(f\"Exception in corrected substitution engine: {e}\")\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return ValidationResult(\n",
    "                step_name=\"Enhanced Lineups & Rim Analytics\",\n",
    "                passed=False,\n",
    "                details=f\"Error in corrected engine: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def debug_segment_analysis(self, player_id: int = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced debugging function to analyze segment calculation for specific players.\n",
    "        Useful for diagnosing Reed Sheppard cases and other timing issues.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'data_summary') or 'enhanced_substitution_debug' not in self.data_summary:\n",
    "            return {\"error\": \"No enhanced substitution data available\"}\n",
    "\n",
    "        debug_data = self.data_summary['enhanced_substitution_debug']\n",
    "\n",
    "        # If specific player requested, focus on them\n",
    "        if player_id:\n",
    "            return self._analyze_player_segments(player_id)\n",
    "\n",
    "        # Otherwise provide overall analysis\n",
    "        return {\n",
    "            'summary': debug_data,\n",
    "            'recommendations': self._generate_debug_recommendations(debug_data)\n",
    "        }\n",
    "\n",
    "    def _analyze_player_segments(self, player_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"Detailed analysis for a specific player's segments\"\"\"\n",
    "        return {\n",
    "            'player_id': player_id,\n",
    "            'analysis': 'Detailed segment analysis would go here',\n",
    "            'recommendations': []\n",
    "        }\n",
    "\n",
    "    def _generate_debug_recommendations(self, debug_data: Dict) -> List[str]:\n",
    "        \"\"\"Generate recommendations based on debug data\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        validation = debug_data.get('validation', {})\n",
    "        offenders = validation.get('offenders', 0)\n",
    "        max_diff = validation.get('max_difference', 0)\n",
    "\n",
    "        if offenders > 0:\n",
    "            recommendations.append(f\"Still have {offenders} players with timing issues\")\n",
    "\n",
    "        if max_diff > 300:  # 5 minutes\n",
    "            recommendations.append(\"Large timing discrepancies detected - check period transitions\")\n",
    "        elif max_diff > 120:  # 2 minutes  \n",
    "            recommendations.append(\"Moderate timing issues - check first-action logic\")\n",
    "\n",
    "        if debug_data.get('validation_errors', 0) > 0:\n",
    "            recommendations.append(\"Segment validation errors detected - check overlap prevention\")\n",
    "\n",
    "        auto_outs = debug_data.get('auto_outs', 0)\n",
    "        if auto_outs > 20:\n",
    "            recommendations.append(\"High auto-out count - may indicate lineup instability\")\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def create_minutes_validation_report(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a detailed validation report for minutes calculation.\n",
    "        Useful for verifying the corrected engine performance.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'data_summary'):\n",
    "            return \"No validation data available\"\n",
    "\n",
    "        # Get validation data from enhanced runs if available\n",
    "        enhanced_data = self.data_summary.get('enhanced_substitution_debug', {})\n",
    "\n",
    "        report_lines = [\n",
    "            \"MINUTES VALIDATION REPORT\",\n",
    "            \"=\" * 50,\n",
    "            \"\"\n",
    "        ]\n",
    "\n",
    "        if enhanced_data:\n",
    "            validation = enhanced_data.get('validation', {})\n",
    "            report_lines.extend([\n",
    "                \"ENHANCED ENGINE RESULTS:\",\n",
    "                f\"Total players: {validation.get('total_players', 0)}\",\n",
    "                f\"Players exceeding tolerance: {validation.get('offenders', 0)}\",\n",
    "                f\"Maximum difference: {validation.get('max_difference', 0):.1f}s\",\n",
    "                f\"Tolerance threshold: {validation.get('tolerance', 120)}s\",\n",
    "                f\"5-on-floor fixes: {enhanced_data.get('always_five_fixes', 0)}\",\n",
    "                \"\"\n",
    "            ])\n",
    "\n",
    "            # Add recommendations\n",
    "            recommendations = self._generate_debug_recommendations(enhanced_data)\n",
    "            if recommendations:\n",
    "                report_lines.extend([\n",
    "                    \"RECOMMENDATIONS:\",\n",
    "                    *[f\"- {rec}\" for rec in recommendations],\n",
    "                    \"\"\n",
    "                ])\n",
    "\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "    def debug_substitution_analysis(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Comprehensive debugger to identify substitution and minutes calculation issues.\n",
    "\n",
    "        This function analyzes:\n",
    "        1. Reed Sheppard's specific case and similar players\n",
    "        2. Minutes calculation discrepancies\n",
    "        3. Substitution event patterns\n",
    "        4. Missing first-action events\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"🔍 STARTING COMPREHENSIVE SUBSTITUTION ANALYSIS\")\n",
    "        logger.info(\"=\" * 80)\n",
    "\n",
    "        analysis_results = {\n",
    "            'reed_sheppard_analysis': {},\n",
    "            'minutes_discrepancies': {},\n",
    "            'substitution_patterns': {},\n",
    "            'first_action_missing': {},\n",
    "            'timeline_analysis': {}\n",
    "        }\n",
    "\n",
    "        # Get all data we need\n",
    "        box_df = self.conn.execute(\"\"\"\n",
    "            SELECT player_id, player_name, team_id, team_abbrev, is_starter, seconds_played\n",
    "            FROM box_score WHERE seconds_played > 0 ORDER BY team_id, seconds_played DESC\n",
    "        \"\"\").df()\n",
    "\n",
    "        events_df = self.conn.execute(\"\"\"\n",
    "            SELECT period, pbp_order, wall_clock_int, game_clock, description,\n",
    "                   team_id_off, team_id_def, msg_type, action_type,\n",
    "                   player_id_1, player_id_2, player_id_3,\n",
    "                   last_name_1, last_name_2, last_name_3, points\n",
    "            FROM pbp ORDER BY period, pbp_order, wall_clock_int\n",
    "        \"\"\").df()\n",
    "\n",
    "        # Create name mappings\n",
    "        name_map = dict(zip(box_df['player_id'], box_df['player_name']))\n",
    "        team_map = dict(zip(box_df['player_id'], box_df['team_abbrev']))\n",
    "\n",
    "        logger.info(f\"📊 Analyzing {len(events_df)} events for {len(box_df)} players\")\n",
    "\n",
    "        # 1. REED SHEPPARD SPECIFIC ANALYSIS\n",
    "        logger.info(\"\\n🎯 REED SHEPPARD CASE ANALYSIS\")\n",
    "        logger.info(\"-\" * 50)\n",
    "\n",
    "        reed_sheppard_id = 1642263  # From the provided data\n",
    "        if reed_sheppard_id in name_map:\n",
    "            reed_events = []\n",
    "\n",
    "            # Find all events involving Reed Sheppard\n",
    "            for _, ev in events_df.iterrows():\n",
    "                if (ev['player_id_1'] == reed_sheppard_id or \n",
    "                    ev['player_id_2'] == reed_sheppard_id or \n",
    "                    ev['player_id_3'] == reed_sheppard_id or\n",
    "                    'sheppard' in str(ev['description']).lower()):\n",
    "\n",
    "                    reed_events.append({\n",
    "                        'period': ev['period'],\n",
    "                        'game_clock': ev['game_clock'],\n",
    "                        'msg_type': ev['msg_type'],\n",
    "                        'description': ev['description'],\n",
    "                        'player_1': ev['player_id_1'],\n",
    "                        'player_2': ev['player_id_2'],\n",
    "                        'player_3': ev['player_id_3'],\n",
    "                        'last_name_1': ev['last_name_1']\n",
    "                    })\n",
    "\n",
    "            logger.info(f\"Reed Sheppard events found: {len(reed_events)}\")\n",
    "            for i, event in enumerate(reed_events):\n",
    "                logger.info(f\"  Event {i+1}: Q{event['period']} {event['game_clock']} | {event['description']}\")\n",
    "                logger.info(f\"    MsgType: {event['msg_type']}, Players: {event['player_1']}, {event['player_2']}, {event['player_3']}\")\n",
    "\n",
    "            analysis_results['reed_sheppard_analysis'] = {\n",
    "                'total_events': len(reed_events),\n",
    "                'events': reed_events,\n",
    "                'box_minutes': box_df[box_df['player_id'] == reed_sheppard_id]['seconds_played'].iloc[0] if reed_sheppard_id in box_df['player_id'].values else 0\n",
    "            }\n",
    "\n",
    "        # 2. FIND PLAYERS WITH FIRST ACTIONS BUT NO SUB-IN\n",
    "        logger.info(\"\\n🚨 PLAYERS WITH ACTIONS BUT NO SUB-IN\")\n",
    "        logger.info(\"-\" * 50)\n",
    "\n",
    "        # Get all substitution events\n",
    "        sub_events = events_df[events_df['msg_type'] == 8].copy()\n",
    "\n",
    "        # Get all players who sub IN\n",
    "        subbed_in_players = set()\n",
    "        for _, sub in sub_events.iterrows():\n",
    "            if pd.notna(sub['player_id_2']):\n",
    "                subbed_in_players.add(int(sub['player_id_2']))\n",
    "\n",
    "        # Get starters\n",
    "        starters = set(box_df[box_df['is_starter'] == True]['player_id'].tolist())\n",
    "\n",
    "        # Find players with actions but never subbed in (and not starters)\n",
    "        action_events = events_df[events_df['msg_type'].isin([1, 2, 4, 5, 6])].copy()\n",
    "\n",
    "        players_with_actions = set()\n",
    "        for _, ev in action_events.iterrows():\n",
    "            for col in ['player_id_1', 'player_id_2', 'player_id_3']:\n",
    "                if pd.notna(ev[col]):\n",
    "                    players_with_actions.add(int(ev[col]))\n",
    "\n",
    "        # Players who have actions but no sub-in and aren't starters\n",
    "        missing_sub_in = players_with_actions - subbed_in_players - starters\n",
    "\n",
    "        logger.info(f\"Players with actions but no sub-in: {len(missing_sub_in)}\")\n",
    "        for pid in missing_sub_in:\n",
    "            if pid in name_map:\n",
    "                logger.info(f\"  {name_map[pid]} (ID: {pid}) - {team_map.get(pid, 'Unknown team')}\")\n",
    "\n",
    "                # Find their first action\n",
    "                first_action = None\n",
    "                for _, ev in action_events.iterrows():\n",
    "                    if (ev['player_id_1'] == pid or ev['player_id_2'] == pid or ev['player_id_3'] == pid):\n",
    "                        first_action = ev\n",
    "                        break\n",
    "\n",
    "                if first_action is not None:\n",
    "                    logger.info(f\"    First action: Q{first_action['period']} {first_action['game_clock']} | {first_action['description']}\")\n",
    "\n",
    "        analysis_results['first_action_missing'] = {\n",
    "            'count': len(missing_sub_in),\n",
    "            'players': [{'id': pid, 'name': name_map.get(pid), 'team': team_map.get(pid)} for pid in missing_sub_in if pid in name_map]\n",
    "        }\n",
    "\n",
    "        # 3. SUBSTITUTION PATTERN ANALYSIS\n",
    "        logger.info(\"\\n🔄 SUBSTITUTION PATTERN ANALYSIS\")\n",
    "        logger.info(\"-\" * 50)\n",
    "\n",
    "        sub_analysis = {\n",
    "            'total_subs': len(sub_events),\n",
    "            'subs_with_both_players': 0,\n",
    "            'subs_with_only_in': 0,\n",
    "            'subs_with_only_out': 0,\n",
    "            'subs_with_neither': 0\n",
    "        }\n",
    "\n",
    "        for _, sub in sub_events.iterrows():\n",
    "            has_out = pd.notna(sub['player_id_1'])\n",
    "            has_in = pd.notna(sub['player_id_2'])\n",
    "\n",
    "            if has_out and has_in:\n",
    "                sub_analysis['subs_with_both_players'] += 1\n",
    "            elif has_in and not has_out:\n",
    "                sub_analysis['subs_with_only_in'] += 1\n",
    "            elif has_out and not has_in:\n",
    "                sub_analysis['subs_with_only_out'] += 1\n",
    "            else:\n",
    "                sub_analysis['subs_with_neither'] += 1\n",
    "\n",
    "        logger.info(f\"Total substitutions: {sub_analysis['total_subs']}\")\n",
    "        logger.info(f\"  Both players: {sub_analysis['subs_with_both_players']}\")\n",
    "        logger.info(f\"  Only IN player: {sub_analysis['subs_with_only_in']}\")\n",
    "        logger.info(f\"  Only OUT player: {sub_analysis['subs_with_only_out']}\")\n",
    "        logger.info(f\"  Neither player: {sub_analysis['subs_with_neither']}\")\n",
    "\n",
    "        analysis_results['substitution_patterns'] = sub_analysis\n",
    "\n",
    "        # 4. MINUTES CALCULATION SIMULATION\n",
    "        logger.info(\"\\n⏱️ MINUTES CALCULATION SIMULATION\")\n",
    "        logger.info(\"-\" * 50)\n",
    "\n",
    "        def parse_game_clock(gc):\n",
    "            if not gc or not isinstance(gc, str):\n",
    "                return None\n",
    "            try:\n",
    "                if ':' in gc:\n",
    "                    mm, ss = gc.split(':')\n",
    "                    return float(mm) * 60.0 + float(ss)\n",
    "            except:\n",
    "                pass\n",
    "            return None\n",
    "\n",
    "        def abs_time(period, rem_sec):\n",
    "            total = 0.0\n",
    "            for p in range(1, period):\n",
    "                total += 720.0 if p <= 4 else 300.0\n",
    "            period_length = 720.0 if period <= 4 else 300.0\n",
    "            if rem_sec is None:\n",
    "                return total\n",
    "            return total + (period_length - rem_sec)\n",
    "\n",
    "        # Simulate simple starter minutes (baseline)\n",
    "        baseline_minutes = {}\n",
    "        starters_per_team = {}\n",
    "\n",
    "        for team in box_df['team_id'].unique():\n",
    "            team_starters = box_df[(box_df['team_id'] == team) & (box_df['is_starter'] == True)]['player_id'].tolist()\n",
    "            starters_per_team[team] = team_starters\n",
    "\n",
    "            # Assume starters play full quarters 1 and 3, then continue in 2 and 4\n",
    "            for pid in team_starters:\n",
    "                baseline_minutes[pid] = 2 * 720.0  # Q1 + Q3 = 24 minutes baseline\n",
    "\n",
    "        logger.info(f\"Baseline starter minutes (Q1+Q3 only): {sum(baseline_minutes.values())/60:.1f} total minutes\")\n",
    "\n",
    "        # Find actual box score total\n",
    "        actual_total = box_df['seconds_played'].sum()\n",
    "        logger.info(f\"Actual box score total: {actual_total/60:.1f} minutes\")\n",
    "        logger.info(f\"Expected game total: {48*10:.1f} minutes (48 min × 10 players)\")\n",
    "\n",
    "        analysis_results['minutes_discrepancies'] = {\n",
    "            'baseline_total': sum(baseline_minutes.values()),\n",
    "            'actual_total': actual_total,\n",
    "            'expected_total': 48 * 60 * 10,\n",
    "            'baseline_vs_actual_diff': actual_total - sum(baseline_minutes.values())\n",
    "        }\n",
    "\n",
    "        # 5. TIMELINE ANALYSIS\n",
    "        logger.info(\"\\n📈 TIMELINE ANALYSIS\")\n",
    "        logger.info(\"-\" * 50)\n",
    "\n",
    "        timeline = []\n",
    "        for _, ev in events_df.iterrows():\n",
    "            if ev['msg_type'] == 8:  # Substitutions\n",
    "                timeline.append({\n",
    "                    'time': abs_time(ev['period'], parse_game_clock(ev['game_clock'])),\n",
    "                    'period': ev['period'],\n",
    "                    'clock': ev['game_clock'],\n",
    "                    'event_type': 'SUB',\n",
    "                    'description': ev['description']\n",
    "                })\n",
    "            elif ev['msg_type'] in [1, 2, 4, 5, 6] and ev['player_id_1'] in missing_sub_in:\n",
    "                timeline.append({\n",
    "                    'time': abs_time(ev['period'], parse_game_clock(ev['game_clock'])),\n",
    "                    'period': ev['period'],\n",
    "                    'clock': ev['game_clock'],\n",
    "                    'event_type': 'MISSING_PLAYER_ACTION',\n",
    "                    'player': name_map.get(ev['player_id_1'], f\"ID_{ev['player_id_1']}\"),\n",
    "                    'description': ev['description']\n",
    "                })\n",
    "\n",
    "        timeline.sort(key=lambda x: x['time'])\n",
    "\n",
    "        logger.info(\"Key timeline events:\")\n",
    "        for event in timeline[:20]:  # First 20 events\n",
    "            if event['event_type'] == 'SUB':\n",
    "                logger.info(f\"  {event['time']:>6.1f}s Q{event['period']} {event['clock']} | SUB: {event['description']}\")\n",
    "            else:\n",
    "                logger.info(f\"  {event['time']:>6.1f}s Q{event['period']} {event['clock']} | MISSING: {event['player']} | {event['description']}\")\n",
    "\n",
    "        analysis_results['timeline_analysis'] = timeline[:50]  # Store first 50 for reference\n",
    "\n",
    "        logger.info(\"\\n✅ ANALYSIS COMPLETE\")\n",
    "        logger.info(\"=\" * 80)\n",
    "\n",
    "        return analysis_results\n",
    "\n",
    "    def debug_minutes_tracker(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create a detailed minute-by-minute tracker to identify exactly where minutes are being miscalculated.\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "\n",
    "        logger.info(\"🔍 CREATING DETAILED MINUTES TRACKER\")\n",
    "\n",
    "        # This will track every single second of every player's time\n",
    "        minute_tracker = {\n",
    "            'player_segments': defaultdict(list),\n",
    "            'period_summaries': {},\n",
    "            'discrepancies': {},\n",
    "            'debug_log': []\n",
    "        }\n",
    "\n",
    "        def log_debug(message):\n",
    "            minute_tracker['debug_log'].append(message)\n",
    "            logger.debug(message)\n",
    "\n",
    "        # Get data\n",
    "        box_df = self.conn.execute(\"\"\"\n",
    "            SELECT player_id, player_name, team_id, team_abbrev, is_starter, seconds_played\n",
    "            FROM box_score WHERE seconds_played > 0 ORDER BY team_id, seconds_played DESC\n",
    "        \"\"\").df()\n",
    "\n",
    "        events_df = self.conn.execute(\"\"\"\n",
    "            SELECT period, pbp_order, wall_clock_int, game_clock, description,\n",
    "                   team_id_off, team_id_def, msg_type, action_type,\n",
    "                   player_id_1, player_id_2, player_id_3,\n",
    "                   last_name_1, last_name_2, last_name_3\n",
    "            FROM pbp ORDER BY period, pbp_order, wall_clock_int\n",
    "        \"\"\").df()\n",
    "\n",
    "        name_map = dict(zip(box_df['player_id'], box_df['player_name']))\n",
    "        team_map = dict(zip(box_df['player_id'], box_df['team_id']))\n",
    "\n",
    "        # Initialize with starters\n",
    "        teams = sorted(box_df['team_id'].unique())\n",
    "        starters = {team: set(box_df[(box_df['team_id'] == team) & (box_df['is_starter'] == True)]['player_id'].tolist()) for team in teams}\n",
    "\n",
    "        current_lineups = {team: set(starters[team]) for team in teams}\n",
    "\n",
    "        log_debug(f\"Initial lineups: {current_lineups}\")\n",
    "\n",
    "        def parse_clock(gc):\n",
    "            if not gc or not isinstance(gc, str) or ':' not in gc:\n",
    "                return None\n",
    "            try:\n",
    "                mm, ss = gc.split(':')\n",
    "                return float(mm) * 60.0 + float(ss)\n",
    "            except:\n",
    "                pass\n",
    "            return None\n",
    "\n",
    "        def abs_time(period, rem_sec):\n",
    "            total = sum(720.0 if p <= 4 else 300.0 for p in range(1, period))\n",
    "            if rem_sec is None:\n",
    "                return total\n",
    "            period_length = 720.0 if period <= 4 else 300.0\n",
    "            return total + (period_length - rem_sec)\n",
    "\n",
    "        # Track time\n",
    "        prev_abs_time = 0.0\n",
    "        prev_period = 0\n",
    "\n",
    "        for idx, ev in events_df.iterrows():\n",
    "            period = int(ev['period'])\n",
    "            curr_clock = parse_clock(ev['game_clock'])\n",
    "            curr_abs_time = abs_time(period, curr_clock)\n",
    "\n",
    "            # Handle period transitions\n",
    "            if period != prev_period:\n",
    "                if prev_period > 0:\n",
    "                    # Credit end of previous period\n",
    "                    period_end_time = abs_time(prev_period, 0.0)\n",
    "                    if period_end_time > prev_abs_time:\n",
    "                        duration = period_end_time - prev_abs_time\n",
    "                        for team in teams:\n",
    "                            for pid in current_lineups[team]:\n",
    "                                minute_tracker['player_segments'][pid].append({\n",
    "                                    'start': prev_abs_time,\n",
    "                                    'end': period_end_time,\n",
    "                                    'duration': duration,\n",
    "                                    'reason': f'PERIOD_{prev_period}_END'\n",
    "                                })\n",
    "                        log_debug(f\"Period {prev_period} end: credited {duration:.1f}s to {sum(len(current_lineups[t]) for t in teams)} players\")\n",
    "\n",
    "                # Reset or continue lineups for new period\n",
    "                if period in [1, 3]:  # Starter reset periods\n",
    "                    current_lineups = {team: set(starters[team]) for team in teams}\n",
    "                    log_debug(f\"Period {period}: Reset to starters\")\n",
    "                else:\n",
    "                    log_debug(f\"Period {period}: Continue lineups\")\n",
    "\n",
    "                prev_period = period\n",
    "\n",
    "            # Credit time between events\n",
    "            if curr_abs_time > prev_abs_time and prev_abs_time > 0:\n",
    "                duration = curr_abs_time - prev_abs_time\n",
    "                players_credited = 0\n",
    "                for team in teams:\n",
    "                    for pid in current_lineups[team]:\n",
    "                        minute_tracker['player_segments'][pid].append({\n",
    "                            'start': prev_abs_time,\n",
    "                            'end': curr_abs_time,\n",
    "                            'duration': duration,\n",
    "                            'reason': f'PERIOD_{period}_PLAY'\n",
    "                        })\n",
    "                        players_credited += 1\n",
    "\n",
    "                if duration > 60:  # Log significant time gaps\n",
    "                    log_debug(f\"Large time gap: {duration:.1f}s credited to {players_credited} players\")\n",
    "\n",
    "            # Handle substitutions\n",
    "            if ev['msg_type'] == 8:\n",
    "                out_pid = int(ev['player_id_1']) if pd.notna(ev['player_id_1']) else None\n",
    "                in_pid = int(ev['player_id_2']) if pd.notna(ev['player_id_2']) else None\n",
    "\n",
    "                # Find which team this substitution is for\n",
    "                sub_team = None\n",
    "                if in_pid and in_pid in team_map:\n",
    "                    sub_team = team_map[in_pid]\n",
    "                elif out_pid and out_pid in team_map:\n",
    "                    sub_team = team_map[out_pid]\n",
    "\n",
    "                if sub_team:\n",
    "                    if out_pid and out_pid in current_lineups[sub_team]:\n",
    "                        current_lineups[sub_team].remove(out_pid)\n",
    "                        log_debug(f\"SUB OUT: {name_map.get(out_pid, out_pid)} from team {sub_team}\")\n",
    "\n",
    "                    if in_pid and in_pid not in current_lineups[sub_team]:\n",
    "                        current_lineups[sub_team].add(in_pid)\n",
    "                        log_debug(f\"SUB IN: {name_map.get(in_pid, in_pid)} to team {sub_team}\")\n",
    "\n",
    "            prev_abs_time = curr_abs_time\n",
    "\n",
    "        # Final period end\n",
    "        if prev_period > 0:\n",
    "            final_end = abs_time(prev_period, 0.0)\n",
    "            if final_end > prev_abs_time:\n",
    "                duration = final_end - prev_abs_time\n",
    "                for team in teams:\n",
    "                    for pid in current_lineups[team]:\n",
    "                        minute_tracker['player_segments'][pid].append({\n",
    "                            'start': prev_abs_time,\n",
    "                            'end': final_end,\n",
    "                            'duration': duration,\n",
    "                            'reason': f'PERIOD_{prev_period}_FINAL_END'\n",
    "                        })\n",
    "                log_debug(f\"Final period end: credited {duration:.1f}s\")\n",
    "\n",
    "        # Calculate totals and compare\n",
    "        for pid in minute_tracker['player_segments']:\n",
    "            calculated_total = sum(seg['duration'] for seg in minute_tracker['player_segments'][pid])\n",
    "            box_total = box_df[box_df['player_id'] == pid]['seconds_played'].iloc[0] if pid in box_df['player_id'].values else 0\n",
    "            diff = calculated_total - box_total\n",
    "\n",
    "            minute_tracker['discrepancies'][pid] = {\n",
    "                'calculated': calculated_total,\n",
    "                'box_score': box_total,\n",
    "                'difference': diff,\n",
    "                'segments_count': len(minute_tracker['player_segments'][pid]),\n",
    "                'player_name': name_map.get(pid, f\"ID_{pid}\")\n",
    "            }\n",
    "\n",
    "        return minute_tracker\n",
    "\n",
    "    def test_reed_sheppard_case(self):\n",
    "        \"\"\"Test function to verify Reed Sheppard case is handled correctly\"\"\"\n",
    "        # Check if Reed Sheppard (ID: 1642263) appears in events\n",
    "        reed_events = self.conn.execute(\"\"\"\n",
    "            SELECT period, game_clock, description, msg_type,\n",
    "                   player_id_1, player_id_2, player_id_3,\n",
    "                   last_name_1, last_name_2, last_name_3\n",
    "            FROM pbp \n",
    "            WHERE player_id_1 = 1642263 \n",
    "               OR player_id_2 = 1642263 \n",
    "               OR player_id_3 = 1642263\n",
    "               OR LOWER(description) LIKE '%sheppard%'\n",
    "            ORDER BY period, pbp_order\n",
    "        \"\"\").df()\n",
    "\n",
    "        print(f\"Reed Sheppard events: {len(reed_events)}\")\n",
    "        for _, ev in reed_events.iterrows():\n",
    "            print(f\"  Q{ev['period']} {ev['game_clock']} | {ev['description']}\")\n",
    "\n",
    "        return reed_events\n",
    "\n",
    "    def create_missing_player_report(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Summarize PBP-only players with names, inferred team, confidence, first/last seen, and event breakdown.\n",
    "\n",
    "        Debug-first policy:\n",
    "        - Do NOT hide missing data via COALESCE in the final outputs. Expose raw + resolved columns.\n",
    "        - Add preflight checks and log actual row counts of intermediates (pbp_only_players, occ, sums).\n",
    "        - Rebuild _pbp_names in this scope if it is not present to avoid hidden coupling.\n",
    "        - Dump the FULL report (all columns, all rows) and schema to the logs when done.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # --- Preconditions: required base tables/views must exist ---\n",
    "            need_tables = [\"pbp\", \"dim_players\", \"dim_teams\"]\n",
    "            for t in need_tables:\n",
    "                exists = self.conn.execute(\n",
    "                    f\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = '{t}'\"\n",
    "                ).fetchone()[0]\n",
    "                if exists == 0:\n",
    "                    return ValidationResult(\n",
    "                        step_name=\"Missing Player Report\",\n",
    "                        passed=False,\n",
    "                        details=f\"Missing required table: {t}\",\n",
    "                        processing_time=time.time() - start_time\n",
    "                    )\n",
    "\n",
    "            # pbp_only_players is created in create_dimensions()\n",
    "            has_pbp_only = self.conn.execute(\n",
    "                \"SELECT COUNT(*) FROM information_schema.views WHERE table_name = 'pbp_only_players'\"\n",
    "            ).fetchone()[0]\n",
    "            if has_pbp_only == 0:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Missing Player Report\",\n",
    "                    passed=False,\n",
    "                    details=\"pbp_only_players view not found. Run create_dimensions() first.\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # --- Ensure _pbp_names exists (recreate locally if absent) ---\n",
    "            has_pbp_names = self.conn.execute(\n",
    "                \"SELECT COUNT(*) FROM information_schema.views WHERE table_name = '_pbp_names'\"\n",
    "            ).fetchone()[0]\n",
    "            if has_pbp_names == 0:\n",
    "                logger.info(\"[Missing Player Report] _pbp_names not found; rebuilding TEMP view locally\")\n",
    "                self.conn.execute(\"\"\"\n",
    "                    CREATE OR REPLACE TEMP VIEW _pbp_names AS\n",
    "                    WITH p1 AS (\n",
    "                        SELECT player_id_1 AS player_id, ANY_VALUE(NULLIF(last_name_1,'')) AS last_name\n",
    "                        FROM pbp\n",
    "                        WHERE player_id_1 IS NOT NULL\n",
    "                        GROUP BY player_id_1\n",
    "                    ),\n",
    "                    p2 AS (\n",
    "                        SELECT player_id_2 AS player_id, ANY_VALUE(NULLIF(last_name_2,'')) AS last_name\n",
    "                        FROM pbp\n",
    "                        WHERE player_id_2 IS NOT NULL\n",
    "                        GROUP BY player_id_2\n",
    "                    ),\n",
    "                    p3 AS (\n",
    "                        SELECT player_id_3 AS player_id, ANY_VALUE(NULLIF(last_name_3,'')) AS last_name\n",
    "                        FROM pbp\n",
    "                        WHERE player_id_3 IS NOT NULL\n",
    "                        GROUP BY player_id_3\n",
    "                    ),\n",
    "                    unioned AS (\n",
    "                        SELECT * FROM p1\n",
    "                        UNION ALL\n",
    "                        SELECT * FROM p2\n",
    "                        UNION ALL\n",
    "                        SELECT * FROM p3\n",
    "                    )\n",
    "                    SELECT player_id, ANY_VALUE(last_name) AS last_name\n",
    "                    FROM unioned\n",
    "                    WHERE last_name IS NOT NULL\n",
    "                    GROUP BY player_id\n",
    "                \"\"\")\n",
    "            else:\n",
    "                logger.info(\"[Missing Player Report] Reusing existing _pbp_names TEMP view\")\n",
    "\n",
    "            # Rebuild team guess confidence (same logic as earlier)\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE OR REPLACE TEMP VIEW _team_guess_conf AS\n",
    "                WITH occ AS (\n",
    "                    SELECT player_id_1 AS player_id, team_id_off AS team_id FROM pbp WHERE player_id_1 IS NOT NULL\n",
    "                    UNION ALL SELECT player_id_2, team_id_off FROM pbp WHERE player_id_2 IS NOT NULL\n",
    "                    UNION ALL SELECT player_id_3, team_id_off FROM pbp WHERE player_id_3 IS NOT NULL\n",
    "                    UNION ALL SELECT player_id_1, team_id_def FROM pbp WHERE player_id_1 IS NOT NULL\n",
    "                    UNION ALL SELECT player_id_2, team_id_def FROM pbp WHERE player_id_2 IS NOT NULL\n",
    "                    UNION ALL SELECT player_id_3, team_id_def FROM pbp WHERE player_id_3 IS NOT NULL\n",
    "                ),\n",
    "                agg AS (\n",
    "                    SELECT player_id, team_id, COUNT(*) AS c\n",
    "                    FROM occ\n",
    "                    GROUP BY player_id, team_id\n",
    "                ),\n",
    "                totals AS (\n",
    "                    SELECT player_id, SUM(c) AS tot\n",
    "                    FROM agg\n",
    "                    GROUP BY player_id\n",
    "                ),\n",
    "                ranked AS (\n",
    "                    SELECT\n",
    "                        a.player_id, a.team_id, a.c, t.tot,\n",
    "                        ROW_NUMBER() OVER (PARTITION BY a.player_id ORDER BY a.c DESC, a.team_id) AS rn\n",
    "                    FROM agg a\n",
    "                    JOIN totals t USING(player_id)\n",
    "                )\n",
    "                SELECT\n",
    "                    player_id,\n",
    "                    team_id AS guessed_team_id,\n",
    "                    c AS guessed_count,\n",
    "                    tot,\n",
    "                    (c::DOUBLE)/NULLIF(tot,0) AS team_confidence\n",
    "                FROM ranked\n",
    "                WHERE rn = 1\n",
    "            \"\"\")\n",
    "\n",
    "            # --- Preflight debug: how many pbp-only players? ---\n",
    "            num_only = self.conn.execute(\"SELECT COUNT(*) FROM pbp_only_players\").fetchone()[0]\n",
    "            logger.info(f\"[Missing Player Report] pbp_only_players count = {num_only}\")\n",
    "\n",
    "            # --- Build the report table (JOIN sums as `s`) ---\n",
    "            self._robust_drop_object(\"missing_player_report\")\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE TABLE missing_player_report AS\n",
    "                WITH occ AS (\n",
    "                    SELECT\n",
    "                        o.player_id,\n",
    "                        p.period,\n",
    "                        p.pbp_order,\n",
    "                        p.wall_clock_int,\n",
    "                        p.game_clock,\n",
    "                        p.description,\n",
    "                        p.msg_type,\n",
    "                        p.points\n",
    "                    FROM pbp_only_players o\n",
    "                    LEFT JOIN pbp p\n",
    "                    ON o.player_id = p.player_id_1\n",
    "                    OR o.player_id = p.player_id_2\n",
    "                    OR o.player_id = p.player_id_3\n",
    "                ),\n",
    "                sums AS (\n",
    "                    SELECT\n",
    "                        player_id,\n",
    "                        COUNT(*) AS total_events,\n",
    "                        SUM(points) AS points,\n",
    "                        SUM(CASE WHEN msg_type IN (1,2) THEN 1 ELSE 0 END) AS shot_events,\n",
    "                        SUM(CASE WHEN msg_type = 1 THEN 1 ELSE 0 END) AS made_fg,\n",
    "                        SUM(CASE WHEN msg_type = 2 THEN 1 ELSE 0 END) AS missed_fg,\n",
    "                        SUM(CASE WHEN msg_type = 3 THEN 1 ELSE 0 END) AS free_throws,\n",
    "                        SUM(CASE WHEN msg_type = 4 THEN 1 ELSE 0 END) AS rebounds,\n",
    "                        SUM(CASE WHEN msg_type = 5 THEN 1 ELSE 0 END) AS turnovers,\n",
    "                        SUM(CASE WHEN msg_type = 6 THEN 1 ELSE 0 END) AS fouls,\n",
    "                        SUM(CASE WHEN msg_type = 8 THEN 1 ELSE 0 END) AS substitutions,\n",
    "                        arg_min(CONCAT('Q', period, ' ', game_clock, ' | ', description), wall_clock_int) AS first_event,\n",
    "                        arg_max(CONCAT('Q', period, ' ', game_clock, ' | ', description), wall_clock_int) AS last_event\n",
    "                    FROM occ\n",
    "                    GROUP BY player_id\n",
    "                )\n",
    "                SELECT\n",
    "                    -- identity\n",
    "                    o.player_id,\n",
    "\n",
    "                    -- names: show raw sources + resolved (do NOT hide missing in raw columns)\n",
    "                    dp.player_name                           AS box_player_name,\n",
    "                    n.last_name                              AS pbp_last_name,\n",
    "                    CASE\n",
    "                        WHEN dp.player_name IS NOT NULL THEN dp.player_name\n",
    "                        WHEN n.last_name   IS NOT NULL THEN n.last_name\n",
    "                        ELSE CAST(o.player_id AS VARCHAR)\n",
    "                    END                                      AS resolved_name,\n",
    "\n",
    "                    -- team IDs & labels: expose raw + guessed + resolved\n",
    "                    dp.team_id                               AS box_team_id,\n",
    "                    tgc.guessed_team_id                      AS guessed_team_id,\n",
    "                    tgc.team_confidence                      AS team_confidence,\n",
    "                    CASE\n",
    "                        WHEN dp.team_id IS NOT NULL THEN dp.team_id\n",
    "                        WHEN tgc.guessed_team_id IS NOT NULL THEN tgc.guessed_team_id\n",
    "                        ELSE NULL\n",
    "                    END                                      AS resolved_team_id,\n",
    "\n",
    "                    dt_res.team_abbrev                       AS resolved_team_abbrev,\n",
    "                    dt_box.team_abbrev                       AS box_team_abbrev,\n",
    "                    dt_guess.team_abbrev                     AS guessed_team_abbrev,\n",
    "\n",
    "                    -- sample text from pbp_only_players\n",
    "                    o.sample_event,\n",
    "\n",
    "                    -- event rollups from sums\n",
    "                    s.total_events,\n",
    "                    s.points,\n",
    "                    s.shot_events,\n",
    "                    s.made_fg,\n",
    "                    s.missed_fg,\n",
    "                    s.free_throws,\n",
    "                    s.rebounds,\n",
    "                    s.turnovers,\n",
    "                    s.fouls,\n",
    "                    s.substitutions,\n",
    "                    s.first_event,\n",
    "                    s.last_event\n",
    "\n",
    "                FROM pbp_only_players o\n",
    "                LEFT JOIN dim_players dp           ON o.player_id = dp.player_id\n",
    "                LEFT JOIN _pbp_names n             ON o.player_id = n.player_id\n",
    "                LEFT JOIN _team_guess_conf tgc     ON o.player_id = tgc.player_id\n",
    "                LEFT JOIN sums s                   ON o.player_id = s.player_id\n",
    "                LEFT JOIN dim_teams dt_res         ON CASE\n",
    "                                                        WHEN dp.team_id IS NOT NULL THEN dp.team_id\n",
    "                                                        ELSE tgc.guessed_team_id\n",
    "                                                    END = dt_res.team_id\n",
    "                LEFT JOIN dim_teams dt_box         ON dp.team_id = dt_box.team_id\n",
    "                LEFT JOIN dim_teams dt_guess       ON tgc.guessed_team_id = dt_guess.team_id\n",
    "                ORDER BY o.player_id\n",
    "            \"\"\")\n",
    "\n",
    "            # --- Postflight debug metrics (surface issues instead of masking) ---\n",
    "            n_rows = self.conn.execute(\"SELECT COUNT(*) FROM missing_player_report\").fetchone()[0]\n",
    "            logger.info(f\"[Missing Player Report] built with {n_rows} rows\")\n",
    "\n",
    "            n_no_sums = self.conn.execute(\"SELECT COUNT(*) FROM missing_player_report WHERE total_events IS NULL\").fetchone()[0]\n",
    "            if n_no_sums > 0:\n",
    "                logger.warning(f\"[Missing Player Report] {n_no_sums} row(s) have NULL total_events (no matching occ/sums)\")\n",
    "\n",
    "            n_no_team = self.conn.execute(\"SELECT COUNT(*) FROM missing_player_report WHERE resolved_team_id IS NULL\").fetchone()[0]\n",
    "            if n_no_team > 0:\n",
    "                logger.warning(f\"[Missing Player Report] {n_no_team} row(s) missing resolved_team_id\")\n",
    "\n",
    "            n_no_name = self.conn.execute(\"SELECT COUNT(*) FROM missing_player_report WHERE resolved_name IS NULL\").fetchone()[0]\n",
    "            if n_no_name > 0:\n",
    "                logger.warning(f\"[Missing Player Report] {n_no_name} row(s) missing resolved_name\")\n",
    "\n",
    "            # --- NEW: Dump the FULL report & schema to the output (no truncation) ---\n",
    "            try:\n",
    "                df = self.conn.execute(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM missing_player_report\n",
    "                    ORDER BY player_id\n",
    "                \"\"\").df()\n",
    "\n",
    "                # Print schema (column name + dtype)\n",
    "                schema_lines = [f\"  - {col}: {str(df[col].dtype)}\" for col in df.columns]\n",
    "                logger.info(\"\\n\" + \"=\"*80 + \"\\nMISSING PLAYER REPORT — SCHEMA\\n\" + \"=\"*80 + \"\\n\" + \"\\n\".join(schema_lines))\n",
    "\n",
    "                # Print full table without truncation\n",
    "                with pd.option_context('display.max_columns', None, 'display.max_colwidth', None, 'display.width', 10000):\n",
    "                    table_str = df.to_string(index=False)\n",
    "                logger.info(\"\\n\" + \"=\"*80 + \"\\nMISSING PLAYER REPORT — FULL DATA\\n\" + \"=\"*80 + f\"\\nrows: {len(df)}\\n\" + table_str + \"\\n\" + \"=\"*80)\n",
    "\n",
    "            except Exception as dump_e:\n",
    "                logger.warning(f\"[Missing Player Report] Could not print full report to logs: {dump_e}\")\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Missing Player Report\",\n",
    "                passed=True,\n",
    "                details=f\"Built missing_player_report with {n_rows} rows\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Missing Player Report\",\n",
    "                passed=False,\n",
    "                details=f\"Error building report: {e}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "def load_all_data_enhanced(data_dir: Path | None = None, db_path: str = \"mavs_enhanced.duckdb\") -> Tuple[bool, Dict[str, Any]]:\n",
    "    \"\"\"Enhanced data loading with comprehensive validation + analytics outputs\"\"\"\n",
    "    print(\"NBA Pipeline - Enhanced Data Loading & Validation\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results = {\n",
    "        'success': True,\n",
    "        'validation_results': [],\n",
    "        'data_summary': {}\n",
    "    }\n",
    "\n",
    "    # Prefer config-managed data directory if not provided\n",
    "    if data_dir is None:\n",
    "        try:\n",
    "            from utils.config import MAVS_DATA_DIR\n",
    "            data_dir = MAVS_DATA_DIR\n",
    "        except Exception:\n",
    "            data_dir = Path('data/mavs_data_engineer_2025')\n",
    "\n",
    "    with EnhancedNBADataLoader(db_path) as loader:\n",
    "\n",
    "        # 1) Box\n",
    "        box_result = loader.load_and_validate_box_score(data_dir / 'box_HOU-DAL.csv')\n",
    "        loader.validator.log_validation(box_result)\n",
    "        results['validation_results'].append(box_result)\n",
    "        if not box_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 2) PBP\n",
    "        pbp_result = loader.load_and_validate_pbp(data_dir / 'pbp_HOU-DAL.csv')\n",
    "        loader.validator.log_validation(pbp_result)\n",
    "        results['validation_results'].append(pbp_result)\n",
    "        if not pbp_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 3) Relationships\n",
    "        rel_result = loader.validate_data_relationships()\n",
    "        loader.validator.log_validation(rel_result)\n",
    "        results['validation_results'].append(rel_result)\n",
    "        if not rel_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 4) Lookups\n",
    "        lookup_result = loader.create_lookup_views()\n",
    "        loader.validator.log_validation(lookup_result)\n",
    "        results['validation_results'].append(lookup_result)\n",
    "        if not lookup_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 5) Dimensions\n",
    "        dims_result = loader.create_dimensions()\n",
    "        loader.validator.log_validation(dims_result)\n",
    "        results['validation_results'].append(dims_result)\n",
    "        if not dims_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 6) Enriched view\n",
    "        enrich_result = loader.create_pbp_enriched_view()\n",
    "        loader.validator.log_validation(enrich_result)\n",
    "        results['validation_results'].append(enrich_result)\n",
    "        if not enrich_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 6.3) UPDATED: Traditional Data-Driven Substitution Tracking (replaces basic)\n",
    "        traditional_result = loader.run_traditional_data_driven_lineups()\n",
    "        loader.validator.log_validation(traditional_result)\n",
    "        results['validation_results'].append(traditional_result)\n",
    "        if not traditional_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 6.4) Enhanced substitution tracking with comprehensive flagging\n",
    "        enhanced_result = loader.run_enhanced_substitution_tracking_with_flags()\n",
    "        loader.validator.log_validation(enhanced_result)\n",
    "        results['validation_results'].append(enhanced_result)\n",
    "        if not enhanced_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 6.5) Missing player report (optional but useful)\n",
    "        missing_result = loader.create_missing_player_report()\n",
    "        loader.validator.log_validation(missing_result)\n",
    "        results['validation_results'].append(missing_result)\n",
    "        if not missing_result.passed:\n",
    "            results['success'] = False\n",
    "            return results['success'], results\n",
    "\n",
    "        # 7) Enhanced estimation engine (projects 1 & 2) - original method\n",
    "        analytics_result = loader.run_lineups_and_rim_analytics()\n",
    "        loader.validator.log_validation(analytics_result)\n",
    "        results['validation_results'].append(analytics_result)\n",
    "\n",
    "        # 7.5) UPDATED: Comprehensive comparison: Traditional Data-Driven vs Enhanced vs Box\n",
    "        compare_result = loader.compare_traditional_vs_enhanced_lineups()\n",
    "        loader.validator.log_validation(compare_result)\n",
    "        results['validation_results'].append(compare_result)\n",
    "\n",
    "        # 7.6) Legacy comparison (for backward compatibility)\n",
    "        legacy_compare_result = loader.compare_basic_vs_estimated_lineups()\n",
    "        loader.validator.log_validation(legacy_compare_result)\n",
    "        results['validation_results'].append(legacy_compare_result)\n",
    "\n",
    "        # 7.7) NEW: Dataset compliance validation\n",
    "        compliance_result = loader.validate_dataset_compliance()\n",
    "        loader.validator.log_validation(compliance_result)\n",
    "        results['validation_results'].append(compliance_result)\n",
    "\n",
    "        # 7.8) NEW: Create final submission artifacts\n",
    "        submission_result = loader.create_project_submission_artifacts()\n",
    "        loader.validator.log_validation(submission_result)\n",
    "        results['validation_results'].append(submission_result)\n",
    "\n",
    "        # 8) Final report\n",
    "        report_result = loader.write_final_report()\n",
    "        loader.validator.log_validation(report_result)\n",
    "        results['validation_results'].append(report_result)\n",
    "\n",
    "        # Summary\n",
    "        results['data_summary'] = loader.data_summary\n",
    "        loader.print_enhanced_summary()\n",
    "        success = loader.validator.print_validation_summary()\n",
    "        results['success'] = success\n",
    "        return success, results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = Path('data/mavs_data_engineer_2025')\n",
    "    database_path = \"mavs_enhanced.duckdb\"\n",
    "\n",
    "    success, results = load_all_data_enhanced(data_directory, database_path)\n",
    "\n",
    "    if success:\n",
    "        print(\"\\n✅ Enhanced data loading completed successfully\")\n",
    "        print(\"🎯 Ready for entity extraction and lineup analysis\")\n",
    "    else:\n",
    "        print(\"\\n❌ Enhanced data loading failed\")\n",
    "        print(\"🔧 Review validation messages above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47c7e4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/data/nba_entities_extractor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/data/nba_entities_extractor.py\n",
    "# Robust NBA Entity Extractor - Step 3 Improvements  \n",
    "\"\"\"\n",
    "NBA Pipeline - Robust Entity Extraction with Proper Validation\n",
    "==============================================================\n",
    "\n",
    "This robust extractor handles the identified issues:\n",
    "1. Uses actual starter data from box score (gs=1)\n",
    "2. Handles missing players transparently \n",
    "3. Creates proper team mappings\n",
    "4. Validates entity completeness without hiding issues\n",
    "\n",
    "Key Features:\n",
    "- Extract exactly 5 starters per team from box score\n",
    "- Handle team mapping correctly (HOU: 1610612745, DAL: 1610612742)\n",
    "- Create canonical player and team entities\n",
    "- Transparent validation and error reporting\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Set\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure we're in the right directory\n",
    "cwd = os.getcwd()\n",
    "if not cwd.endswith(\"airflow_project\"):\n",
    "    os.chdir('api/src/airflow_project')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from eda.utils.nba_pipeline_analysis import NBADataValidator, ValidationResult\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class GameEntities:\n",
    "    \"\"\"Container for all canonical game entities\"\"\"\n",
    "    unique_players: pd.DataFrame = None\n",
    "    starters: Dict[str, List[Dict]] = field(default_factory=dict)\n",
    "    team_mapping: Dict[int, str] = field(default_factory=dict)\n",
    "    game_info: Dict = field(default_factory=dict)\n",
    "\n",
    "    def validate_completeness(self) -> List[str]:\n",
    "        \"\"\"Validate all entities are present and complete\"\"\"\n",
    "        errors = []\n",
    "\n",
    "        if self.unique_players is None or len(self.unique_players) == 0:\n",
    "            errors.append(\"unique_players is empty\")\n",
    "\n",
    "        if len(self.starters) == 0:\n",
    "            errors.append(\"No starters defined\")\n",
    "\n",
    "        if len(self.team_mapping) == 0:\n",
    "            errors.append(\"No team mapping defined\")\n",
    "\n",
    "        return errors\n",
    "\n",
    "class RobustEntityExtractor:\n",
    "    \"\"\"Extract and validate canonical entities from NBA data\"\"\"\n",
    "\n",
    "    def __init__(self, db_path: str):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self.validator = NBADataValidator()\n",
    "        self.entities = GameEntities()\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = duckdb.connect(self.db_path)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "    def _robust_drop_object(self, object_name: str) -> None:\n",
    "        \"\"\"Robustly drop any DuckDB object regardless of type (same as Step 2)\"\"\"\n",
    "        try:\n",
    "            self.conn.execute(f\"DROP TABLE IF EXISTS {object_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            self.conn.execute(f\"DROP VIEW IF EXISTS {object_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            self.conn.execute(f\"DROP SEQUENCE IF EXISTS {object_name}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _get_object_type(self, object_name: str) -> Optional[str]:\n",
    "        \"\"\"Get the type of a DuckDB object if it exists\"\"\"\n",
    "        try:\n",
    "            result = self.conn.execute(f\"\"\"\n",
    "                SELECT table_type \n",
    "                FROM information_schema.tables \n",
    "                WHERE table_name = '{object_name}'\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if result:\n",
    "                return result[0][0]\n",
    "                \n",
    "            # Check views separately\n",
    "            result = self.conn.execute(f\"\"\"\n",
    "                SELECT 'VIEW' as table_type\n",
    "                FROM information_schema.views \n",
    "                WHERE table_name = '{object_name}'\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if result:\n",
    "                return 'VIEW'\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Could not check object type for {object_name}: {e}\")\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def extract_unique_players(self) -> ValidationResult:\n",
    "        \"\"\"Extract all unique active players with validation\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Extracting unique players from box score...\")\n",
    "\n",
    "            # Get all active players from box score\n",
    "            players_query = \"\"\"\n",
    "            SELECT \n",
    "                player_id,\n",
    "                player_name,\n",
    "                team_id,\n",
    "                team_abbrev,\n",
    "                is_home,\n",
    "                is_starter,\n",
    "                seconds_played,\n",
    "                points,\n",
    "                rebounds,\n",
    "                assists,\n",
    "                jersey_number\n",
    "            FROM box_score\n",
    "            WHERE seconds_played > 0\n",
    "            ORDER BY team_id, seconds_played DESC\n",
    "            \"\"\"\n",
    "\n",
    "            self.entities.unique_players = self.conn.execute(players_query).df()\n",
    "\n",
    "            if len(self.entities.unique_players) == 0:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Extract Unique Players\",\n",
    "                    passed=False,\n",
    "                    details=\"No players found in box score\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            warnings = []\n",
    "\n",
    "            # Validate player data quality\n",
    "            null_names = self.entities.unique_players['player_name'].isnull().sum()\n",
    "            if null_names > 0:\n",
    "                warnings.append(f\"{null_names} players have null names\")\n",
    "\n",
    "            # Validate team distribution\n",
    "            team_counts = self.entities.unique_players['team_abbrev'].value_counts()\n",
    "            for team, count in team_counts.items():\n",
    "                if count < 8:\n",
    "                    warnings.append(f\"Team {team} has only {count} players (minimum 8 expected)\")\n",
    "                elif count > 20:\n",
    "                    warnings.append(f\"Team {team} has {count} players (unusually high)\")\n",
    "\n",
    "            # Check for duplicate player IDs\n",
    "            duplicate_players = self.entities.unique_players['player_id'].duplicated().sum()\n",
    "            if duplicate_players > 0:\n",
    "                warnings.append(f\"{duplicate_players} duplicate player IDs found\")\n",
    "                # Remove duplicates, keeping first occurrence\n",
    "                self.entities.unique_players = self.entities.unique_players.drop_duplicates(\n",
    "                    subset=['player_id'], keep='first'\n",
    "                )\n",
    "\n",
    "            details = f\"Extracted {len(self.entities.unique_players)} players across {len(team_counts)} teams\"\n",
    "            details += f\". Team distribution: {dict(team_counts)}\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Unique Players\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(self.entities.unique_players),\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Unique Players\",\n",
    "                passed=False,\n",
    "                details=f\"Error extracting players: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def extract_starters(self) -> ValidationResult:\n",
    "        \"\"\"Extract starting lineups with strict validation\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Extracting starting lineups...\")\n",
    "\n",
    "            # Get starters from box score (gs=1 as specified in requirements)\n",
    "            starters_query = \"\"\"\n",
    "            SELECT \n",
    "                team_id,\n",
    "                team_abbrev,\n",
    "                player_id,\n",
    "                player_name,\n",
    "                jersey_number,\n",
    "                seconds_played,\n",
    "                points\n",
    "            FROM box_score\n",
    "            WHERE is_starter = TRUE\n",
    "            ORDER BY team_abbrev, seconds_played DESC\n",
    "            \"\"\"\n",
    "\n",
    "            starters_df = self.conn.execute(starters_query).df()\n",
    "\n",
    "            if len(starters_df) == 0:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Extract Starters\",\n",
    "                    passed=False,\n",
    "                    details=\"No starters found in box score\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            warnings = []\n",
    "            teams_with_issues = []\n",
    "\n",
    "            # Process starters by team\n",
    "            for team_abbrev in starters_df['team_abbrev'].unique():\n",
    "                team_starters = starters_df[starters_df['team_abbrev'] == team_abbrev]\n",
    "\n",
    "                # Validate exactly 5 starters per team (as specified in requirements)\n",
    "                if len(team_starters) != 5:\n",
    "                    warnings.append(f\"Team {team_abbrev} has {len(team_starters)} starters (expected 5)\")\n",
    "                    teams_with_issues.append(team_abbrev)\n",
    "\n",
    "                    # If we don't have exactly 5, try to fix it\n",
    "                    if len(team_starters) < 5:\n",
    "                        # Get additional players from the same team\n",
    "                        additional_players = self.conn.execute(f\"\"\"\n",
    "                            SELECT player_id, player_name, jersey_number, seconds_played, points\n",
    "                            FROM box_score \n",
    "                            WHERE team_abbrev = '{team_abbrev}' \n",
    "                            AND is_starter = FALSE\n",
    "                            AND seconds_played > 0\n",
    "                            ORDER BY seconds_played DESC\n",
    "                            LIMIT {5 - len(team_starters)}\n",
    "                        \"\"\").df()\n",
    "\n",
    "                        if len(additional_players) > 0:\n",
    "                            # Add team info to additional players\n",
    "                            team_info = team_starters.iloc[0][['team_id', 'team_abbrev']]\n",
    "                            for col in ['team_id', 'team_abbrev']:\n",
    "                                additional_players[col] = team_info[col]\n",
    "\n",
    "                            # Combine with existing starters\n",
    "                            team_starters = pd.concat([team_starters, additional_players], ignore_index=True)\n",
    "                            warnings.append(f\"Added {len(additional_players)} non-starters to {team_abbrev} lineup\")\n",
    "\n",
    "                    elif len(team_starters) > 5:\n",
    "                        # Keep top 5 by playing time\n",
    "                        team_starters = team_starters.head(5)\n",
    "                        warnings.append(f\"Reduced {team_abbrev} starters to top 5 by playing time\")\n",
    "\n",
    "                # Create starters list for this team\n",
    "                starters_list = []\n",
    "                for i, (_, player) in enumerate(team_starters.iterrows()):\n",
    "                    starters_list.append({\n",
    "                        'player_id': int(player['player_id']),\n",
    "                        'player_name': player['player_name'],\n",
    "                        'jersey_number': int(player['jersey_number']) if pd.notna(player['jersey_number']) else None,\n",
    "                        'position': f\"P{i+1}\",  # Generic position since we don't have actual positions\n",
    "                        'seconds_played': int(player['seconds_played']),\n",
    "                        'points': int(player['points'])\n",
    "                    })\n",
    "\n",
    "                # Store starters for this team\n",
    "                self.entities.starters[team_abbrev] = starters_list\n",
    "                self.entities.starters[f\"{team_abbrev}_ids\"] = tuple(sorted(p['player_id'] for p in starters_list))\n",
    "\n",
    "                logger.info(f\"Team {team_abbrev} starters: {[p['player_name'] for p in starters_list]}\")\n",
    "\n",
    "            total_starters = sum(len(v) for k, v in self.entities.starters.items() if isinstance(v, list))\n",
    "\n",
    "            details = f\"Extracted starters for {len([k for k in self.entities.starters if not k.endswith('_ids')])} teams\"\n",
    "            details += f\", {total_starters} total starters\"\n",
    "\n",
    "            if teams_with_issues:\n",
    "                details += f\". Issues resolved for: {', '.join(teams_with_issues)}\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Starters\",\n",
    "                passed=len(teams_with_issues) == 0,\n",
    "                details=details,\n",
    "                data_count=total_starters,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Starters\",\n",
    "                passed=False,\n",
    "                details=f\"Error extracting starters: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def extract_team_mapping(self) -> ValidationResult:\n",
    "        \"\"\"Create team ID → abbreviation mapping with home/away flags\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Creating team mapping...\")\n",
    "\n",
    "            # Get team information from box score\n",
    "            team_query = \"\"\"\n",
    "            SELECT DISTINCT\n",
    "                team_id,\n",
    "                team_abbrev,\n",
    "                is_home\n",
    "            FROM box_score\n",
    "            ORDER BY team_id\n",
    "            \"\"\"\n",
    "\n",
    "            team_df = self.conn.execute(team_query).df()\n",
    "\n",
    "            if len(team_df) == 0:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Extract Team Mapping\",\n",
    "                    passed=False,\n",
    "                    details=\"No teams found in box score\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            warnings = []\n",
    "\n",
    "            # Build team mapping\n",
    "            for _, row in team_df.iterrows():\n",
    "                self.entities.team_mapping[int(row['team_id'])] = row['team_abbrev']\n",
    "\n",
    "            # Identify home and away teams\n",
    "            home_teams = team_df[team_df['is_home'] == True]['team_abbrev'].tolist()\n",
    "            away_teams = team_df[team_df['is_home'] == False]['team_abbrev'].tolist()\n",
    "\n",
    "            # Validate exactly one home and one away team\n",
    "            if len(home_teams) != 1 or len(away_teams) != 1:\n",
    "                warnings.append(f\"Invalid home/away setup: home={home_teams}, away={away_teams}\")\n",
    "            else:\n",
    "                self.entities.team_mapping['home_team'] = home_teams[0]\n",
    "                self.entities.team_mapping['away_team'] = away_teams[0]\n",
    "\n",
    "            # Validate expected teams (based on file name HOU-DAL)\n",
    "            expected_teams = {'HOU', 'DAL'}\n",
    "            actual_teams = set(team_df['team_abbrev'])\n",
    "\n",
    "            if expected_teams != actual_teams:\n",
    "                warnings.append(f\"Expected teams {expected_teams}, found {actual_teams}\")\n",
    "\n",
    "            # Validate expected home/away (DAL should be home based on file naming convention)\n",
    "            if home_teams and home_teams[0] != 'DAL':\n",
    "                warnings.append(f\"Expected DAL to be home team, but {home_teams[0]} is home\")\n",
    "\n",
    "            if away_teams and away_teams[0] != 'HOU':\n",
    "                warnings.append(f\"Expected HOU to be away team, but {away_teams[0]} is away\")\n",
    "\n",
    "            # Create reverse mapping for convenience\n",
    "            team_id_to_abbrev = {k: v for k, v in self.entities.team_mapping.items() if isinstance(k, int)}\n",
    "\n",
    "            details = f\"Created mapping for {len(team_id_to_abbrev)} teams: {team_id_to_abbrev}\"\n",
    "            if 'home_team' in self.entities.team_mapping:\n",
    "                details += f\". Home: {self.entities.team_mapping['home_team']}, Away: {self.entities.team_mapping['away_team']}\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Team Mapping\",\n",
    "                passed=len(warnings) == 0,\n",
    "                details=details,\n",
    "                data_count=len(team_id_to_abbrev),\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Team Mapping\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating team mapping: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def extract_game_info(self) -> ValidationResult:\n",
    "        \"\"\"Extract basic game information and metadata\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Extracting game information...\")\n",
    "\n",
    "            # Get game info from box score\n",
    "            game_info_query = \"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT team_id) as num_teams,\n",
    "                COUNT(*) as total_players,\n",
    "                COUNT(CASE WHEN is_starter THEN 1 END) as total_starters,\n",
    "                SUM(seconds_played) as total_seconds_played,\n",
    "                SUM(points) as total_points,\n",
    "                MIN(team_id) as team1_id,\n",
    "                MAX(team_id) as team2_id\n",
    "            FROM box_score\n",
    "            \"\"\"\n",
    "\n",
    "            game_info = self.conn.execute(game_info_query).df().iloc[0].to_dict()\n",
    "\n",
    "            # Get team names\n",
    "            teams = [self.entities.team_mapping[int(game_info['team1_id'])], \n",
    "                    self.entities.team_mapping[int(game_info['team2_id'])]]\n",
    "\n",
    "            self.entities.game_info = {\n",
    "                'num_teams': int(game_info['num_teams']),\n",
    "                'total_players': int(game_info['total_players']),\n",
    "                'total_starters': int(game_info['total_starters']),\n",
    "                'total_seconds_played': int(game_info['total_seconds_played']),\n",
    "                'total_points': int(game_info['total_points']),\n",
    "                'teams': teams,\n",
    "                'matchup': f\"{teams[1]} @ {teams[0]}\" if 'home_team' in self.entities.team_mapping else f\"{teams[0]} vs {teams[1]}\"\n",
    "            }\n",
    "\n",
    "            warnings = []\n",
    "\n",
    "            # Validate game info\n",
    "            if game_info['num_teams'] != 2:\n",
    "                warnings.append(f\"Expected 2 teams, found {game_info['num_teams']}\")\n",
    "\n",
    "            if game_info['total_starters'] != 10:\n",
    "                warnings.append(f\"Expected 10 starters, found {game_info['total_starters']}\")\n",
    "\n",
    "            if game_info['total_players'] < 16:\n",
    "                warnings.append(f\"Only {game_info['total_players']} players found (minimum 16 expected)\")\n",
    "\n",
    "            details = f\"Game: {self.entities.game_info['matchup']}, {game_info['total_players']} players, {game_info['total_starters']} starters\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Game Info\",\n",
    "                passed=len(warnings) == 0,\n",
    "                details=details,\n",
    "                data_count=1,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Extract Game Info\",\n",
    "                passed=False,\n",
    "                details=f\"Error extracting game info: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def create_canonical_tables(self) -> ValidationResult:\n",
    "        \"\"\"Create optimized tables for canonical entities\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Creating canonical entity tables...\")\n",
    "\n",
    "            # Create canonical players table with robust object handling\n",
    "            self._robust_drop_object(\"canonical_players\")\n",
    "            self.conn.execute(\"\"\"\n",
    "            CREATE VIEW canonical_players AS\n",
    "            SELECT \n",
    "                player_id,\n",
    "                player_name,\n",
    "                team_id,\n",
    "                team_abbrev,\n",
    "                is_home,\n",
    "                is_starter,\n",
    "                seconds_played,\n",
    "                points,\n",
    "                rebounds,\n",
    "                assists,\n",
    "                jersey_number\n",
    "            FROM box_score\n",
    "            ORDER BY team_abbrev, seconds_played DESC\n",
    "            \"\"\")\n",
    "\n",
    "            # Create canonical starters table\n",
    "            starters_data = []\n",
    "            for team_abbrev, starters_list in self.entities.starters.items():\n",
    "                if isinstance(starters_list, list):  # Skip _ids entries\n",
    "                    team_id = None\n",
    "                    for tid, abbrev in self.entities.team_mapping.items():\n",
    "                        if isinstance(tid, int) and abbrev == team_abbrev:\n",
    "                            team_id = tid\n",
    "                            break\n",
    "\n",
    "                    for i, starter in enumerate(starters_list):\n",
    "                        starters_data.append({\n",
    "                            'team_id': team_id,\n",
    "                            'team_abbrev': team_abbrev,\n",
    "                            'lineup_position': i + 1,\n",
    "                            'player_id': starter['player_id'],\n",
    "                            'player_name': starter['player_name'],\n",
    "                            'jersey_number': starter['jersey_number'],\n",
    "                            'position': starter['position'],\n",
    "                            'seconds_played': starter['seconds_played'],\n",
    "                            'points': starter['points']\n",
    "                        })\n",
    "\n",
    "            if starters_data:\n",
    "                # Robust object handling\n",
    "                self._robust_drop_object(\"canonical_starters\")\n",
    "                \n",
    "                starters_df = pd.DataFrame(starters_data)\n",
    "                self.conn.register(\"starters_temp\", starters_df)\n",
    "\n",
    "                self.conn.execute(\"\"\"\n",
    "                CREATE TABLE canonical_starters AS\n",
    "                SELECT * FROM starters_temp\n",
    "                ORDER BY team_id, lineup_position\n",
    "                \"\"\")\n",
    "\n",
    "                self.conn.execute(\"DROP VIEW IF EXISTS starters_temp\")\n",
    "\n",
    "                # Create indexes for performance with error handling\n",
    "                try:\n",
    "                    self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_canonical_starters_team ON canonical_starters(team_id)\")\n",
    "                    self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_canonical_starters_player ON canonical_starters(player_id)\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not create starter indexes: {e}\")\n",
    "\n",
    "            # Create team mapping table\n",
    "            team_data = []\n",
    "            for team_id, team_abbrev in self.entities.team_mapping.items():\n",
    "                if isinstance(team_id, int):  # Skip special keys like 'home_team'\n",
    "                    is_home = team_abbrev == self.entities.team_mapping.get('home_team', '')\n",
    "                    team_data.append({\n",
    "                        'team_id': team_id,\n",
    "                        'team_abbrev': team_abbrev,\n",
    "                        'is_home': is_home\n",
    "                    })\n",
    "\n",
    "            if team_data:\n",
    "                # Robust object handling\n",
    "                self._robust_drop_object(\"canonical_teams\")\n",
    "                \n",
    "                teams_df = pd.DataFrame(team_data)\n",
    "                self.conn.register(\"teams_temp\", teams_df)\n",
    "\n",
    "                self.conn.execute(\"\"\"\n",
    "                CREATE TABLE canonical_teams AS\n",
    "                SELECT * FROM teams_temp\n",
    "                ORDER BY team_id\n",
    "                \"\"\")\n",
    "\n",
    "                self.conn.execute(\"DROP VIEW IF EXISTS teams_temp\")\n",
    "\n",
    "            details = f\"Created canonical tables: players (view), starters ({len(starters_data)}), teams ({len(team_data)})\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Canonical Tables\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(starters_data) + len(team_data),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Canonical Tables\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating canonical tables: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def validate_entity_completeness(self) -> ValidationResult:\n",
    "        \"\"\"Final validation that all entities are complete and consistent\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Performing final entity validation...\")\n",
    "\n",
    "            # Use the GameEntities validation method\n",
    "            entity_errors = self.entities.validate_completeness()\n",
    "\n",
    "            warnings = []\n",
    "\n",
    "            # Cross-validate entities\n",
    "            if self.entities.unique_players is not None and len(self.entities.starters) > 0:\n",
    "                # Check all starters are in unique_players\n",
    "                all_player_ids = set(self.entities.unique_players['player_id'])\n",
    "\n",
    "                for team, starters_list in self.entities.starters.items():\n",
    "                    if isinstance(starters_list, list):\n",
    "                        for starter in starters_list:\n",
    "                            if starter['player_id'] not in all_player_ids:\n",
    "                                warnings.append(f\"Starter {starter['player_name']} not found in unique_players\")\n",
    "\n",
    "            # Validate team consistency\n",
    "            if self.entities.unique_players is not None:\n",
    "                player_teams = set(self.entities.unique_players['team_id'])\n",
    "                mapping_teams = {k for k in self.entities.team_mapping.keys() if isinstance(k, int)}\n",
    "\n",
    "                if player_teams != mapping_teams:\n",
    "                    warnings.append(f\"Team ID mismatch: players have {player_teams}, mapping has {mapping_teams}\")\n",
    "\n",
    "            # Check starter counts\n",
    "            starter_counts = {}\n",
    "            for team, starters_list in self.entities.starters.items():\n",
    "                if isinstance(starters_list, list):\n",
    "                    starter_counts[team] = len(starters_list)\n",
    "\n",
    "            for team, count in starter_counts.items():\n",
    "                if count != 5:\n",
    "                    warnings.append(f\"Team {team} has {count} starters (expected 5)\")\n",
    "\n",
    "            passed = len(entity_errors) == 0 and all(count == 5 for count in starter_counts.values())\n",
    "\n",
    "            details = f\"Entity validation: {len(entity_errors)} errors, {len(warnings)} warnings\"\n",
    "            details += f\". Starter counts: {starter_counts}\"\n",
    "\n",
    "            if entity_errors:\n",
    "                details += f\" - Errors: {', '.join(entity_errors)}\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Entity Completeness\",\n",
    "                passed=passed,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings + entity_errors\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Entity Completeness\",\n",
    "                passed=False,\n",
    "                details=f\"Error validating entities: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def print_entities_summary(self):\n",
    "        \"\"\"Print comprehensive summary of extracted entities\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ROBUST NBA ENTITY EXTRACTION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Game info\n",
    "        if self.entities.game_info:\n",
    "            print(f\"🏀 GAME: {self.entities.game_info.get('matchup', 'Unknown')}\")\n",
    "            print(f\"   Teams: {', '.join(self.entities.game_info.get('teams', []))}\")\n",
    "            print(f\"   Players: {self.entities.game_info.get('total_players', 0)}\")\n",
    "            print(f\"   Starters: {self.entities.game_info.get('total_starters', 0)}\")\n",
    "            print()\n",
    "\n",
    "        # Players summary\n",
    "        if self.entities.unique_players is not None:\n",
    "            print(\"👥 PLAYERS BY TEAM:\")\n",
    "            for team in self.entities.unique_players['team_abbrev'].unique():\n",
    "                team_players = self.entities.unique_players[self.entities.unique_players['team_abbrev'] == team]\n",
    "                starters = team_players[team_players['is_starter'] == True]\n",
    "                print(f\"   {team}: {len(team_players)} players ({len(starters)} starters)\")\n",
    "            print()\n",
    "\n",
    "        # Starters detail\n",
    "        print(\"🏆 STARTING LINEUPS:\")\n",
    "        for team, starters_list in self.entities.starters.items():\n",
    "            if isinstance(starters_list, list):\n",
    "                print(f\"   {team}:\")\n",
    "                for i, starter in enumerate(starters_list):\n",
    "                    jersey = starter.get('jersey_number', 'N/A')\n",
    "                    seconds = starter.get('seconds_played', 0)\n",
    "                    points = starter.get('points', 0)\n",
    "                    print(f\"     {i+1}. {starter['player_name']} (#{jersey}, {seconds//60}:{seconds%60:02d}, {points}pts)\")\n",
    "        print()\n",
    "\n",
    "        # Team mapping\n",
    "        print(\"🏟️  TEAM MAPPING:\")\n",
    "        for team_id, team_abbrev in self.entities.team_mapping.items():\n",
    "            if isinstance(team_id, int):\n",
    "                home_away = \"🏠\" if team_abbrev == self.entities.team_mapping.get('home_team') else \"✈️\"\n",
    "                print(f\"   {team_id} → {team_abbrev} {home_away}\")\n",
    "\n",
    "        print(\"=\"*80)\n",
    "\n",
    "def extract_all_entities_robust(db_path: str = \"mavs_enhanced.duckdb\") -> Tuple[bool, GameEntities]:\n",
    "    \"\"\"Extract all canonical entities with robust validation\"\"\"\n",
    "\n",
    "    print(\"🏀 NBA Pipeline - Robust Entity Extraction\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    with RobustEntityExtractor(db_path) as extractor:\n",
    "\n",
    "        # Extract unique players\n",
    "        logger.info(\"Step 3a: Extracting unique players...\")\n",
    "        result = extractor.extract_unique_players()\n",
    "        extractor.validator.log_validation(result)\n",
    "\n",
    "        if not result.passed:\n",
    "            logger.error(\"❌ Failed to extract players - stopping\")\n",
    "            return False, extractor.entities\n",
    "\n",
    "        # Extract starters (using actual gs=1 data from box score)\n",
    "        logger.info(\"Step 3b: Extracting starters from box score...\")\n",
    "        result = extractor.extract_starters()\n",
    "        extractor.validator.log_validation(result)\n",
    "\n",
    "        if not result.passed:\n",
    "            logger.error(\"❌ Failed to extract starters - stopping\")\n",
    "            return False, extractor.entities\n",
    "\n",
    "        # Extract team mapping\n",
    "        logger.info(\"Step 3c: Creating team mapping...\")\n",
    "        result = extractor.extract_team_mapping()\n",
    "        extractor.validator.log_validation(result)\n",
    "\n",
    "        # Extract game info\n",
    "        logger.info(\"Step 3d: Extracting game info...\")\n",
    "        result = extractor.extract_game_info()\n",
    "        extractor.validator.log_validation(result)\n",
    "\n",
    "        # Create canonical tables\n",
    "        logger.info(\"Step 3e: Creating canonical tables...\")\n",
    "        result = extractor.create_canonical_tables()\n",
    "        extractor.validator.log_validation(result)\n",
    "\n",
    "        # Final validation\n",
    "        logger.info(\"Step 3f: Final validation...\")\n",
    "        result = extractor.validate_entity_completeness()\n",
    "        extractor.validator.log_validation(result)\n",
    "\n",
    "        # Print summary\n",
    "        extractor.print_entities_summary()\n",
    "        success = extractor.validator.print_validation_summary()\n",
    "\n",
    "        return success, extractor.entities\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    database_path = \"mavs_enhanced.duckdb\"\n",
    "\n",
    "    success, entities = extract_all_entities_robust(database_path)\n",
    "\n",
    "    if success:\n",
    "        print(\"\\n✅ Robust entity extraction completed successfully\")\n",
    "        print(\"🎯 Ready for lineup tracking and possession analysis\")\n",
    "    else:\n",
    "        print(\"\\n❌ Robust entity extraction failed\")\n",
    "        print(\"🔧 Review validation messages above\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4175be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/data/nba_pbp_processor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/data/nba_pbp_processor.py\n",
    "# Step 4: Play-by-Play Processing & Lineup State Machine (Updated with Step 2 Integration)\n",
    "\"\"\"\n",
    "NBA Pipeline - Step 4: Process PBP Events & Track Lineups (UPDATED)\n",
    "==================================================================\n",
    "\n",
    "UPDATED to integrate Step 2 findings:\n",
    "- Traditional Data-Driven Method: Follows raw substitution data strictly (3-6 man lineups)\n",
    "- Enhanced Estimation Method: Uses intelligent inference to maintain 5-man lineups\n",
    "- Both methods run in parallel to provide comparison and validation\n",
    "- Comprehensive flagging system from Step 2 integrated\n",
    "- Config-driven approach for easy switching between methods\n",
    "\n",
    "Key Integration Points from Step 2:\n",
    "1. Traditional method: msgType=8, playerId1=IN, playerId2=OUT, allows variable lineup sizes\n",
    "2. Enhanced method: First-action rules, inactivity detection, always-5 enforcement\n",
    "3. Comprehensive flagging and validation system\n",
    "4. Both methods track the same events but with different lineup management strategies\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "# Ensure we're in the right directory\n",
    "cwd = os.getcwd()\n",
    "if not cwd.endswith(\"airflow_project\"):\n",
    "    os.chdir('api/src/airflow_project')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Set, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "from eda.utils.nba_pipeline_analysis import NBADataValidator, ValidationResult\n",
    "from eda.data.nba_entities_extractor import GameEntities\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    from utils.config import (\n",
    "        NBA_SUBSTITUTION_CONFIG,\n",
    "        RIM_DISTANCE_FEET,\n",
    "        COORDINATE_SCALE,\n",
    "        MINIMUM_SECONDS_PLAYED,\n",
    "        DUCKDB_PATH\n",
    "    )\n",
    "    CONFIG = NBA_SUBSTITUTION_CONFIG\n",
    "    RIM_THRESHOLD = RIM_DISTANCE_FEET\n",
    "    COORD_SCALE = COORDINATE_SCALE\n",
    "    MIN_SECONDS = MINIMUM_SECONDS_PLAYED\n",
    "    DB_PATH = str(DUCKDB_PATH)\n",
    "except ImportError:\n",
    "    logger.warning(\"Config not available, using defaults\")\n",
    "    CONFIG = {\n",
    "        \"starter_reset_periods\": [1, 3],\n",
    "        \"msg_types\": {\"substitution\": 8, \"shot_made\": 1, \"shot_missed\": 2, \"rebound\": 4},\n",
    "        \"one_direction\": {\"enabled\": True, \"appearance_via_last_name\": True},\n",
    "        \"validation\": {\"validate_team_membership\": True, \"min_lineup_size\": 5},\n",
    "        \"debug\": {\"log_all_substitutions\": True}\n",
    "    }\n",
    "    RIM_THRESHOLD = 4.0\n",
    "    COORD_SCALE = 10.0\n",
    "    MIN_SECONDS = 30\n",
    "    DB_PATH = \"mavs_enhanced.duckdb\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TraditionalLineupState:\n",
    "    \"\"\"Traditional data-driven lineup state - follows raw data strictly\"\"\"\n",
    "    team_lineups: Dict[int, Set[int]] = field(default_factory=dict)\n",
    "    period: int = 0\n",
    "    flags: List[Dict] = field(default_factory=list)\n",
    "    substitution_log: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    def add_flag(self, flag_type: str, team_id: int, player_id: int = None, details: str = \"\"):\n",
    "        \"\"\"Add a flag for data quality issues\"\"\"\n",
    "        self.flags.append({\n",
    "            'type': flag_type,\n",
    "            'team_id': team_id,\n",
    "            'player_id': player_id,\n",
    "            'details': details,\n",
    "            'period': self.period\n",
    "        })\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnhancedLineupState:\n",
    "    \"\"\"Enhanced lineup state with intelligent inference - maintains 5-man lineups\"\"\"\n",
    "    team_lineups: Dict[int, Set[int]] = field(default_factory=dict)\n",
    "    period: int = 0\n",
    "    last_action_time: Dict[int, float] = field(default_factory=dict)\n",
    "    recent_out: Dict[int, deque] = field(default_factory=dict)\n",
    "    flags: List[Dict] = field(default_factory=list)\n",
    "    substitution_log: List[Dict] = field(default_factory=list)\n",
    "    first_action_events: List[Dict] = field(default_factory=list)\n",
    "    auto_out_events: List[Dict] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        for team_id in self.team_lineups.keys():\n",
    "            self.recent_out[team_id] = deque(maxlen=10)\n",
    "    \n",
    "    def add_flag(self, flag_type: str, team_id: int, player_id: int = None, details: str = \"\"):\n",
    "        \"\"\"Add a flag for tracking intelligent inference actions\"\"\"\n",
    "        self.flags.append({\n",
    "            'type': flag_type,\n",
    "                    'team_id': team_id,\n",
    "            'player_id': player_id,\n",
    "            'details': details,\n",
    "            'period': self.period\n",
    "        })\n",
    "\n",
    "\n",
    "@dataclass  \n",
    "class ProcessedEvent:\n",
    "    \"\"\"Represents a processed play-by-play event with context from both methods\"\"\"\n",
    "    pbp_id: int\n",
    "    period: int\n",
    "    pbp_order: int\n",
    "    wall_clock_int: int\n",
    "    msg_type: int\n",
    "    action_type: int = None\n",
    "    description: str = \"\"\n",
    "    off_team_id: int = None\n",
    "    def_team_id: int = None\n",
    "    player_id_1: int = None\n",
    "    player_id_2: int = None\n",
    "    player_id_3: int = None\n",
    "    loc_x: int = None\n",
    "    loc_y: int = None\n",
    "    points: int = 0\n",
    "\n",
    "    # Computed fields\n",
    "    is_shot: bool = False\n",
    "    is_rim_attempt: bool = False\n",
    "    is_rim_make: bool = False\n",
    "    distance_ft: float = None\n",
    "    is_substitution: bool = False\n",
    "    sub_out_player: int = None\n",
    "    sub_in_player: int = None\n",
    "\n",
    "    # Lineup context from BOTH methods\n",
    "    traditional_off_lineup: Tuple[int, ...] = None\n",
    "    traditional_def_lineup: Tuple[int, ...] = None\n",
    "    enhanced_off_lineup: Tuple[int, ...] = None\n",
    "    enhanced_def_lineup: Tuple[int, ...] = None\n",
    "\n",
    "\n",
    "class PBPProcessor:\n",
    "    \"\"\"UPDATED: Integrated processor using both Step 2 methods\"\"\"\n",
    "\n",
    "    def __init__(self, db_path: str = None, entities: GameEntities = None):\n",
    "        \"\"\"Initialize with both traditional and enhanced tracking methods\"\"\"\n",
    "        self.db_path = db_path or DB_PATH\n",
    "        self.conn = None\n",
    "        self.entities = entities\n",
    "        self.validator = NBADataValidator()\n",
    "\n",
    "        # DUAL STATE TRACKING - Both methods run in parallel\n",
    "        self.traditional_state = TraditionalLineupState()\n",
    "        self.enhanced_state = EnhancedLineupState()\n",
    "        \n",
    "        self.processed_events: List[ProcessedEvent] = []\n",
    "\n",
    "        # Build team rosters and reference data\n",
    "        self.team_rosters = self._build_team_rosters_from_entities()\n",
    "        self.player_names = self._build_player_names()\n",
    "        self.team_names = self._build_team_names()\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.traditional_stats = {\n",
    "            'substitutions': 0, 'flags': 0, 'lineup_size_deviations': 0\n",
    "        }\n",
    "        self.enhanced_stats = {\n",
    "            'substitutions': 0, 'first_actions': 0, 'auto_outs': 0, 'flags': 0\n",
    "        }\n",
    "\n",
    "    def _build_team_rosters_from_entities(self) -> Dict[int, Set[int]]:\n",
    "        \"\"\"Build complete team rosters from entities\"\"\"\n",
    "        rosters = {}\n",
    "        if hasattr(self.entities, 'unique_players') and self.entities.unique_players is not None:\n",
    "            for _, player in self.entities.unique_players.iterrows():\n",
    "                team_id = int(player['team_id'])\n",
    "                player_id = int(player['player_id'])\n",
    "                if team_id not in rosters:\n",
    "                    rosters[team_id] = set()\n",
    "                rosters[team_id].add(player_id)\n",
    "        return rosters\n",
    "\n",
    "    def _build_player_names(self) -> Dict[int, str]:\n",
    "        \"\"\"Build player ID to name mapping\"\"\"\n",
    "        names = {}\n",
    "        if hasattr(self.entities, 'unique_players') and self.entities.unique_players is not None:\n",
    "            for _, player in self.entities.unique_players.iterrows():\n",
    "                names[int(player['player_id'])] = str(player['player_name'])\n",
    "        return names\n",
    "\n",
    "    def _build_team_names(self) -> Dict[int, str]:\n",
    "        \"\"\"Build team ID to abbreviation mapping\"\"\"\n",
    "        if hasattr(self.entities, 'team_mapping'):\n",
    "            return {int(k): str(v) for k, v in self.entities.team_mapping.items() if isinstance(k, int)}\n",
    "        return {}\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = duckdb.connect(self.db_path)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "    def initialize_lineups(self) -> ValidationResult:\n",
    "        \"\"\"Initialize starting lineups for BOTH tracking methods\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Initializing lineups for both tracking methods...\")\n",
    "\n",
    "            if not self.entities.starters:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Initialize Lineups\",\n",
    "                    passed=False,\n",
    "                    details=\"No starting lineups available in entities\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            warnings = []\n",
    "\n",
    "            # Initialize both states with same starting lineups\n",
    "            for team_abbrev, starters_list in self.entities.starters.items():\n",
    "                if isinstance(starters_list, list):\n",
    "                    # Find team ID\n",
    "                    team_id = None\n",
    "                    for tid, tabbrev in self.entities.team_mapping.items():\n",
    "                        if isinstance(tid, int) and tabbrev == team_abbrev:\n",
    "                            team_id = tid\n",
    "                            break\n",
    "\n",
    "                    if team_id is None:\n",
    "                        warnings.append(f\"Could not find team ID for {team_abbrev}\")\n",
    "                        continue\n",
    "\n",
    "                    starter_ids = {starter['player_id'] for starter in starters_list}\n",
    "\n",
    "                    if len(starter_ids) != 5:\n",
    "                        warnings.append(f\"Team {team_abbrev} has {len(starter_ids)} starters (expected 5)\")\n",
    "\n",
    "                    # Set for BOTH methods\n",
    "                    self.traditional_state.team_lineups[team_id] = starter_ids.copy()\n",
    "                    self.enhanced_state.team_lineups[team_id] = starter_ids.copy()\n",
    "                    \n",
    "                    # Initialize enhanced state tracking\n",
    "                    self.enhanced_state.recent_out[team_id] = deque(maxlen=10)\n",
    "                    for player_id in starter_ids:\n",
    "                        self.enhanced_state.last_action_time[player_id] = 0.0\n",
    "\n",
    "                    logger.info(f\"Initialized {team_abbrev} starters: {sorted(starter_ids)}\")\n",
    "\n",
    "            details = f\"Initialized lineups for both tracking methods: {len(self.traditional_state.team_lineups)} teams\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Initialize Lineups\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Initialize Lineups\",\n",
    "                passed=False,\n",
    "                details=f\"Error initializing lineups: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def load_pbp_events(self) -> ValidationResult:\n",
    "        \"\"\"Load PBP events with Step 2 integration\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Loading PBP events with Step 2 classification...\")\n",
    "\n",
    "            # Use same canonical view approach from Step 2\n",
    "            pbp_view = self._ensure_canonical_pbp_view()\n",
    "\n",
    "            events_df = self.conn.execute(f\"\"\"\n",
    "            SELECT \n",
    "                    pbp_id, period, pbp_order, wall_clock_int,\n",
    "                    game_clock, description, msg_type, action_type,\n",
    "                    off_team_id, def_team_id,\n",
    "                    player_id_1, player_id_2, player_id_3,\n",
    "                    loc_x, loc_y, points\n",
    "            FROM {pbp_view}\n",
    "            ORDER BY period, pbp_order, wall_clock_int\n",
    "            \"\"\").df()\n",
    "\n",
    "            if len(events_df) == 0:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Load PBP Events\", \n",
    "                    passed=False,\n",
    "                    details=\"No valid PBP events found\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Convert to ProcessedEvent objects with Step 2 classification\n",
    "            self.processed_events = []\n",
    "            for _, row in events_df.iterrows():\n",
    "                event = ProcessedEvent(\n",
    "                    pbp_id=int(row['pbp_id']),\n",
    "                    period=int(row['period']),\n",
    "                    pbp_order=int(row['pbp_order']),\n",
    "                    wall_clock_int=int(row['wall_clock_int']) if pd.notna(row['wall_clock_int']) else 0,\n",
    "                    msg_type=int(row['msg_type']),\n",
    "                    action_type=int(row['action_type']) if pd.notna(row['action_type']) else None,\n",
    "                    description=str(row['description']) if pd.notna(row['description']) else \"\",\n",
    "                    off_team_id=int(row['off_team_id']) if pd.notna(row['off_team_id']) else None,\n",
    "                    def_team_id=int(row['def_team_id']) if pd.notna(row['def_team_id']) else None,\n",
    "                    player_id_1=int(row['player_id_1']) if pd.notna(row['player_id_1']) else None,\n",
    "                    player_id_2=int(row['player_id_2']) if pd.notna(row['player_id_2']) else None,\n",
    "                    player_id_3=int(row['player_id_3']) if pd.notna(row['player_id_3']) else None,\n",
    "                    loc_x=int(row['loc_x']) if pd.notna(row['loc_x']) and row['loc_x'] != 0 else None,\n",
    "                    loc_y=int(row['loc_y']) if pd.notna(row['loc_y']) and row['loc_y'] != 0 else None,\n",
    "                    points=int(row['points']) if pd.notna(row['points']) else 0\n",
    "                )\n",
    "\n",
    "                # Classify event using Step 2 logic\n",
    "                self._classify_event_step2(event)\n",
    "                self.processed_events.append(event)\n",
    "\n",
    "            details = f\"Loaded {len(self.processed_events)} events with Step 2 classification\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load PBP Events\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(self.processed_events),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load PBP Events\",\n",
    "                passed=False,\n",
    "                details=f\"Error loading events: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _ensure_canonical_pbp_view(self) -> str:\n",
    "        \"\"\"Create canonical PBP view with robust column detection (no nulling).\"\"\"\n",
    "        # Discover a pbp-like table (prefer enriched if present)\n",
    "        tables = [r[0] for r in self.conn.execute(\"\"\"\n",
    "            SELECT table_name FROM information_schema.tables\n",
    "            WHERE table_schema NOT IN ('information_schema','pg_catalog')\n",
    "            ORDER BY table_name\n",
    "        \"\"\").fetchall()]\n",
    "\n",
    "        pbp_candidates = [t for t in tables if t.lower() in (\"pbp_enriched\", \"pbp\")]\n",
    "        if pbp_candidates:\n",
    "            pbp_table = pbp_candidates[0]\n",
    "        else:\n",
    "            # last resort heuristic\n",
    "            pbp_table = next((t for t in tables if \"pbp\" in t.lower()), None) or \"pbp\"\n",
    "\n",
    "        # Inspect columns to build a robust SELECT\n",
    "        cols = {r[1].lower(): r[1] for r in self.conn.execute(f\"PRAGMA table_info('{pbp_table}')\").fetchall()}\n",
    "\n",
    "        # Map required fields with safe coalesces\n",
    "        period_col        = cols.get(\"period\",        \"period\")\n",
    "        pbp_order_col     = cols.get(\"pbp_order\",     \"pbp_order\")\n",
    "        wall_clock_col    = cols.get(\"wall_clock_int\",\"wall_clock_int\")\n",
    "        game_clock_col    = cols.get(\"game_clock\",    \"game_clock\")\n",
    "        desc_col          = cols.get(\"description\",   \"description\")\n",
    "        msg_type_col      = cols.get(\"msg_type\",      \"msg_type\")\n",
    "        action_type_col   = cols.get(\"action_type\",   \"action_type\")\n",
    "        off_team_col      = cols.get(\"team_id_off\",   cols.get(\"off_team_id\", \"team_id_off\"))\n",
    "        def_team_col      = cols.get(\"team_id_def\",   cols.get(\"def_team_id\", \"team_id_def\"))\n",
    "        p1_col            = cols.get(\"player_id_1\",   \"player_id_1\")\n",
    "        p2_col            = cols.get(\"player_id_2\",   \"player_id_2\")\n",
    "        p3_col            = cols.get(\"player_id_3\",   \"player_id_3\")\n",
    "        loc_x_col         = cols.get(\"loc_x\",         cols.get(\"x\", \"loc_x\"))\n",
    "        # Common alternates for loc_y\n",
    "        loc_y_source      = cols.get(\"loc_y\", cols.get(\"y\", cols.get(\"pos_y\")))\n",
    "        # Common alternates for points\n",
    "        points_source     = cols.get(\"points\", cols.get(\"points_scored\", cols.get(\"score_change\")))\n",
    "\n",
    "        # Build COALESCE expressions where necessary\n",
    "        loc_y_expr   = (loc_y_source or \"NULL\")\n",
    "        points_expr  = (points_source or \"0\")\n",
    "\n",
    "        # pbp_id mapping (fallback to row_number if missing)\n",
    "        pbp_id_expr = \"pbp_id\" if \"pbp_id\" in cols else f\"row_number() over (order by {period_col}, {pbp_order_col}) as pbp_id\"\n",
    "\n",
    "        self.conn.execute(\"DROP VIEW IF EXISTS canonical_pbp\")\n",
    "        self.conn.execute(f\"\"\"\n",
    "            CREATE VIEW canonical_pbp AS\n",
    "            SELECT \n",
    "                {pbp_id_expr},\n",
    "                {period_col}       AS period,\n",
    "                {pbp_order_col}    AS pbp_order,\n",
    "                {wall_clock_col}   AS wall_clock_int,\n",
    "                {game_clock_col}   AS game_clock,\n",
    "                {desc_col}         AS description,\n",
    "                {msg_type_col}     AS msg_type,\n",
    "                {action_type_col}  AS action_type,\n",
    "                {off_team_col}     AS off_team_id,\n",
    "                {def_team_col}     AS def_team_id,\n",
    "                {p1_col}           AS player_id_1,\n",
    "                {p2_col}           AS player_id_2,\n",
    "                {p3_col}           AS player_id_3,\n",
    "                {loc_x_col}        AS loc_x,\n",
    "                {loc_y_expr}       AS loc_y,\n",
    "                {points_expr}      AS points\n",
    "            FROM {pbp_table}\n",
    "            WHERE {off_team_col} IS NOT NULL AND {def_team_col} IS NOT NULL\n",
    "        \"\"\")\n",
    "        return \"canonical_pbp\"\n",
    "\n",
    "\n",
    "    def _classify_event_step2(self, event: ProcessedEvent):\n",
    "        \"\"\"Classify events using Step 2 methodology\"\"\"\n",
    "        # Shot classification\n",
    "        if event.msg_type in [1, 2]:  # Made/Missed shots\n",
    "            event.is_shot = True\n",
    "\n",
    "            if event.loc_x is not None and event.loc_y is not None:\n",
    "                event.distance_ft = np.sqrt(event.loc_x**2 + event.loc_y**2) / COORD_SCALE\n",
    "                event.is_rim_attempt = event.distance_ft <= RIM_THRESHOLD\n",
    "                event.is_rim_make = event.is_rim_attempt and event.msg_type == 1\n",
    "\n",
    "        # Substitution classification - Step 2 methodology\n",
    "        if event.msg_type == CONFIG[\"msg_types\"][\"substitution\"]:\n",
    "            event.is_substitution = True\n",
    "            # Step 2 finding: playerId1 = IN, playerId2 = OUT for traditional\n",
    "            event.sub_in_player = event.player_id_1\n",
    "            event.sub_out_player = event.player_id_2\n",
    "\n",
    "    def process_traditional_substitution(self, event: ProcessedEvent) -> bool:\n",
    "        \"\"\"Process substitution using TRADITIONAL DATA-DRIVEN method from Step 2\"\"\"\n",
    "        if not event.is_substitution:\n",
    "            return False\n",
    "\n",
    "        in_player = event.sub_in_player\n",
    "        out_player = event.sub_out_player\n",
    "\n",
    "        # Traditional method: follow data strictly, allow variable lineup sizes\n",
    "        try:\n",
    "            # Determine team (prefer out_player's current team)\n",
    "            team_id = None\n",
    "            for tid, lineup in self.traditional_state.team_lineups.items():\n",
    "                if out_player and out_player in lineup:\n",
    "                    team_id = tid\n",
    "                    break\n",
    "            \n",
    "            if not team_id:\n",
    "                # Fallback to roster check\n",
    "                for tid, roster in self.team_rosters.items():\n",
    "                    if (out_player and out_player in roster) or (in_player and in_player in roster):\n",
    "                        team_id = tid\n",
    "                        break\n",
    "\n",
    "            if not team_id:\n",
    "                self.traditional_state.add_flag(\n",
    "                    \"unknown_team_substitution\", 0, in_player,\n",
    "                    f\"Cannot determine team for sub: {out_player} -> {in_player}\"\n",
    "                )\n",
    "                return False\n",
    "\n",
    "            lineup = self.traditional_state.team_lineups[team_id]\n",
    "\n",
    "            # Traditional method flags (from Step 2)\n",
    "            if out_player and out_player not in lineup:\n",
    "                self.traditional_state.add_flag(\n",
    "                    \"sub_out_player_not_in_lineup\", team_id, out_player,\n",
    "                    f\"OUT player {out_player} not in current lineup\"\n",
    "                )\n",
    "\n",
    "            if in_player and in_player in lineup:\n",
    "                self.traditional_state.add_flag(\n",
    "                    \"sub_in_player_already_in_lineup\", team_id, in_player,\n",
    "                    f\"IN player {in_player} already in lineup\"\n",
    "                )\n",
    "\n",
    "            # Execute substitution strictly as recorded\n",
    "            if out_player and out_player in lineup:\n",
    "                lineup.remove(out_player)\n",
    "            if in_player:\n",
    "                lineup.add(in_player)\n",
    "\n",
    "            # Flag lineup size deviations (Step 2 finding: 3-6 man lineups)\n",
    "            if len(lineup) != 5:\n",
    "                self.traditional_state.add_flag(\n",
    "                    \"lineup_size_deviation\", team_id, None,\n",
    "                    f\"Lineup size {len(lineup)}/5 after substitution\"\n",
    "                )\n",
    "                self.traditional_stats['lineup_size_deviations'] += 1\n",
    "\n",
    "            self.traditional_stats['substitutions'] += 1\n",
    "            self.traditional_state.substitution_log.append({\n",
    "                'period': event.period,\n",
    "                'team_id': team_id,\n",
    "                'in_player': in_player,\n",
    "                'out_player': out_player,\n",
    "                'lineup_size_after': len(lineup)\n",
    "            })\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.traditional_state.add_flag(\n",
    "                \"substitution_error\", team_id or 0, None, f\"Error: {str(e)}\"\n",
    "            )\n",
    "            return False\n",
    "\n",
    "    def process_enhanced_substitution(self, event: ProcessedEvent, current_time: float) -> bool:\n",
    "        \"\"\"Process substitution using ENHANCED method from Step 2\"\"\"\n",
    "        if not event.is_substitution:\n",
    "            return False\n",
    "\n",
    "        in_player = event.sub_in_player\n",
    "        out_player = event.sub_out_player\n",
    "\n",
    "        # Enhanced method: intelligent inference to maintain 5-man lineups\n",
    "        try:\n",
    "            # Determine team with fallbacks\n",
    "            team_id = self._determine_team_enhanced(in_player, out_player, event)\n",
    "            if not team_id:\n",
    "                return False\n",
    "\n",
    "            lineup = self.enhanced_state.team_lineups[team_id]\n",
    "\n",
    "            # Enhanced method: prepare lineup for substitution\n",
    "            if out_player and out_player not in lineup:\n",
    "                # Try to find and move player\n",
    "                for other_tid, other_lineup in self.enhanced_state.team_lineups.items():\n",
    "                    if other_tid != team_id and out_player in other_lineup:\n",
    "                        other_lineup.remove(out_player)\n",
    "                        lineup.add(out_player)\n",
    "                        self.enhanced_state.add_flag(\n",
    "                            \"moved_player_between_teams\", team_id, out_player,\n",
    "                            f\"Moved OUT player from team {other_tid} to {team_id}\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "            if in_player and in_player in lineup:\n",
    "                # Remove duplicate\n",
    "                lineup.remove(in_player)\n",
    "                self.enhanced_state.add_flag(\n",
    "                    \"removed_duplicate_in_player\", team_id, in_player,\n",
    "                    \"Removed duplicate IN player\"\n",
    "                )\n",
    "\n",
    "            # Remove from other teams\n",
    "            for other_tid, other_lineup in self.enhanced_state.team_lineups.items():\n",
    "                if other_tid != team_id and in_player and in_player in other_lineup:\n",
    "                    other_lineup.remove(in_player)\n",
    "\n",
    "            # Execute substitution\n",
    "            if out_player and out_player in lineup:\n",
    "                lineup.remove(out_player)\n",
    "                self.enhanced_state.recent_out[team_id].append(out_player)\n",
    "\n",
    "            if in_player:\n",
    "                lineup.add(in_player)\n",
    "\n",
    "            # Enhanced method: ensure exactly 5 players\n",
    "            self._ensure_five_players_enhanced(team_id, current_time)\n",
    "\n",
    "            self.enhanced_stats['substitutions'] += 1\n",
    "            self.enhanced_state.substitution_log.append({\n",
    "                'period': event.period,\n",
    "                'team_id': team_id,\n",
    "                'in_player': in_player,\n",
    "                'out_player': out_player,\n",
    "                'lineup_size_after': len(self.enhanced_state.team_lineups[team_id])\n",
    "            })\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.enhanced_state.add_flag(\n",
    "                \"substitution_error\", team_id or 0, None, f\"Error: {str(e)}\"\n",
    "            )\n",
    "            return False\n",
    "\n",
    "    def _determine_team_enhanced(self, in_player: int, out_player: int, event) -> Optional[int]:\n",
    "        \"\"\"Enhanced team determination with multiple fallbacks\"\"\"\n",
    "        # Check current lineups\n",
    "        for team_id, lineup in self.enhanced_state.team_lineups.items():\n",
    "            if out_player and out_player in lineup:\n",
    "                return team_id\n",
    "\n",
    "        # Check rosters\n",
    "        for team_id, roster in self.team_rosters.items():\n",
    "            if (out_player and out_player in roster) or (in_player and in_player in roster):\n",
    "                    return team_id\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _ensure_five_players_enhanced(self, team_id: int, current_time: float):\n",
    "        \"\"\"Ensure exactly 5 players using Enhanced method logic from Step 2\"\"\"\n",
    "        lineup = self.enhanced_state.team_lineups[team_id]\n",
    "\n",
    "        # Remove excess players (auto-out logic)\n",
    "        while len(lineup) > 5:\n",
    "            # Find least active player for auto-out\n",
    "            candidate = None\n",
    "            max_idle = -1\n",
    "            \n",
    "            for player_id in lineup:\n",
    "                idle_time = current_time - self.enhanced_state.last_action_time.get(player_id, 0)\n",
    "                if idle_time > max_idle:\n",
    "                    max_idle = idle_time\n",
    "                    candidate = player_id\n",
    "\n",
    "            if candidate:\n",
    "                lineup.remove(candidate)\n",
    "                self.enhanced_state.recent_out[team_id].append(candidate)\n",
    "                self.enhanced_state.add_flag(\n",
    "                    \"auto_out_excess_player\", team_id, candidate,\n",
    "                    f\"Auto-out due to excess players (idle: {max_idle:.1f}s)\"\n",
    "                )\n",
    "                self.enhanced_stats['auto_outs'] += 1\n",
    "\n",
    "        # Add players if under 5\n",
    "        if len(lineup) < 5:\n",
    "            available = self.team_rosters.get(team_id, set()) - lineup\n",
    "            # Prefer recently out players\n",
    "            recent = [p for p in self.enhanced_state.recent_out[team_id] if p in available]\n",
    "            \n",
    "            for player_id in (recent + list(available))[:5-len(lineup)]:\n",
    "                lineup.add(player_id)\n",
    "                self.enhanced_state.add_flag(\n",
    "                    \"auto_in_fill_lineup\", team_id, player_id,\n",
    "                    \"Auto-in to fill lineup to 5 players\"\n",
    "                )\n",
    "\n",
    "    def handle_first_action_events(self, event: ProcessedEvent, current_time: float):\n",
    "        \"\"\"Handle first-action events (Reed Sheppard case) from Step 2\"\"\"\n",
    "        if event.msg_type not in [1, 2, 4, 5, 6]:  # Only for action events\n",
    "            return\n",
    "\n",
    "        action_player = event.player_id_1\n",
    "        if not action_player:\n",
    "            return\n",
    "\n",
    "        # Check if player is in any lineup\n",
    "        player_team = None\n",
    "        player_in_lineup = False\n",
    "\n",
    "        for team_id, lineup in self.enhanced_state.team_lineups.items():\n",
    "            if action_player in lineup:\n",
    "                player_in_lineup = True\n",
    "                break\n",
    "            elif action_player in self.team_rosters.get(team_id, set()):\n",
    "                player_team = team_id\n",
    "\n",
    "        # First-action injection (Reed Sheppard case)\n",
    "        if not player_in_lineup and player_team:\n",
    "            self.enhanced_state.team_lineups[player_team].add(action_player)\n",
    "            self.enhanced_state.add_flag(\n",
    "                \"first_action_injection\", player_team, action_player,\n",
    "                f\"First-action injection: {event.description}\"\n",
    "            )\n",
    "            self.enhanced_stats['first_actions'] += 1\n",
    "            self.enhanced_state.first_action_events.append({\n",
    "                'period': event.period,\n",
    "                'player_id': action_player,\n",
    "                'team_id': player_team,\n",
    "                'event_type': event.msg_type,\n",
    "                'description': event.description\n",
    "            })\n",
    "            \n",
    "            # Ensure 5 players after injection\n",
    "            self._ensure_five_players_enhanced(player_team, current_time)\n",
    "\n",
    "        # Update activity time\n",
    "        if action_player:\n",
    "            self.enhanced_state.last_action_time[action_player] = current_time\n",
    "\n",
    "    def process_all_events(self) -> ValidationResult:\n",
    "        \"\"\"Process all events with BOTH tracking methods from Step 2\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Processing {len(self.processed_events)} events with both methods...\")\n",
    "\n",
    "            periods_seen = set()\n",
    "            current_time = 0.0\n",
    "\n",
    "            for i, event in enumerate(self.processed_events):\n",
    "                current_time = float(event.wall_clock_int)\n",
    "\n",
    "                # Handle period transitions\n",
    "                if event.period not in periods_seen:\n",
    "                    periods_seen.add(event.period)\n",
    "                    logger.info(f\"Processing period {event.period}\")\n",
    "\n",
    "                    # Reset periods for enhanced method (Step 2 logic)\n",
    "                    if event.period in CONFIG[\"starter_reset_periods\"]:\n",
    "                        self._reset_to_starters_enhanced()\n",
    "\n",
    "                    # Update period in states\n",
    "                    self.traditional_state.period = event.period\n",
    "                    self.enhanced_state.period = event.period\n",
    "\n",
    "                # Capture lineup context BEFORE processing\n",
    "                event.traditional_off_lineup = tuple(sorted(self.traditional_state.team_lineups.get(event.off_team_id, set())))\n",
    "                event.traditional_def_lineup = tuple(sorted(self.traditional_state.team_lineups.get(event.def_team_id, set())))\n",
    "                event.enhanced_off_lineup = tuple(sorted(self.enhanced_state.team_lineups.get(event.off_team_id, set())))\n",
    "                event.enhanced_def_lineup = tuple(sorted(self.enhanced_state.team_lineups.get(event.def_team_id, set())))\n",
    "\n",
    "                # Process substitutions with BOTH methods\n",
    "                if event.is_substitution:\n",
    "                    traditional_success = self.process_traditional_substitution(event)\n",
    "                    enhanced_success = self.process_enhanced_substitution(event, current_time)\n",
    "\n",
    "                    if CONFIG[\"debug\"][\"log_all_substitutions\"]:\n",
    "                        logger.info(f\"Substitution P{event.period}: Traditional={traditional_success}, Enhanced={enhanced_success}\")\n",
    "\n",
    "                # Handle first-action events (Enhanced method only)\n",
    "                self.handle_first_action_events(event, current_time)\n",
    "\n",
    "            # Collect final statistics\n",
    "            self.traditional_stats['flags'] = len(self.traditional_state.flags)\n",
    "            self.enhanced_stats['flags'] = len(self.enhanced_state.flags)\n",
    "\n",
    "            # Calculate lineup size distribution for traditional method\n",
    "            traditional_sizes = defaultdict(int)\n",
    "            for team_lineup in self.traditional_state.team_lineups.values():\n",
    "                traditional_sizes[len(team_lineup)] += 1\n",
    "\n",
    "            details = f\"Processed {len(self.processed_events)} events. \"\n",
    "            details += f\"Traditional: {self.traditional_stats['substitutions']} subs, {self.traditional_stats['flags']} flags, {self.traditional_stats['lineup_size_deviations']} size deviations. \"\n",
    "            details += f\"Enhanced: {self.enhanced_stats['substitutions']} subs, {self.enhanced_stats['first_actions']} first-actions, {self.enhanced_stats['auto_outs']} auto-outs.\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Process All Events\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(self.processed_events),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Process All Events\",\n",
    "                passed=False,\n",
    "                details=f\"Error processing events: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _reset_to_starters_enhanced(self):\n",
    "        \"\"\"Reset to starters for enhanced method (Q1, Q3)\"\"\"\n",
    "        if hasattr(self.entities, 'starters'):\n",
    "            for team_abbrev, starters_list in self.entities.starters.items():\n",
    "                if isinstance(starters_list, list):\n",
    "                    team_id = None\n",
    "                    for tid, tabbrev in self.entities.team_mapping.items():\n",
    "                        if isinstance(tid, int) and tabbrev == team_abbrev:\n",
    "                            team_id = tid\n",
    "                            break\n",
    "                    \n",
    "                    if team_id:\n",
    "                        starter_ids = {starter['player_id'] for starter in starters_list}\n",
    "                        self.enhanced_state.team_lineups[team_id] = starter_ids.copy()\n",
    "\n",
    "\n",
    "\n",
    "    def _step4_required_columns(self) -> Set[str]:\n",
    "        \"\"\"\n",
    "        The required dual-method lineup columns that Step 5 relies on.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"traditional_off_lineup\", \"traditional_def_lineup\",\n",
    "            \"enhanced_off_lineup\", \"enhanced_def_lineup\"\n",
    "        }\n",
    "\n",
    "    def _write_step4_contract_stamp(self, table_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Write a versioned contract stamp indicating Step 4 produced the expected schema.\n",
    "        This does not alter data; it records meta only.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Discover columns & row count of the output table\n",
    "            cols = [r[1] for r in self.conn.execute(f\"PRAGMA table_info('{table_name}')\").fetchall()]\n",
    "            row_count = self.conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "\n",
    "            # Create contract table if missing\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS pipeline_contract (\n",
    "                    component    VARCHAR,\n",
    "                    version      VARCHAR,\n",
    "                    table_name   VARCHAR,\n",
    "                    columns_json VARCHAR,\n",
    "                    row_count    BIGINT,\n",
    "                    created_at   TIMESTAMP\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "            # Insert a new stamp\n",
    "            self.conn.execute(\"\"\"\n",
    "                INSERT INTO pipeline_contract(component, version, table_name, columns_json, row_count, created_at)\n",
    "                VALUES (?, ?, ?, ?, ?, now())\n",
    "            \"\"\", (\"step4\", \"dual_lineups_v1\", table_name, json.dumps(cols, ensure_ascii=True), row_count))\n",
    "\n",
    "            logger.info(f\"[CONTRACT] Step 4 stamped version 'dual_lineups_v1' for table '{table_name}' ({row_count} rows)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[CONTRACT] Failed to write Step 4 contract stamp: {e}\")\n",
    "\n",
    "    def validate_step4_schema(self) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that step4_processed_events contains the expected dual-method columns.\n",
    "        Logs full column list and returns True/False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cols = [r[1] for r in self.conn.execute(\"PRAGMA table_info('step4_processed_events')\").fetchall()]\n",
    "            required = self._step4_required_columns()\n",
    "            ok = required.issubset(set(cols))\n",
    "            logger.info(f\"[VALIDATE] Step 4 schema columns: {cols}\")\n",
    "            if not ok:\n",
    "                missing = sorted(required - set(cols))\n",
    "                logger.error(f\"[VALIDATE] Step 4 schema missing required columns: {missing}\")\n",
    "            return ok\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[VALIDATE] Error validating Step 4 schema: {e}\")\n",
    "            return False\n",
    "\n",
    "    def create_step4_output_tables(self) -> ValidationResult:\n",
    "        \"\"\"Create Step 4 output tables integrating both methods (schema-safe)\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Creating Step 4 output tables with both tracking methods...\")\n",
    "\n",
    "            # --- Hard drop both VIEW and TABLE (handles stale views/tables) ---\n",
    "            for obj in (\"step4_processed_events\", \"step4_traditional_flags\", \"step4_enhanced_flags\"):\n",
    "                try:\n",
    "                    self.conn.execute(f\"DROP VIEW IF EXISTS {obj}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    self.conn.execute(f\"DROP TABLE IF EXISTS {obj}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Build events dataframe\n",
    "            events_data = []\n",
    "            for event in self.processed_events:\n",
    "                events_data.append({\n",
    "                    'pbp_id': event.pbp_id,\n",
    "                    'period': event.period,\n",
    "                    'pbp_order': event.pbp_order,\n",
    "                    'wall_clock_int': event.wall_clock_int,\n",
    "                    'description': event.description,\n",
    "                    'msg_type': event.msg_type,\n",
    "                    'action_type': event.action_type,\n",
    "                    'off_team_id': event.off_team_id,\n",
    "                    'def_team_id': event.def_team_id,\n",
    "                    'player_id_1': event.player_id_1,\n",
    "                    'player_id_2': event.player_id_2,\n",
    "                    'player_id_3': event.player_id_3,\n",
    "                    'is_shot': bool(event.is_shot),\n",
    "                    'is_rim_attempt': bool(event.is_rim_attempt),\n",
    "                    'is_rim_make': bool(event.is_rim_make),\n",
    "                    'distance_ft': float(event.distance_ft) if event.distance_ft is not None else None,\n",
    "                    'is_substitution': bool(event.is_substitution),\n",
    "                    'points': int(event.points) if event.points is not None else 0,\n",
    "                    # Store lineups as ASCII JSON arrays for portability\n",
    "                    'traditional_off_lineup': json.dumps(list(event.traditional_off_lineup), ensure_ascii=True) if event.traditional_off_lineup else None,\n",
    "                    'traditional_def_lineup': json.dumps(list(event.traditional_def_lineup), ensure_ascii=True) if event.traditional_def_lineup else None,\n",
    "                    'enhanced_off_lineup': json.dumps(list(event.enhanced_off_lineup), ensure_ascii=True) if event.enhanced_off_lineup else None,\n",
    "                    'enhanced_def_lineup': json.dumps(list(event.enhanced_def_lineup), ensure_ascii=True) if event.enhanced_def_lineup else None\n",
    "                })\n",
    "\n",
    "            events_df = pd.DataFrame(events_data)\n",
    "\n",
    "            # Persist processed events\n",
    "            self.conn.register(\"events_temp\", events_df)\n",
    "            try:\n",
    "                self.conn.execute(\"\"\"\n",
    "                    CREATE TABLE step4_processed_events AS\n",
    "                    SELECT * FROM events_temp\n",
    "                    ORDER BY period, pbp_order, wall_clock_int\n",
    "                \"\"\")\n",
    "            finally:\n",
    "                self.conn.unregister(\"events_temp\")\n",
    "\n",
    "            # Traditional flags\n",
    "            traditional_flags_df = pd.DataFrame(self.traditional_state.flags)\n",
    "            if not traditional_flags_df.empty:\n",
    "                self.conn.register(\"trad_flags_temp\", traditional_flags_df)\n",
    "                try:\n",
    "                    self.conn.execute(\"CREATE TABLE step4_traditional_flags AS SELECT * FROM trad_flags_temp\")\n",
    "                finally:\n",
    "                    self.conn.unregister(\"trad_flags_temp\")\n",
    "\n",
    "            # Enhanced flags\n",
    "            enhanced_flags_df = pd.DataFrame(self.enhanced_state.flags)\n",
    "            if not enhanced_flags_df.empty:\n",
    "                self.conn.register(\"enh_flags_temp\", enhanced_flags_df)\n",
    "                try:\n",
    "                    self.conn.execute(\"CREATE TABLE step4_enhanced_flags AS SELECT * FROM enh_flags_temp\")\n",
    "                finally:\n",
    "                    self.conn.unregister(\"enh_flags_temp\")\n",
    "\n",
    "            # Method comparison (quick summary table)\n",
    "            comparison_data = [{\n",
    "                'method': 'Traditional',\n",
    "                'substitutions_processed': self.traditional_stats['substitutions'],\n",
    "                'flags_generated': self.traditional_stats['flags'],\n",
    "                'lineup_size_deviations': self.traditional_stats['lineup_size_deviations'],\n",
    "                'maintains_5_man_lineups': False\n",
    "            }, {\n",
    "                'method': 'Enhanced',\n",
    "                'substitutions_processed': self.enhanced_stats['substitutions'],\n",
    "                'flags_generated': self.enhanced_stats['flags'],\n",
    "                'first_action_injections': self.enhanced_stats['first_actions'],\n",
    "                'auto_out_corrections': self.enhanced_stats['auto_outs'],\n",
    "                'maintains_5_man_lineups': True\n",
    "            }]\n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            self.conn.register(\"comp_temp\", comparison_df)\n",
    "            try:\n",
    "                self.conn.execute(\"CREATE OR REPLACE TABLE step4_method_comparison AS SELECT * FROM comp_temp\")\n",
    "            finally:\n",
    "                self.conn.unregister(\"comp_temp\")\n",
    "\n",
    "            # --- Post-create schema validation & contract stamp ---\n",
    "            ok = self.validate_step4_schema()\n",
    "            self._write_step4_contract_stamp(\"step4_processed_events\")\n",
    "\n",
    "            details = (\n",
    "                f\"Created Step 4 output tables: processed_events ({len(events_data)} rows), \"\n",
    "                f\"traditional_flags ({len(traditional_flags_df)} rows), \"\n",
    "                f\"enhanced_flags ({len(enhanced_flags_df)} rows), method_comparison\"\n",
    "            )\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Step 4 Output Tables\",\n",
    "                passed=ok,\n",
    "                details=details if ok else details + \" [SCHEMA INVALID]\",\n",
    "                data_count=len(events_data),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Step 4 Output Tables\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating output tables: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "    def print_step4_summary(self):\n",
    "        \"\"\"Print Step 4 summary with both methods (ASCII only)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"NBA PIPELINE - STEP 4 SUMMARY (INTEGRATED WITH STEP 2)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"TRADITIONAL DATA-DRIVEN METHOD:\")\n",
    "        print(f\"  Substitutions Processed: {self.traditional_stats['substitutions']}\")\n",
    "        print(f\"  Flags Generated: {self.traditional_stats['flags']}\")\n",
    "        print(f\"  Lineup Size Deviations: {self.traditional_stats['lineup_size_deviations']}\")\n",
    "        print(\"  Current Lineup Sizes:\")\n",
    "        for team_id, lineup in self.traditional_state.team_lineups.items():\n",
    "            team_name = self.team_names.get(team_id, f\"Team_{team_id}\")\n",
    "            print(f\"    {team_name}: {len(lineup)} players\")\n",
    "\n",
    "        print(\"\\nENHANCED ESTIMATION METHOD:\")\n",
    "        print(f\"  Substitutions Processed: {self.enhanced_stats['substitutions']}\")\n",
    "        print(f\"  First-Action Injections: {self.enhanced_stats['first_actions']}\")\n",
    "        print(f\"  Auto-Out Corrections: {self.enhanced_stats['auto_outs']}\")\n",
    "        print(f\"  Flags Generated: {self.enhanced_stats['flags']}\")\n",
    "        print(\"  Current Lineup Sizes:\")\n",
    "        for team_id, lineup in self.enhanced_state.team_lineups.items():\n",
    "            team_name = self.team_names.get(team_id, f\"Team_{team_id}\")\n",
    "            print(f\"    {team_name}: {len(lineup)} players\")\n",
    "\n",
    "        print(f\"\\nTOTAL EVENTS PROCESSED: {len(self.processed_events)}\")\n",
    "\n",
    "        trad_correct = sum(1 for lineup in self.traditional_state.team_lineups.values() if len(lineup) == 5)\n",
    "        enh_correct = sum(1 for lineup in self.enhanced_state.team_lineups.values() if len(lineup) == 5)\n",
    "        total_lineups = max(1, len(self.traditional_state.team_lineups))\n",
    "\n",
    "        print(\"LINEUP SIZE ACCURACY:\")\n",
    "        print(f\"  Traditional: {trad_correct}/{total_lineups} teams have 5-man lineups \"\n",
    "            f\"({trad_correct/total_lineups*100:.1f}%)\")\n",
    "        print(f\"  Enhanced: {enh_correct}/{total_lineups} teams have 5-man lineups \"\n",
    "            f\"({enh_correct/total_lineups*100:.1f}%)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "def process_pbp_with_step2_integration(db_path: str = None, \n",
    "                                      entities: GameEntities = None) -> Tuple[bool, PBPProcessor]:\n",
    "    \"\"\"Process PBP events using integrated Step 2 methods\"\"\"\n",
    "    \n",
    "    print(\"NBA Pipeline - Step 4: Integrated PBP Processing (Updated with Step 2)\")\n",
    "    print(\"=\"*75)\n",
    "\n",
    "    if entities is None:\n",
    "        logger.error(\"GameEntities required for PBP processing\")\n",
    "        return False, None\n",
    "\n",
    "    with PBPProcessor(db_path, entities) as processor:\n",
    "\n",
    "        # Initialize lineups for both methods\n",
    "        logger.info(\"Step 4a: Initializing lineups for both tracking methods...\")\n",
    "        result = processor.initialize_lineups()\n",
    "        processor.validator.log_validation(result)\n",
    "        if not result.passed:\n",
    "            logger.error(\"Failed to initialize lineups\")\n",
    "            return False, processor\n",
    "\n",
    "        # Load PBP events with Step 2 classification\n",
    "        logger.info(\"Step 4b: Loading PBP events with Step 2 classification...\")\n",
    "        result = processor.load_pbp_events()\n",
    "        processor.validator.log_validation(result)\n",
    "        if not result.passed:\n",
    "            logger.error(\"Failed to load PBP events\")\n",
    "            return False, processor\n",
    "\n",
    "        # Process all events with both methods\n",
    "        logger.info(\"Step 4c: Processing events with both Traditional and Enhanced methods...\")\n",
    "        result = processor.process_all_events()\n",
    "        processor.validator.log_validation(result)\n",
    "\n",
    "        # Create output tables\n",
    "        logger.info(\"Step 4d: Creating Step 4 output tables...\")\n",
    "        result = processor.create_step4_output_tables()\n",
    "        print('output tables results===============', result)\n",
    "        processor.validator.log_validation(result)\n",
    "\n",
    "        # Print summary\n",
    "        processor.print_step4_summary()\n",
    "\n",
    "        success = processor.validator.print_validation_summary()\n",
    "        return success, processor\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from eda.data.nba_entities_extractor import extract_all_entities_robust\n",
    "\n",
    "    # Extract entities first\n",
    "    entities_success, entities = extract_all_entities_robust()\n",
    "    \n",
    "    if entities_success:\n",
    "        success, processor = process_pbp_with_step2_integration(entities=entities)\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\n✅ Step 4 Complete: Integrated processing with Step 2 methods\")\n",
    "            print(\"🎯 Both Traditional and Enhanced methods available for comparison\")\n",
    "        else:\n",
    "            print(\"\\n❌ Step 4 Failed: Review validation messages\")\n",
    "    else:\n",
    "        print(\"❌ Failed to get entities - cannot proceed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096613f",
   "metadata": {},
   "source": [
    "Step 5: Possession Engine & Lineup Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f33b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/data/nba_possession_engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/data/nba_possession_engine.py\n",
    "# Step 5: Dual-Method Possession Engine & Lineup Statistics Calculation\n",
    "\"\"\"\n",
    "NBA Pipeline - UPDATED Step 5: Dual-Method Possession Engine & Statistics\n",
    "=========================================================================\n",
    "\n",
    "UPDATED to integrate Step 2 findings:\n",
    "- Processes possessions using BOTH traditional and enhanced lineup methods\n",
    "- Generates separate statistics for each method\n",
    "- Includes comprehensive violation and validation reporting\n",
    "- Config-driven approach for automation settings\n",
    "- Exports both result sets for comparison\n",
    "\n",
    "Key Integration Points from Step 2:\n",
    "1. Uses traditional_lineup_state (variable lineup sizes, raw data adherence)\n",
    "2. Uses enhanced_lineup_state (5-man lineups, intelligent inference)  \n",
    "3. Generates violation reports for traditional method\n",
    "4. Comprehensive method comparison and validation\n",
    "5. Config-driven automation paths\n",
    "\n",
    "The possession engine determines when possessions change hands based on:\n",
    "- Made field goals\n",
    "- Turnovers\n",
    "- Defensive rebounds (after missed shots)\n",
    "- Free throw sequences\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "# Ensure we're in the right directory\n",
    "cwd = os.getcwd()\n",
    "if not cwd.endswith(\"airflow_project\"):\n",
    "    os.chdir('api/src/airflow_project')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Set, Any, NamedTuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "from eda.utils.nba_pipeline_analysis import NBADataValidator, ValidationResult\n",
    "from eda.data.nba_entities_extractor import GameEntities\n",
    "from eda.data.nba_pbp_processor import PBPProcessor, ProcessedEvent\n",
    "import ast\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    from utils.config import (\n",
    "        NBA_SUBSTITUTION_CONFIG,\n",
    "        RIM_DISTANCE_FEET,\n",
    "        COORDINATE_SCALE,\n",
    "        MINIMUM_SECONDS_PLAYED,\n",
    "        DUCKDB_PATH,\n",
    "        DUCKDB_DIR,\n",
    "        EXPORTS_DIR\n",
    "    )\n",
    "    CONFIG = NBA_SUBSTITUTION_CONFIG\n",
    "    RIM_THRESHOLD = RIM_DISTANCE_FEET\n",
    "    COORD_SCALE = COORDINATE_SCALE\n",
    "    MIN_SECONDS = MINIMUM_SECONDS_PLAYED\n",
    "    DB_PATH = str(DUCKDB_PATH)\n",
    "    EXPORT_DIR = EXPORTS_DIR\n",
    "except ImportError:\n",
    "    logger.warning(\"Config not available, using defaults\")\n",
    "    CONFIG = {\"debug\": {\"log_all_substitutions\": True}}\n",
    "    RIM_THRESHOLD = 4.0\n",
    "    COORD_SCALE = 10.0\n",
    "    MIN_SECONDS = 30\n",
    "    DB_PATH = \"mavs_enhanced.duckdb\"\n",
    "    EXPORT_DIR = Path(\"exports\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DualPossession(NamedTuple):\n",
    "    \"\"\"Represents a possession with both traditional and enhanced lineup contexts\"\"\"\n",
    "    possession_id: int\n",
    "    period: int\n",
    "    start_pbp_order: int\n",
    "    end_pbp_order: int\n",
    "    off_team_id: int\n",
    "    def_team_id: int\n",
    "\n",
    "    # Traditional method lineups (may not be 5 players)\n",
    "    traditional_off_lineup: Tuple[int, ...]\n",
    "    traditional_def_lineup: Tuple[int, ...]\n",
    "\n",
    "    # Enhanced method lineups (always 5 players)\n",
    "    enhanced_off_lineup: Tuple[int, ...]\n",
    "    enhanced_def_lineup: Tuple[int, ...]\n",
    "\n",
    "    points_scored: int\n",
    "    ended_by: str\n",
    "\n",
    "@dataclass\n",
    "class DualLineupStats:\n",
    "    \"\"\"Statistics for lineup with both method contexts\"\"\"\n",
    "    team_id: int\n",
    "    team_abbrev: str\n",
    "    lineup_method: str  # 'traditional' or 'enhanced'\n",
    "    player_ids: Tuple[int, ...]\n",
    "    player_names: List[str]\n",
    "    lineup_size: int  # Actual size (may vary for traditional)\n",
    "\n",
    "    # Possession counts\n",
    "    off_possessions: int = 0\n",
    "    def_possessions: int = 0\n",
    "\n",
    "    # Scoring\n",
    "    points_for: int = 0\n",
    "    points_against: int = 0\n",
    "\n",
    "    # Ratings (per 100 possessions)\n",
    "    off_rating: float = 0.0\n",
    "    def_rating: float = 0.0\n",
    "    net_rating: float = 0.0\n",
    "\n",
    "    # Validation flags\n",
    "    lineup_violations: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass  \n",
    "class DualPlayerRimStats:\n",
    "    \"\"\"Player rim defense statistics with method context\"\"\"\n",
    "    player_id: int\n",
    "    player_name: str\n",
    "    team_id: int\n",
    "    team_abbrev: str\n",
    "    method: str  # 'traditional' or 'enhanced'\n",
    "\n",
    "    # Possession counts\n",
    "    off_possessions: int = 0\n",
    "    def_possessions: int = 0\n",
    "\n",
    "    # Rim defense (when on court)\n",
    "    opp_rim_attempts_on: int = 0\n",
    "    opp_rim_makes_on: int = 0\n",
    "\n",
    "    # Rim defense (when off court)  \n",
    "    opp_rim_attempts_off: int = 0\n",
    "    opp_rim_makes_off: int = 0\n",
    "\n",
    "    # Calculated percentages\n",
    "    opp_rim_fg_pct_on: float = None\n",
    "    opp_rim_fg_pct_off: float = None\n",
    "    rim_defense_on_off: float = None\n",
    "\n",
    "class DualMethodPossessionEngine:\n",
    "    \"\"\"UPDATED: Possession engine that processes both traditional and enhanced methods\"\"\"\n",
    "\n",
    "    def __init__(self, db_path: str = None, entities: GameEntities = None):\n",
    "        self.db_path = db_path or DB_PATH\n",
    "        self.conn = None\n",
    "        self.entities = entities\n",
    "        self.validator = NBADataValidator()\n",
    "\n",
    "        # Dual possession tracking\n",
    "        self.dual_possessions = []\n",
    "\n",
    "        # Dual statistics containers\n",
    "        self.traditional_lineup_stats = {}  # (team_id, lineup_tuple) -> DualLineupStats\n",
    "        self.enhanced_lineup_stats = {}     # (team_id, lineup_tuple) -> DualLineupStats\n",
    "        self.traditional_player_stats = {}  # player_id -> DualPlayerRimStats\n",
    "        self.enhanced_player_stats = {}     # player_id -> DualPlayerRimStats\n",
    "\n",
    "        # Violation tracking\n",
    "        self.traditional_violations = []\n",
    "        self.enhanced_violations = []\n",
    "\n",
    "        # Method comparison metrics\n",
    "        self.method_comparison = {}\n",
    "\n",
    "        # Build entity mappings\n",
    "        self.player_team = {}\n",
    "        self.team_roster = {}\n",
    "        self.team_abbrev = {}\n",
    "        self.player_names = {}\n",
    "\n",
    "        self._build_entity_mappings()\n",
    "\n",
    "\n",
    "    def diagnose_pipeline_state(self) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive diagnostic of pipeline state, with schema/NULL audits for step4 + contract stamp if present.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"=== PIPELINE DIAGNOSTIC ===\")\n",
    "\n",
    "            # All table names\n",
    "            all_tables = self.conn.execute(\n",
    "                \"SELECT table_name FROM information_schema.tables ORDER BY table_name\"\n",
    "            ).fetchall()\n",
    "            table_names = [t[0] for t in all_tables]\n",
    "\n",
    "            diag = {\n",
    "                \"all_tables\": table_names,\n",
    "                \"step_requirements\": {\n",
    "                    \"step4_processed_events\": \"step4_processed_events\" in table_names,\n",
    "                    \"traditional_lineup_state\": \"traditional_lineup_state\" in table_names,\n",
    "                    \"enhanced_lineup_state\": \"enhanced_lineup_state\" in table_names,\n",
    "                    \"traditional_lineup_flags\": \"traditional_lineup_flags\" in table_names,\n",
    "                    \"enhanced_lineup_flags\": \"enhanced_lineup_flags\" in table_names,\n",
    "                },\n",
    "                \"alternative_tables\": {\n",
    "                    \"step4_traditional_flags\": \"step4_traditional_flags\" in table_names,\n",
    "                    \"step4_enhanced_flags\": \"step4_enhanced_flags\" in table_names,\n",
    "                    \"processed_events\": \"processed_events\" in table_names,\n",
    "                    \"traditional_violation_report\": \"traditional_violation_report\" in table_names,\n",
    "                    \"enhanced_violation_report\": \"enhanced_violation_report\" in table_names,\n",
    "                },\n",
    "                \"table_counts\": {},\n",
    "                \"sample_data\": {},\n",
    "                \"step4_schema\": {},\n",
    "                \"step4_null_audit\": {},\n",
    "                \"contract\": None,\n",
    "            }\n",
    "\n",
    "            # Counts & samples\n",
    "            key_tables = [\n",
    "                \"step4_processed_events\",\n",
    "                \"traditional_lineup_state\",\n",
    "                \"enhanced_lineup_state\",\n",
    "                \"traditional_lineup_flags\",\n",
    "                \"enhanced_lineup_flags\",\n",
    "                \"step4_traditional_flags\",\n",
    "                \"step4_enhanced_flags\",\n",
    "                \"traditional_violation_report\",\n",
    "                \"enhanced_violation_report\",\n",
    "            ]\n",
    "            for t in key_tables:\n",
    "                if t in table_names:\n",
    "                    try:\n",
    "                        cnt = self.conn.execute(f\"SELECT COUNT(*) FROM {t}\").fetchone()[0]\n",
    "                        diag[\"table_counts\"][t] = cnt\n",
    "                        if t in (\"step4_processed_events\", \"traditional_lineup_state\", \"enhanced_lineup_state\"):\n",
    "                            sample = self.conn.execute(f\"SELECT * FROM {t} LIMIT 3\").df()\n",
    "                            diag[\"sample_data\"][t] = sample.to_dict(\"records\") if not sample.empty else []\n",
    "                    except Exception as e:\n",
    "                        diag[\"table_counts\"][t] = f\"Error: {e}\"\n",
    "\n",
    "            # Deep audit of step4_processed_events schema + null counts\n",
    "            if \"step4_processed_events\" in table_names:\n",
    "                cols = self.conn.execute(\"PRAGMA table_info('step4_processed_events')\").fetchall()\n",
    "                colnames = [c[1] for c in cols]\n",
    "                diag[\"step4_schema\"][\"columns\"] = colnames\n",
    "                diag[\"step4_schema\"][\"has_legacy_lineups\"] = (\"off_lineup\" in colnames and \"def_lineup\" in colnames)\n",
    "                diag[\"step4_schema\"][\"has_traditional_lineups\"] = (\n",
    "                    \"traditional_off_lineup\" in colnames and \"traditional_def_lineup\" in colnames\n",
    "                )\n",
    "                diag[\"step4_schema\"][\"has_enhanced_lineups\"] = (\n",
    "                    \"enhanced_off_lineup\" in colnames and \"enhanced_def_lineup\" in colnames\n",
    "                )\n",
    "                diag[\"step4_schema\"][\"has_points\"] = \"points\" in colnames\n",
    "                diag[\"step4_schema\"][\"has_rim_flags\"] = all(c in colnames for c in (\"is_rim_attempt\", \"is_rim_make\"))\n",
    "\n",
    "                # Null audits (counts of non-null values)\n",
    "                to_audit = [\n",
    "                    \"off_lineup\", \"def_lineup\",\n",
    "                    \"traditional_off_lineup\", \"traditional_def_lineup\",\n",
    "                    \"enhanced_off_lineup\", \"enhanced_def_lineup\",\n",
    "                    \"points\", \"is_rim_attempt\", \"is_rim_make\"\n",
    "                ]\n",
    "                for c in to_audit:\n",
    "                    if c in colnames:\n",
    "                        try:\n",
    "                            n = self.conn.execute(\n",
    "                                f\"SELECT COUNT(*) FROM step4_processed_events WHERE {c} IS NOT NULL\"\n",
    "                            ).fetchone()[0]\n",
    "                            diag[\"step4_null_audit\"][c] = n\n",
    "                        except Exception as e:\n",
    "                            diag[\"step4_null_audit\"][c] = f\"Error: {e}\"\n",
    "\n",
    "            # Contract stamp if present\n",
    "            if \"pipeline_contract\" in table_names:\n",
    "                try:\n",
    "                    dfc = self.conn.execute(\"\"\"\n",
    "                        SELECT *\n",
    "                        FROM pipeline_contract\n",
    "                        WHERE component='step4'\n",
    "                        ORDER BY created_at DESC\n",
    "                        LIMIT 1\n",
    "                    \"\"\").df()\n",
    "                    if not dfc.empty:\n",
    "                        diag[\"contract\"] = dfc.to_dict(\"records\")[0]\n",
    "                except Exception as e:\n",
    "                    diag[\"contract\"] = f\"Error reading contract: {e}\"\n",
    "\n",
    "            logger.info(f\"Diagnostic results: {diag}\")\n",
    "            return diag\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in pipeline diagnostic: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "    def _build_entity_mappings(self):\n",
    "        \"\"\"Build entity mappings from provided entities\"\"\"\n",
    "        if not self.entities or not hasattr(self.entities, 'unique_players'):\n",
    "            return\n",
    "\n",
    "        up = self.entities.unique_players\n",
    "        if up is not None and not up.empty:\n",
    "            # Player mappings\n",
    "            for _, r in up.iterrows():\n",
    "                pid = int(r[\"player_id\"])\n",
    "                tid = int(r[\"team_id\"])\n",
    "                self.player_team[pid] = tid\n",
    "                self.player_names[pid] = str(r[\"player_name\"])\n",
    "\n",
    "                if tid not in self.team_roster:\n",
    "                    self.team_roster[tid] = set()\n",
    "                self.team_roster[tid].add(pid)\n",
    "\n",
    "            # Team mappings\n",
    "            for _, r in up.drop_duplicates(\"team_id\").iterrows():\n",
    "                tid = int(r[\"team_id\"])\n",
    "                self.team_abbrev[tid] = str(r[\"team_abbrev\"])\n",
    "\n",
    "    def _validate_lineup(self,\n",
    "                        lineup: Optional[Tuple[int, ...]],\n",
    "                        label: str,\n",
    "                        poss_id: int,\n",
    "                        method: str = \"enhanced\") -> None:\n",
    "        \"\"\"\n",
    "        Validate lineup contents according to method rules.\n",
    "\n",
    "        enhanced: exactly 5 unique players.\n",
    "        traditional: non-empty, all unique (size may be != 5).\n",
    "        \"\"\"\n",
    "        if lineup is None:\n",
    "            raise AssertionError(f\"[Possession {poss_id}] {label} lineup is None\")\n",
    "\n",
    "        if method == \"enhanced\":\n",
    "            if len(lineup) != 5:\n",
    "                raise AssertionError(\n",
    "                    f\"[Possession {poss_id}] {label} enhanced lineup len={len(lineup)} != 5 -> {lineup}\"\n",
    "                )\n",
    "            if len(set(lineup)) != 5:\n",
    "                raise AssertionError(\n",
    "                    f\"[Possession {poss_id}] {label} enhanced lineup has duplicates -> {lineup}\"\n",
    "                )\n",
    "        else:\n",
    "            if len(lineup) == 0:\n",
    "                raise AssertionError(f\"[Possession {poss_id}] {label} traditional lineup is empty\")\n",
    "            if len(set(lineup)) != len(lineup):\n",
    "                raise AssertionError(\n",
    "                    f\"[Possession {poss_id}] {label} traditional lineup has duplicates -> {lineup}\"\n",
    "                )\n",
    "\n",
    "    def _rebound_team(self, event) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Infer rebounder team for msgType==4 using player_id_1 if available,\n",
    "        falling back to def/off teams only if we cannot resolve player→team.\n",
    "        \"\"\"\n",
    "        # Correct attribute name on ProcessedEvent is player_id_1\n",
    "        pid = getattr(event, \"player_id_1\", None)\n",
    "        # Be tolerant if a dict-like row sneaks in\n",
    "        if pid is None and hasattr(event, \"__getitem__\"):\n",
    "            try:\n",
    "                pid = event[\"player_id_1\"]\n",
    "            except Exception:\n",
    "                pid = None\n",
    "\n",
    "        if pid is not None and pid in self.player_team:\n",
    "            return self.player_team[pid]\n",
    "\n",
    "        # Fallbacks (keep same order of preference)\n",
    "        if getattr(event, \"def_team_id\", None) is not None:\n",
    "            return event.def_team_id\n",
    "        if getattr(event, \"off_team_id\", None) is not None:\n",
    "            return event.off_team_id\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = duckdb.connect(self.db_path)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "\n",
    "\n",
    "    def _load_events_from_db_as_processed(self) -> List[ProcessedEvent]:\n",
    "        \"\"\"\n",
    "        Fallback loader: reconstruct ProcessedEvent objects from DuckDB table\n",
    "        'processed_events' created by Step 4. Returns [] if not found/empty.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure we have a connection\n",
    "            if self.conn is None:\n",
    "                self.conn = duckdb.connect(self.db_path)\n",
    "\n",
    "            # Check table existence\n",
    "            tables = set(t[0].lower() for t in self.conn.execute(\n",
    "                \"SELECT table_name FROM information_schema.tables\"\n",
    "            ).fetchall())\n",
    "            if \"processed_events\" not in tables:\n",
    "                return []\n",
    "\n",
    "            df = self.conn.execute(\"\"\"\n",
    "                SELECT\n",
    "                    pbp_id, period, pbp_order, wall_clock_int, description,\n",
    "                    msg_type, action_type, off_team_id, def_team_id,\n",
    "                    player_id_1, player_id_2, player_id_3,\n",
    "                    loc_x, loc_y, points,\n",
    "                    is_shot, is_rim_attempt, is_rim_make, distance_ft,\n",
    "                    is_substitution, sub_out_player, sub_in_player,\n",
    "                    off_lineup, def_lineup\n",
    "                FROM processed_events\n",
    "                ORDER BY period, pbp_order, wall_clock_int\n",
    "            \"\"\").df()\n",
    "            if df.empty:\n",
    "                return []\n",
    "\n",
    "            events: List[ProcessedEvent] = []\n",
    "            for _, r in df.iterrows():\n",
    "                # Parse lineup strings back to tuples\n",
    "                def _to_tuple(s):\n",
    "                    if pd.isna(s) or s is None or s == \"\":\n",
    "                        return None\n",
    "                    try:\n",
    "                        t = ast.literal_eval(str(s))\n",
    "                        # normalize to sorted tuple of ints\n",
    "                        if isinstance(t, (list, tuple)):\n",
    "                            return tuple(int(x) for x in t)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return None\n",
    "\n",
    "                ev = ProcessedEvent(\n",
    "                    pbp_id         = int(r[\"pbp_id\"]),\n",
    "                    period         = int(r[\"period\"]),\n",
    "                    pbp_order      = int(r[\"pbp_order\"]),\n",
    "                    wall_clock_int = int(r[\"wall_clock_int\"]) if pd.notna(r[\"wall_clock_int\"]) else 0,\n",
    "                    msg_type       = int(r[\"msg_type\"]) if pd.notna(r[\"msg_type\"]) else None,\n",
    "                    action_type    = int(r[\"action_type\"]) if pd.notna(r[\"action_type\"]) else None,\n",
    "                    description    = str(r[\"description\"]) if pd.notna(r[\"description\"]) else \"\",\n",
    "                    off_team_id    = int(r[\"off_team_id\"]) if pd.notna(r[\"off_team_id\"]) else None,\n",
    "                    def_team_id    = int(r[\"def_team_id\"]) if pd.notna(r[\"def_team_id\"]) else None,\n",
    "                    player_id_1    = int(r[\"player_id_1\"]) if pd.notna(r[\"player_id_1\"]) else None,\n",
    "                    player_id_2    = int(r[\"player_id_2\"]) if pd.notna(r[\"player_id_2\"]) else None,\n",
    "                    player_id_3    = int(r[\"player_id_3\"]) if pd.notna(r[\"player_id_3\"]) else None,\n",
    "                    loc_x          = int(r[\"loc_x\"]) if pd.notna(r[\"loc_x\"]) and r[\"loc_x\"] != 0 else None,\n",
    "                    loc_y          = int(r[\"loc_y\"]) if pd.notna(r[\"loc_y\"]) and r[\"loc_y\"] != 0 else None,\n",
    "                    points         = int(r[\"points\"]) if pd.notna(r[\"points\"]) else 0,\n",
    "\n",
    "                    is_shot        = bool(r[\"is_shot\"]) if pd.notna(r[\"is_shot\"]) else False,\n",
    "                    is_rim_attempt = bool(r[\"is_rim_attempt\"]) if pd.notna(r[\"is_rim_attempt\"]) else False,\n",
    "                    is_rim_make    = bool(r[\"is_rim_make\"]) if pd.notna(r[\"is_rim_make\"]) else False,\n",
    "                    distance_ft    = float(r[\"distance_ft\"]) if pd.notna(r[\"distance_ft\"]) else None,\n",
    "                    is_substitution= bool(r[\"is_substitution\"]) if pd.notna(r[\"is_substitution\"]) else False,\n",
    "                    sub_out_player = int(r[\"sub_out_player\"]) if pd.notna(r[\"sub_out_player\"]) else None,\n",
    "                    sub_in_player  = int(r[\"sub_in_player\"]) if pd.notna(r[\"sub_in_player\"]) else None,\n",
    "\n",
    "                    off_lineup     = _to_tuple(r[\"off_lineup\"]),\n",
    "                    def_lineup     = _to_tuple(r[\"def_lineup\"]),\n",
    "                )\n",
    "                events.append(ev)\n",
    "\n",
    "            return events\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load processed_events from DB: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _assert_step4_schema_or_rebuild(self, autorun: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Ensure 'step4_processed_events' contains the required dual-method lineup columns.\n",
    "        - If missing and autorun=True, invoke Step 4 to rebuild once, then re-check.\n",
    "        - Returns a dict: {\"ok\": bool, \"details\": str}\n",
    "        \"\"\"\n",
    "        required = {\n",
    "            \"traditional_off_lineup\", \"traditional_def_lineup\",\n",
    "            \"enhanced_off_lineup\", \"enhanced_def_lineup\"\n",
    "        }\n",
    "\n",
    "        # existence\n",
    "        tables = {t[0] for t in self.conn.execute(\"SELECT table_name FROM information_schema.tables\").fetchall()}\n",
    "        if \"step4_processed_events\" not in tables:\n",
    "            return {\"ok\": False, \"details\": \"Missing table 'step4_processed_events' (Step 4 not run).\"}\n",
    "\n",
    "        # schema\n",
    "        cols = {r[1] for r in self.conn.execute(\"PRAGMA table_info('step4_processed_events')\").fetchall()}\n",
    "        if required.issubset(cols):\n",
    "            return {\"ok\": True, \"details\": \"Step 4 schema satisfied (dual-method columns present).\"}\n",
    "\n",
    "        missing = sorted(required - cols)\n",
    "        msg = f\"Step 4 schema missing required columns: {missing}. Present columns: {sorted(cols)}\"\n",
    "\n",
    "        if not autorun:\n",
    "            # Do not fix; just report\n",
    "            logger.error(f\"[CONTRACT] {msg}\")\n",
    "            return {\"ok\": False, \"details\": msg + \" | autorun=False\"}\n",
    "\n",
    "        # Try to rebuild by running the real Step 4\n",
    "        logger.warning(f\"[CONTRACT] {msg} -> autorun=True, invoking Step 4 to rebuild...\")\n",
    "        try:\n",
    "            from eda.data.nba_pbp_processor import process_pbp_with_step2_integration\n",
    "            ok, _ = process_pbp_with_step2_integration(db_path=self.db_path, entities=self.entities)\n",
    "            if not ok:\n",
    "                return {\"ok\": False, \"details\": \"Invoked Step 4, but it reported failure.\"}\n",
    "\n",
    "            # Re-check\n",
    "            cols2 = {r[1] for r in self.conn.execute(\"PRAGMA table_info('step4_processed_events')\").fetchall()}\n",
    "            if required.issubset(cols2):\n",
    "                return {\"ok\": True, \"details\": \"Step 4 rebuilt and schema now satisfies contract.\"}\n",
    "            return {\"ok\": False, \"details\": \"Step 4 rebuilt but schema still missing required dual-method columns.\"}\n",
    "        except Exception as e:\n",
    "            return {\"ok\": False, \"details\": f\"Failed to invoke Step 4: {e}\"}\n",
    "\n",
    "\n",
    "    def load_dual_method_data(self, autorun_rebuild: bool = False) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Load upstream artifacts needed by Step 5.\n",
    "\n",
    "        Hard requirement:\n",
    "        - step4_processed_events with dual-method lineup columns (contract)\n",
    "\n",
    "        Optional (diagnostics/enrichment; absence should NOT block Step 5):\n",
    "        - traditional_lineup_state / enhanced_lineup_state\n",
    "        - traditional_lineup_flags / enhanced_lineup_flags\n",
    "        - step4_traditional_flags / step4_enhanced_flags (older naming)\n",
    "        - traditional_violation_report / enhanced_violation_report (read-only, previous Step 5 outputs)\n",
    "\n",
    "        We DO NOT fill in or synthesize missing data. We enforce the contract, report, and\n",
    "        optionally rebuild via Step 4 when autorun_rebuild=True.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Loading dual-method data from Step 2/4 integration...\")\n",
    "\n",
    "            # ---- Enforce Step 4 → Step 5 contract up front ----\n",
    "            contract = self._assert_step4_schema_or_rebuild(autorun=autorun_rebuild)\n",
    "            if not contract.get(\"ok\"):\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Load Dual Method Data\",\n",
    "                    passed=False,\n",
    "                    details=(\"Contract failed: \" + contract.get(\"details\", \"\")),\n",
    "                    processing_time=time.time() - start_time,\n",
    "                )\n",
    "\n",
    "            all_tables = self.conn.execute(\n",
    "                \"SELECT table_name FROM information_schema.tables ORDER BY table_name\"\n",
    "            ).fetchall()\n",
    "            logger.info(f\"DEBUG: Available tables in database: {[t[0] for t in all_tables]}\")\n",
    "\n",
    "            existing = {r[0] for r in all_tables}\n",
    "\n",
    "            # ---- Optional sources (don't fail if missing) ----\n",
    "            optional_expected = [\n",
    "                \"traditional_lineup_state\",\n",
    "                \"enhanced_lineup_state\",\n",
    "                \"traditional_lineup_flags\",\n",
    "                \"enhanced_lineup_flags\",\n",
    "                \"step4_traditional_flags\",\n",
    "                \"step4_enhanced_flags\",\n",
    "                # read-only outputs from previous Step 5 runs (if present)\n",
    "                \"traditional_violation_report\",\n",
    "                \"enhanced_violation_report\",\n",
    "            ]\n",
    "            present_optional = [t for t in optional_expected if t in existing]\n",
    "            missing_optional = [t for t in optional_expected if t not in existing]\n",
    "\n",
    "            logger.info(f\"DEBUG: Optional tables present: {present_optional}\")\n",
    "            logger.info(f\"DEBUG: Optional tables missing (non-blocking): {missing_optional}\")\n",
    "\n",
    "            # ---- Load flags (non-blocking): prefer official names, then step4_*, then violation_report ----\n",
    "            trad_flags = self._load_flags_with_fallback(\"traditional_lineup_flags\", \"step4_traditional_flags\")\n",
    "            enh_flags  = self._load_flags_with_fallback(\"enhanced_lineup_flags\", \"step4_enhanced_flags\")\n",
    "\n",
    "            self.traditional_violations = trad_flags.to_dict(\"records\") if not trad_flags.empty else []\n",
    "            self.enhanced_violations    = enh_flags.to_dict(\"records\") if not enh_flags.empty else []\n",
    "\n",
    "            details = (\n",
    "                \"Loaded Step 5 inputs. \"\n",
    "                f\"Flags: traditional={len(self.traditional_violations)}, enhanced={len(self.enhanced_violations)}. \"\n",
    "                f\"Optional sources missing (non-blocking): {missing_optional}.\"\n",
    "            )\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load Dual Method Data\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(self.traditional_violations) + len(self.enhanced_violations),\n",
    "                processing_time=time.time() - start_time,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Load Dual Method Data\",\n",
    "                passed=False,\n",
    "                details=f\"Error loading dual-method data: {e}\",\n",
    "                processing_time=time.time() - start_time,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def _create_missing_tables_from_alternatives(self, missing: List[str], existing: set) -> bool:\n",
    "        \"\"\"Create missing tables from alternative sources (never synthesizes step4_processed_events).\"\"\"\n",
    "        try:\n",
    "            logger.info(\"DEBUG: Attempting to create missing tables from alternatives...\")\n",
    "\n",
    "            # Only allow flag tables to be backfilled from their step4_* equivalents.\n",
    "            # DO NOT create step4_processed_events from legacy processed_events (schema would be wrong).\n",
    "            table_mappings = {\n",
    "                'step4_traditional_flags': 'traditional_lineup_flags',\n",
    "                'step4_enhanced_flags': 'enhanced_lineup_flags',\n",
    "                # 'processed_events': 'step4_processed_events'  # intentionally removed to avoid legacy schema propagation\n",
    "            }\n",
    "\n",
    "            created_count = 0\n",
    "            for alt_name, expected_name in table_mappings.items():\n",
    "                if expected_name.lower() in missing and alt_name.lower() in existing:\n",
    "                    logger.info(f\"DEBUG: Creating '{expected_name}' from '{alt_name}'\")\n",
    "                    try:\n",
    "                        self.conn.execute(f\"CREATE OR REPLACE TABLE {expected_name} AS SELECT * FROM {alt_name}\")\n",
    "                        created_count += 1\n",
    "                        logger.info(f\"DEBUG: Successfully created '{expected_name}'\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"DEBUG: Failed to create '{expected_name}' from '{alt_name}': {e}\")\n",
    "\n",
    "            logger.info(f\"DEBUG: Created {created_count} missing tables from alternatives\")\n",
    "            return created_count > 0\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DEBUG: Error creating missing tables: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _load_flags_with_fallback(self, primary_table: str, fallback_table: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load a flags-like table with sensible fallbacks and explicit debugs.\n",
    "\n",
    "        Resolution order:\n",
    "        1) primary_table (e.g., traditional_lineup_flags)\n",
    "        2) fallback_table (e.g., step4_traditional_flags)\n",
    "        3) read-only violation report (e.g., traditional_violation_report / enhanced_violation_report)\n",
    "\n",
    "        We DO NOT synthesize records. If none are found, return an empty DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tables = {\n",
    "                t[0].lower()\n",
    "                for t in self.conn.execute(\"SELECT table_name FROM information_schema.tables\").fetchall()\n",
    "            }\n",
    "\n",
    "            # try primary\n",
    "            target = None\n",
    "            if primary_table.lower() in tables:\n",
    "                target = primary_table\n",
    "                logger.info(f\"DEBUG: Using primary flags table '{primary_table}'\")\n",
    "            # else fallback\n",
    "            elif fallback_table.lower() in tables:\n",
    "                target = fallback_table\n",
    "                logger.info(f\"DEBUG: Using fallback flags table '{fallback_table}'\")\n",
    "            else:\n",
    "                # consider read-only prior outputs if they exist\n",
    "                alt_report = None\n",
    "                if primary_table.lower().startswith(\"traditional\"):\n",
    "                    alt_report = \"traditional_violation_report\"\n",
    "                elif primary_table.lower().startswith(\"enhanced\"):\n",
    "                    alt_report = \"enhanced_violation_report\"\n",
    "\n",
    "                if alt_report and alt_report.lower() in tables:\n",
    "                    target = alt_report\n",
    "                    logger.info(\n",
    "                        f\"DEBUG: Using read-only prior output '{alt_report}' as violation source \"\n",
    "                        f\"(no synthesis; purely for context)\"\n",
    "                    )\n",
    "\n",
    "            if not target:\n",
    "                logger.warning(f\"DEBUG: No available tables among: '{primary_table}', '{fallback_table}', prior reports\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            # Determine a stable ordering column if present\n",
    "            order_cols = [c[1] for c in self.conn.execute(f\"PRAGMA table_info('{target}')\").fetchall()]\n",
    "            if \"abs_time\" in order_cols:\n",
    "                order_expr = \"abs_time\"\n",
    "            elif \"wall_clock_int\" in order_cols:\n",
    "                order_expr = \"wall_clock_int\"\n",
    "            elif \"pbp_order\" in order_cols:\n",
    "                order_expr = \"pbp_order\"\n",
    "            else:\n",
    "                order_expr = None\n",
    "\n",
    "            if order_expr:\n",
    "                df = self.conn.execute(f\"SELECT * FROM {target} ORDER BY {order_expr}\").df()\n",
    "            else:\n",
    "                df = self.conn.execute(f\"SELECT * FROM {target}\").df()\n",
    "\n",
    "            logger.info(f\"DEBUG: Loaded {len(df)} rows from '{target}' for flags/violations\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"DEBUG: Error loading flags from '{primary_table}'/'{fallback_table}': {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    ALLOW_LEGACY_FALLBACK = False\n",
    "\n",
    "    def identify_dual_possessions(self) -> ValidationResult:\n",
    "        \"\"\"FIXED: Enhanced possession identification with proper team attribution\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Identifying possessions with dual-method lineup contexts (FIXED MODE)...\")\n",
    "\n",
    "            event_count = self.conn.execute(\"SELECT COUNT(*) FROM step4_processed_events\").fetchone()[0]\n",
    "            logger.info(f\"DEBUG: step4_processed_events has {event_count} rows\")\n",
    "\n",
    "            cols = {r[1] for r in self.conn.execute(\"PRAGMA table_info('step4_processed_events')\").fetchall()}\n",
    "            have_trad = {\"traditional_off_lineup\", \"traditional_def_lineup\"}.issubset(cols)\n",
    "            have_enh  = {\"enhanced_off_lineup\", \"enhanced_def_lineup\"}.issubset(cols)\n",
    "\n",
    "            if not (have_trad and have_enh):\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Identify Dual Possessions\",\n",
    "                    passed=False,\n",
    "                    details=(\"Required columns missing in 'step4_processed_events'. \"\n",
    "                            \"Expected traditional_/enhanced_ lineups. Aborting without fallback.\"),\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            select_sql = \"\"\"\n",
    "                SELECT \n",
    "                    pbp_id, period, pbp_order, wall_clock_int, msg_type,\n",
    "                    off_team_id, def_team_id, points,\n",
    "                    traditional_off_lineup, traditional_def_lineup,\n",
    "                    enhanced_off_lineup, enhanced_def_lineup,\n",
    "                    player_id_1, description\n",
    "                FROM step4_processed_events\n",
    "                WHERE off_team_id IS NOT NULL AND def_team_id IS NOT NULL\n",
    "                ORDER BY period, pbp_order, wall_clock_int\n",
    "            \"\"\"\n",
    "\n",
    "            events_df = self.conn.execute(select_sql).df()\n",
    "            logger.info(f\"DEBUG: Retrieved {len(events_df)} events with valid team IDs\")\n",
    "\n",
    "            if events_df.empty:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Identify Dual Possessions\",\n",
    "                    passed=False,\n",
    "                    details=\"No events with dual lineup context found\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            def _parse_lineup(val) -> Optional[Tuple[int, ...]]:\n",
    "                if val is None or (isinstance(val, float) and pd.isna(val)) or str(val).strip() == \"\":\n",
    "                    return None\n",
    "                try:\n",
    "                    obj = json.loads(val) if isinstance(val, str) else val\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        obj = ast.literal_eval(str(val))\n",
    "                    except Exception:\n",
    "                        logger.debug(f\"Lineup parse failed; raw={val!r}\")\n",
    "                        return None\n",
    "                if isinstance(obj, (list, tuple, set)):\n",
    "                    try:\n",
    "                        tup = tuple(sorted(int(x) for x in obj))\n",
    "                        return tup\n",
    "                    except Exception:\n",
    "                        logger.debug(f\"Lineup normalization failed; raw={obj!r}\")\n",
    "                        return None\n",
    "                logger.debug(f\"Lineup had unexpected type; raw={obj!r}\")\n",
    "                return None\n",
    "\n",
    "            self.dual_possessions = []\n",
    "            possession_id = 0\n",
    "\n",
    "            # FIXED: Initialize possession state properly\n",
    "            current_possession = None\n",
    "            \n",
    "            # Enhanced debug tracking\n",
    "            points_by_possession = []\n",
    "            scoring_events_processed = []\n",
    "            team_points_tracker = {\"DAL\": 0, \"HOU\": 0}  # Track cumulative points by team\n",
    "\n",
    "            for idx, row in events_df.iterrows():\n",
    "                msg_type = int(row[\"msg_type\"]) if pd.notna(row[\"msg_type\"]) else None\n",
    "                event_team_id = int(row[\"off_team_id\"])\n",
    "                event_def_team = int(row[\"def_team_id\"])\n",
    "                event_points = int(row[\"points\"]) if pd.notna(row[\"points\"]) and row[\"points\"] > 0 else 0\n",
    "                \n",
    "                # Track every scoring event with detailed context\n",
    "                if event_points > 0:\n",
    "                    team_abbrev = self.team_abbrev.get(event_team_id, f\"Team{event_team_id}\")\n",
    "                    team_points_tracker[team_abbrev] += event_points\n",
    "                    \n",
    "                    scoring_event = {\n",
    "                        \"pbp_id\": int(row[\"pbp_id\"]),\n",
    "                        \"period\": int(row[\"period\"]),\n",
    "                        \"pbp_order\": int(row[\"pbp_order\"]),\n",
    "                        \"points\": event_points,\n",
    "                        \"off_team_id\": event_team_id,\n",
    "                        \"description\": str(row[\"description\"]),\n",
    "                        \"msg_type\": msg_type,\n",
    "                        \"current_possession_team\": current_possession[\"off_team_id\"] if current_possession else None,\n",
    "                        \"team_abbrev\": team_abbrev,\n",
    "                        \"cumulative_team_points\": team_points_tracker[team_abbrev]\n",
    "                    }\n",
    "                    scoring_events_processed.append(scoring_event)\n",
    "\n",
    "                # FIXED: Determine if we need to end current possession\n",
    "                should_end_possession = (\n",
    "                    current_possession is None or  # No current possession\n",
    "                    current_possession[\"off_team_id\"] != event_team_id or  # Team changed\n",
    "                    current_possession[\"def_team_id\"] != event_def_team or  # Defense changed\n",
    "                    msg_type in (1, 5, 12, 13) or  # Made shot, turnover, period boundaries\n",
    "                    (msg_type == 4 and self._is_defensive_rebound(row))  # Defensive rebound\n",
    "                )\n",
    "\n",
    "                # End current possession if needed\n",
    "                if should_end_possession and current_possession is not None:\n",
    "                    # Close current possession\n",
    "                    possession_info = {\n",
    "                        \"possession_id\": possession_id,\n",
    "                        \"team_id\": current_possession[\"off_team_id\"],\n",
    "                        \"points\": current_possession[\"points\"],\n",
    "                        \"period\": current_possession[\"period\"],\n",
    "                        \"start_order\": current_possession[\"start_order\"],\n",
    "                        \"end_order\": int(row[\"pbp_order\"]) - 1,\n",
    "                        \"ended_by\": self._determine_end_reason(row)\n",
    "                    }\n",
    "                    points_by_possession.append(possession_info)\n",
    "                    \n",
    "                    self.dual_possessions.append(\n",
    "                        DualPossession(\n",
    "                            possession_id=possession_id,\n",
    "                            period=current_possession[\"period\"],\n",
    "                            start_pbp_order=current_possession[\"start_order\"],\n",
    "                            end_pbp_order=int(row[\"pbp_order\"]) - 1,\n",
    "                            off_team_id=current_possession[\"off_team_id\"],\n",
    "                            def_team_id=current_possession[\"def_team_id\"],\n",
    "                            traditional_off_lineup=current_possession[\"trad_off\"],\n",
    "                            traditional_def_lineup=current_possession[\"trad_def\"],\n",
    "                            enhanced_off_lineup=current_possession[\"enh_off\"],\n",
    "                            enhanced_def_lineup=current_possession[\"enh_def\"],\n",
    "                            points_scored=current_possession[\"points\"],\n",
    "                            ended_by=possession_info[\"ended_by\"],\n",
    "                        )\n",
    "                    )\n",
    "                    possession_id += 1\n",
    "                    current_possession = None\n",
    "\n",
    "                # FIXED: Start new possession if needed (always when no current possession)\n",
    "                if current_possession is None:\n",
    "                    current_possession = {\n",
    "                        \"off_team_id\": event_team_id,\n",
    "                        \"def_team_id\": event_def_team,\n",
    "                        \"start_order\": int(row[\"pbp_order\"]),\n",
    "                        \"period\": int(row[\"period\"]),\n",
    "                        \"points\": 0,  # Will accumulate points for this possession\n",
    "                        \"trad_off\": _parse_lineup(row[\"traditional_off_lineup\"]),\n",
    "                        \"trad_def\": _parse_lineup(row[\"traditional_def_lineup\"]),\n",
    "                        \"enh_off\": _parse_lineup(row[\"enhanced_off_lineup\"]),\n",
    "                        \"enh_def\": _parse_lineup(row[\"enhanced_def_lineup\"])\n",
    "                    }\n",
    "\n",
    "                    # Validate enhanced lineups\n",
    "                    try:\n",
    "                        if current_possession[\"enh_off\"] is not None:\n",
    "                            self._validate_lineup(current_possession[\"enh_off\"], \"off\", possession_id, \"enhanced\")\n",
    "                        if current_possession[\"enh_def\"] is not None:\n",
    "                            self._validate_lineup(current_possession[\"enh_def\"], \"def\", possession_id, \"enhanced\")\n",
    "                    except AssertionError as ae:\n",
    "                        logger.warning(f\"Enhanced lineup validation failed: {ae}\")\n",
    "\n",
    "                # FIXED: Add points to current possession only if teams match\n",
    "                if event_points > 0 and current_possession[\"off_team_id\"] == event_team_id:\n",
    "                    current_possession[\"points\"] += event_points\n",
    "                elif event_points > 0:\n",
    "                    # Points belong to different team - log this mismatch but don't add to possession\n",
    "                    logger.warning(f\"SCORING EVENT TEAM MISMATCH: pbp_id={row['pbp_id']}, \"\n",
    "                                 f\"possession_team={current_possession['off_team_id']}, \"\n",
    "                                 f\"event_team={event_team_id}, points={event_points}\")\n",
    "\n",
    "            # Close the final possession\n",
    "            if current_possession is not None and not events_df.empty:\n",
    "                final_possession = {\n",
    "                    \"possession_id\": possession_id,\n",
    "                    \"team_id\": current_possession[\"off_team_id\"],\n",
    "                    \"points\": current_possession[\"points\"],\n",
    "                    \"period\": current_possession[\"period\"],\n",
    "                    \"start_order\": current_possession[\"start_order\"],\n",
    "                    \"end_order\": int(events_df.iloc[-1][\"pbp_order\"])\n",
    "                }\n",
    "                points_by_possession.append(final_possession)\n",
    "                \n",
    "                self.dual_possessions.append(\n",
    "                    DualPossession(\n",
    "                        possession_id=possession_id,\n",
    "                        period=current_possession[\"period\"],\n",
    "                        start_pbp_order=current_possession[\"start_order\"],\n",
    "                        end_pbp_order=int(events_df.iloc[-1][\"pbp_order\"]),\n",
    "                        off_team_id=current_possession[\"off_team_id\"],\n",
    "                        def_team_id=current_possession[\"def_team_id\"],\n",
    "                        traditional_off_lineup=current_possession[\"trad_off\"],\n",
    "                        traditional_def_lineup=current_possession[\"trad_def\"],\n",
    "                        enhanced_off_lineup=current_possession[\"enh_off\"],\n",
    "                        enhanced_def_lineup=current_possession[\"enh_def\"],\n",
    "                        points_scored=current_possession[\"points\"],\n",
    "                        ended_by=\"game_end\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # FIXED: Enhanced debug output with corrected calculations\n",
    "            logger.info(\"=== FIXED POSSESSION DEBUG ANALYSIS ===\")\n",
    "            total_possession_points = sum(p[\"points\"] for p in points_by_possession)\n",
    "            total_scoring_events = len(scoring_events_processed)\n",
    "            total_event_points = sum(e[\"points\"] for e in scoring_events_processed)\n",
    "            \n",
    "            logger.info(f\"Total Possessions Created: {len(self.dual_possessions):,}\")\n",
    "            logger.info(f\"Total Possession Points: {total_possession_points:,}\")\n",
    "            logger.info(f\"Total Scoring Events Processed: {total_scoring_events:,}\")\n",
    "            logger.info(f\"Total Event Points: {total_event_points:,}\")\n",
    "            \n",
    "            # Team-specific analysis with corrected logic\n",
    "            team_possession_points = {}\n",
    "            team_event_points = {}\n",
    "            \n",
    "            for p in points_by_possession:\n",
    "                team_abbrev = self.team_abbrev.get(p[\"team_id\"], f\"Team{p['team_id']}\")\n",
    "                team_possession_points[team_abbrev] = team_possession_points.get(team_abbrev, 0) + p[\"points\"]\n",
    "            \n",
    "            for e in scoring_events_processed:\n",
    "                team_abbrev = e[\"team_abbrev\"]\n",
    "                team_event_points[team_abbrev] = team_event_points.get(team_abbrev, 0) + e[\"points\"]\n",
    "            \n",
    "            logger.info(\"FIXED TEAM-BY-TEAM ANALYSIS:\")\n",
    "            for team in [\"DAL\", \"HOU\"]:\n",
    "                poss_pts = team_possession_points.get(team, 0)\n",
    "                event_pts = team_event_points.get(team, 0)\n",
    "                diff = poss_pts - event_pts  # Possession points should match event points\n",
    "                \n",
    "                logger.info(f\"  {team}:\")\n",
    "                logger.info(f\"    Possession Points: {poss_pts}\")\n",
    "                logger.info(f\"    Event Points: {event_pts}\")\n",
    "                logger.info(f\"    Difference: {diff:+}\")\n",
    "                \n",
    "                if diff != 0:\n",
    "                    logger.warning(f\"    *** {team} POINTS DISCREPANCY: {diff:+} points ***\")\n",
    "\n",
    "            details = (f\"FIXED: Identified {len(self.dual_possessions)} dual-method possessions, \"\n",
    "                      f\"{total_possession_points} possession points vs {total_event_points} event points.\")\n",
    "            \n",
    "            return ValidationResult(\n",
    "                step_name=\"Identify Dual Possessions\",\n",
    "                passed=len(self.dual_possessions) > 0,\n",
    "                details=details,\n",
    "                data_count=len(self.dual_possessions),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Identify Dual Possessions\",\n",
    "                passed=False,\n",
    "                details=f\"Error identifying dual possessions: {e}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def _is_defensive_rebound(self, row) -> bool:\n",
    "        \"\"\"Determine if rebound is defensive based on player team\"\"\"\n",
    "        if row['msg_type'] != 4:  # Not a rebound\n",
    "            return False\n",
    "\n",
    "        rebounder_id = row.get('player_id_1')\n",
    "        if not rebounder_id or rebounder_id not in self.player_team:\n",
    "            return False\n",
    "\n",
    "        rebounder_team = self.player_team[rebounder_id]\n",
    "        return rebounder_team == row['def_team_id']\n",
    "\n",
    "    def _determine_end_reason(self, row) -> str:\n",
    "        \"\"\"Determine why possession ended (made FG, turnover, defensive rebound, or period boundary).\"\"\"\n",
    "        mt = row.get('msg_type') if hasattr(row, 'get') else row['msg_type']\n",
    "        if mt == 1:\n",
    "            return \"made_fg\"\n",
    "        elif mt == 5:\n",
    "            return \"turnover\"\n",
    "        elif mt == 4 and self._is_defensive_rebound(row):\n",
    "            return \"def_rebound\"\n",
    "        elif mt in (12, 13):  # 12: start period; 13: end period\n",
    "            return \"period_boundary\"\n",
    "        else:\n",
    "            return \"team_change\"\n",
    "\n",
    "    def debug_points_flow_comprehensive(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Comprehensive points flow analysis to identify where the 4-point HOU discrepancy originates.\n",
    "        Traces from raw events -> processed events -> possessions -> lineups.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"=== COMPREHENSIVE POINTS FLOW DEBUG ===\")\n",
    "            \n",
    "            debug_info = {\n",
    "                \"raw_pbp_points\": {},\n",
    "                \"step4_processed_points\": {},\n",
    "                \"possession_points\": {},\n",
    "                \"lineup_points\": {},\n",
    "                \"discrepancies\": [],\n",
    "                \"detailed_scoring_events\": []\n",
    "            }\n",
    "            \n",
    "            # 1. Raw PBP points by team\n",
    "            raw_points = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    CASE \n",
    "                        WHEN team_id_off = 1610612742 THEN 'DAL'\n",
    "                        WHEN team_id_off = 1610612745 THEN 'HOU'\n",
    "                        ELSE CAST(team_id_off AS VARCHAR)\n",
    "                    END as team,\n",
    "                    SUM(COALESCE(points, 0)) as total_points,\n",
    "                    COUNT(*) as scoring_events\n",
    "                FROM pbp \n",
    "                WHERE points > 0 AND team_id_off IS NOT NULL\n",
    "                GROUP BY team_id_off\n",
    "                ORDER BY team\n",
    "            \"\"\").df()\n",
    "            \n",
    "            for _, row in raw_points.iterrows():\n",
    "                debug_info[\"raw_pbp_points\"][row[\"team\"]] = {\n",
    "                    \"points\": int(row[\"total_points\"]),\n",
    "                    \"events\": int(row[\"scoring_events\"])\n",
    "                }\n",
    "            \n",
    "            # 2. Step4 processed points by team\n",
    "            step4_points = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    CASE \n",
    "                        WHEN off_team_id = 1610612742 THEN 'DAL'\n",
    "                        WHEN off_team_id = 1610612745 THEN 'HOU'\n",
    "                        ELSE CAST(off_team_id AS VARCHAR)\n",
    "                    END as team,\n",
    "                    SUM(COALESCE(points, 0)) as total_points,\n",
    "                    COUNT(*) as scoring_events\n",
    "                FROM step4_processed_events \n",
    "                WHERE points > 0 AND off_team_id IS NOT NULL\n",
    "                GROUP BY off_team_id\n",
    "                ORDER BY team\n",
    "            \"\"\").df()\n",
    "            \n",
    "            for _, row in step4_points.iterrows():\n",
    "                debug_info[\"step4_processed_points\"][row[\"team\"]] = {\n",
    "                    \"points\": int(row[\"total_points\"]),\n",
    "                    \"events\": int(row[\"scoring_events\"])\n",
    "                }\n",
    "            \n",
    "            # 3. Possession-level points (from our dual possessions)\n",
    "            possession_totals = {}\n",
    "            for poss in self.dual_possessions:\n",
    "                team_abbrev = self.team_abbrev.get(poss.off_team_id, f\"Team{poss.off_team_id}\")\n",
    "                if team_abbrev not in possession_totals:\n",
    "                    possession_totals[team_abbrev] = {\"points\": 0, \"possessions\": 0}\n",
    "                possession_totals[team_abbrev][\"points\"] += poss.points_scored\n",
    "                possession_totals[team_abbrev][\"possessions\"] += 1\n",
    "            \n",
    "            debug_info[\"possession_points\"] = possession_totals\n",
    "            \n",
    "            # 4. Lineup-level points (both methods)\n",
    "            debug_info[\"lineup_points\"] = {}\n",
    "            for method in [\"traditional\", \"enhanced\"]:\n",
    "                lineup_stats = (self.traditional_lineup_stats if method == \"traditional\" \n",
    "                              else self.enhanced_lineup_stats)\n",
    "                method_totals = {}\n",
    "                for (team_id, lineup), stats in lineup_stats.items():\n",
    "                    team_abbrev = self.team_abbrev.get(team_id, f\"Team{team_id}\")\n",
    "                    if team_abbrev not in method_totals:\n",
    "                        method_totals[team_abbrev] = {\"points\": 0, \"lineups\": 0}\n",
    "                    method_totals[team_abbrev][\"points\"] += stats.points_for\n",
    "                    method_totals[team_abbrev][\"lineups\"] += 1\n",
    "                debug_info[\"lineup_points\"][method] = method_totals\n",
    "            \n",
    "            # 5. Detailed scoring events for HOU (the problematic team)\n",
    "            hou_events = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    pbp_id, period, pbp_order, description, points,\n",
    "                    off_team_id, msg_type, action_type,\n",
    "                    traditional_off_lineup, enhanced_off_lineup\n",
    "                FROM step4_processed_events \n",
    "                WHERE off_team_id = 1610612745 AND points > 0\n",
    "                ORDER BY period, pbp_order\n",
    "            \"\"\").df()\n",
    "            \n",
    "            debug_info[\"detailed_scoring_events\"] = hou_events.to_dict('records')\n",
    "            \n",
    "            # 6. Calculate discrepancies at each stage\n",
    "            teams = ['DAL', 'HOU']\n",
    "            for team in teams:\n",
    "                raw_pts = debug_info[\"raw_pbp_points\"].get(team, {}).get(\"points\", 0)\n",
    "                step4_pts = debug_info[\"step4_processed_points\"].get(team, {}).get(\"points\", 0)\n",
    "                poss_pts = debug_info[\"possession_points\"].get(team, {}).get(\"points\", 0)\n",
    "                \n",
    "                trad_lineup_pts = debug_info[\"lineup_points\"].get(\"traditional\", {}).get(team, {}).get(\"points\", 0)\n",
    "                enh_lineup_pts = debug_info[\"lineup_points\"].get(\"enhanced\", {}).get(team, {}).get(\"points\", 0)\n",
    "                \n",
    "                discrepancy = {\n",
    "                    \"team\": team,\n",
    "                    \"raw_pbp_points\": raw_pts,\n",
    "                    \"step4_processed_points\": step4_pts,\n",
    "                    \"possession_points\": poss_pts,\n",
    "                    \"traditional_lineup_points\": trad_lineup_pts,\n",
    "                    \"enhanced_lineup_points\": enh_lineup_pts,\n",
    "                    \"raw_to_step4_diff\": step4_pts - raw_pts,\n",
    "                    \"step4_to_possession_diff\": poss_pts - step4_pts,\n",
    "                    \"possession_to_traditional_diff\": trad_lineup_pts - poss_pts,\n",
    "                    \"possession_to_enhanced_diff\": enh_lineup_pts - poss_pts\n",
    "                }\n",
    "                debug_info[\"discrepancies\"].append(discrepancy)\n",
    "            \n",
    "            # Log findings\n",
    "            logger.info(\"POINTS FLOW ANALYSIS:\")\n",
    "            for disc in debug_info[\"discrepancies\"]:\n",
    "                team = disc[\"team\"]\n",
    "                logger.info(f\"  {team}:\")\n",
    "                logger.info(f\"    Raw PBP: {disc['raw_pbp_points']}\")\n",
    "                logger.info(f\"    Step4 Processed: {disc['step4_processed_points']} (diff: {disc['raw_to_step4_diff']:+})\")\n",
    "                logger.info(f\"    Possessions: {disc['possession_points']} (diff: {disc['step4_to_possession_diff']:+})\")\n",
    "                logger.info(f\"    Traditional Lineups: {disc['traditional_lineup_points']} (diff: {disc['possession_to_traditional_diff']:+})\")\n",
    "                logger.info(f\"    Enhanced Lineups: {disc['enhanced_lineup_points']} (diff: {disc['possession_to_enhanced_diff']:+})\")\n",
    "            \n",
    "            return debug_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in comprehensive points flow debug: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def debug_hou_scoring_events_detailed(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Detailed analysis of every HOU scoring event to identify potential double-counting\n",
    "        or attribution errors causing the 4-point discrepancy.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"=== DETAILED HOU SCORING EVENTS ANALYSIS ===\")\n",
    "            \n",
    "            # Get all HOU scoring events with context\n",
    "            hou_scoring = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    se.pbp_id, se.period, se.pbp_order, se.wall_clock_int,\n",
    "                    se.description, se.points, se.msg_type, se.action_type,\n",
    "                    se.player_id_1, se.player_id_2, se.player_id_3,\n",
    "                    se.traditional_off_lineup, se.enhanced_off_lineup,\n",
    "                    -- Get original PBP data for comparison\n",
    "                    pbp.points as original_points,\n",
    "                    pbp.description as original_description\n",
    "                FROM step4_processed_events se\n",
    "                LEFT JOIN pbp ON se.pbp_id = pbp.pbp_id\n",
    "                WHERE se.off_team_id = 1610612745 AND se.points > 0\n",
    "                ORDER BY se.period, se.pbp_order, se.wall_clock_int\n",
    "            \"\"\").df()\n",
    "            \n",
    "            analysis = {\n",
    "                \"total_hou_scoring_events\": len(hou_scoring),\n",
    "                \"total_hou_points_calculated\": int(hou_scoring['points'].sum()),\n",
    "                \"scoring_events_by_type\": {},\n",
    "                \"potential_issues\": [],\n",
    "                \"event_details\": []\n",
    "            }\n",
    "            \n",
    "            # Analyze by scoring event type\n",
    "            if not hou_scoring.empty:\n",
    "                by_type = hou_scoring.groupby('msg_type').agg({\n",
    "                    'points': ['count', 'sum'],\n",
    "                    'pbp_id': 'count'\n",
    "                }).round(2)\n",
    "                \n",
    "                analysis[\"scoring_events_by_type\"] = by_type.to_dict()\n",
    "                \n",
    "                # Check for potential issues\n",
    "                for _, event in hou_scoring.iterrows():\n",
    "                    event_detail = {\n",
    "                        \"pbp_id\": int(event['pbp_id']),\n",
    "                        \"period\": int(event['period']),\n",
    "                        \"pbp_order\": int(event['pbp_order']),\n",
    "                        \"description\": str(event['description']),\n",
    "                        \"points\": int(event['points']),\n",
    "                        \"original_points\": int(event['original_points']) if pd.notna(event['original_points']) else None,\n",
    "                        \"msg_type\": int(event['msg_type']),\n",
    "                        \"has_traditional_lineup\": bool(pd.notna(event['traditional_off_lineup']) and \n",
    "                                                     str(event['traditional_off_lineup']).strip() not in ['', '[]']),\n",
    "                        \"has_enhanced_lineup\": bool(pd.notna(event['enhanced_off_lineup']) and \n",
    "                                                  str(event['enhanced_off_lineup']).strip() not in ['', '[]'])\n",
    "                    }\n",
    "                    \n",
    "                    # Flag potential issues\n",
    "                    if event_detail[\"points\"] != event_detail[\"original_points\"]:\n",
    "                        analysis[\"potential_issues\"].append(f\"Points mismatch for pbp_id {event_detail['pbp_id']}: processed={event_detail['points']}, original={event_detail['original_points']}\")\n",
    "                    \n",
    "                    if not event_detail[\"has_traditional_lineup\"]:\n",
    "                        analysis[\"potential_issues\"].append(f\"No traditional lineup for scoring event pbp_id {event_detail['pbp_id']}\")\n",
    "                    \n",
    "                    if not event_detail[\"has_enhanced_lineup\"]:\n",
    "                        analysis[\"potential_issues\"].append(f\"No enhanced lineup for scoring event pbp_id {event_detail['pbp_id']}\")\n",
    "                    \n",
    "                    analysis[\"event_details\"].append(event_detail)\n",
    "            \n",
    "            # Check for possession attribution\n",
    "            hou_possession_points = sum(p.points_scored for p in self.dual_possessions if p.off_team_id == 1610612745)\n",
    "            analysis[\"possession_attribution\"] = {\n",
    "                \"total_possession_points\": hou_possession_points,\n",
    "                \"difference_from_events\": hou_possession_points - analysis[\"total_hou_points_calculated\"]\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"HOU Scoring Analysis:\")\n",
    "            logger.info(f\"  Total Scoring Events: {analysis['total_hou_scoring_events']}\")\n",
    "            logger.info(f\"  Total Points from Events: {analysis['total_hou_points_calculated']}\")\n",
    "            logger.info(f\"  Total Points from Possessions: {hou_possession_points}\")\n",
    "            logger.info(f\"  Potential Issues Found: {len(analysis['potential_issues'])}\")\n",
    "            \n",
    "            for issue in analysis[\"potential_issues\"][:10]:  # Log first 10 issues\n",
    "                logger.warning(f\"    {issue}\")\n",
    "            \n",
    "            return analysis\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in detailed HOU scoring analysis: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_dual_lineup_stats(self) -> ValidationResult:\n",
    "        \"\"\"Calculate lineup statistics for both traditional and enhanced methods\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Calculating dual-method lineup statistics...\")\n",
    "\n",
    "            if not self.dual_possessions:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Calculate Dual Lineup Stats\",\n",
    "                    passed=False,\n",
    "                    details=\"No dual possessions available\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Initialize stats containers\n",
    "            self.traditional_lineup_stats = {}\n",
    "            self.enhanced_lineup_stats = {}\n",
    "\n",
    "            # Process each possession for both methods\n",
    "            for poss in self.dual_possessions:\n",
    "                # Traditional method stats\n",
    "                if poss.traditional_off_lineup and poss.traditional_def_lineup:\n",
    "                    self._update_lineup_stats(\n",
    "                        poss, \"traditional\",\n",
    "                        poss.traditional_off_lineup, poss.traditional_def_lineup\n",
    "                    )\n",
    "\n",
    "                # Enhanced method stats\n",
    "                if poss.enhanced_off_lineup and poss.enhanced_def_lineup:\n",
    "                    self._update_lineup_stats(\n",
    "                        poss, \"enhanced\", \n",
    "                        poss.enhanced_off_lineup, poss.enhanced_def_lineup\n",
    "                    )\n",
    "\n",
    "            # Calculate ratings for both methods\n",
    "            self._calculate_ratings(self.traditional_lineup_stats)\n",
    "            self._calculate_ratings(self.enhanced_lineup_stats)\n",
    "\n",
    "            # Add violation context to traditional lineups\n",
    "            self._add_violation_context()\n",
    "\n",
    "            trad_count = len(self.traditional_lineup_stats)\n",
    "            enh_count = len(self.enhanced_lineup_stats)\n",
    "\n",
    "            details = f\"Calculated lineup stats: {trad_count} traditional, {enh_count} enhanced lineups\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Calculate Dual Lineup Stats\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=trad_count + enh_count,\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Calculate Dual Lineup Stats\",\n",
    "                passed=False,\n",
    "                details=f\"Error calculating dual lineup stats: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _update_lineup_stats(self, poss: DualPossession, method: str, \n",
    "                           off_lineup: Tuple[int, ...], def_lineup: Tuple[int, ...]):\n",
    "        \"\"\"Update lineup statistics for given method\"\"\"\n",
    "        stats_container = (self.traditional_lineup_stats if method == \"traditional\" \n",
    "                          else self.enhanced_lineup_stats)\n",
    "\n",
    "        # Offensive lineup\n",
    "        off_key = (poss.off_team_id, off_lineup)\n",
    "        if off_key not in stats_container:\n",
    "            stats_container[off_key] = self._create_lineup_stats(\n",
    "                poss.off_team_id, off_lineup, method\n",
    "            )\n",
    "\n",
    "        off_stats = stats_container[off_key]\n",
    "        off_stats.off_possessions += 1\n",
    "        off_stats.points_for += poss.points_scored\n",
    "\n",
    "        # Defensive lineup\n",
    "        def_key = (poss.def_team_id, def_lineup)\n",
    "        if def_key not in stats_container:\n",
    "            stats_container[def_key] = self._create_lineup_stats(\n",
    "                poss.def_team_id, def_lineup, method\n",
    "            )\n",
    "\n",
    "        def_stats = stats_container[def_key]\n",
    "        def_stats.def_possessions += 1\n",
    "        def_stats.points_against += poss.points_scored\n",
    "\n",
    "    def _create_lineup_stats(self, team_id: int, lineup: Tuple[int, ...], method: str) -> DualLineupStats:\n",
    "        \"\"\"Create lineup statistics object\"\"\"\n",
    "        team_abbrev = self.team_abbrev.get(team_id, f\"Team{team_id}\")\n",
    "        player_names = [self.player_names.get(pid, f\"Player{pid}\") for pid in lineup]\n",
    "\n",
    "        return DualLineupStats(\n",
    "            team_id=team_id,\n",
    "            team_abbrev=team_abbrev,\n",
    "            lineup_method=method,\n",
    "            player_ids=lineup,\n",
    "            player_names=player_names,\n",
    "            lineup_size=len(lineup)\n",
    "        )\n",
    "\n",
    "    def _calculate_ratings(self, stats_container: Dict):\n",
    "        \"\"\"Calculate offensive/defensive/net ratings\"\"\"\n",
    "        for stats in stats_container.values():\n",
    "            if stats.off_possessions > 0:\n",
    "                stats.off_rating = (100.0 * stats.points_for / stats.off_possessions)\n",
    "            if stats.def_possessions > 0:\n",
    "                stats.def_rating = (100.0 * stats.points_against / stats.def_possessions)\n",
    "            stats.net_rating = stats.off_rating - stats.def_rating\n",
    "\n",
    "    def _add_violation_context(self):\n",
    "        \"\"\"Add violation flags to traditional lineup stats\"\"\"\n",
    "        # Group violations by lineup characteristics if possible\n",
    "        violation_summary = defaultdict(list)\n",
    "\n",
    "        for violation in self.traditional_violations:\n",
    "            flag_type = violation.get('flag_type', 'unknown')\n",
    "            team_id = violation.get('team_id')\n",
    "            details = violation.get('description', '')\n",
    "\n",
    "            violation_summary[f\"team_{team_id}\"].append(f\"{flag_type}: {details}\")\n",
    "\n",
    "        # Add violations to lineup stats where applicable\n",
    "        for (team_id, lineup), stats in self.traditional_lineup_stats.items():\n",
    "            team_violations = violation_summary.get(f\"team_{team_id}\", [])\n",
    "            stats.lineup_violations = team_violations[:5]  # Top 5 violations\n",
    "\n",
    "\n",
    "    def calculate_dual_player_rim_stats(self) -> ValidationResult:\n",
    "        \"\"\"Calculate player rim defense statistics for both methods\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Calculating dual-method player rim defense statistics...\")\n",
    "\n",
    "            if not self.dual_possessions:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Calculate Dual Player Rim Stats\",\n",
    "                    passed=False,\n",
    "                    details=\"No dual possessions available\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Initialize player stats for both methods\n",
    "            self.traditional_player_stats = {}\n",
    "            self.enhanced_player_stats = {}\n",
    "\n",
    "            # Initialize all active players\n",
    "            if hasattr(self.entities, 'unique_players') and self.entities.unique_players is not None:\n",
    "                for _, r in self.entities.unique_players.iterrows():\n",
    "                    pid = int(r[\"player_id\"])\n",
    "\n",
    "                    # Traditional method player\n",
    "                    self.traditional_player_stats[pid] = DualPlayerRimStats(\n",
    "                        player_id=pid,\n",
    "                        player_name=str(r.get(\"player_name\", pid)),\n",
    "                        team_id=int(r.get(\"team_id\")) if pd.notna(r.get(\"team_id\")) else None,\n",
    "                        team_abbrev=str(r.get(\"team_abbrev\")) if pd.notna(r.get(\"team_abbrev\")) else None,\n",
    "                        method=\"traditional\"\n",
    "                    )\n",
    "\n",
    "                    # Enhanced method player\n",
    "                    self.enhanced_player_stats[pid] = DualPlayerRimStats(\n",
    "                        player_id=pid,\n",
    "                        player_name=str(r.get(\"player_name\", pid)),\n",
    "                        team_id=int(r.get(\"team_id\")) if pd.notna(r.get(\"team_id\")) else None,\n",
    "                        team_abbrev=str(r.get(\"team_abbrev\")) if pd.notna(r.get(\"team_abbrev\")) else None,\n",
    "                        method=\"enhanced\"\n",
    "                    )\n",
    "\n",
    "            # Count possessions for both methods\n",
    "            for poss in self.dual_possessions:\n",
    "                # Traditional method\n",
    "                if poss.traditional_off_lineup:\n",
    "                    for pid in poss.traditional_off_lineup:\n",
    "                        if pid in self.traditional_player_stats:\n",
    "                            self.traditional_player_stats[pid].off_possessions += 1\n",
    "\n",
    "                if poss.traditional_def_lineup:\n",
    "                    for pid in poss.traditional_def_lineup:\n",
    "                        if pid in self.traditional_player_stats:\n",
    "                            self.traditional_player_stats[pid].def_possessions += 1\n",
    "\n",
    "                # Enhanced method\n",
    "                if poss.enhanced_off_lineup:\n",
    "                    for pid in poss.enhanced_off_lineup:\n",
    "                        if pid in self.enhanced_player_stats:\n",
    "                            self.enhanced_player_stats[pid].off_possessions += 1\n",
    "\n",
    "                if poss.enhanced_def_lineup:\n",
    "                    for pid in poss.enhanced_def_lineup:\n",
    "                        if pid in self.enhanced_player_stats:\n",
    "                            self.enhanced_player_stats[pid].def_possessions += 1\n",
    "\n",
    "            # Calculate rim defense stats\n",
    "            self._calculate_dual_rim_defense()\n",
    "\n",
    "            trad_with_rim = sum(1 for s in self.traditional_player_stats.values() \n",
    "                               if s.opp_rim_attempts_on > 0)\n",
    "            enh_with_rim = sum(1 for s in self.enhanced_player_stats.values() \n",
    "                              if s.opp_rim_attempts_on > 0)\n",
    "\n",
    "            details = (f\"Calculated player rim stats: {trad_with_rim} traditional, \"\n",
    "                      f\"{enh_with_rim} enhanced players with rim data\")\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Calculate Dual Player Rim Stats\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(self.traditional_player_stats) + len(self.enhanced_player_stats),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Calculate Dual Player Rim Stats\",\n",
    "                passed=False,\n",
    "                details=f\"Error calculating dual player rim stats: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _calculate_dual_rim_defense(self):\n",
    "        \"\"\"Calculate rim defense stats using rim attempt events\"\"\"\n",
    "        try:\n",
    "            rim_events_df = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    def_team_id, is_rim_make,\n",
    "                    traditional_def_lineup, enhanced_def_lineup\n",
    "                FROM step4_processed_events\n",
    "                WHERE is_rim_attempt = TRUE AND def_team_id IS NOT NULL\n",
    "            \"\"\").df()\n",
    "            if rim_events_df.empty:\n",
    "                logger.debug(\"No rim events found; skipping rim-defense aggregation.\")\n",
    "                return\n",
    "\n",
    "            def _as_set(val) -> Set[int]:\n",
    "                if val is None or (isinstance(val, float) and pd.isna(val)) or str(val).strip() == \"\":\n",
    "                    return set()\n",
    "                try:\n",
    "                    obj = json.loads(val) if isinstance(val, str) else val\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        obj = ast.literal_eval(str(val))\n",
    "                    except Exception:\n",
    "                        logger.debug(f\"Rim lineup parse failed; raw={val!r}\")\n",
    "                        return set()\n",
    "                if not isinstance(obj, (list, tuple, set)):\n",
    "                    logger.debug(f\"Rim lineup had unexpected type; raw={obj!r}\")\n",
    "                    return set()\n",
    "                out = set()\n",
    "                for x in obj:\n",
    "                    try:\n",
    "                        out.add(int(x))\n",
    "                    except Exception:\n",
    "                        logger.debug(f\"Rim lineup member non-int; raw={x!r}\")\n",
    "                return out\n",
    "\n",
    "            for _, row in rim_events_df.iterrows():\n",
    "                def_team_id = int(row[\"def_team_id\"])\n",
    "                is_make = bool(row[\"is_rim_make\"])\n",
    "                trad_def = _as_set(row[\"traditional_def_lineup\"])\n",
    "                enh_def = _as_set(row[\"enhanced_def_lineup\"])\n",
    "                roster = self.team_roster.get(def_team_id, set())\n",
    "\n",
    "                # Traditional: on\n",
    "                for pid in trad_def:\n",
    "                    if pid in self.traditional_player_stats:\n",
    "                        s = self.traditional_player_stats[pid]\n",
    "                        s.opp_rim_attempts_on += 1\n",
    "                        if is_make:\n",
    "                            s.opp_rim_makes_on += 1\n",
    "                # Traditional: off (rest of roster)\n",
    "                for pid in (roster - trad_def):\n",
    "                    if pid in self.traditional_player_stats:\n",
    "                        s = self.traditional_player_stats[pid]\n",
    "                        s.opp_rim_attempts_off += 1\n",
    "                        if is_make:\n",
    "                            s.opp_rim_makes_off += 1\n",
    "\n",
    "                # Enhanced: on\n",
    "                for pid in enh_def:\n",
    "                    if pid in self.enhanced_player_stats:\n",
    "                        s = self.enhanced_player_stats[pid]\n",
    "                        s.opp_rim_attempts_on += 1\n",
    "                        if is_make:\n",
    "                            s.opp_rim_makes_on += 1\n",
    "                # Enhanced: off\n",
    "                for pid in (roster - enh_def):\n",
    "                    if pid in self.enhanced_player_stats:\n",
    "                        s = self.enhanced_player_stats[pid]\n",
    "                        s.opp_rim_attempts_off += 1\n",
    "                        if is_make:\n",
    "                            s.opp_rim_makes_off += 1\n",
    "\n",
    "            # Percentages (diagnostic only; no fill-ins)\n",
    "            for s in list(self.traditional_player_stats.values()) + list(self.enhanced_player_stats.values()):\n",
    "                if s.opp_rim_attempts_on > 0:\n",
    "                    s.opp_rim_fg_pct_on = s.opp_rim_makes_on / s.opp_rim_attempts_on\n",
    "                if s.opp_rim_attempts_off > 0:\n",
    "                    s.opp_rim_fg_pct_off = s.opp_rim_makes_off / s.opp_rim_attempts_off\n",
    "                if s.opp_rim_fg_pct_on is not None and s.opp_rim_fg_pct_off is not None:\n",
    "                    s.rim_defense_on_off = s.opp_rim_fg_pct_on - s.opp_rim_fg_pct_off\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating rim defense: {e}\")\n",
    "\n",
    "    def create_dual_method_tables(self) -> ValidationResult:\n",
    "        \"\"\"Create comprehensive tables for both traditional and enhanced methods\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Creating dual-method output tables...\")\n",
    "\n",
    "            if self.conn is None:\n",
    "                self.conn = duckdb.connect(self.db_path)\n",
    "\n",
    "            # Create traditional lineups table\n",
    "            trad_lineups_data = []\n",
    "            for (team_id, lineup), stats in self.traditional_lineup_stats.items():\n",
    "                # Pad lineup to 5 players if needed, or truncate if more\n",
    "                padded_lineup = list(lineup) + [None] * (5 - len(lineup))\n",
    "                padded_names = list(stats.player_names) + [\"\"] * (5 - len(stats.player_names))\n",
    "\n",
    "                row = {\n",
    "                    \"method\": \"traditional\",\n",
    "                    \"team_id\": team_id,\n",
    "                    \"team_abbrev\": stats.team_abbrev,\n",
    "                    \"lineup_size\": stats.lineup_size,\n",
    "                    \"player_1_id\": padded_lineup[0],\n",
    "                    \"player_1_name\": padded_names[0] if padded_names[0] else \"\",\n",
    "                    \"player_2_id\": padded_lineup[1] if len(padded_lineup) > 1 else None,\n",
    "                    \"player_2_name\": padded_names[1] if len(padded_names) > 1 and padded_names[1] else \"\",\n",
    "                    \"player_3_id\": padded_lineup[2] if len(padded_lineup) > 2 else None,\n",
    "                    \"player_3_name\": padded_names[2] if len(padded_names) > 2 and padded_names[2] else \"\",\n",
    "                    \"player_4_id\": padded_lineup[3] if len(padded_lineup) > 3 else None,\n",
    "                    \"player_4_name\": padded_names[3] if len(padded_names) > 3 and padded_names[3] else \"\",\n",
    "                    \"player_5_id\": padded_lineup[4] if len(padded_lineup) > 4 else None,\n",
    "                    \"player_5_name\": padded_names[4] if len(padded_names) > 4 and padded_names[4] else \"\",\n",
    "                    \"off_possessions\": stats.off_possessions,\n",
    "                    \"def_possessions\": stats.def_possessions,\n",
    "                    \"points_for\": stats.points_for,\n",
    "                    \"points_against\": stats.points_against,\n",
    "                    \"off_rating\": round(stats.off_rating, 1),\n",
    "                    \"def_rating\": round(stats.def_rating, 1),\n",
    "                    \"net_rating\": round(stats.net_rating, 1),\n",
    "                    \"violation_count\": len(stats.lineup_violations),\n",
    "                    \"violation_summary\": \"; \".join(stats.lineup_violations[:3])  # Top 3 violations\n",
    "                }\n",
    "                trad_lineups_data.append(row)\n",
    "\n",
    "            # Create enhanced lineups table\n",
    "            enh_lineups_data = []\n",
    "            for (team_id, lineup), stats in self.enhanced_lineup_stats.items():\n",
    "                row = {\n",
    "                    \"method\": \"enhanced\",\n",
    "                    \"team_id\": team_id,\n",
    "                    \"team_abbrev\": stats.team_abbrev,\n",
    "                    \"lineup_size\": stats.lineup_size,\n",
    "                    \"player_1_id\": lineup[0],\n",
    "                    \"player_1_name\": stats.player_names[0],\n",
    "                    \"player_2_id\": lineup[1],\n",
    "                    \"player_2_name\": stats.player_names[1],\n",
    "                    \"player_3_id\": lineup[2],\n",
    "                    \"player_3_name\": stats.player_names[2],\n",
    "                    \"player_4_id\": lineup[3],\n",
    "                    \"player_4_name\": stats.player_names[3],\n",
    "                    \"player_5_id\": lineup[4],\n",
    "                    \"player_5_name\": stats.player_names[4],\n",
    "                    \"off_possessions\": stats.off_possessions,\n",
    "                    \"def_possessions\": stats.def_possessions,\n",
    "                    \"points_for\": stats.points_for,\n",
    "                    \"points_against\": stats.points_against,\n",
    "                    \"off_rating\": round(stats.off_rating, 1),\n",
    "                    \"def_rating\": round(stats.def_rating, 1),\n",
    "                    \"net_rating\": round(stats.net_rating, 1),\n",
    "                    \"violation_count\": 0,  # Enhanced method maintains 5-man lineups\n",
    "                    \"violation_summary\": \"\"\n",
    "                }\n",
    "                enh_lineups_data.append(row)\n",
    "\n",
    "            # Combine and create unified lineups table\n",
    "            all_lineups_data = trad_lineups_data + enh_lineups_data\n",
    "            lineups_df = pd.DataFrame(all_lineups_data)\n",
    "\n",
    "            if not lineups_df.empty:\n",
    "                self.conn.register(\"dual_lineups_temp\", lineups_df)\n",
    "                try:\n",
    "                    self.conn.execute(\"\"\"\n",
    "                        CREATE OR REPLACE TABLE final_dual_lineups AS\n",
    "                        SELECT * FROM dual_lineups_temp\n",
    "                        ORDER BY method, team_abbrev, off_possessions DESC\n",
    "                    \"\"\")\n",
    "                finally:\n",
    "                    self.conn.unregister(\"dual_lineups_temp\")\n",
    "\n",
    "            # Create dual players table\n",
    "            trad_players_data = []\n",
    "            for pid, stats in self.traditional_player_stats.items():\n",
    "                trad_players_data.append({\n",
    "                    \"method\": \"traditional\",\n",
    "                    \"player_id\": pid,\n",
    "                    \"player_name\": stats.player_name,\n",
    "                    \"team_id\": stats.team_id,\n",
    "                    \"team_abbrev\": stats.team_abbrev,\n",
    "                    \"off_possessions\": stats.off_possessions,\n",
    "                    \"def_possessions\": stats.def_possessions,\n",
    "                    \"opp_rim_attempts_on\": stats.opp_rim_attempts_on,\n",
    "                    \"opp_rim_makes_on\": stats.opp_rim_makes_on,\n",
    "                    \"opp_rim_attempts_off\": stats.opp_rim_attempts_off,\n",
    "                    \"opp_rim_makes_off\": stats.opp_rim_makes_off,\n",
    "                    \"opp_rim_fg_pct_on\": round(stats.opp_rim_fg_pct_on, 3) if stats.opp_rim_fg_pct_on is not None else None,\n",
    "                    \"opp_rim_fg_pct_off\": round(stats.opp_rim_fg_pct_off, 3) if stats.opp_rim_fg_pct_off is not None else None,\n",
    "                    \"rim_defense_on_off\": round(stats.rim_defense_on_off, 3) if stats.rim_defense_on_off is not None else None\n",
    "                })\n",
    "\n",
    "            enh_players_data = []\n",
    "            for pid, stats in self.enhanced_player_stats.items():\n",
    "                enh_players_data.append({\n",
    "                    \"method\": \"enhanced\",\n",
    "                    \"player_id\": pid,\n",
    "                    \"player_name\": stats.player_name,\n",
    "                    \"team_id\": stats.team_id,\n",
    "                    \"team_abbrev\": stats.team_abbrev,\n",
    "                    \"off_possessions\": stats.off_possessions,\n",
    "                    \"def_possessions\": stats.def_possessions,\n",
    "                    \"opp_rim_attempts_on\": stats.opp_rim_attempts_on,\n",
    "                    \"opp_rim_makes_on\": stats.opp_rim_makes_on,\n",
    "                    \"opp_rim_attempts_off\": stats.opp_rim_attempts_off,\n",
    "                    \"opp_rim_makes_off\": stats.opp_rim_makes_off,\n",
    "                    \"opp_rim_fg_pct_on\": round(stats.opp_rim_fg_pct_on, 3) if stats.opp_rim_fg_pct_on is not None else None,\n",
    "                    \"opp_rim_fg_pct_off\": round(stats.opp_rim_fg_pct_off, 3) if stats.opp_rim_fg_pct_off is not None else None,\n",
    "                    \"rim_defense_on_off\": round(stats.rim_defense_on_off, 3) if stats.rim_defense_on_off is not None else None\n",
    "                })\n",
    "\n",
    "            all_players_data = trad_players_data + enh_players_data\n",
    "            players_df = pd.DataFrame(all_players_data)\n",
    "\n",
    "            if not players_df.empty:\n",
    "                self.conn.register(\"dual_players_temp\", players_df)\n",
    "                try:\n",
    "                    self.conn.execute(\"\"\"\n",
    "                        CREATE OR REPLACE TABLE final_dual_players AS\n",
    "                        SELECT * FROM dual_players_temp\n",
    "                        ORDER BY method, team_abbrev, player_name\n",
    "                    \"\"\")\n",
    "                finally:\n",
    "                    self.conn.unregister(\"dual_players_temp\")\n",
    "\n",
    "\n",
    "            # Create method comparison summary\n",
    "            self._create_method_comparison_table()\n",
    "\n",
    "            # Create violation reports\n",
    "            self._create_violation_reports()\n",
    "\n",
    "            details = (f\"Created dual-method tables: lineups({len(lineups_df)}), \"\n",
    "                      f\"players({len(players_df)}), comparisons, violations\")\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Dual Method Tables\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(lineups_df) + len(players_df),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Dual Method Tables\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating dual-method tables: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _create_method_comparison_table(self):\n",
    "        \"\"\"Create comprehensive method comparison table\"\"\"\n",
    "        try:\n",
    "            # Calculate comparison metrics\n",
    "            trad_total_lineups = len(self.traditional_lineup_stats)\n",
    "            enh_total_lineups = len(self.enhanced_lineup_stats)\n",
    "\n",
    "            trad_5man_lineups = sum(1 for stats in self.traditional_lineup_stats.values() \n",
    "                                   if stats.lineup_size == 5)\n",
    "            enh_5man_lineups = sum(1 for stats in self.enhanced_lineup_stats.values() \n",
    "                                  if stats.lineup_size == 5)\n",
    "\n",
    "            trad_avg_possessions = np.mean([stats.off_possessions + stats.def_possessions \n",
    "                                          for stats in self.traditional_lineup_stats.values()])\n",
    "            enh_avg_possessions = np.mean([stats.off_possessions + stats.def_possessions \n",
    "                                         for stats in self.enhanced_lineup_stats.values()])\n",
    "\n",
    "            comparison_data = [\n",
    "                {\n",
    "                    \"metric\": \"Total Lineups\",\n",
    "                    \"traditional_value\": trad_total_lineups,\n",
    "                    \"enhanced_value\": enh_total_lineups,\n",
    "                    \"difference\": enh_total_lineups - trad_total_lineups,\n",
    "                    \"better_method\": \"Enhanced\" if enh_total_lineups < trad_total_lineups else \"Traditional\"\n",
    "                },\n",
    "                {\n",
    "                    \"metric\": \"5-Man Lineups\",\n",
    "                    \"traditional_value\": trad_5man_lineups,\n",
    "                    \"enhanced_value\": enh_5man_lineups,\n",
    "                    \"difference\": enh_5man_lineups - trad_5man_lineups,\n",
    "                    \"better_method\": \"Enhanced\" if enh_5man_lineups > trad_5man_lineups else \"Traditional\"\n",
    "                },\n",
    "                {\n",
    "                    \"metric\": \"5-Man Percentage\",\n",
    "                    \"traditional_value\": round(100 * trad_5man_lineups / max(1, trad_total_lineups), 1),\n",
    "                    \"enhanced_value\": round(100 * enh_5man_lineups / max(1, enh_total_lineups), 1),\n",
    "                    \"difference\": round(100 * (enh_5man_lineups / max(1, enh_total_lineups) - \n",
    "                                             trad_5man_lineups / max(1, trad_total_lineups)), 1),\n",
    "                    \"better_method\": \"Enhanced\"\n",
    "                },\n",
    "                {\n",
    "                    \"metric\": \"Avg Possessions per Lineup\",\n",
    "                    \"traditional_value\": round(trad_avg_possessions, 1),\n",
    "                    \"enhanced_value\": round(enh_avg_possessions, 1),\n",
    "                    \"difference\": round(enh_avg_possessions - trad_avg_possessions, 1),\n",
    "                    \"better_method\": \"Enhanced\" if enh_avg_possessions > trad_avg_possessions else \"Traditional\"\n",
    "                },\n",
    "                {\n",
    "                    \"metric\": \"Violation Flags\",\n",
    "                    \"traditional_value\": len(self.traditional_violations),\n",
    "                    \"enhanced_value\": len(self.enhanced_violations),\n",
    "                    \"difference\": len(self.enhanced_violations) - len(self.traditional_violations),\n",
    "                    \"better_method\": \"Traditional\" if len(self.traditional_violations) < len(self.enhanced_violations) else \"Enhanced\"\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            self.conn.register(\"comparison_temp\", comparison_df)\n",
    "            self.conn.execute(\"\"\"\n",
    "                CREATE OR REPLACE TABLE method_comparison_summary AS\n",
    "                SELECT * FROM comparison_temp\n",
    "            \"\"\")\n",
    "            self.conn.unregister(\"comparison_temp\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating method comparison: {e}\")\n",
    "\n",
    "    def _create_violation_reports(self):\n",
    "        \"\"\"Create detailed violation reports for both methods with normalized schema and stable ordering.\n",
    "\n",
    "        Normalization rules (no synthesis of unknown values):\n",
    "        - Add 'order_ts' = first available of ['abs_time','wall_clock_int','pbp_order'] (as-is).\n",
    "        - Ensure 'period' exists (if not, leave absent).\n",
    "        - Ensure 'team_abbrev' exists; if only 'team_id' present, map using self.team_abbrev (deterministic).\n",
    "        - Ensure 'player_name' exists; if only 'player_id' present, map using self.player_names (deterministic).\n",
    "        - Pass through 'flag_type','description','flag_details' if present; omit if not.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            def _normalize(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "                if df_in is None or df_in.empty:\n",
    "                    return pd.DataFrame()\n",
    "\n",
    "                df = df_in.copy()\n",
    "\n",
    "                cols = set(c.lower() for c in df.columns)\n",
    "\n",
    "                # Standardize casing to avoid surprises later\n",
    "                rename_map = {c: c for c in df.columns}  # identity\n",
    "                # common alternates could be added here if you encounter them:\n",
    "                # e.g., rename_map['team'] = 'team_abbrev' if 'team' in df\n",
    "\n",
    "                df = df.rename(columns=rename_map)\n",
    "\n",
    "                # Build order_ts without assuming existence\n",
    "                order_candidates = [c for c in [\"abs_time\", \"wall_clock_int\", \"pbp_order\"] if c in df.columns]\n",
    "                if order_candidates:\n",
    "                    first = order_candidates[0]\n",
    "                    df[\"order_ts\"] = df[first]\n",
    "                # else: no order column at all; exporter will handle lack of ordering gracefully\n",
    "\n",
    "                # team_abbrev from team_id if needed\n",
    "                if \"team_abbrev\" not in df.columns and \"team_id\" in df.columns and self.team_abbrev:\n",
    "                    df[\"team_abbrev\"] = df[\"team_id\"].map(lambda tid: self.team_abbrev.get(int(tid)) if pd.notna(tid) else None)\n",
    "\n",
    "                # player_name from player_id if needed\n",
    "                if \"player_name\" not in df.columns and \"player_id\" in df.columns and self.player_names:\n",
    "                    df[\"player_name\"] = df[\"player_id\"].map(lambda pid: self.player_names.get(int(pid)) if pd.notna(pid) else None)\n",
    "\n",
    "                # Keep only a sensible set + anything extra that came in (don’t drop unknowns)\n",
    "                preferred_order = [\n",
    "                    \"period\", \"order_ts\", \"abs_time\", \"wall_clock_int\", \"pbp_order\",\n",
    "                    \"team_abbrev\", \"team_id\", \"flag_type\", \"player_name\", \"player_id\",\n",
    "                    \"description\", \"flag_details\"\n",
    "                ]\n",
    "                # Reorder columns: known first, then the rest in original order\n",
    "                known = [c for c in preferred_order if c in df.columns]\n",
    "                rest  = [c for c in df.columns if c not in known]\n",
    "                df = df[known + rest]\n",
    "\n",
    "                return df\n",
    "\n",
    "            def _write_table(temp_name: str, final_name: str, df: pd.DataFrame):\n",
    "                if df is None or df.empty:\n",
    "                    return\n",
    "                self.conn.register(temp_name, df)\n",
    "                try:\n",
    "                    cols = [c[1] for c in self.conn.execute(f\"PRAGMA table_info('{temp_name}')\").fetchall()]\n",
    "                    order_expr = \"order_ts\" if \"order_ts\" in cols else None\n",
    "                    if order_expr:\n",
    "                        self.conn.execute(f\"\"\"\n",
    "                            CREATE OR REPLACE TABLE {final_name} AS\n",
    "                            SELECT * FROM {temp_name}\n",
    "                            ORDER BY {order_expr}\n",
    "                        \"\"\")\n",
    "                    else:\n",
    "                        self.conn.execute(f\"\"\"\n",
    "                            CREATE OR REPLACE TABLE {final_name} AS\n",
    "                            SELECT * FROM {temp_name}\n",
    "                        \"\"\")\n",
    "                finally:\n",
    "                    self.conn.unregister(temp_name)\n",
    "\n",
    "            # Traditional\n",
    "            if self.traditional_violations:\n",
    "                trad_df = _normalize(pd.DataFrame(self.traditional_violations))\n",
    "                _write_table(\"trad_violations_temp\", \"traditional_violation_report\", trad_df)\n",
    "\n",
    "            # Enhanced\n",
    "            if self.enhanced_violations:\n",
    "                enh_df = _normalize(pd.DataFrame(self.enhanced_violations))\n",
    "                _write_table(\"enh_violations_temp\", \"enhanced_violation_report\", enh_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating violation reports: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def print_dual_method_summary(self):\n",
    "        \"\"\"Print comprehensive summary of both methods\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"NBA PIPELINE - STEP 5 DUAL-METHOD SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        print(\"POSSESSION ANALYSIS:\")\n",
    "        print(f\"  Total Dual Possessions: {len(self.dual_possessions):,}\")\n",
    "        if self.dual_possessions:\n",
    "            total_points = sum(p.points_scored for p in self.dual_possessions)\n",
    "            periods = len(set(p.period for p in self.dual_possessions))\n",
    "            print(f\"  Total Points: {total_points:,}\")\n",
    "            print(f\"  Periods: {periods}\")\n",
    "\n",
    "        print(\"\\nTRADITIONAL METHOD RESULTS:\")\n",
    "        print(f\"  Unique Lineups: {len(self.traditional_lineup_stats):,}\")\n",
    "        if self.traditional_lineup_stats:\n",
    "            trad_5man = sum(1 for s in self.traditional_lineup_stats.values() if s.lineup_size == 5)\n",
    "            print(f\"  5-Man Lineups: {trad_5man:,} ({100*trad_5man/len(self.traditional_lineup_stats):.1f}%)\")\n",
    "            trad_violations = len(self.traditional_violations)\n",
    "            print(f\"  Violation Flags: {trad_violations:,}\")\n",
    "\n",
    "        print(\"\\nENHANCED METHOD RESULTS:\")\n",
    "        print(f\"  Unique Lineups: {len(self.enhanced_lineup_stats):,}\")\n",
    "        if self.enhanced_lineup_stats:\n",
    "            enh_5man = sum(1 for s in self.enhanced_lineup_stats.values() if s.lineup_size == 5)\n",
    "            print(f\"  5-Man Lineups: {enh_5man:,} ({100*enh_5man/len(self.enhanced_lineup_stats):.1f}%)\")\n",
    "            enh_violations = len(self.enhanced_violations)\n",
    "            print(f\"  Violation Flags: {enh_violations:,}\")\n",
    "\n",
    "        print(\"\\nPLAYER RIM DEFENSE:\")\n",
    "        trad_with_rim = sum(1 for s in self.traditional_player_stats.values() if s.opp_rim_attempts_on > 0)\n",
    "        enh_with_rim = sum(1 for s in self.enhanced_player_stats.values() if s.opp_rim_attempts_on > 0)\n",
    "        print(f\"  Traditional Players with Rim Data: {trad_with_rim:,}\")\n",
    "        print(f\"  Enhanced Players with Rim Data: {enh_with_rim:,}\")\n",
    "\n",
    "        print(\"\\nMETHOD EFFECTIVENESS:\")\n",
    "        if self.traditional_lineup_stats and self.enhanced_lineup_stats:\n",
    "            improvement = len(self.traditional_lineup_stats) - len(self.enhanced_lineup_stats)\n",
    "            print(f\"  Lineup Count Change: {improvement:+,} (Enhanced has fewer unique lineups)\")\n",
    "\n",
    "            trad_5_pct = 100 * sum(1 for s in self.traditional_lineup_stats.values() if s.lineup_size == 5) / len(self.traditional_lineup_stats)\n",
    "            enh_5_pct = 100 * sum(1 for s in self.enhanced_lineup_stats.values() if s.lineup_size == 5) / len(self.enhanced_lineup_stats)\n",
    "            print(f\"  5-Man Accuracy: Traditional {trad_5_pct:.1f}% vs Enhanced {enh_5_pct:.1f}%\")\n",
    "\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    def create_project_output_tables(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        Project deliverables (no fallbacks):\n",
    "        - project1_lineups: 5-man lineups per team (ENHANCED ONLY) with Off/Def/Net ratings.\n",
    "        - project2_players: player-level rim defense on/off (ENHANCED ONLY).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # --- Project 1: 5-man lineups (enhanced only) ---\n",
    "            p1_rows = []\n",
    "            for (team_id, lineup), stats in self.enhanced_lineup_stats.items():\n",
    "                # Enhanced is guaranteed 5\n",
    "                if len(lineup) != 5:\n",
    "                    continue\n",
    "                row = {\n",
    "                    \"team_id\": team_id,\n",
    "                    \"team_abbrev\": stats.team_abbrev,\n",
    "                    \"player_1_id\": lineup[0], \"player_1_name\": stats.player_names[0],\n",
    "                    \"player_2_id\": lineup[1], \"player_2_name\": stats.player_names[1],\n",
    "                    \"player_3_id\": lineup[2], \"player_3_name\": stats.player_names[2],\n",
    "                    \"player_4_id\": lineup[3], \"player_4_name\": stats.player_names[3],\n",
    "                    \"player_5_id\": lineup[4], \"player_5_name\": stats.player_names[4],\n",
    "                    \"off_possessions\": stats.off_possessions,\n",
    "                    \"def_possessions\": stats.def_possessions,\n",
    "                    \"off_rating\": round(stats.off_rating, 1),\n",
    "                    \"def_rating\": round(stats.def_rating, 1),\n",
    "                    \"net_rating\": round(stats.net_rating, 1),\n",
    "                }\n",
    "                p1_rows.append(row)\n",
    "            p1_df = pd.DataFrame(p1_rows).sort_values(\n",
    "                [\"team_abbrev\", \"off_possessions\"], ascending=[True, False]\n",
    "            )\n",
    "            if not p1_df.empty:\n",
    "                self.conn.register(\"project1_temp\", p1_df)\n",
    "                try:\n",
    "                    self.conn.execute(\"\"\"\n",
    "                        CREATE OR REPLACE TABLE project1_lineups AS\n",
    "                        SELECT * FROM project1_temp\n",
    "                    \"\"\")\n",
    "                finally:\n",
    "                    self.conn.unregister(\"project1_temp\")\n",
    "\n",
    "            # --- Project 2: player rim defense on/off (enhanced only) ---\n",
    "            p2_rows = []\n",
    "            for pid, s in self.enhanced_player_stats.items():\n",
    "                p2_rows.append({\n",
    "                    \"player_id\": pid,\n",
    "                    \"player_name\": s.player_name,\n",
    "                    \"team_id\": s.team_id,\n",
    "                    \"team_abbrev\": s.team_abbrev,\n",
    "                    \"off_possessions\": s.off_possessions,\n",
    "                    \"def_possessions\": s.def_possessions,\n",
    "                    \"opp_rim_attempts_on\": s.opp_rim_attempts_on,\n",
    "                    \"opp_rim_makes_on\": s.opp_rim_makes_on,\n",
    "                    \"opp_rim_attempts_off\": s.opp_rim_attempts_off,\n",
    "                    \"opp_rim_makes_off\": s.opp_rim_makes_off,\n",
    "                    \"opp_rim_fg_pct_on\": round(s.opp_rim_fg_pct_on, 3) if s.opp_rim_fg_pct_on is not None else None,\n",
    "                    \"opp_rim_fg_pct_off\": round(s.opp_rim_fg_pct_off, 3) if s.opp_rim_fg_pct_off is not None else None,\n",
    "                    \"rim_defense_on_off\": round(s.rim_defense_on_off, 3) if s.rim_defense_on_off is not None else None,\n",
    "                })\n",
    "            p2_df = pd.DataFrame(p2_rows).sort_values([\"team_abbrev\", \"player_name\"])\n",
    "            if not p2_df.empty:\n",
    "                self.conn.register(\"project2_temp\", p2_df)\n",
    "                try:\n",
    "                    self.conn.execute(\"\"\"\n",
    "                        CREATE OR REPLACE TABLE project2_players AS\n",
    "                        SELECT * FROM project2_temp\n",
    "                    \"\"\")\n",
    "                finally:\n",
    "                    self.conn.unregister(\"project2_temp\")\n",
    "\n",
    "            details = (f\"Project outputs created: \"\n",
    "                    f\"project1_lineups({len(p1_df)}), project2_players({len(p2_df)})\")\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Project Output Tables\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(p1_df) + len(p2_df),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Create Project Output Tables\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating project outputs: {e}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "def run_dual_method_possession_engine(db_path: str = None,\n",
    "                                    entities: GameEntities = None) -> Tuple[bool, DualMethodPossessionEngine]:\n",
    "    \"\"\"Run the complete dual-method possession engine pipeline\"\"\"\n",
    "\n",
    "    print(\"NBA Pipeline - UPDATED Step 5: Dual-Method Possession Engine\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    if entities is None:\n",
    "        logger.error(\"GameEntities required for dual-method possession engine\")\n",
    "        return False, None\n",
    "\n",
    "    with DualMethodPossessionEngine(db_path, entities) as engine:\n",
    "\n",
    "        # Run comprehensive diagnostic first\n",
    "        logger.info(\"Step 5a: Running pipeline diagnostic...\")\n",
    "        diagnostic = engine.diagnose_pipeline_state()\n",
    "        logger.info(f\"Pipeline diagnostic completed: {diagnostic}\")\n",
    "\n",
    "        # Load dual-method data from Step 2/4 (autorun rebuild ON)\n",
    "        logger.info(\"Step 5b: Loading dual-method data...\")\n",
    "        result = engine.load_dual_method_data(autorun_rebuild=True)\n",
    "        engine.validator.log_validation(result)\n",
    "        if not result.passed:\n",
    "            return False, engine\n",
    "\n",
    "        # Identify possessions with dual lineup contexts\n",
    "        logger.info(\"Step 5c: Identifying dual-method possessions...\")\n",
    "        result = engine.identify_dual_possessions()\n",
    "        engine.validator.log_validation(result)\n",
    "        if not result.passed:\n",
    "            return False, engine\n",
    "\n",
    "        # Calculate lineup statistics for both methods\n",
    "        logger.info(\"Step 5d: Calculating dual-method lineup statistics...\")\n",
    "        result = engine.calculate_dual_lineup_stats()\n",
    "        engine.validator.log_validation(result)\n",
    "\n",
    "        # Calculate player rim statistics for both methods\n",
    "        logger.info(\"Step 5e: Calculating dual-method player rim statistics...\")\n",
    "        result = engine.calculate_dual_player_rim_stats()\n",
    "        engine.validator.log_validation(result)\n",
    "\n",
    "        # Run comprehensive debugging to identify points discrepancy\n",
    "        logger.info(\"Step 5f: Running comprehensive points flow debugging...\")\n",
    "        debug_info = engine.debug_points_flow_comprehensive()\n",
    "        logger.info(f\"Points flow debug completed: {debug_info}\")\n",
    "        \n",
    "        # Run detailed HOU analysis\n",
    "        logger.info(\"Step 5g: Running detailed HOU scoring events analysis...\")\n",
    "        hou_analysis = engine.debug_hou_scoring_events_detailed()\n",
    "        logger.info(f\"HOU analysis completed: {hou_analysis}\")\n",
    "\n",
    "        # Create comprehensive output tables (dual-method)\n",
    "        logger.info(\"Step 5f: Creating dual-method output tables...\")\n",
    "        result = engine.create_dual_method_tables()\n",
    "        engine.validator.log_validation(result)\n",
    "\n",
    "        # ---> NEW: Project deliverables (enhanced-only) <---\n",
    "        logger.info(\"Step 5g: Creating project deliverable tables (enhanced only)...\")\n",
    "        result = engine.create_project_output_tables()\n",
    "        engine.validator.log_validation(result)\n",
    "\n",
    "        # Print summary\n",
    "        engine.print_dual_method_summary()\n",
    "\n",
    "        success = engine.validator.print_validation_summary()\n",
    "        return success, engine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from eda.data.nba_entities_extractor import extract_all_entities_robust\n",
    "\n",
    "    database_path = \"mavs_enhanced.duckdb\"\n",
    "    ok, entities = extract_all_entities_robust(database_path)\n",
    "\n",
    "    if ok:\n",
    "        success, engine = run_dual_method_possession_engine(database_path, entities)\n",
    "        if success:\n",
    "            print(\"\\nUPDATED Step 5 Complete: Dual-method possession engine\")\n",
    "            print(\"Both Traditional and Enhanced statistics calculated\")\n",
    "            print(\"Ready for comprehensive dual-method export (Step 6)\")\n",
    "        else:\n",
    "            print(\"\\nUPDATED Step 5 Failed: Review validation messages\")\n",
    "    else:\n",
    "        print(\"Failed to get entities - cannot proceed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154503e",
   "metadata": {},
   "source": [
    "Step 6: Final Validation & Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8a88408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/data/nba_final_export.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/data/nba_final_export.py\n",
    "# Step 6: Dual-Method Final Validation & Export Results\n",
    "\"\"\"\n",
    "NBA Pipeline - UPDATED Step 6: Dual-Method Final Validation & Export\n",
    "====================================================================\n",
    "\n",
    "UPDATED to integrate Step 2 findings and dual-method approach:\n",
    "- Validates and exports BOTH traditional and enhanced method results\n",
    "- Includes comprehensive violation reports for traditional lineups\n",
    "- Generates method comparison and effectiveness analysis\n",
    "- Uses config-driven automation for paths and settings\n",
    "- Creates base dataset reports for final project submission\n",
    "- Comprehensive validation against box score data\n",
    "\n",
    "Key Integration Points from Step 2/5:\n",
    "1. Exports traditional results (variable lineup sizes, violations included)\n",
    "2. Exports enhanced results (5-man lineups, intelligent inference)\n",
    "3. Creates violation analysis and comparison reports\n",
    "4. Uses CONFIG for automation settings and paths\n",
    "5. Generates final project deliverables in specified format\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "# Ensure we're in the right directory\n",
    "cwd = os.getcwd()\n",
    "if not cwd.endswith(\"airflow_project\"):\n",
    "    os.chdir('api/src/airflow_project')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, field\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from eda.utils.nba_pipeline_analysis import NBADataValidator, ValidationResult\n",
    "from eda.data.nba_entities_extractor import GameEntities\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    from utils.config import (\n",
    "        NBA_SUBSTITUTION_CONFIG,\n",
    "        DUCKDB_PATH,\n",
    "        EXPORTS_DIR,\n",
    "        PROCESSED_DIR,\n",
    "        LOGS_DIR,\n",
    "        LINEUPS_OUTPUT_COLUMNS,\n",
    "        PLAYERS_OUTPUT_COLUMNS,\n",
    "        get_column_usage_report\n",
    "    )\n",
    "    CONFIG = NBA_SUBSTITUTION_CONFIG\n",
    "    DB_PATH = str(DUCKDB_PATH)\n",
    "    EXPORT_DIR = EXPORTS_DIR\n",
    "    PROCESSED_DIR = PROCESSED_DIR\n",
    "    LOGS_DIR = LOGS_DIR\n",
    "    LINEUP_COLUMNS = LINEUPS_OUTPUT_COLUMNS\n",
    "    PLAYER_COLUMNS = PLAYERS_OUTPUT_COLUMNS\n",
    "except ImportError:\n",
    "    logger.warning(\"Config not available, using defaults\")\n",
    "    CONFIG = {\"debug\": {\"log_all_substitutions\": True}}\n",
    "    DB_PATH = \"mavs_enhanced.duckdb\"\n",
    "    EXPORT_DIR = Path(\"exports\")\n",
    "    PROCESSED_DIR = Path(\"processed\")\n",
    "    LOGS_DIR = Path(\"logs\")\n",
    "    LINEUP_COLUMNS = [\"Team\", \"Player 1\", \"Player 2\", \"Player 3\", \"Player 4\", \"Player 5\", \n",
    "                     \"Offensive possessions played\", \"Defensive possessions played\",\n",
    "                     \"Offensive rating\", \"Defensive rating\", \"Net rating\"]\n",
    "    PLAYER_COLUMNS = [\"Player ID\", \"Player Name\", \"Team\", \"Offensive possessions played\", \n",
    "                     \"Defensive possessions played\", \n",
    "                     \"Opponent rim field goal percentage when player is on the court\",\n",
    "                     \"Opponent rim field goal percentage when player is off the court\",\n",
    "                     \"Opponent rim field goal percentage on/off difference (on-off)\"]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class DualMethodValidationTolerance:\n",
    "    \"\"\"Enhanced tolerance levels for dual-method validation\"\"\"\n",
    "    points_tolerance: int = 2\n",
    "    possession_tolerance_pct: float = 0.05\n",
    "    minutes_tolerance_sec: int = 30\n",
    "    rating_sanity_min: float = 50.0\n",
    "    rating_sanity_max: float = 200.0\n",
    "\n",
    "    # Method comparison tolerances\n",
    "    lineup_count_difference_max: int = 10  # Max acceptable difference in lineup counts\n",
    "    possession_difference_max_pct: float = 0.10  # Max 10% difference in possessions\n",
    "    rating_difference_max: float = 20.0  # Max 20 point rating difference\n",
    "\n",
    "@dataclass\n",
    "class ExportSummary:\n",
    "    \"\"\"Summary of exported files and metrics\"\"\"\n",
    "    files_exported: List[str] = field(default_factory=list)\n",
    "    traditional_lineups: int = 0\n",
    "    enhanced_lineups: int = 0\n",
    "    traditional_players: int = 0\n",
    "    enhanced_players: int = 0\n",
    "    violation_reports: int = 0\n",
    "    comparison_reports: int = 0\n",
    "    total_file_size_mb: float = 0.0\n",
    "\n",
    "class DualMethodFinalValidator:\n",
    "    \"\"\"UPDATED: Comprehensive dual-method validation and export\"\"\"\n",
    "\n",
    "    def __init__(self, db_path: str = None, entities: GameEntities = None, \n",
    "                 ascii_only: bool = True):\n",
    "        self.db_path = db_path or DB_PATH\n",
    "        self.conn = None\n",
    "        self.entities = entities\n",
    "        self.validator = NBADataValidator()\n",
    "        self.tolerance = DualMethodValidationTolerance()\n",
    "        self.ascii_only = ascii_only\n",
    "\n",
    "        # Export configuration using config system\n",
    "        self.export_dir = EXPORT_DIR\n",
    "        self.export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Processing directories\n",
    "        self.processed_dir = PROCESSED_DIR\n",
    "        self.processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.logs_dir = LOGS_DIR\n",
    "        self.logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Export summary\n",
    "        self.export_summary = ExportSummary()\n",
    "\n",
    "    # --- helper: map status icons to ASCII ---\n",
    "    @staticmethod\n",
    "    def _status_label(passed: bool) -> str:\n",
    "        return \"[PASS]\" if passed else \"[FAIL]\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _warn_label() -> str:\n",
    "        return \"WARN\"\n",
    "\n",
    "    def _sanitize_for_file(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Keep report readable everywhere. If ascii_only is True, strip/replace\n",
    "        non-ASCII chars. We intentionally replace just decoration; content remains.\n",
    "        \"\"\"\n",
    "        if not self.ascii_only:\n",
    "            return text\n",
    "        # Encode-decode via 'ascii' with 'replace' to surface any stray glyphs as '?'\n",
    "        # But first do targeted replacements for known emojis so we don't lose meaning.\n",
    "        text = (text\n",
    "                .replace(\"✅\", \"[PASS]\")\n",
    "                .replace(\"❌\", \"[FAIL]\")\n",
    "                .replace(\"⚠️\", \"WARN\")\n",
    "                .replace(\"⚠\", \"WARN\")\n",
    "                .replace(\"📁\", \"FILES\")\n",
    "                .replace(\"📄\", \"-\")\n",
    "                .replace(\"🏟️\", \"ARENA\")\n",
    "                .replace(\"🏠\", \"HOME\")\n",
    "                .replace(\"✈️\", \"AWAY\")\n",
    "                .replace(\"→\", \"->\"))\n",
    "        try:\n",
    "            return text.encode(\"ascii\", \"replace\").decode(\"ascii\")\n",
    "        except Exception:\n",
    "            # As a last resort, drop non-ascii\n",
    "            return \"\".join(ch if ord(ch) < 128 else \"?\" for ch in text)\n",
    "\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = duckdb.connect(self.db_path)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "    def validate_dual_method_tables_exist(self) -> ValidationResult:\n",
    "        \"\"\"Validate that both traditional and enhanced method tables exist\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Validating dual-method tables exist...\")\n",
    "\n",
    "            # Check for required dual-method tables\n",
    "            required_tables = [\n",
    "                'final_dual_lineups',\n",
    "                'final_dual_players', \n",
    "                'method_comparison_summary',\n",
    "                'traditional_violation_report',\n",
    "                'enhanced_violation_report'\n",
    "            ]\n",
    "\n",
    "            existing_tables = [\n",
    "                row[0] for row in self.conn.execute(\n",
    "                    \"SELECT table_name FROM information_schema.tables\"\n",
    "                ).fetchall()\n",
    "            ]\n",
    "\n",
    "            missing_tables = [t for t in required_tables if t not in existing_tables]\n",
    "            warnings = []\n",
    "\n",
    "            if missing_tables:\n",
    "                warnings.append(f\"Missing dual-method tables: {missing_tables}\")\n",
    "\n",
    "            # Check table contents\n",
    "            table_counts = {}\n",
    "            for table in [t for t in required_tables if t not in missing_tables]:\n",
    "                try:\n",
    "                    count = self.conn.execute(f\"SELECT COUNT(*) FROM {table}\").fetchone()[0]\n",
    "                    table_counts[table] = count\n",
    "                    if count == 0:\n",
    "                        warnings.append(f\"Table {table} exists but is empty\")\n",
    "                except Exception as e:\n",
    "                    warnings.append(f\"Error checking table {table}: {e}\")\n",
    "\n",
    "            # Validate dual-method structure\n",
    "            if 'final_dual_lineups' in table_counts:\n",
    "                method_counts = self.conn.execute(\"\"\"\n",
    "                    SELECT method, COUNT(*) as count\n",
    "                    FROM final_dual_lineups\n",
    "                    GROUP BY method\n",
    "                \"\"\").df()\n",
    "\n",
    "                if not method_counts.empty:\n",
    "                    methods = set(method_counts['method'].tolist())\n",
    "                    expected_methods = {'traditional', 'enhanced'}\n",
    "                    if methods != expected_methods:\n",
    "                        warnings.append(f\"Expected methods {expected_methods}, found {methods}\")\n",
    "\n",
    "                    for _, row in method_counts.iterrows():\n",
    "                        if row['method'] == 'traditional':\n",
    "                            self.export_summary.traditional_lineups = int(row['count'])\n",
    "                        elif row['method'] == 'enhanced':\n",
    "                            self.export_summary.enhanced_lineups = int(row['count'])\n",
    "\n",
    "            passed = len(missing_tables) == 0 and len([w for w in warnings if 'empty' in w]) == 0\n",
    "            details = f\"Dual-method tables validated: {table_counts}\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Dual Method Tables Check\",\n",
    "                passed=passed,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Dual Method Tables Check\",\n",
    "                passed=False,\n",
    "                details=f\"Error validating dual-method tables: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "    def validate_against_box_score_dual(self) -> ValidationResult:\n",
    "        \"\"\"Enhanced version with detailed debugging of the points mismatch\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Validating dual-method results against box score (DEBUG MODE)...\")\n",
    "\n",
    "            # 1) Box score totals with more detail\n",
    "            box_totals = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    team_abbrev,\n",
    "                    SUM(points) AS total_points,\n",
    "                    SUM(seconds_played) AS total_seconds,\n",
    "                    COUNT(*) AS active_players,\n",
    "                    STRING_AGG(player_name || ':' || CAST(points AS VARCHAR), ', ') AS player_breakdown\n",
    "                FROM box_score\n",
    "                WHERE status = 'ACTIVE'\n",
    "                GROUP BY team_abbrev\n",
    "                ORDER BY team_abbrev\n",
    "            \"\"\").df()\n",
    "\n",
    "            # 2) Calculated lineup totals with debugging info\n",
    "            method_totals = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    method,\n",
    "                    team_abbrev,\n",
    "                    SUM(points_for) AS calc_points_for,\n",
    "                    SUM(points_against) AS calc_points_against,\n",
    "                    SUM(off_possessions) AS total_off_poss,\n",
    "                    SUM(def_possessions) AS total_def_poss,\n",
    "                    COUNT(*) AS unique_lineups,\n",
    "                    AVG(off_rating) AS avg_off_rating,\n",
    "                    AVG(def_rating) AS avg_def_rating\n",
    "                FROM final_dual_lineups\n",
    "                GROUP BY method, team_abbrev\n",
    "                ORDER BY method, team_abbrev\n",
    "            \"\"\").df()\n",
    "\n",
    "            # 3) Raw event totals for comparison\n",
    "            raw_event_totals = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    CASE \n",
    "                        WHEN off_team_id = 1610612742 THEN 'DAL'\n",
    "                        WHEN off_team_id = 1610612745 THEN 'HOU'\n",
    "                        ELSE CAST(off_team_id AS VARCHAR)\n",
    "                    END as team_abbrev,\n",
    "                    SUM(COALESCE(points, 0)) AS raw_event_points,\n",
    "                    COUNT(CASE WHEN points > 0 THEN 1 END) AS scoring_events\n",
    "                FROM step4_processed_events\n",
    "                WHERE off_team_id IS NOT NULL\n",
    "                GROUP BY off_team_id\n",
    "                ORDER BY team_abbrev\n",
    "            \"\"\").df()\n",
    "\n",
    "            warnings = []\n",
    "            debug_lines = []\n",
    "\n",
    "            # 4) Enhanced per-team comparison\n",
    "            for _, box_row in box_totals.iterrows():\n",
    "                team = box_row['team_abbrev']\n",
    "                box_pts = int(box_row['total_points'])\n",
    "                \n",
    "                # Get raw event points\n",
    "                raw_row = raw_event_totals[raw_event_totals['team_abbrev'] == team]\n",
    "                raw_pts = int(raw_row.iloc[0]['raw_event_points']) if not raw_row.empty else 0\n",
    "                raw_events = int(raw_row.iloc[0]['scoring_events']) if not raw_row.empty else 0\n",
    "                \n",
    "                debug_lines.append(f\"=== {team} DEBUG ===\")\n",
    "                debug_lines.append(f\"Box Score: {box_pts} points ({box_row['active_players']} players)\")\n",
    "                debug_lines.append(f\"Raw Events: {raw_pts} points ({raw_events} scoring events)\")\n",
    "                debug_lines.append(f\"Player Breakdown: {box_row['player_breakdown']}\")\n",
    "                \n",
    "                # Check each method\n",
    "                for method in ['traditional', 'enhanced']:\n",
    "                    method_row = method_totals[\n",
    "                        (method_totals['method'] == method) & \n",
    "                        (method_totals['team_abbrev'] == team)\n",
    "                    ]\n",
    "                    \n",
    "                    if method_row.empty:\n",
    "                        warnings.append(f\"{method.title()} method missing data for team {team}\")\n",
    "                        debug_lines.append(f\"{method.title()}: MISSING DATA\")\n",
    "                        continue\n",
    "                    \n",
    "                    calc_pts = int(method_row.iloc[0]['calc_points_for'])\n",
    "                    lineups = int(method_row.iloc[0]['unique_lineups'])\n",
    "                    possessions = int(method_row.iloc[0]['total_off_poss'])\n",
    "                    avg_off_rating = float(method_row.iloc[0]['avg_off_rating'])\n",
    "                    \n",
    "                    diff = calc_pts - box_pts\n",
    "                    raw_diff = calc_pts - raw_pts\n",
    "                    \n",
    "                    tag = \"OK\" if diff == 0 else (\"SHORT\" if diff < 0 else \"OVER\")\n",
    "                    \n",
    "                    debug_lines.append(f\"{method.title()}: {calc_pts} points ({lineups} lineups, {possessions} poss)\")\n",
    "                    debug_lines.append(f\"  vs Box: {diff:+} | vs Raw Events: {raw_diff:+} | Rating: {avg_off_rating:.1f}\")\n",
    "                    \n",
    "                    if abs(diff) > self.tolerance.points_tolerance:\n",
    "                        warnings.append(f\"{method.title()} {team} points mismatch: box={box_pts}, calc={calc_pts} (diff={diff})\")\n",
    "                        \n",
    "                        # Additional debugging for mismatched teams\n",
    "                        if team == \"HOU\" and diff > 0:  # Focus on HOU overage\n",
    "                            debug_lines.append(f\"  *** HOU OVERAGE DETECTED: +{diff} points ***\")\n",
    "                            debug_lines.append(f\"  This suggests either:\")\n",
    "                            debug_lines.append(f\"    1. Double-counting of scoring events\")\n",
    "                            debug_lines.append(f\"    2. Attribution of points to wrong possessions\")\n",
    "                            debug_lines.append(f\"    3. Events not properly filtered\")\n",
    "\n",
    "                debug_lines.append(\"\")  # Spacing between teams\n",
    "\n",
    "            # 5) Log all debug information\n",
    "            for line in debug_lines:\n",
    "                logger.info(line)\n",
    "\n",
    "            # 6) Summary comparison\n",
    "            summary_lines = []\n",
    "            for _, row in box_totals.iterrows():\n",
    "                team = row['team_abbrev']\n",
    "                box_pts = int(row['total_points'])\n",
    "                \n",
    "                for method in ['traditional', 'enhanced']:\n",
    "                    method_data = method_totals[\n",
    "                        (method_totals['method'] == method) & \n",
    "                        (method_totals['team_abbrev'] == team)\n",
    "                    ]\n",
    "                    if not method_data.empty:\n",
    "                        calc_pts = int(method_data.iloc[0]['calc_points_for'])\n",
    "                        diff = calc_pts - box_pts\n",
    "                        tag = \"OK\" if diff == 0 else (\"SHORT\" if diff < 0 else \"OVER\")\n",
    "                        summary_lines.append(f\"{method.title()} {team}: calc={calc_pts}, box={box_pts}, diff={diff} ({tag})\")\n",
    "\n",
    "            detail_str = \" | \".join(summary_lines)\n",
    "            \n",
    "            return ValidationResult(\n",
    "                step_name=\"Dual Method Box Score Validation\",\n",
    "                passed=all((\"mismatch\" not in w and \"missing\" not in w) for w in warnings),\n",
    "                details=f\"Per-team results: {detail_str}\",\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Dual Method Box Score Validation\",\n",
    "                passed=False,\n",
    "                details=f\"Error validating dual methods against box score: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def validate_data_completeness(self) -> ValidationResult:\n",
    "        \"\"\"Validate completeness and quality of final results (robust, no UNION alias traps).\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Validating data completeness...\")\n",
    "\n",
    "            warnings = []\n",
    "\n",
    "            # ---- Lineup totals & averages ----\n",
    "            lineup_row = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) AS total_lineups,\n",
    "                    COUNT(CASE WHEN off_possessions > 0 THEN 1 END) AS lineups_with_off_poss,\n",
    "                    COUNT(CASE WHEN def_possessions > 0 THEN 1 END) AS lineups_with_def_poss,\n",
    "                    COUNT(CASE WHEN off_possessions > 0 AND def_possessions > 0 THEN 1 END) AS complete_lineups,\n",
    "                    AVG(off_possessions) AS avg_off_poss,\n",
    "                    AVG(def_possessions) AS avg_def_poss\n",
    "                FROM final_lineups\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            total_lineups      = lineup_row[0] or 0\n",
    "            with_off_lineups   = lineup_row[1] or 0\n",
    "            with_def_lineups   = lineup_row[2] or 0\n",
    "            complete_lineups   = lineup_row[3] or 0\n",
    "            avg_off_lineups    = float(lineup_row[4]) if lineup_row[4] is not None else 0.0\n",
    "            avg_def_lineups    = float(lineup_row[5]) if lineup_row[5] is not None else 0.0\n",
    "\n",
    "            if total_lineups == 0:\n",
    "                warnings.append(\"No lineups in final table\")\n",
    "            if complete_lineups < total_lineups * 0.8:\n",
    "                warnings.append(f\"Only {complete_lineups}/{total_lineups} lineups have both offensive and defensive data\")\n",
    "            if avg_off_lineups < 5:\n",
    "                warnings.append(f\"Low average offensive possessions per lineup: {avg_off_lineups:.1f}\")\n",
    "\n",
    "            # ---- Player totals ----\n",
    "            player_row = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) AS total_players,\n",
    "                    COUNT(CASE WHEN off_possessions > 0 THEN 1 END) AS players_with_off_poss,\n",
    "                    COUNT(CASE WHEN def_possessions > 0 THEN 1 END) AS players_with_def_poss,\n",
    "                    COUNT(CASE WHEN opp_rim_attempts_on  > 0 THEN 1 END) AS players_with_rim_on,\n",
    "                    COUNT(CASE WHEN opp_rim_attempts_off > 0 THEN 1 END) AS players_with_rim_off,\n",
    "                    COUNT(CASE WHEN rim_defense_on_off IS NOT NULL THEN 1 END) AS players_with_rim_diff\n",
    "                FROM final_players\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            total_players      = player_row[0] or 0\n",
    "            with_off_players   = player_row[1] or 0\n",
    "            with_def_players   = player_row[2] or 0\n",
    "            with_rim_on        = player_row[3] or 0\n",
    "            with_rim_off       = player_row[4] or 0\n",
    "            with_rim_diff      = player_row[5] or 0\n",
    "\n",
    "            if total_players == 0:\n",
    "                warnings.append(\"No players in final table\")\n",
    "            if with_rim_on == 0:\n",
    "                warnings.append(\"No players have rim defense data (on court)\")\n",
    "            if with_rim_diff < max(1, total_players * 0.5):\n",
    "                warnings.append(f\"Only {with_rim_diff}/{total_players} players have complete rim on/off data\")\n",
    "\n",
    "            # ---- Null checks (run as separate queries to avoid UNION alias mismatch) ----\n",
    "            lineup_nulls = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    SUM(CASE WHEN team_abbrev IS NULL THEN 1 ELSE 0 END) AS null_team,\n",
    "                    SUM(CASE WHEN off_possessions > 0 AND off_rating IS NULL THEN 1 ELSE 0 END) AS null_off_rating,\n",
    "                    SUM(CASE WHEN def_possessions > 0 AND def_rating IS NULL THEN 1 ELSE 0 END) AS null_def_rating\n",
    "                FROM final_lineups\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            players_nulls = self.conn.execute(\"\"\"\n",
    "                SELECT\n",
    "                    SUM(CASE WHEN team_abbrev IS NULL THEN 1 ELSE 0 END) AS null_team,\n",
    "                    -- treat blank/whitespace names as null for quality\n",
    "                    SUM(CASE WHEN player_name IS NULL OR LENGTH(TRIM(player_name)) = 0 THEN 1 ELSE 0 END) AS null_name\n",
    "                FROM final_players\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            # Lineups nulls\n",
    "            ln_null_team       = int(lineup_nulls[0] or 0)\n",
    "            ln_null_off_rating = int(lineup_nulls[1] or 0)\n",
    "            ln_null_def_rating = int(lineup_nulls[2] or 0)\n",
    "\n",
    "            if ln_null_team > 0:\n",
    "                warnings.append(f\"lineups table has {ln_null_team} records with null team\")\n",
    "            if ln_null_off_rating > 0:\n",
    "                warnings.append(f\"lineups table has {ln_null_off_rating} records with null offensive rating\")\n",
    "            if ln_null_def_rating > 0:\n",
    "                warnings.append(f\"lineups table has {ln_null_def_rating} records with null defensive rating\")\n",
    "\n",
    "            # Players nulls\n",
    "            pl_null_team = int(players_nulls[0] or 0)\n",
    "            pl_null_name = int(players_nulls[1] or 0)\n",
    "\n",
    "            if pl_null_team > 0:\n",
    "                warnings.append(f\"players table has {pl_null_team} records with null team\")\n",
    "            if pl_null_name > 0:\n",
    "                warnings.append(f\"players table has {pl_null_name} records with null or blank names\")\n",
    "\n",
    "            details = (\n",
    "                f\"Completeness check: {total_lineups} lineups, {total_players} players, \"\n",
    "                f\"{complete_lineups} complete lineups, {with_rim_diff} players with rim on/off\"\n",
    "            )\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Data Completeness\",\n",
    "                passed=(len(warnings) == 0),\n",
    "                details=details,\n",
    "                data_count=(total_lineups + total_players),\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Data Completeness\",\n",
    "                passed=False,\n",
    "                details=f\"Error validating completeness: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "    def export_project_deliverables(self) -> ValidationResult:\n",
    "        \"\"\"Export final project deliverables in required format\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Exporting project deliverables for both methods...\")\n",
    "\n",
    "            exported_files = []\n",
    "\n",
    "            # PROJECT 1: Lineup Tables (Both Methods)\n",
    "\n",
    "            # Traditional Method - Project 1\n",
    "            traditional_lineups = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    team_abbrev as \"{LINEUP_COLUMNS[0]}\",\n",
    "                    player_1_name as \"{LINEUP_COLUMNS[1]}\",\n",
    "                    player_2_name as \"{LINEUP_COLUMNS[2]}\",\n",
    "                    player_3_name as \"{LINEUP_COLUMNS[3]}\",\n",
    "                    player_4_name as \"{LINEUP_COLUMNS[4]}\",\n",
    "                    player_5_name as \"{LINEUP_COLUMNS[5]}\",\n",
    "                    off_possessions as \"{LINEUP_COLUMNS[6]}\",\n",
    "                    def_possessions as \"{LINEUP_COLUMNS[7]}\",\n",
    "                    off_rating as \"{LINEUP_COLUMNS[8]}\",\n",
    "                    def_rating as \"{LINEUP_COLUMNS[9]}\",\n",
    "                    net_rating as \"{LINEUP_COLUMNS[10]}\"\n",
    "                FROM final_dual_lineups\n",
    "                WHERE method = 'traditional' \n",
    "                AND (off_possessions > 0 OR def_possessions > 0)\n",
    "                ORDER BY team_abbrev, off_possessions DESC\n",
    "            \"\"\").df()\n",
    "\n",
    "            traditional_file = self.export_dir / \"project1_lineups_traditional.csv\"\n",
    "            traditional_lineups.to_csv(traditional_file, index=False)\n",
    "            exported_files.append(f\"project1_lineups_traditional.csv ({len(traditional_lineups)} lineups)\")\n",
    "\n",
    "            # Enhanced Method - Project 1\n",
    "            enhanced_lineups = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    team_abbrev as \"{LINEUP_COLUMNS[0]}\",\n",
    "                    player_1_name as \"{LINEUP_COLUMNS[1]}\",\n",
    "                    player_2_name as \"{LINEUP_COLUMNS[2]}\",\n",
    "                    player_3_name as \"{LINEUP_COLUMNS[3]}\",\n",
    "                    player_4_name as \"{LINEUP_COLUMNS[4]}\",\n",
    "                    player_5_name as \"{LINEUP_COLUMNS[5]}\",\n",
    "                    off_possessions as \"{LINEUP_COLUMNS[6]}\",\n",
    "                    def_possessions as \"{LINEUP_COLUMNS[7]}\",\n",
    "                    off_rating as \"{LINEUP_COLUMNS[8]}\",\n",
    "                    def_rating as \"{LINEUP_COLUMNS[9]}\",\n",
    "                    net_rating as \"{LINEUP_COLUMNS[10]}\"\n",
    "                FROM final_dual_lineups\n",
    "                WHERE method = 'enhanced'\n",
    "                AND (off_possessions > 0 OR def_possessions > 0)\n",
    "                ORDER BY team_abbrev, off_possessions DESC\n",
    "            \"\"\").df()\n",
    "\n",
    "            enhanced_file = self.export_dir / \"project1_lineups_enhanced.csv\"\n",
    "            enhanced_lineups.to_csv(enhanced_file, index=False)\n",
    "            exported_files.append(f\"project1_lineups_enhanced.csv ({len(enhanced_lineups)} lineups)\")\n",
    "\n",
    "            # PROJECT 2: Player Tables (Both Methods)\n",
    "\n",
    "            # Traditional Method - Project 2\n",
    "            traditional_players = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    player_id as \"{PLAYER_COLUMNS[0]}\",\n",
    "                    player_name as \"{PLAYER_COLUMNS[1]}\",\n",
    "                    team_abbrev as \"{PLAYER_COLUMNS[2]}\",\n",
    "                    off_possessions as \"{PLAYER_COLUMNS[3]}\",\n",
    "                    def_possessions as \"{PLAYER_COLUMNS[4]}\",\n",
    "                    COALESCE(opp_rim_fg_pct_on, 0) as \"{PLAYER_COLUMNS[5]}\",\n",
    "                    COALESCE(opp_rim_fg_pct_off, 0) as \"{PLAYER_COLUMNS[6]}\",\n",
    "                    COALESCE(rim_defense_on_off, 0) as \"{PLAYER_COLUMNS[7]}\"\n",
    "                FROM final_dual_players\n",
    "                WHERE method = 'traditional'\n",
    "                AND (off_possessions > 0 OR def_possessions > 0)\n",
    "                ORDER BY team_abbrev, player_name\n",
    "            \"\"\").df()\n",
    "\n",
    "            trad_players_file = self.export_dir / \"project2_players_traditional.csv\"\n",
    "            traditional_players.to_csv(trad_players_file, index=False)\n",
    "            exported_files.append(f\"project2_players_traditional.csv ({len(traditional_players)} players)\")\n",
    "\n",
    "            # Enhanced Method - Project 2\n",
    "            enhanced_players = self.conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    player_id as \"{PLAYER_COLUMNS[0]}\",\n",
    "                    player_name as \"{PLAYER_COLUMNS[1]}\",\n",
    "                    team_abbrev as \"{PLAYER_COLUMNS[2]}\",\n",
    "                    off_possessions as \"{PLAYER_COLUMNS[3]}\",\n",
    "                    def_possessions as \"{PLAYER_COLUMNS[4]}\",\n",
    "                    COALESCE(opp_rim_fg_pct_on, 0) as \"{PLAYER_COLUMNS[5]}\",\n",
    "                    COALESCE(opp_rim_fg_pct_off, 0) as \"{PLAYER_COLUMNS[6]}\",\n",
    "                    COALESCE(rim_defense_on_off, 0) as \"{PLAYER_COLUMNS[7]}\"\n",
    "                FROM final_dual_players\n",
    "                WHERE method = 'enhanced'\n",
    "                AND (off_possessions > 0 OR def_possessions > 0)\n",
    "                ORDER BY team_abbrev, player_name\n",
    "            \"\"\").df()\n",
    "\n",
    "            enh_players_file = self.export_dir / \"project2_players_enhanced.csv\"\n",
    "            enhanced_players.to_csv(enh_players_file, index=False)\n",
    "            exported_files.append(f\"project2_players_enhanced.csv ({len(enhanced_players)} players)\")\n",
    "\n",
    "            # Update export summary\n",
    "            self.export_summary.traditional_lineups = len(traditional_lineups)\n",
    "            self.export_summary.enhanced_lineups = len(enhanced_lineups)\n",
    "            self.export_summary.traditional_players = len(traditional_players)\n",
    "            self.export_summary.enhanced_players = len(enhanced_players)\n",
    "\n",
    "            details = f\"Exported project deliverables: {len(exported_files)} files\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Export Project Deliverables\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(exported_files),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Export Project Deliverables\",\n",
    "                passed=False,\n",
    "                details=f\"Error exporting project deliverables: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "    def export_violation_reports(self) -> ValidationResult:\n",
    "        \"\"\"Export violation reports (traditional/enhanced) without assuming any specific time column.\n",
    "\n",
    "        Strategy:\n",
    "        - Introspect available columns per table.\n",
    "        - Build a SELECT list that only references existing columns.\n",
    "        - Choose an ORDER BY among ['order_ts','abs_time','wall_clock_int','pbp_order'] if present.\n",
    "        - Prefer readable fields (team_abbrev/player_name); fall back to ids if names not present.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Exporting violation reports for traditional lineups...\")\n",
    "\n",
    "            exported_files = []\n",
    "\n",
    "            def _cols(table: str) -> List[str]:\n",
    "                try:\n",
    "                    return [r[1] for r in self.conn.execute(f\"PRAGMA table_info('{table}')\").fetchall()]\n",
    "                except Exception:\n",
    "                    return []\n",
    "\n",
    "            def _build_sql(table: str) -> Optional[str]:\n",
    "                cols = _cols(table)\n",
    "                if not cols:\n",
    "                    return None\n",
    "\n",
    "                # pick time/order column\n",
    "                time_col = next((c for c in [\"order_ts\", \"abs_time\", \"wall_clock_int\", \"pbp_order\"] if c in cols), None)\n",
    "\n",
    "                # assemble select columns, only if they exist\n",
    "                select_cols = []\n",
    "                for c in [\"period\"]:\n",
    "                    if c in cols: select_cols.append(c)\n",
    "\n",
    "                if time_col:\n",
    "                    select_cols.append(f\"{time_col} AS time_order\")\n",
    "\n",
    "                # prefer readable names; fallback to ids\n",
    "                if \"team_abbrev\" in cols:\n",
    "                    select_cols.append(\"team_abbrev\")\n",
    "                elif \"team_id\" in cols:\n",
    "                    select_cols.append(\"team_id\")\n",
    "\n",
    "                select_cols += [c for c in [\"flag_type\"] if c in cols]\n",
    "\n",
    "                if \"player_name\" in cols:\n",
    "                    select_cols.append(\"player_name\")\n",
    "                elif \"player_id\" in cols:\n",
    "                    select_cols.append(\"player_id\")\n",
    "\n",
    "                select_cols += [c for c in [\"description\", \"flag_details\"] if c in cols]\n",
    "\n",
    "                if not select_cols:\n",
    "                    # Nothing exportable\n",
    "                    return None\n",
    "\n",
    "                select_list = \", \".join(select_cols)\n",
    "                sql = f\"SELECT {select_list} FROM {table}\"\n",
    "                if time_col:\n",
    "                    sql += \" ORDER BY time_order\"\n",
    "                return sql\n",
    "\n",
    "            # Traditional\n",
    "            trad_sql = _build_sql(\"traditional_violation_report\")\n",
    "            if trad_sql:\n",
    "                trad_df = self.conn.execute(trad_sql).df()\n",
    "                if not trad_df.empty:\n",
    "                    fpath = self.export_dir / \"traditional_lineup_violations.csv\"\n",
    "                    trad_df.to_csv(fpath, index=False)\n",
    "                    exported_files.append(f\"traditional_lineup_violations.csv ({len(trad_df)} violations)\")\n",
    "                    # Summarize by flag_type if present\n",
    "                    if \"flag_type\" in trad_df.columns:\n",
    "                        counts = trad_df[\"flag_type\"].value_counts().to_dict()\n",
    "                        with open(self.export_dir / \"violation_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(\"TRADITIONAL LINEUP VIOLATION SUMMARY\\n\")\n",
    "                            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "                            f.write(f\"Total Violations: {len(trad_df):,}\\n\\n\")\n",
    "                            f.write(\"Violation Types:\\n\")\n",
    "                            for k, v in counts.items():\n",
    "                                f.write(f\"  {k}: {v:,} ({100.0 * v / max(1, len(trad_df)):.1f}%)\\n\")\n",
    "                            f.write(f\"\\nGenerated: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                        exported_files.append(\"violation_summary.txt\")\n",
    "                        self.export_summary.violation_reports += 1\n",
    "            else:\n",
    "                logger.info(\"No traditional_violation_report table or no exportable columns.\")\n",
    "\n",
    "            # Enhanced\n",
    "            enh_sql = _build_sql(\"enhanced_violation_report\")\n",
    "            if enh_sql:\n",
    "                enh_df = self.conn.execute(enh_sql).df()\n",
    "                if not enh_df.empty:\n",
    "                    fpath = self.export_dir / \"enhanced_method_flags.csv\"\n",
    "                    enh_df.to_csv(fpath, index=False)\n",
    "                    exported_files.append(f\"enhanced_method_flags.csv ({len(enh_df)} flags)\")\n",
    "                    self.export_summary.violation_reports += 1\n",
    "            else:\n",
    "                logger.info(\"No enhanced_violation_report table or no exportable columns.\")\n",
    "\n",
    "            # Base-dataset quality memo (unchanged)\n",
    "            self._create_base_dataset_violation_report()\n",
    "            exported_files.append(\"base_dataset_violations.txt\")\n",
    "\n",
    "            details = f\"Exported violation reports: {len(exported_files)} files\"\n",
    "            return ValidationResult(\n",
    "                step_name=\"Export Violation Reports\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(exported_files),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Export Violation Reports\",\n",
    "                passed=False,\n",
    "                details=f\"Error exporting violation reports: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "    def _create_base_dataset_violation_report(self):\n",
    "        \"\"\"Create base dataset violation report for final analysis\"\"\"\n",
    "        try:\n",
    "            # Analyze base dataset quality issues\n",
    "            base_dataset_analysis = {\n",
    "                'data_quality_issues': [],\n",
    "                'lineup_tracking_challenges': [],\n",
    "                'recommendations': []\n",
    "            }\n",
    "\n",
    "            # Check for common data issues in the base dataset\n",
    "            pbp_issues = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_events,\n",
    "                    SUM(CASE WHEN team_id_off IS NULL OR team_id_def IS NULL THEN 1 ELSE 0 END) as missing_team_events,\n",
    "                    SUM(CASE WHEN player_id_1 IS NULL AND player_id_2 IS NULL AND player_id_3 IS NULL THEN 1 ELSE 0 END) as no_player_events,\n",
    "                    SUM(CASE WHEN msg_type = 8 AND player_id_1 IS NULL AND player_id_2 IS NULL THEN 1 ELSE 0 END) as incomplete_substitutions\n",
    "                FROM pbp\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            if pbp_issues:\n",
    "                total_events = pbp_issues[0]\n",
    "                missing_team_pct = (pbp_issues[1] / total_events * 100) if total_events > 0 else 0\n",
    "                no_player_pct = (pbp_issues[2] / total_events * 100) if total_events > 0 else 0\n",
    "                incomplete_sub_pct = (pbp_issues[3] / total_events * 100) if total_events > 0 else 0\n",
    "\n",
    "                base_dataset_analysis['data_quality_issues'] = [\n",
    "                    f\"Missing team data in {missing_team_pct:.1f}% of events\",\n",
    "                    f\"No player data in {no_player_pct:.1f}% of events\", \n",
    "                    f\"Incomplete substitutions in {incomplete_sub_pct:.1f}% of substitution events\"\n",
    "                ]\n",
    "\n",
    "            # Lineup tracking challenges from traditional method\n",
    "            if hasattr(self, 'export_summary'):\n",
    "                if self.export_summary.traditional_lineups > 0:\n",
    "                    lineup_5man_pct = self.conn.execute(\"\"\"\n",
    "                        SELECT AVG(CASE WHEN lineup_size = 5 THEN 1.0 ELSE 0.0 END) * 100 as pct\n",
    "                        FROM final_dual_lineups WHERE method = 'traditional'\n",
    "                    \"\"\").fetchone()[0]\n",
    "\n",
    "                    base_dataset_analysis['lineup_tracking_challenges'] = [\n",
    "                        f\"Only {lineup_5man_pct:.1f}% of traditional lineups have exactly 5 players\",\n",
    "                        f\"Substitution data requires intelligent inference for accurate lineup tracking\",\n",
    "                        f\"Enhanced method achieves 100% 5-player lineup accuracy through automation\"\n",
    "                    ]\n",
    "\n",
    "            base_dataset_analysis['recommendations'] = [\n",
    "                \"Use enhanced method for production lineup tracking\",\n",
    "                \"Traditional method useful for data quality validation\", \n",
    "                \"Violation reports highlight areas needing manual review\",\n",
    "                \"Consider implementing enhanced automation for real-time applications\"\n",
    "            ]\n",
    "\n",
    "            # Write report\n",
    "            report_file = self.export_dir / \"base_dataset_violations.txt\"\n",
    "            with open(report_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"BASE DATASET VIOLATIONS AND RECOMMENDATIONS\\n\")\n",
    "                f.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "                f.write(\"DATA QUALITY ISSUES:\\n\")\n",
    "                for issue in base_dataset_analysis['data_quality_issues']:\n",
    "                    f.write(f\"  - {issue}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                f.write(\"LINEUP TRACKING CHALLENGES:\\n\")\n",
    "                for challenge in base_dataset_analysis['lineup_tracking_challenges']:\n",
    "                    f.write(f\"  - {challenge}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                f.write(\"RECOMMENDATIONS:\\n\")\n",
    "                for rec in base_dataset_analysis['recommendations']:\n",
    "                    f.write(f\"  - {rec}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                f.write(f\"Report generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "                f.write(\"For detailed violation data, see traditional_lineup_violations.csv\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create base dataset violation report: {e}\")\n",
    "\n",
    "    def export_method_comparison_reports(self) -> ValidationResult:\n",
    "        \"\"\"Export comprehensive method comparison and effectiveness analysis\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Exporting method comparison reports...\")\n",
    "\n",
    "            exported_files = []\n",
    "\n",
    "            # Method Comparison Summary\n",
    "            try:\n",
    "                comparison_summary = self.conn.execute(\"\"\"\n",
    "                    SELECT * FROM method_comparison_summary\n",
    "                    ORDER BY metric\n",
    "                \"\"\").df()\n",
    "\n",
    "                if not comparison_summary.empty:\n",
    "                    comparison_file = self.export_dir / \"method_comparison_summary.csv\"\n",
    "                    comparison_summary.to_csv(comparison_file, index=False)\n",
    "                    exported_files.append(f\"method_comparison_summary.csv ({len(comparison_summary)} metrics)\")\n",
    "                    self.export_summary.comparison_reports += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not export method comparison: {e}\")\n",
    "\n",
    "            # Create method effectiveness report\n",
    "            self._create_method_effectiveness_report()\n",
    "            exported_files.append(\"method_effectiveness_report.txt\")\n",
    "\n",
    "            details = f\"Exported method comparison reports: {len(exported_files)} files\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Export Method Comparison Reports\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(exported_files),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Export Method Comparison Reports\",\n",
    "                passed=False,\n",
    "                details=f\"Error exporting method comparison reports: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def _create_method_effectiveness_report(self):\n",
    "        \"\"\"Create comprehensive method effectiveness analysis report\"\"\"\n",
    "        try:\n",
    "            # Calculate effectiveness metrics\n",
    "            effectiveness_metrics = {}\n",
    "\n",
    "            # Lineup accuracy metrics\n",
    "            traditional_accuracy = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_lineups,\n",
    "                    SUM(CASE WHEN lineup_size = 5 THEN 1 ELSE 0 END) as five_man_lineups,\n",
    "                    AVG(lineup_size) as avg_size,\n",
    "                    MIN(lineup_size) as min_size,\n",
    "                    MAX(lineup_size) as max_size\n",
    "                FROM final_dual_lineups WHERE method = 'traditional'\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            enhanced_accuracy = self.conn.execute(\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_lineups,\n",
    "                    SUM(CASE WHEN lineup_size = 5 THEN 1 ELSE 0 END) as five_man_lineups,\n",
    "                    AVG(lineup_size) as avg_size\n",
    "                FROM final_dual_lineups WHERE method = 'enhanced'\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            if traditional_accuracy and enhanced_accuracy:\n",
    "                effectiveness_metrics['lineup_accuracy'] = {\n",
    "                    'traditional': {\n",
    "                        'total_lineups': traditional_accuracy[0],\n",
    "                        'five_man_accuracy': (traditional_accuracy[1] / traditional_accuracy[0] * 100) if traditional_accuracy[0] > 0 else 0,\n",
    "                        'avg_size': traditional_accuracy[2],\n",
    "                        'min_size': traditional_accuracy[3],\n",
    "                        'max_size': traditional_accuracy[4]\n",
    "                    },\n",
    "                    'enhanced': {\n",
    "                        'total_lineups': enhanced_accuracy[0],\n",
    "                        'five_man_accuracy': (enhanced_accuracy[1] / enhanced_accuracy[0] * 100) if enhanced_accuracy[0] > 0 else 0,\n",
    "                        'avg_size': enhanced_accuracy[2]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            # Create effectiveness report\n",
    "            report_file = self.export_dir / \"method_effectiveness_report.txt\"\n",
    "            with open(report_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"METHOD EFFECTIVENESS ANALYSIS\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "                f.write(\"EXECUTIVE SUMMARY:\\n\")\n",
    "                if 'lineup_accuracy' in effectiveness_metrics:\n",
    "                    trad_acc = effectiveness_metrics['lineup_accuracy']['traditional']['five_man_accuracy']\n",
    "                    enh_acc = effectiveness_metrics['lineup_accuracy']['enhanced']['five_man_accuracy']\n",
    "                    improvement = enh_acc - trad_acc\n",
    "\n",
    "                    f.write(f\"  Enhanced method achieves {improvement:+.1f} percentage point improvement\\n\")\n",
    "                    f.write(f\"  in 5-man lineup accuracy over traditional method.\\n\\n\")\n",
    "\n",
    "                f.write(\"DETAILED METRICS:\\n\\n\")\n",
    "\n",
    "                f.write(\"Traditional Data-Driven Method:\\n\")\n",
    "                if 'lineup_accuracy' in effectiveness_metrics:\n",
    "                    trad = effectiveness_metrics['lineup_accuracy']['traditional']\n",
    "                    f.write(f\"  Total Lineups: {trad['total_lineups']:,}\\n\")\n",
    "                    f.write(f\"  5-Man Accuracy: {trad['five_man_accuracy']:.1f}%\\n\")\n",
    "                    f.write(f\"  Average Size: {trad['avg_size']:.1f} players\\n\")\n",
    "                    f.write(f\"  Size Range: {trad['min_size']}-{trad['max_size']} players\\n\")\n",
    "                    f.write(f\"  Strength: Faithful to raw data, highlights data quality issues\\n\")\n",
    "                    f.write(f\"  Use Case: Data validation and quality assessment\\n\\n\")\n",
    "\n",
    "                f.write(\"Enhanced Automation Method:\\n\")\n",
    "                if 'lineup_accuracy' in effectiveness_metrics:\n",
    "                    enh = effectiveness_metrics['lineup_accuracy']['enhanced']\n",
    "                    f.write(f\"  Total Lineups: {enh['total_lineups']:,}\\n\")\n",
    "                    f.write(f\"  5-Man Accuracy: {enh['five_man_accuracy']:.1f}%\\n\")\n",
    "                    f.write(f\"  Average Size: {enh['avg_size']:.1f} players\\n\")\n",
    "                    f.write(f\"  Strength: Consistent 5-man lineups, intelligent inference\\n\")\n",
    "                    f.write(f\"  Use Case: Production analytics and reporting\\n\\n\")\n",
    "\n",
    "                f.write(\"RECOMMENDATIONS:\\n\")\n",
    "                f.write(\"  1. Use Enhanced method for production lineup analytics\\n\")\n",
    "                f.write(\"  2. Use Traditional method for data quality validation\\n\")\n",
    "                f.write(\"  3. Review violation reports to identify systematic data issues\\n\")\n",
    "                f.write(\"  4. Consider automated data quality monitoring based on violation patterns\\n\\n\")\n",
    "\n",
    "                f.write(f\"Analysis completed: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not create method effectiveness report: {e}\")\n",
    "\n",
    "    def generate_quality_report(self) -> ValidationResult:\n",
    "        \"\"\"Generate comprehensive quality and validation report (ASCII-safe, explicit encoding).\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"Generating quality report...\")\n",
    "\n",
    "            # Copy current validations logged so far\n",
    "            all_validations = self.validator.validation_results.copy()\n",
    "\n",
    "            # -------- Lineup metrics --------\n",
    "            lm = self.conn.execute(\"\"\"\n",
    "                SELECT\n",
    "                    COUNT(*) AS total_records,\n",
    "                    AVG(off_possessions) AS avg_off_poss,\n",
    "                    AVG(def_possessions) AS avg_def_poss,\n",
    "                    AVG(off_rating)      AS avg_off_rating,\n",
    "                    AVG(def_rating)      AS avg_def_rating\n",
    "                FROM final_lineups\n",
    "                WHERE off_possessions > 0\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            lineup_metrics = {\n",
    "                \"total_records\": int(lm[0] or 0),\n",
    "                \"avg_off_poss\": float(lm[1]) if lm[1] is not None else 0.0,\n",
    "                \"avg_def_poss\": float(lm[2]) if lm[2] is not None else 0.0,\n",
    "                \"avg_off_rating\": float(lm[3]) if lm[3] is not None else 0.0,\n",
    "                \"avg_def_rating\": float(lm[4]) if lm[4] is not None else 0.0,\n",
    "            }\n",
    "\n",
    "            # -------- Player rim-defense metrics (FIXED) --------\n",
    "            pm = self.conn.execute(\"\"\"\n",
    "                SELECT\n",
    "                    COUNT(*) AS total_records,\n",
    "                    -- Raw event counts per player\n",
    "                    AVG(opp_rim_attempts_on)  AS avg_attempts_on_raw,\n",
    "                    AVG(opp_rim_attempts_off) AS avg_attempts_off_raw,\n",
    "                    AVG(opp_rim_makes_on)     AS avg_makes_on_raw,\n",
    "                    AVG(opp_rim_makes_off)    AS avg_makes_off_raw,\n",
    "                    AVG(opp_rim_fg_pct_on)    AS avg_fg_pct_on,\n",
    "                    AVG(opp_rim_fg_pct_off)   AS avg_fg_pct_off,\n",
    "                    AVG(rim_defense_on_off)   AS avg_rim_defense_diff,\n",
    "                    -- Calculated normalized metrics using existing columns\n",
    "                    AVG(CASE WHEN def_possessions > 0 THEN opp_rim_attempts_on::FLOAT / def_possessions ELSE 0 END) AS avg_attempts_per_def_poss_on,\n",
    "                    AVG(CASE WHEN def_possessions > 0 THEN opp_rim_attempts_off::FLOAT / def_possessions ELSE 0 END) AS avg_attempts_per_def_poss_off,\n",
    "                    AVG(CASE WHEN def_possessions > 0 THEN 100.0 * opp_rim_attempts_on::FLOAT / def_possessions ELSE 0 END) AS attempts_on_per100_def_poss,\n",
    "                    AVG(CASE WHEN def_possessions > 0 THEN 100.0 * opp_rim_attempts_off::FLOAT / def_possessions ELSE 0 END) AS attempts_off_per100_def_poss\n",
    "                FROM final_players\n",
    "                WHERE opp_rim_attempts_on > 0 OR opp_rim_attempts_off > 0\n",
    "            \"\"\").fetchone()\n",
    "\n",
    "            player_metrics = {\n",
    "                \"total_records\": int(pm[0] or 0),\n",
    "                \"avg_attempts_on_raw\": float(pm[1]) if pm[1] is not None else 0.0,\n",
    "                \"avg_attempts_off_raw\": float(pm[2]) if pm[2] is not None else 0.0,\n",
    "                \"avg_makes_on_raw\": float(pm[3]) if pm[3] is not None else 0.0,\n",
    "                \"avg_makes_off_raw\": float(pm[4]) if pm[4] is not None else 0.0,\n",
    "                \"avg_fg_pct_on\": float(pm[5]) if pm[5] is not None else 0.0,\n",
    "                \"avg_fg_pct_off\": float(pm[6]) if pm[6] is not None else 0.0,\n",
    "                \"avg_rim_defense_diff\": float(pm[7]) if pm[7] is not None else 0.0,\n",
    "                \"avg_attempts_per_def_poss_on\": float(pm[8]) if pm[8] is not None else 0.0,\n",
    "                \"avg_attempts_per_def_poss_off\": float(pm[9]) if pm[9] is not None else 0.0,\n",
    "                \"attempts_on_per100_def_poss\": float(pm[10]) if pm[10] is not None else 0.0,\n",
    "                \"attempts_off_per100_def_poss\": float(pm[11]) if pm[11] is not None else 0.0,\n",
    "            }\n",
    "\n",
    "            # -------- Assemble report text (ASCII labels, no emojis) --------\n",
    "            report_lines = [\n",
    "                \"NBA PLAY-BY-PLAY PIPELINE - QUALITY REPORT\",\n",
    "                \"=\" * 80,\n",
    "                f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "                \"\",\n",
    "                \"PIPELINE EXECUTION SUMMARY:\",\n",
    "                f\"  Total Validation Steps: {len(all_validations)}\",\n",
    "                f\"  Passed: {sum(1 for v in all_validations if v.passed)}\",\n",
    "                f\"  Failed: {sum(1 for v in all_validations if not v.passed)}\",\n",
    "                f\"  Total Warnings: {sum(len(v.warnings) for v in all_validations)}\",\n",
    "                \"\"\n",
    "            ]\n",
    "\n",
    "            # Validation step details\n",
    "            report_lines.append(\"VALIDATION STEP DETAILS:\")\n",
    "            for validation in all_validations:\n",
    "                status = self._status_label(validation.passed)\n",
    "                report_lines.append(f\"  {status} {validation.step_name}\")\n",
    "                report_lines.append(f\"    Details: {validation.details}\")\n",
    "                report_lines.append(f\"    Time: {validation.processing_time:.3f}s\")\n",
    "                if validation.warnings:\n",
    "                    for warning in validation.warnings:\n",
    "                        report_lines.append(f\"    {self._warn_label()} {warning}\")\n",
    "                report_lines.append(\"\")\n",
    "\n",
    "            # Data quality sections (UPDATED)\n",
    "            report_lines.extend([\n",
    "                \"DATA QUALITY METRICS:\",\n",
    "                \"\",\n",
    "                \"  LINEUP STATISTICS:\",\n",
    "                f\"    Total Records: {lineup_metrics['total_records']:,}\",\n",
    "                f\"    Avg Offensive Possessions: {lineup_metrics['avg_off_poss']:.1f}\",\n",
    "                f\"    Avg Defensive Possessions: {lineup_metrics['avg_def_poss']:.1f}\",\n",
    "                f\"    Avg Offensive Rating: {lineup_metrics['avg_off_rating']:.1f}\",\n",
    "                f\"    Avg Defensive Rating: {lineup_metrics['avg_def_rating']:.1f}\",\n",
    "                \"\",\n",
    "                \"  PLAYER RIM DEFENSE:\",\n",
    "                f\"    Player Records: {player_metrics['total_records']:,}\",\n",
    "                f\"    Avg Rim Attempts (On): {player_metrics['avg_attempts_on_raw']:.2f}\",\n",
    "                f\"    Avg Rim Attempts (Off): {player_metrics['avg_attempts_off_raw']:.2f}\",\n",
    "                f\"    Avg Rim Makes (On): {player_metrics['avg_makes_on_raw']:.2f}\",\n",
    "                f\"    Avg Rim Makes (Off): {player_metrics['avg_makes_off_raw']:.2f}\",\n",
    "                f\"    Avg Rim FG% (On): {player_metrics['avg_fg_pct_on']:.1%}\",\n",
    "                f\"    Avg Rim FG% (Off): {player_metrics['avg_fg_pct_off']:.1%}\",\n",
    "                f\"    Avg On/Off Difference: {player_metrics['avg_rim_defense_diff']:.3f}\",\n",
    "                f\"    Rim Attempts per Def Possession (On): {player_metrics['avg_attempts_per_def_poss_on']:.4f}\",\n",
    "                f\"    Rim Attempts per Def Possession (Off): {player_metrics['avg_attempts_per_def_poss_off']:.4f}\",\n",
    "                f\"    Rim Attempts per 100 Def Poss (On): {player_metrics['attempts_on_per100_def_poss']:.2f}\",\n",
    "                f\"    Rim Attempts per 100 Def Poss (Off): {player_metrics['attempts_off_per100_def_poss']:.2f}\",\n",
    "                \"\"\n",
    "            ])\n",
    "\n",
    "            # Save to disk (sanitize + explicit encoding)\n",
    "            report_text = \"\\n\".join(report_lines)\n",
    "            report_text = self._sanitize_for_file(report_text)\n",
    "            report_file = self.export_dir / \"quality_report.txt\"\n",
    "            report_file.write_text(report_text, encoding=\"utf-8\")\n",
    "\n",
    "            details = \"Generated quality report with corrected player rim defense metrics\"\n",
    "            return ValidationResult(\n",
    "                step_name=\"Quality Report\",\n",
    "                passed=True,\n",
    "                details=details,\n",
    "                data_count=len(all_validations),\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Quality Report\",\n",
    "                passed=False,\n",
    "                details=f\"Error generating quality report: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "    def print_final_summary(self):\n",
    "        \"\"\"Print final pipeline summary (ASCII-only to avoid console encoding issues).\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"NBA PIPELINE - FINAL EXPORT & VALIDATION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Show export directory contents\n",
    "        if self.export_dir.exists():\n",
    "            export_files = list(self.export_dir.glob(\"*.csv\")) + list(self.export_dir.glob(\"*.txt\"))\n",
    "            print(f\"EXPORTED FILES ({len(export_files)}):\")\n",
    "            for file in sorted(export_files):\n",
    "                size_kb = file.stat().st_size / 1024\n",
    "                print(f\"   - {file.name} ({size_kb:.1f} KB)\")\n",
    "            print()\n",
    "\n",
    "        # Show key metrics\n",
    "        try:\n",
    "            lineup_count = self.conn.execute(\"SELECT COUNT(*) FROM final_lineups\").fetchone()[0]\n",
    "            player_count = self.conn.execute(\"SELECT COUNT(*) FROM final_players\").fetchone()[0]\n",
    "\n",
    "            print(\"FINAL RESULTS:\")\n",
    "            print(f\"   Unique Lineups: {lineup_count:,}\")\n",
    "            print(f\"   Active Players: {player_count:,}\")\n",
    "\n",
    "            if hasattr(self.engine, 'possessions') and self.engine.possessions:\n",
    "                print(f\"   Total Possessions: {len(self.engine.possessions):,}\")\n",
    "\n",
    "            if hasattr(self.processor, 'processed_events') and self.processor.processed_events:\n",
    "                rim_attempts = sum(1 for e in self.processor.processed_events if getattr(e, 'is_rim_attempt', False))\n",
    "                print(f\"   Rim Attempts: {rim_attempts:,}\")\n",
    "\n",
    "        except Exception:\n",
    "            print(\"   Unable to retrieve final metrics\")\n",
    "\n",
    "        print(\"=\"*80)\n",
    "\n",
    "    # Add this to your FinalValidator class in nba_final_export.py\n",
    "\n",
    "    def validate_final_tables_exist(self) -> ValidationResult:\n",
    "        \"\"\"Validate final tables exist with expected structure before generating reports.\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            warnings = []\n",
    "\n",
    "            # Check tables exist\n",
    "            tables = [row[0] for row in self.conn.execute(\"\"\"\n",
    "                SELECT table_name FROM information_schema.tables \n",
    "                WHERE table_name IN ('final_lineups', 'final_players')\n",
    "            \"\"\").fetchall()]\n",
    "\n",
    "            missing_tables = set(['final_lineups', 'final_players']) - set(tables)\n",
    "            if missing_tables:\n",
    "                return ValidationResult(\n",
    "                    step_name=\"Final Tables Check\",\n",
    "                    passed=False,\n",
    "                    details=f\"Missing tables: {missing_tables}\",\n",
    "                    processing_time=time.time() - start_time\n",
    "                )\n",
    "\n",
    "            # Check final_players has required columns for quality report\n",
    "            player_cols = [row[0] for row in self.conn.execute(\"\"\"\n",
    "                SELECT column_name FROM information_schema.columns \n",
    "                WHERE table_name = 'final_players'\n",
    "            \"\"\").fetchall()]\n",
    "\n",
    "            required_cols = [\n",
    "                'player_id', 'player_name', 'team_abbrev', \n",
    "                'off_possessions', 'def_possessions',\n",
    "                'opp_rim_attempts_on', 'opp_rim_attempts_off',\n",
    "                'opp_rim_fg_pct_on', 'opp_rim_fg_pct_off'\n",
    "            ]\n",
    "\n",
    "            missing_cols = set(required_cols) - set(player_cols)\n",
    "            if missing_cols:\n",
    "                warnings.append(f\"Missing columns in final_players: {missing_cols}\")\n",
    "\n",
    "            # Check data exists\n",
    "            lineup_count = self.conn.execute(\"SELECT COUNT(*) FROM final_lineups\").fetchone()[0]\n",
    "            player_count = self.conn.execute(\"SELECT COUNT(*) FROM final_players\").fetchone()[0]\n",
    "\n",
    "            if lineup_count == 0:\n",
    "                warnings.append(\"final_lineups table is empty\")\n",
    "            if player_count == 0:\n",
    "                warnings.append(\"final_players table is empty\")\n",
    "\n",
    "            details = f\"Tables validated: final_lineups({lineup_count}), final_players({player_count})\"\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Final Tables Check\",\n",
    "                passed=len(missing_cols) == 0 and lineup_count > 0 and player_count > 0,\n",
    "                details=details,\n",
    "                processing_time=time.time() - start_time,\n",
    "                warnings=warnings\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Final Tables Check\", \n",
    "                passed=False,\n",
    "                details=f\"Error checking final tables: {str(e)}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "    def export_points_attribution_audit(self) -> ValidationResult:\n",
    "        \"\"\"\n",
    "        ENHANCED: Comprehensive audit of where points are lost/gained between stages.\n",
    "        Produces detailed CSVs under exports/debug_points_audit/ with granular analysis.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            logger.info(\"=== ENHANCED POINTS ATTRIBUTION AUDIT ===\")\n",
    "            \n",
    "            out_dir = self.export_dir / \"debug_points_audit\"\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            existing_tables = {r[0] for r in self.conn.execute(\n",
    "                \"SELECT table_name FROM information_schema.tables\").fetchall()}\n",
    "\n",
    "            audit_results = {\n",
    "                \"box_score_totals\": {},\n",
    "                \"raw_pbp_totals\": {},\n",
    "                \"step4_processed_totals\": {},\n",
    "                \"lineup_totals\": {},\n",
    "                \"discrepancies\": {}\n",
    "            }\n",
    "\n",
    "            # 1) Box score totals (ground truth)\n",
    "            box_df = pd.DataFrame()\n",
    "            if \"box_score\" in existing_tables:\n",
    "                box_df = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        team_abbrev,\n",
    "                        SUM(points) AS box_points,\n",
    "                        COUNT(*) AS active_players,\n",
    "                        SUM(seconds_played) AS total_seconds\n",
    "                    FROM box_score\n",
    "                    WHERE status = 'ACTIVE'\n",
    "                    GROUP BY team_abbrev\n",
    "                    ORDER BY team_abbrev\n",
    "                \"\"\").df()\n",
    "                \n",
    "                for _, row in box_df.iterrows():\n",
    "                    audit_results[\"box_score_totals\"][row[\"team_abbrev\"]] = {\n",
    "                        \"points\": int(row[\"box_points\"]),\n",
    "                        \"players\": int(row[\"active_players\"]),\n",
    "                        \"seconds\": int(row[\"total_seconds\"])\n",
    "                    }\n",
    "\n",
    "            # 2) Raw PBP totals\n",
    "            raw_pbp_df = pd.DataFrame()\n",
    "            if \"pbp\" in existing_tables:\n",
    "                raw_pbp_df = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        CASE \n",
    "                            WHEN team_id_off = 1610612742 THEN 'DAL'\n",
    "                            WHEN team_id_off = 1610612745 THEN 'HOU'\n",
    "                            ELSE CAST(team_id_off AS VARCHAR)\n",
    "                        END as team_abbrev,\n",
    "                        SUM(COALESCE(points, 0)) AS raw_pbp_points,\n",
    "                        COUNT(*) AS scoring_events,\n",
    "                        COUNT(DISTINCT pbp_id) AS unique_events\n",
    "                    FROM pbp \n",
    "                    WHERE points > 0 AND team_id_off IS NOT NULL\n",
    "                    GROUP BY team_id_off\n",
    "                    ORDER BY team_abbrev\n",
    "                \"\"\").df()\n",
    "                \n",
    "                for _, row in raw_pbp_df.iterrows():\n",
    "                    audit_results[\"raw_pbp_totals\"][row[\"team_abbrev\"]] = {\n",
    "                        \"points\": int(row[\"raw_pbp_points\"]),\n",
    "                        \"events\": int(row[\"scoring_events\"]),\n",
    "                        \"unique_events\": int(row[\"unique_events\"])\n",
    "                    }\n",
    "\n",
    "            # 3) Step4 processed totals\n",
    "            step4_df = pd.DataFrame()\n",
    "            if \"step4_processed_events\" in existing_tables:\n",
    "                step4_df = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        CASE \n",
    "                            WHEN off_team_id = 1610612742 THEN 'DAL'\n",
    "                            WHEN off_team_id = 1610612745 THEN 'HOU'\n",
    "                            ELSE CAST(off_team_id AS VARCHAR)\n",
    "                        END as team_abbrev,\n",
    "                        SUM(COALESCE(points, 0)) AS step4_points,\n",
    "                        COUNT(*) AS processing_events,\n",
    "                        COUNT(CASE WHEN points > 0 THEN 1 END) AS scoring_events,\n",
    "                        COUNT(DISTINCT pbp_id) AS unique_pbp_ids\n",
    "                    FROM step4_processed_events\n",
    "                    WHERE off_team_id IS NOT NULL\n",
    "                    GROUP BY off_team_id\n",
    "                    ORDER BY team_abbrev\n",
    "                \"\"\").df()\n",
    "                \n",
    "                for _, row in step4_df.iterrows():\n",
    "                    audit_results[\"step4_processed_totals\"][row[\"team_abbrev\"]] = {\n",
    "                        \"points\": int(row[\"step4_points\"]),\n",
    "                        \"events\": int(row[\"processing_events\"]),\n",
    "                        \"scoring_events\": int(row[\"scoring_events\"]),\n",
    "                        \"unique_pbp_ids\": int(row[\"unique_pbp_ids\"])\n",
    "                    }\n",
    "\n",
    "            # 4) Lineup totals by method\n",
    "            lineup_df = pd.DataFrame()\n",
    "            if \"final_dual_lineups\" in existing_tables:\n",
    "                lineup_df = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        method,\n",
    "                        team_abbrev,\n",
    "                        SUM(points_for) AS lineup_points_for,\n",
    "                        SUM(off_possessions) AS total_off_poss,\n",
    "                        COUNT(*) AS unique_lineups\n",
    "                    FROM final_dual_lineups\n",
    "                    GROUP BY method, team_abbrev\n",
    "                    ORDER BY method, team_abbrev\n",
    "                \"\"\").df()\n",
    "                \n",
    "                audit_results[\"lineup_totals\"] = {}\n",
    "                for _, row in lineup_df.iterrows():\n",
    "                    method = row[\"method\"]\n",
    "                    team = row[\"team_abbrev\"]\n",
    "                    if method not in audit_results[\"lineup_totals\"]:\n",
    "                        audit_results[\"lineup_totals\"][method] = {}\n",
    "                    audit_results[\"lineup_totals\"][method][team] = {\n",
    "                        \"points\": int(row[\"lineup_points_for\"]),\n",
    "                        \"possessions\": int(row[\"total_off_poss\"]),\n",
    "                        \"lineups\": int(row[\"unique_lineups\"])\n",
    "                    }\n",
    "\n",
    "            # 5) Calculate discrepancies at each stage\n",
    "            teams = [\"DAL\", \"HOU\"]\n",
    "            for team in teams:\n",
    "                box_pts = audit_results[\"box_score_totals\"].get(team, {}).get(\"points\", 0)\n",
    "                raw_pts = audit_results[\"raw_pbp_totals\"].get(team, {}).get(\"points\", 0)\n",
    "                step4_pts = audit_results[\"step4_processed_totals\"].get(team, {}).get(\"points\", 0)\n",
    "                \n",
    "                trad_pts = audit_results[\"lineup_totals\"].get(\"traditional\", {}).get(team, {}).get(\"points\", 0)\n",
    "                enh_pts = audit_results[\"lineup_totals\"].get(\"enhanced\", {}).get(team, {}).get(\"points\", 0)\n",
    "                \n",
    "                audit_results[\"discrepancies\"][team] = {\n",
    "                    \"box_score\": box_pts,\n",
    "                    \"raw_pbp\": raw_pts,\n",
    "                    \"step4_processed\": step4_pts,\n",
    "                    \"traditional_lineups\": trad_pts,\n",
    "                    \"enhanced_lineups\": enh_pts,\n",
    "                    \"raw_to_step4_diff\": step4_pts - raw_pts,\n",
    "                    \"step4_to_traditional_diff\": trad_pts - step4_pts,\n",
    "                    \"step4_to_enhanced_diff\": enh_pts - step4_pts,\n",
    "                    \"box_to_traditional_diff\": trad_pts - box_pts,\n",
    "                    \"box_to_enhanced_diff\": enh_pts - box_pts\n",
    "                }\n",
    "\n",
    "            # 6) Export comprehensive summary\n",
    "            summary_rows = []\n",
    "            for team in teams:\n",
    "                disc = audit_results[\"discrepancies\"][team]\n",
    "                summary_rows.append({\n",
    "                    \"team\": team,\n",
    "                    \"box_score_points\": disc[\"box_score\"],\n",
    "                    \"raw_pbp_points\": disc[\"raw_pbp\"],\n",
    "                    \"step4_processed_points\": disc[\"step4_processed\"],\n",
    "                    \"traditional_lineup_points\": disc[\"traditional_lineups\"],\n",
    "                    \"enhanced_lineup_points\": disc[\"enhanced_lineups\"],\n",
    "                    \"raw_to_step4_diff\": disc[\"raw_to_step4_diff\"],\n",
    "                    \"step4_to_traditional_diff\": disc[\"step4_to_traditional_diff\"],\n",
    "                    \"step4_to_enhanced_diff\": disc[\"step4_to_enhanced_diff\"],\n",
    "                    \"box_to_traditional_diff\": disc[\"box_to_traditional_diff\"],\n",
    "                    \"box_to_enhanced_diff\": disc[\"box_to_enhanced_diff\"]\n",
    "                })\n",
    "            \n",
    "            pd.DataFrame(summary_rows).to_csv(out_dir / \"001_comprehensive_points_flow.csv\", index=False)\n",
    "\n",
    "            # 7) Detailed HOU analysis (the problematic team)\n",
    "            if \"step4_processed_events\" in existing_tables:\n",
    "                hou_events = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        pbp_id, period, pbp_order, wall_clock_int,\n",
    "                        description, points, msg_type, action_type,\n",
    "                        player_id_1, player_id_2, player_id_3,\n",
    "                        traditional_off_lineup, enhanced_off_lineup,\n",
    "                        CASE \n",
    "                            WHEN traditional_off_lineup IS NULL \n",
    "                            OR TRIM(CAST(traditional_off_lineup AS VARCHAR)) IN ('', '[]')\n",
    "                            THEN 'NO_LINEUP'\n",
    "                            ELSE 'HAS_LINEUP'\n",
    "                        END as trad_lineup_status,\n",
    "                        CASE \n",
    "                            WHEN enhanced_off_lineup IS NULL \n",
    "                            OR TRIM(CAST(enhanced_off_lineup AS VARCHAR)) IN ('', '[]')\n",
    "                            THEN 'NO_LINEUP'\n",
    "                            ELSE 'HAS_LINEUP'\n",
    "                        END as enh_lineup_status\n",
    "                    FROM step4_processed_events \n",
    "                    WHERE off_team_id = 1610612745 AND points > 0\n",
    "                    ORDER BY period, pbp_order, wall_clock_int\n",
    "                \"\"\").df()\n",
    "                \n",
    "                hou_events.to_csv(out_dir / \"002_hou_scoring_events_detailed.csv\", index=False)\n",
    "                \n",
    "                # HOU lineup attribution analysis\n",
    "                hou_attribution = hou_events.groupby(['trad_lineup_status', 'enh_lineup_status']).agg({\n",
    "                    'points': ['count', 'sum'],\n",
    "                    'pbp_id': 'count'\n",
    "                }).round(2)\n",
    "                hou_attribution.to_csv(out_dir / \"003_hou_lineup_attribution_breakdown.csv\")\n",
    "\n",
    "            # 8) Cross-reference with original PBP\n",
    "            if \"pbp\" in existing_tables and \"step4_processed_events\" in existing_tables:\n",
    "                cross_ref = self.conn.execute(\"\"\"\n",
    "                    SELECT \n",
    "                        pbp.pbp_id,\n",
    "                        pbp.points as original_points,\n",
    "                        pbp.description as original_description,\n",
    "                        se.points as processed_points,\n",
    "                        se.description as processed_description,\n",
    "                        CASE \n",
    "                            WHEN pbp.team_id_off = 1610612742 THEN 'DAL'\n",
    "                            WHEN pbp.team_id_off = 1610612745 THEN 'HOU'\n",
    "                            ELSE CAST(pbp.team_id_off AS VARCHAR)\n",
    "                        END as team,\n",
    "                        (se.points - COALESCE(pbp.points, 0)) as points_diff\n",
    "                    FROM pbp \n",
    "                    LEFT JOIN step4_processed_events se ON pbp.pbp_id = se.pbp_id\n",
    "                    WHERE pbp.points > 0 OR se.points > 0\n",
    "                    ORDER BY team, pbp.pbp_id\n",
    "                \"\"\").df()\n",
    "                \n",
    "                cross_ref.to_csv(out_dir / \"004_pbp_to_step4_cross_reference.csv\", index=False)\n",
    "                \n",
    "                # Points difference summary\n",
    "                diff_summary = cross_ref.groupby('team')['points_diff'].agg(['sum', 'count', 'mean']).round(3)\n",
    "                diff_summary.to_csv(out_dir / \"005_points_transformation_summary.csv\")\n",
    "\n",
    "            # 9) Log findings\n",
    "            logger.info(\"=== ENHANCED AUDIT FINDINGS ===\")\n",
    "            for team in teams:\n",
    "                disc = audit_results[\"discrepancies\"][team]\n",
    "                logger.info(f\"{team} POINTS FLOW:\")\n",
    "                logger.info(f\"  Box Score (Ground Truth): {disc['box_score']}\")\n",
    "                logger.info(f\"  Raw PBP: {disc['raw_pbp']} (diff: {disc['raw_pbp'] - disc['box_score']:+})\")\n",
    "                logger.info(f\"  Step4 Processed: {disc['step4_processed']} (diff: {disc['raw_to_step4_diff']:+})\")\n",
    "                logger.info(f\"  Traditional Lineups: {disc['traditional_lineups']} (diff: {disc['step4_to_traditional_diff']:+})\")\n",
    "                logger.info(f\"  Enhanced Lineups: {disc['enhanced_lineups']} (diff: {disc['step4_to_enhanced_diff']:+})\")\n",
    "                logger.info(f\"  FINAL DISCREPANCY vs Box: Traditional={disc['box_to_traditional_diff']:+}, Enhanced={disc['box_to_enhanced_diff']:+}\")\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Enhanced Points Attribution Audit\",\n",
    "                passed=True,\n",
    "                details=f\"Comprehensive audit completed. Key finding: HOU has {audit_results['discrepancies']['HOU']['box_to_enhanced_diff']:+} point discrepancy. Check {out_dir} for detailed analysis.\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Enhanced Points Attribution Audit\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating enhanced audit: {e}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "            # Build a temp teams map if needed\n",
    "            teams_df = pd.DataFrame()\n",
    "            if \"teams\" in existing_tables:\n",
    "                teams_df = self.conn.execute(\"SELECT DISTINCT team_id, team_abbrev FROM teams\").df()\n",
    "            elif \"box_score\" in existing_tables:\n",
    "                teams_df = self.conn.execute(\"SELECT DISTINCT team_id, team_abbrev FROM box_score\").df()\n",
    "\n",
    "            # Register map if we had to synthesize\n",
    "            if not teams_df.empty:\n",
    "                self.conn.register(\"teams_temp_map\", teams_df)\n",
    "                try:\n",
    "                    step4_df = self.conn.execute(\"\"\"\n",
    "                        SELECT m.team_abbrev, SUM(se.points) AS step4_points\n",
    "                        FROM step4_processed_events se\n",
    "                        JOIN teams_temp_map m ON m.team_id = se.off_team_id\n",
    "                        WHERE se.points > 0\n",
    "                        GROUP BY m.team_abbrev\n",
    "                        ORDER BY m.team_abbrev\n",
    "                    \"\"\").df()\n",
    "                finally:\n",
    "                    self.conn.unregister(\"teams_temp_map\")\n",
    "            else:\n",
    "                # fallback: just sum by team id if we cannot resolve abbrev\n",
    "                step4_df = self.conn.execute(\"\"\"\n",
    "                    SELECT CAST(off_team_id AS VARCHAR) AS team_abbrev, SUM(points) AS step4_points\n",
    "                        FROM step4_processed_events\n",
    "                        WHERE points > 0\n",
    "                        GROUP BY off_team_id\n",
    "                    \"\"\").df()\n",
    "\n",
    "            # 4) Merge the three sources into one side-by-side table\n",
    "            merged = None\n",
    "            try:\n",
    "                merged = box_df.copy()\n",
    "                if not step4_df.empty:\n",
    "                    merged = merged.merge(step4_df, on=\"team_abbrev\", how=\"outer\")\n",
    "                if not lineup_df.empty:\n",
    "                    for method in lineup_df['method'].unique():\n",
    "                        sub = lineup_df[lineup_df['method'] == method][[\"team_abbrev\", \"lineup_points_for\"]].rename(\n",
    "                            columns={\"lineup_points_for\": f\"lineup_points_for_{method}\"}\n",
    "                        )\n",
    "                        merged = merged.merge(sub, on=\"team_abbrev\", how=\"outer\")\n",
    "                # diffs\n",
    "                if \"box_points\" in merged.columns and \"lineup_points_for_traditional\" in merged.columns:\n",
    "                    merged[\"diff_traditional_vs_box\"] = merged[\"lineup_points_for_traditional\"] - merged[\"box_points\"]\n",
    "                if \"box_points\" in merged.columns and \"lineup_points_for_enhanced\" in merged.columns:\n",
    "                    merged[\"diff_enhanced_vs_box\"] = merged[\"lineup_points_for_enhanced\"] - merged[\"box_points\"]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if merged is not None:\n",
    "                merged.to_csv(out_dir / \"001_dual_vs_box_points.csv\", index=False)\n",
    "\n",
    "            # 5) Identify unattributed scoring events per method (needs step4)\n",
    "            if \"step4_processed_events\" in existing_tables:\n",
    "                # detect a time column to help downstream inspection\n",
    "                cols = [r[1] for r in self.conn.execute(\"PRAGMA table_info('step4_processed_events')\").fetchall()]\n",
    "                time_col = next((c for c in [\"order_ts\",\"abs_time\",\"wall_clock_int\",\"pbp_order\",\"eventnum\"] if c in cols), None)\n",
    "\n",
    "                def dump_unattr(method: str, lineup_col: str):\n",
    "                    sql = f\"\"\"\n",
    "                        SELECT se.*\n",
    "                        FROM step4_processed_events se\n",
    "                        WHERE se.points > 0\n",
    "                        AND (\n",
    "                            {lineup_col} IS NULL\n",
    "                            OR TRIM(CAST({lineup_col} AS VARCHAR)) = ''\n",
    "                            OR TRIM(CAST({lineup_col} AS VARCHAR)) = '[]'\n",
    "                        )\n",
    "                    \"\"\"\n",
    "                    df = self.conn.execute(sql).df()\n",
    "                    if time_col and time_col in df.columns:\n",
    "                        df = df.sort_values(time_col)\n",
    "                    df.to_csv(out_dir / f\"020_unattributed_scoring_events_{method}.csv\", index=False)\n",
    "                    return df\n",
    "\n",
    "                trad_df = dump_unattr(\"traditional\", \"traditional_off_lineup\") if \"traditional_off_lineup\" in cols else pd.DataFrame()\n",
    "                enh_df  = dump_unattr(\"enhanced\", \"enhanced_off_lineup\")       if \"enhanced_off_lineup\" in cols       else pd.DataFrame()\n",
    "\n",
    "                # 6) Coverage table: how many scoring events had a valid lineup?\n",
    "                coverage_rows = []\n",
    "                for method, lineup_col in [(\"traditional\",\"traditional_off_lineup\"), (\"enhanced\",\"enhanced_off_lineup\")]:\n",
    "                    if lineup_col not in cols:\n",
    "                        continue\n",
    "                    tot = int(self.conn.execute(\"SELECT COUNT(*) FROM step4_processed_events WHERE points > 0\").fetchone()[0])\n",
    "                    ok = int(self.conn.execute(f\"\"\"\n",
    "                        SELECT COUNT(*)\n",
    "                        FROM step4_processed_events\n",
    "                        WHERE points > 0\n",
    "                        AND {lineup_col} IS NOT NULL\n",
    "                        AND TRIM(CAST({lineup_col} AS VARCHAR)) NOT IN ('','[]')\n",
    "                    \"\"\").fetchone()[0])\n",
    "                    coverage_rows.append({\n",
    "                        \"method\": method,\n",
    "                        \"scoring_event_count\": tot,\n",
    "                        \"with_valid_off_lineup\": ok,\n",
    "                        \"coverage_pct\": (ok / tot * 100.0) if tot > 0 else 0.0\n",
    "                    })\n",
    "                pd.DataFrame(coverage_rows).to_csv(out_dir / \"030_scoring_event_lineup_coverage.csv\", index=False)\n",
    "\n",
    "                # 7) Team-level unattributed points (quick lens on HOU -4)\n",
    "                team_unattr_rows = []\n",
    "                for method, lineup_col in [(\"traditional\",\"traditional_off_lineup\"), (\"enhanced\",\"enhanced_off_lineup\")]:\n",
    "                    if lineup_col not in cols:\n",
    "                        continue\n",
    "                    team_map_df = pd.DataFrame()\n",
    "                    if \"teams\" in existing_tables:\n",
    "                        team_map_df = self.conn.execute(\"SELECT DISTINCT team_id, team_abbrev FROM teams\").df()\n",
    "                    elif \"box_score\" in existing_tables:\n",
    "                        team_map_df = self.conn.execute(\"SELECT DISTINCT team_id, team_abbrev FROM box_score\").df()\n",
    "                    if not team_map_df.empty:\n",
    "                        self.conn.register(\"teams_temp_map2\", team_map_df)\n",
    "                        try:\n",
    "                            unattributed = self.conn.execute(f\"\"\"\n",
    "                                SELECT m.team_abbrev, SUM(se.points) AS unattributed_points\n",
    "                                FROM step4_processed_events se\n",
    "                                JOIN teams_temp_map2 m ON m.team_id = se.off_team_id\n",
    "                                WHERE se.points > 0\n",
    "                                AND ({lineup_col} IS NULL\n",
    "                                    OR TRIM(CAST({lineup_col} AS VARCHAR)) IN ('','[]'))\n",
    "                                GROUP BY m.team_abbrev\n",
    "                                ORDER BY m.team_abbrev\n",
    "                            \"\"\").df()\n",
    "                        finally:\n",
    "                            self.conn.unregister(\"teams_temp_map2\")\n",
    "                        unattributed.to_csv(out_dir / f\"040_unattributed_points_by_team_{method}.csv\", index=False)\n",
    "                        team_unattr_rows.append((method, unattributed))\n",
    "\n",
    "            return ValidationResult(\n",
    "                step_name=\"Points Attribution Audit\",\n",
    "                passed=True,\n",
    "                details=f\"Wrote audit CSVs to {out_dir}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return ValidationResult(\n",
    "                step_name=\"Points Attribution Audit\",\n",
    "                passed=False,\n",
    "                details=f\"Error creating audit: {e}\",\n",
    "                processing_time=time.time() - start_time\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_dual_method_final_export(db_path: str = \"mavs_enhanced.duckdb\") -> Tuple[bool, DualMethodFinalValidator]:\n",
    "    \"\"\"Run dual-method final validation and export for both traditional and enhanced approaches\"\"\"\n",
    "    print(\"NBA Pipeline - Step 6: Dual-Method Final Validation & Export\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    with DualMethodFinalValidator(db_path) as validator:\n",
    "        results = []\n",
    "\n",
    "        # Step 6a\n",
    "        logger.info(\"Step 6a: Validating dual-method tables...\")\n",
    "        table_result = validator.validate_dual_method_tables_exist()\n",
    "        results.append(table_result)\n",
    "        if not table_result.passed:\n",
    "            logger.error(\"Dual-method tables validation failed - stopping\")\n",
    "            return False, validator\n",
    "\n",
    "        # Step 6b\n",
    "        logger.info(\"Step 6b: Validating against box score (dual-method)...\")\n",
    "        box_score_result = validator.validate_against_box_score_dual()\n",
    "        results.append(box_score_result)\n",
    "        if not box_score_result.passed:\n",
    "            logger.warning(\"Box score validation issues detected.\")\n",
    "            logger.warning(box_score_result.details)   # richer per-team info\n",
    "            for w in box_score_result.warnings:\n",
    "                logger.warning(w)\n",
    "\n",
    "        # >>> NEW: DEBUG AUDIT right after the mismatch appears <<<\n",
    "        logger.info(\"Step 6b.1: Running points attribution audit (debug-only)...\")\n",
    "        audit_result = validator.export_points_attribution_audit()\n",
    "        results.append(audit_result)\n",
    "        if not audit_result.passed:\n",
    "            logger.warning(f\"Audit issues: {audit_result.details}\")\n",
    "        # <<< END NEW >>>\n",
    "\n",
    "        # Step 6c\n",
    "        logger.info(\"Step 6c: Validating data completeness (dual-method)...\")\n",
    "        completeness_result = validator.validate_data_completeness()\n",
    "        results.append(completeness_result)\n",
    "        if not completeness_result.passed:\n",
    "            logger.warning(f\"Data completeness issues: {completeness_result.details}\")\n",
    "\n",
    "        # Step 6d\n",
    "        logger.info(\"Step 6d: Exporting project deliverables (dual-method)...\")\n",
    "        export_result = validator.export_project_deliverables()\n",
    "        results.append(export_result)\n",
    "        if not export_result.passed:\n",
    "            logger.error(f\"Export failed: {export_result.details}\")\n",
    "            return False, validator\n",
    "\n",
    "        # Step 6e\n",
    "        logger.info(\"Step 6e: Exporting violation reports...\")\n",
    "        violation_result = validator.export_violation_reports()\n",
    "        results.append(violation_result)\n",
    "        if not violation_result.passed:\n",
    "            logger.warning(f\"Violation export had issues: {violation_result.details}\")\n",
    "\n",
    "        # Step 6f\n",
    "        logger.info(\"Step 6f: Exporting method comparison reports...\")\n",
    "        comparison_result = validator.export_method_comparison_reports()\n",
    "        results.append(comparison_result)\n",
    "        if not comparison_result.passed:\n",
    "            logger.warning(f\"Comparison export had issues: {comparison_result.details}\")\n",
    "\n",
    "        # Step 6g\n",
    "        logger.info(\"Step 6g: Generating comprehensive quality report...\")\n",
    "        quality_result = validator.generate_quality_report()\n",
    "        results.append(quality_result)\n",
    "        if not quality_result.passed:\n",
    "            logger.warning(f\"Quality report issues: {quality_result.details}\")\n",
    "\n",
    "        # Summary\n",
    "        validator.print_final_summary()\n",
    "\n",
    "        critical_failures = [r for r in results if not r.passed and r.step_name in \n",
    "                           [\"Validate Dual-Method Tables\", \"Export Project Deliverables\"]]\n",
    "        success = len(critical_failures) == 0\n",
    "        return success, validator\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from eda.data.nba_possession_engine import run_dual_method_possession_engine\n",
    "\n",
    "    database_path = \"mavs_enhanced.duckdb\"\n",
    "\n",
    "    # Run Step 5 first to ensure dual-method data is available\n",
    "    print(\"Running Step 5: Dual-Method Possession Engine...\")\n",
    "    step5_success = run_dual_method_possession_engine(database_path)\n",
    "    if not step5_success:\n",
    "        print(\"❌ Step 5 failed - dual-method data not available\")\n",
    "        exit(1)\n",
    "\n",
    "    # Run Step 6: Dual-Method Final Export\n",
    "    print(\"\\nRunning Step 6: Dual-Method Final Export...\")\n",
    "    success, validator = run_dual_method_final_export(database_path)\n",
    "\n",
    "    if success:\n",
    "        print(\"\\n✅ Step 6 Complete: Dual-method results validated and exported\")\n",
    "        print(\"📊 Exported files:\")\n",
    "        print(\"   - project1_lineups_traditional.csv\")\n",
    "        print(\"   - project1_lineups_enhanced.csv\") \n",
    "        print(\"   - project2_players_traditional.csv\")\n",
    "        print(\"   - project2_players_enhanced.csv\")\n",
    "        print(\"   - traditional_lineup_violations.csv\")\n",
    "        print(\"   - method_comparison_summary.csv\")\n",
    "        print(\"   - method_effectiveness_report.txt\")\n",
    "        print(\"   - base_dataset_violations.txt\")\n",
    "        print(\"🎯 Both traditional and enhanced methods ready for project submission\")\n",
    "    else:\n",
    "        print(\"\\n❌ Step 6 Failed: Dual-method validation errors\")\n",
    "        print(\"🔧 Review validation messages above and fix export issues\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2afc06",
   "metadata": {},
   "source": [
    "Step 7: Complete Pipeline Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00c2b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: c:\\docker_projects\\interview_hackathon\\api\\src\\airflow_project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 18:45:29,304 - WARNING - Ignoring invalid argument (appears to be kernel file): --f=c:\\Users\\ghadf\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3313d9cfac109dbeb75fe9c92c45b8d0cacfe4956.json\n",
      "2025-09-05 18:45:29,306 - INFO - Starting complete pipeline with database: mavs_enhanced.duckdb\n",
      "2025-09-05 18:45:29,306 - INFO - Using database path: c:\\docker_projects\\interview_hackathon\\api\\src\\airflow_project\\mavs_enhanced.duckdb\n",
      "2025-09-05 18:45:29,307 - INFO - ================================================================================\n",
      "2025-09-05 18:45:29,307 - INFO -  NBA COMPLETE PIPELINE RUNNER\n",
      "2025-09-05 18:45:29,307 - INFO - ================================================================================\n",
      "2025-09-05 18:45:29,308 - INFO - Database: c:\\docker_projects\\interview_hackathon\\api\\src\\airflow_project\\mavs_enhanced.duckdb\n",
      "2025-09-05 18:45:29,308 - INFO - Working Directory: c:\\docker_projects\\interview_hackathon\\api\\src\\airflow_project\n",
      "2025-09-05 18:45:29,308 - INFO - Start time: 2025-09-05 18:45:29\n",
      "2025-09-05 18:45:29,309 - INFO - \n",
      "2025-09-05 18:45:29,309 - INFO - STEP 1: Loading NBA Data...\n",
      "2025-09-05 18:45:29,328 - INFO - Loading box score from C:\\docker_projects\\interview_hackathon\\api\\src\\airflow_project\\data\\mavs_data_engineer_2025\\box_HOU-DAL.csv\n",
      "2025-09-05 18:45:29,331 - INFO - Raw box score: 35 rows\n",
      "2025-09-05 18:45:29,332 - INFO - Active players: 28 rows\n",
      "2025-09-05 18:45:29,349 - INFO - [PASS] Load Box Score: Processed box score: 35 → 28 active → 21 final rows. Teams: ['HOU', 'DAL'], Starters: {'HOU': 5, 'DAL': 5}\n",
      "2025-09-05 18:45:29,350 - WARNING - [WARN] Load Box Score: Removed 7 players with no playing time\n",
      "2025-09-05 18:45:29,350 - INFO - Loading PBP from C:\\docker_projects\\interview_hackathon\\api\\src\\airflow_project\\data\\mavs_data_engineer_2025\\pbp_HOU-DAL.csv\n",
      "2025-09-05 18:45:29,354 - INFO - Admin rows: 1, Game events: 506\n",
      "2025-09-05 18:45:29,378 - INFO - [PASS] Load PBP: Processed PBP: 507 → 506 game events → 506 final rows. Shots: 183, Rim attempts: 52\n",
      "2025-09-05 18:45:29,379 - WARNING - [WARN] Load PBP: 1 shots beyond 35.0 feet (max: 41.0ft)\n",
      "2025-09-05 18:45:29,379 - WARNING - [WARN] Load PBP: Flagged 1 extreme-distance shots (> 35.0 ft)\n",
      "2025-09-05 18:45:29,379 - INFO - Validating data relationships...\n",
      "2025-09-05 18:45:29,387 - INFO - Extra PBP players: [101283.0, 1627963.0, 1628954.0]\n",
      "2025-09-05 18:45:29,387 - INFO - [PASS] Data Relationships: Relationship validation: Box teams: 2, PBP teams: 2, Final PBP events: 506\n",
      "2025-09-05 18:45:29,387 - WARNING - [WARN] Data Relationships: 3 players in PBP not in box score: players = {1627963.0, 1628954.0, 101283.0}\n",
      "2025-09-05 18:45:29,389 - INFO - Creating lookup views...\n",
      "2025-09-05 18:45:29,407 - INFO - [PASS] Create Lookup Views: Created/Replaced 3 lookup tables: pbp_event_msg_types, pbp_action_types, pbp_option_types\n",
      "2025-09-05 18:45:29,413 - INFO - 🔍 Detecting referees/officials for filtering...\n",
      "2025-09-05 18:45:29,418 - INFO - 🔍 Analyzing 3 potential referee/official IDs...\n",
      "2025-09-05 18:45:29,420 - INFO -   ✅  (ID: 1627963.0) - REFEREE/OFFICIAL\n",
      "2025-09-05 18:45:29,421 - INFO -       Events: {6: 20, 5: 2, 7: 1}\n",
      "2025-09-05 18:45:29,422 - INFO -   ✅  (ID: 1628954.0) - REFEREE/OFFICIAL\n",
      "2025-09-05 18:45:29,423 - INFO -       Events: {6: 12, 5: 5}\n",
      "2025-09-05 18:45:29,424 - INFO -   ✅  (ID: 101283.0) - REFEREE/OFFICIAL\n",
      "2025-09-05 18:45:29,425 - INFO -       Events: {6: 10, 18: 3, 5: 1, 7: 1}\n",
      "2025-09-05 18:45:29,425 - INFO - 🎯 Identified 3 confirmed referees/officials\n",
      "2025-09-05 18:45:29,431 - INFO - 📋 Created dim_officials table with 3 officials\n",
      "2025-09-05 18:45:29,437 - INFO - 🚫 Filtering out 3 referee IDs: [101283.0, 1627963.0, 1628954.0]\n",
      "2025-09-05 18:45:29,459 - INFO - ✅ Created dim_players: 21 players\n",
      "2025-09-05 18:45:29,461 - INFO - ✅ Created pbp_only_players: 0 PBP-only players\n",
      "2025-09-05 18:45:29,462 - INFO - ✅ Created dim_officials: 3 referees/officials\n",
      "2025-09-05 18:45:29,462 - INFO - 🚫 Filtered out referees: [101283.0, 1627963.0, 1628954.0]\n",
      "2025-09-05 18:45:29,464 - INFO - [PASS] Create Dimensions: dim_players: 21 rows; pbp_only_players: 0 rows; dim_officials: 3 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] All pipeline modules imported successfully\n",
      "NBA Pipeline - Enhanced Data Loading & Validation\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 18:45:29,493 - INFO - [PASS] Create PBP Enriched View: Created view pbp_enriched with 506 rows (matches pbp)\n",
      "2025-09-05 18:45:29,498 - INFO - Processing 506 events with TRADITIONAL DATA-DRIVEN approach...\n",
      "2025-09-05 18:45:29,500 - INFO - [TRADITIONAL SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,501 - INFO - [TRADITIONAL SUB-IN] Klay Thompson to DAL\n",
      "2025-09-05 18:45:29,502 - INFO - [TRADITIONAL SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,503 - INFO - [TRADITIONAL SUB-IN] Kyrie Irving to DAL\n",
      "2025-09-05 18:45:29,504 - INFO - [TRADITIONAL SUB-IN] P.J. Washington to DAL\n",
      "2025-09-05 18:45:29,505 - INFO - [TRADITIONAL SUB-OUT] Dillon Brooks from HOU\n",
      "2025-09-05 18:45:29,505 - INFO - [TRADITIONAL SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,506 - INFO - [TRADITIONAL SUB-IN] Alperen Sengun to HOU\n",
      "2025-09-05 18:45:29,507 - INFO - [TRADITIONAL SUB-IN] Amen Thompson to HOU\n",
      "2025-09-05 18:45:29,508 - INFO - [TRADITIONAL SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,509 - INFO - [TRADITIONAL SUB-IN] Anthony Davis to DAL\n",
      "2025-09-05 18:45:29,509 - INFO - [TRADITIONAL SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,510 - INFO - [TRADITIONAL SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,513 - INFO - [TRADITIONAL SUB-OUT] Alperen Sengun from HOU\n",
      "2025-09-05 18:45:29,513 - INFO - [TRADITIONAL SUB-IN] Jae'Sean Tate to HOU\n",
      "2025-09-05 18:45:29,514 - INFO - [TRADITIONAL SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,515 - INFO - [TRADITIONAL SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,516 - INFO - [TRADITIONAL SUB-OUT] Anthony Davis from DAL\n",
      "2025-09-05 18:45:29,516 - INFO - [TRADITIONAL SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,518 - INFO - [TRADITIONAL SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,519 - INFO - [TRADITIONAL SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,520 - INFO - [TRADITIONAL SUB-IN] Cam Whitmore to HOU\n",
      "2025-09-05 18:45:29,520 - INFO - [TRADITIONAL SUB-OUT] Jalen Green from HOU\n",
      "2025-09-05 18:45:29,521 - INFO - [TRADITIONAL SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,523 - INFO - [TRADITIONAL SUB-IN] P.J. Washington to DAL\n",
      "2025-09-05 18:45:29,524 - INFO - [TRADITIONAL SUB-IN] Klay Thompson to DAL\n",
      "2025-09-05 18:45:29,525 - INFO - [TRADITIONAL SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,525 - INFO - [TRADITIONAL SUB-IN] Reed Sheppard to HOU\n",
      "2025-09-05 18:45:29,527 - INFO - [TRADITIONAL SUB-OUT] P.J. Washington from DAL\n",
      "2025-09-05 18:45:29,528 - INFO - [TRADITIONAL SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,529 - INFO - [TRADITIONAL SUB-OUT] Max Christie from DAL\n",
      "2025-09-05 18:45:29,529 - INFO - [TRADITIONAL SUB-IN] P.J. Washington to DAL\n",
      "2025-09-05 18:45:29,533 - INFO - [TRADITIONAL SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,533 - INFO - [TRADITIONAL SUB-IN] Klay Thompson to DAL\n",
      "2025-09-05 18:45:29,534 - INFO - [TRADITIONAL SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,536 - INFO - [TRADITIONAL SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,536 - INFO - [TRADITIONAL SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,538 - INFO - [TRADITIONAL SUB-IN] Amen Thompson to HOU\n",
      "2025-09-05 18:45:29,539 - INFO - [TRADITIONAL SUB-OUT] Max Christie from DAL\n",
      "2025-09-05 18:45:29,539 - INFO - [TRADITIONAL SUB-IN] Kyrie Irving to DAL\n",
      "2025-09-05 18:45:29,540 - INFO - [TRADITIONAL SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,540 - INFO - [TRADITIONAL SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,541 - INFO - [TRADITIONAL SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,541 - INFO - [TRADITIONAL SUB-IN] Anthony Davis to DAL\n",
      "2025-09-05 18:45:29,542 - INFO - [TRADITIONAL SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,543 - INFO - [TRADITIONAL SUB-OUT] Jalen Green from HOU\n",
      "2025-09-05 18:45:29,544 - INFO - [TRADITIONAL SUB-IN] Aaron Holiday to HOU\n",
      "2025-09-05 18:45:29,544 - INFO - [TRADITIONAL SUB-OUT] Dante Exum from DAL\n",
      "2025-09-05 18:45:29,545 - INFO - [TRADITIONAL SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,546 - INFO - [TRADITIONAL SUB-OUT] Alperen Sengun from HOU\n",
      "2025-09-05 18:45:29,546 - INFO - [TRADITIONAL SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,547 - INFO - [TRADITIONAL SUB-OUT] Dillon Brooks from HOU\n",
      "2025-09-05 18:45:29,547 - INFO - [TRADITIONAL SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,549 - INFO - [TRADITIONAL SUB-IN] Klay Thompson to DAL\n",
      "2025-09-05 18:45:29,550 - INFO - [TRADITIONAL SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,551 - INFO - [TRADITIONAL SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,551 - INFO - [TRADITIONAL SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,552 - INFO - [TRADITIONAL SUB-IN] Amen Thompson to HOU\n",
      "2025-09-05 18:45:29,552 - INFO - [TRADITIONAL SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,552 - INFO - [TRADITIONAL SUB-IN] Cam Whitmore to HOU\n",
      "2025-09-05 18:45:29,554 - INFO - [TRADITIONAL SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,555 - INFO - [TRADITIONAL SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,555 - INFO - [TRADITIONAL SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,555 - INFO - [TRADITIONAL SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,557 - INFO - [TRADITIONAL SUB-OUT] Dante Exum from DAL\n",
      "2025-09-05 18:45:29,557 - INFO - [TRADITIONAL SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,558 - INFO - [TRADITIONAL SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,558 - INFO - [TRADITIONAL SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,559 - INFO - [TRADITIONAL SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,560 - INFO - [TRADITIONAL SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,561 - INFO - [TRADITIONAL SUB-IN] Kyrie Irving to DAL\n",
      "2025-09-05 18:45:29,561 - INFO - [TRADITIONAL SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,562 - INFO - [TRADITIONAL SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,562 - INFO - [TRADITIONAL SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,593 - INFO - [TRADITIONAL DATA-DRIVEN] 46 total substitutions\n",
      "2025-09-05 18:45:29,594 - INFO - [TRADITIONAL DATA-DRIVEN] 873 flags generated\n",
      "2025-09-05 18:45:29,594 - INFO - [TRADITIONAL DATA-DRIVEN] Lineup size distribution: {'4': 509, '5': 377, '3': 88, '6': 42}\n",
      "2025-09-05 18:45:29,594 - INFO - [PASS] Traditional Data-Driven Lineups: Traditional data-driven tracking: 1016 states, 873 flags, 37.1% correct lineup size\n",
      "2025-09-05 18:45:29,598 - INFO - Processing 506 events with ENHANCED substitution rules and flagging...\n",
      "2025-09-05 18:45:29,598 - INFO - [ENHANCED PERIOD 1] Reset to starters\n",
      "2025-09-05 18:45:29,600 - INFO - [ENHANCED SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,600 - INFO - [ENHANCED SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,600 - INFO - [ENHANCED SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,601 - INFO - [ENHANCED SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,601 - INFO - [ENHANCED SUB-OUT] Dillon Brooks from HOU\n",
      "2025-09-05 18:45:29,601 - INFO - [ENHANCED SUB-IN] Cam Whitmore to HOU\n",
      "2025-09-05 18:45:29,602 - INFO - [ENHANCED SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,603 - INFO - [ENHANCED SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,604 - INFO - [ENHANCED SUB-OUT] P.J. Washington from DAL\n",
      "2025-09-05 18:45:29,604 - INFO - [ENHANCED SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,605 - INFO - [ENHANCED SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,606 - INFO - [ENHANCED SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,606 - INFO - [ENHANCED SUB-OUT] Alperen Sengun from HOU\n",
      "2025-09-05 18:45:29,606 - INFO - [ENHANCED SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,607 - INFO - [ENHANCED SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,607 - INFO - [ENHANCED SUB-IN] Jae'Sean Tate to HOU\n",
      "2025-09-05 18:45:29,608 - INFO - [ENHANCED SUB-OUT] Anthony Davis from DAL\n",
      "2025-09-05 18:45:29,608 - INFO - [ENHANCED SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,609 - INFO - [ENHANCED SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,610 - INFO - [ENHANCED SUB-IN] Kyrie Irving to DAL\n",
      "2025-09-05 18:45:29,611 - INFO - [ENHANCED PERIOD 2] Continue lineups\n",
      "2025-09-05 18:45:29,611 - INFO - [ENHANCED FIRST-ACTION] Klay Thompson -> DAL (msg: 1)\n",
      "2025-09-05 18:45:29,611 - INFO - [ENHANCED AUTO-OUT] Naji Marshall from DAL (idle: 68.6s)\n",
      "2025-09-05 18:45:29,612 - INFO - [ENHANCED FIRST-ACTION] Tari Eason -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,612 - INFO - [ENHANCED AUTO-OUT] Jae'Sean Tate from HOU (idle: 163.0s)\n",
      "2025-09-05 18:45:29,613 - INFO - [ENHANCED FIRST-ACTION] Reed Sheppard -> HOU (msg: 4)\n",
      "2025-09-05 18:45:29,613 - INFO - [ENHANCED AUTO-OUT] Jalen Green from HOU (idle: 244.0s)\n",
      "2025-09-05 18:45:29,614 - INFO - [ENHANCED FIRST-ACTION] Jae'Sean Tate -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,614 - INFO - [ENHANCED AUTO-OUT] Cam Whitmore from HOU (idle: 180.0s)\n",
      "2025-09-05 18:45:29,615 - INFO - [ENHANCED FIRST-ACTION] Cam Whitmore -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,616 - INFO - [ENHANCED AUTO-OUT] Steven Adams from HOU (idle: 79.0s)\n",
      "2025-09-05 18:45:29,616 - INFO - [ENHANCED SUB-OUT] Jae'Sean Tate from HOU\n",
      "2025-09-05 18:45:29,617 - INFO - [ENHANCED SUB-IN] Alperen Sengun to HOU\n",
      "2025-09-05 18:45:29,617 - INFO - [ENHANCED SUB-IN] Amen Thompson to HOU\n",
      "2025-09-05 18:45:29,617 - INFO - [ENHANCED AUTO-OUT] Reed Sheppard from HOU (idle: 30.0s)\n",
      "2025-09-05 18:45:29,618 - INFO - [ENHANCED SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,618 - INFO - [ENHANCED SUB-IN] Anthony Davis to DAL\n",
      "2025-09-05 18:45:29,619 - INFO - [ENHANCED FIRST-ACTION] Reed Sheppard -> HOU (msg: 6)\n",
      "2025-09-05 18:45:29,619 - INFO - [ENHANCED AUTO-OUT] Dillon Brooks from HOU (idle: 242.3s)\n",
      "2025-09-05 18:45:29,619 - INFO - [ENHANCED SUB-OUT] Max Christie from DAL\n",
      "2025-09-05 18:45:29,620 - INFO - [ENHANCED SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,620 - INFO - [ENHANCED FIRST-ACTION] P.J. Washington -> DAL (msg: 6)\n",
      "2025-09-05 18:45:29,621 - INFO - [ENHANCED AUTO-OUT] Dante Exum from DAL (idle: 257.8s)\n",
      "2025-09-05 18:45:29,622 - INFO - [ENHANCED SUB-OUT] Cam Whitmore from HOU\n",
      "2025-09-05 18:45:29,622 - INFO - [ENHANCED SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,622 - INFO - [ENHANCED SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,623 - INFO - [ENHANCED SUB-IN] Jalen Green to HOU\n",
      "2025-09-05 18:45:29,624 - INFO - [ENHANCED SUB-OUT] P.J. Washington from DAL\n",
      "2025-09-05 18:45:29,624 - INFO - [ENHANCED SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,624 - INFO - [ENHANCED SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,624 - INFO - [ENHANCED SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,626 - INFO - [ENHANCED SUB-OUT] Reed Sheppard from HOU\n",
      "2025-09-05 18:45:29,626 - INFO - [ENHANCED SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,627 - INFO - [ENHANCED SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,628 - INFO - [ENHANCED SUB-IN] P.J. Washington to DAL\n",
      "2025-09-05 18:45:29,629 - INFO - [ENHANCED SUB-OUT] P.J. Washington from DAL\n",
      "2025-09-05 18:45:29,629 - INFO - [ENHANCED SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,630 - INFO - [ENHANCED PERIOD 3] Reset to starters\n",
      "2025-09-05 18:45:29,631 - INFO - [ENHANCED FIRST-ACTION] Steven Adams -> HOU (msg: 4)\n",
      "2025-09-05 18:45:29,631 - INFO - [ENHANCED AUTO-OUT] Dillon Brooks from HOU (idle: 298.0s)\n",
      "2025-09-05 18:45:29,632 - INFO - [ENHANCED FIRST-ACTION] Max Christie -> DAL (msg: 1)\n",
      "2025-09-05 18:45:29,632 - INFO - [ENHANCED AUTO-OUT] Kyrie Irving from DAL (idle: 273.0s)\n",
      "2025-09-05 18:45:29,633 - INFO - [ENHANCED FIRST-ACTION] Dillon Brooks -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,633 - INFO - [ENHANCED AUTO-OUT] Tari Eason from HOU (idle: 166.2s)\n",
      "2025-09-05 18:45:29,634 - INFO - [ENHANCED SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,634 - INFO - [ENHANCED SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,635 - INFO - [ENHANCED SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,635 - INFO - [ENHANCED SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,636 - INFO - [ENHANCED FIRST-ACTION] Kyrie Irving -> DAL (msg: 2)\n",
      "2025-09-05 18:45:29,636 - INFO - [ENHANCED AUTO-OUT] P.J. Washington from DAL (idle: 391.0s)\n",
      "2025-09-05 18:45:29,637 - INFO - [ENHANCED SUB-OUT] Max Christie from DAL\n",
      "2025-09-05 18:45:29,638 - INFO - [ENHANCED SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,638 - INFO - [ENHANCED SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,639 - INFO - [ENHANCED SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,639 - INFO - [ENHANCED SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,640 - INFO - [ENHANCED SUB-IN] Cam Whitmore to HOU\n",
      "2025-09-05 18:45:29,640 - INFO - [ENHANCED SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,640 - INFO - [ENHANCED SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,641 - INFO - [ENHANCED SUB-OUT] Dante Exum from DAL\n",
      "2025-09-05 18:45:29,641 - INFO - [ENHANCED SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,642 - INFO - [ENHANCED SUB-OUT] Anthony Davis from DAL\n",
      "2025-09-05 18:45:29,642 - INFO - [ENHANCED SUB-IN] Kyrie Irving to DAL\n",
      "2025-09-05 18:45:29,642 - INFO - [ENHANCED SUB-OUT] Dillon Brooks from HOU\n",
      "2025-09-05 18:45:29,643 - INFO - [ENHANCED SUB-IN] Aaron Holiday to HOU\n",
      "2025-09-05 18:45:29,643 - INFO - [ENHANCED PERIOD 4] Continue lineups\n",
      "2025-09-05 18:45:29,644 - INFO - [ENHANCED FIRST-ACTION] Steven Adams -> HOU (msg: 4)\n",
      "2025-09-05 18:45:29,644 - INFO - [ENHANCED AUTO-OUT] Jalen Green from HOU (idle: 201.0s)\n",
      "2025-09-05 18:45:29,645 - INFO - [ENHANCED FIRST-ACTION] Klay Thompson -> DAL (msg: 4)\n",
      "2025-09-05 18:45:29,645 - INFO - [ENHANCED AUTO-OUT] Spencer Dinwiddie from DAL (idle: 183.0s)\n",
      "2025-09-05 18:45:29,645 - INFO - [ENHANCED FIRST-ACTION] Amen Thompson -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,645 - INFO - [ENHANCED AUTO-OUT] Alperen Sengun from HOU (idle: 245.0s)\n",
      "2025-09-05 18:45:29,645 - INFO - [ENHANCED FIRST-ACTION] Spencer Dinwiddie -> DAL (msg: 2)\n",
      "2025-09-05 18:45:29,647 - INFO - [ENHANCED AUTO-OUT] Naji Marshall from DAL (idle: 177.3s)\n",
      "2025-09-05 18:45:29,648 - INFO - [ENHANCED SUB-OUT] Aaron Holiday from HOU\n",
      "2025-09-05 18:45:29,648 - INFO - [ENHANCED SUB-IN] Jalen Green to HOU\n",
      "2025-09-05 18:45:29,649 - INFO - [ENHANCED SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,649 - INFO - [ENHANCED SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,649 - INFO - [ENHANCED SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,650 - INFO - [ENHANCED SUB-IN] Alperen Sengun to HOU\n",
      "2025-09-05 18:45:29,650 - INFO - [ENHANCED SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,651 - INFO - [ENHANCED SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,652 - INFO - [ENHANCED SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,652 - INFO - [ENHANCED SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,652 - INFO - [ENHANCED SUB-OUT] Dante Exum from DAL\n",
      "2025-09-05 18:45:29,653 - INFO - [ENHANCED SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,653 - INFO - [ENHANCED SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,654 - INFO - [ENHANCED SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,654 - INFO - [ENHANCED SUB-OUT] Cam Whitmore from HOU\n",
      "2025-09-05 18:45:29,654 - INFO - [ENHANCED SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,656 - INFO - [ENHANCED SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,657 - INFO - [ENHANCED SUB-IN] Amen Thompson to HOU\n",
      "2025-09-05 18:45:29,658 - INFO - [ENHANCED SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,658 - INFO - [ENHANCED SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,659 - INFO - [ENHANCED SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,660 - INFO - [ENHANCED SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,660 - INFO - [ENHANCED SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,661 - INFO - [ENHANCED SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,661 - INFO - [ENHANCED SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,661 - INFO - [ENHANCED SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,662 - INFO - [ENHANCED SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,663 - INFO - [ENHANCED SUB-IN] Olivier-Maxence Prosper to DAL\n",
      "2025-09-05 18:45:29,663 - INFO - [ENHANCED SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,664 - INFO - [ENHANCED SUB-IN] Kessler Edwards to DAL\n",
      "2025-09-05 18:45:29,664 - INFO - [ENHANCED SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,665 - INFO - [ENHANCED SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,695 - INFO - Created enhanced_flags table with 114 flag records\n",
      "2025-09-05 18:45:29,696 - INFO - [ENHANCED COMPLETE] 46 subs, 15 first-actions, 16 auto-outs\n",
      "2025-09-05 18:45:29,697 - INFO - [ENHANCED FLAGS] Missing sub-ins: 15, Inactivity periods: 52\n",
      "2025-09-05 18:45:29,698 - INFO - [PASS] Enhanced Substitution Tracking with Flags: Enhanced tracking complete: 46 subs, 15 first-actions, 114 total flags\n",
      "2025-09-05 18:45:29,707 - INFO - [Missing Player Report] Reusing existing _pbp_names TEMP view\n",
      "2025-09-05 18:45:29,711 - INFO - [Missing Player Report] pbp_only_players count = 0\n",
      "2025-09-05 18:45:29,733 - INFO - [Missing Player Report] built with 0 rows\n",
      "2025-09-05 18:45:29,736 - INFO - \n",
      "================================================================================\n",
      "MISSING PLAYER REPORT — SCHEMA\n",
      "================================================================================\n",
      "  - player_id: float64\n",
      "  - box_player_name: object\n",
      "  - pbp_last_name: object\n",
      "  - resolved_name: object\n",
      "  - box_team_id: int64\n",
      "  - guessed_team_id: int64\n",
      "  - team_confidence: float64\n",
      "  - resolved_team_id: int64\n",
      "  - resolved_team_abbrev: object\n",
      "  - box_team_abbrev: object\n",
      "  - guessed_team_abbrev: object\n",
      "  - sample_event: object\n",
      "  - total_events: int64\n",
      "  - points: float64\n",
      "  - shot_events: float64\n",
      "  - made_fg: float64\n",
      "  - missed_fg: float64\n",
      "  - free_throws: float64\n",
      "  - rebounds: float64\n",
      "  - turnovers: float64\n",
      "  - fouls: float64\n",
      "  - substitutions: float64\n",
      "  - first_event: object\n",
      "  - last_event: object\n",
      "2025-09-05 18:45:29,739 - INFO - \n",
      "================================================================================\n",
      "MISSING PLAYER REPORT — FULL DATA\n",
      "================================================================================\n",
      "rows: 0\n",
      "Empty DataFrame\n",
      "Columns: [player_id, box_player_name, pbp_last_name, resolved_name, box_team_id, guessed_team_id, team_confidence, resolved_team_id, resolved_team_abbrev, box_team_abbrev, guessed_team_abbrev, sample_event, total_events, points, shot_events, made_fg, missed_fg, free_throws, rebounds, turnovers, fouls, substitutions, first_event, last_event]\n",
      "Index: []\n",
      "================================================================================\n",
      "2025-09-05 18:45:29,740 - INFO - [PASS] Missing Player Report: Built missing_player_report with 0 rows\n",
      "2025-09-05 18:45:29,747 - INFO - Starting lineup processing: DAL vs HOU\n",
      "2025-09-05 18:45:29,748 - INFO - [PERIOD 1] Reset to starters\n",
      "2025-09-05 18:45:29,750 - INFO - [SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,751 - INFO - [SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,751 - INFO - [SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,751 - INFO - [SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,752 - INFO - [SUB-OUT] Dillon Brooks from HOU\n",
      "2025-09-05 18:45:29,753 - INFO - [SUB-IN] Cam Whitmore to HOU\n",
      "2025-09-05 18:45:29,754 - INFO - [SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,754 - INFO - [SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,755 - INFO - [SUB-OUT] P.J. Washington from DAL\n",
      "2025-09-05 18:45:29,756 - INFO - [SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,757 - INFO - [SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,757 - INFO - [SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,758 - INFO - [SUB-OUT] Alperen Sengun from HOU\n",
      "2025-09-05 18:45:29,758 - INFO - [SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,759 - INFO - [SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,759 - INFO - [SUB-IN] Jae'Sean Tate to HOU\n",
      "2025-09-05 18:45:29,762 - INFO - [SUB-OUT] Anthony Davis from DAL\n",
      "2025-09-05 18:45:29,762 - INFO - [SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,763 - INFO - [SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,763 - INFO - [SUB-IN] Kyrie Irving to DAL\n",
      "2025-09-05 18:45:29,764 - INFO - [PERIOD 2] Continue lineups\n",
      "2025-09-05 18:45:29,765 - INFO - [FIRST-ACTION] Klay Thompson -> DAL (msg: 1)\n",
      "2025-09-05 18:45:29,766 - INFO - [AUTO-OUT] Naji Marshall from DAL (idle: 68.6s)\n",
      "2025-09-05 18:45:29,767 - INFO - [FIRST-ACTION] Tari Eason -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,767 - INFO - [AUTO-OUT] Jae'Sean Tate from HOU (idle: 163.0s)\n",
      "2025-09-05 18:45:29,767 - INFO - [FIRST-ACTION] Reed Sheppard -> HOU (msg: 4)\n",
      "2025-09-05 18:45:29,768 - INFO - [AUTO-OUT] Jalen Green from HOU (idle: 244.0s)\n",
      "2025-09-05 18:45:29,769 - INFO - [FIRST-ACTION] Jae'Sean Tate -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,770 - INFO - [AUTO-OUT] Cam Whitmore from HOU (idle: 180.0s)\n",
      "2025-09-05 18:45:29,771 - INFO - [FIRST-ACTION] Cam Whitmore -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,771 - INFO - [AUTO-OUT] Steven Adams from HOU (idle: 79.0s)\n",
      "2025-09-05 18:45:29,772 - INFO - [SUB-OUT] Jae'Sean Tate from HOU\n",
      "2025-09-05 18:45:29,772 - INFO - [SUB-IN] Alperen Sengun to HOU\n",
      "2025-09-05 18:45:29,773 - INFO - [SUB-IN] Amen Thompson to HOU\n",
      "2025-09-05 18:45:29,774 - INFO - [AUTO-OUT] Reed Sheppard from HOU (idle: 30.0s)\n",
      "2025-09-05 18:45:29,775 - INFO - [SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,775 - INFO - [SUB-IN] Anthony Davis to DAL\n",
      "2025-09-05 18:45:29,776 - INFO - [FIRST-ACTION] Reed Sheppard -> HOU (msg: 6)\n",
      "2025-09-05 18:45:29,777 - INFO - [AUTO-OUT] Dillon Brooks from HOU (idle: 242.3s)\n",
      "2025-09-05 18:45:29,777 - INFO - [SUB-OUT] Max Christie from DAL\n",
      "2025-09-05 18:45:29,777 - INFO - [SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,778 - INFO - [FIRST-ACTION] P.J. Washington -> DAL (msg: 6)\n",
      "2025-09-05 18:45:29,778 - INFO - [AUTO-OUT] Dante Exum from DAL (idle: 257.8s)\n",
      "2025-09-05 18:45:29,779 - INFO - [SUB-OUT] Cam Whitmore from HOU\n",
      "2025-09-05 18:45:29,779 - INFO - [SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,780 - INFO - [SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,780 - INFO - [SUB-IN] Jalen Green to HOU\n",
      "2025-09-05 18:45:29,781 - INFO - [SUB-OUT] P.J. Washington from DAL\n",
      "2025-09-05 18:45:29,782 - INFO - [SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,783 - INFO - [SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,784 - INFO - [SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,784 - INFO - [SUB-OUT] Reed Sheppard from HOU\n",
      "2025-09-05 18:45:29,785 - INFO - [SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,786 - INFO - [SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,787 - INFO - [SUB-IN] P.J. Washington to DAL\n",
      "2025-09-05 18:45:29,788 - INFO - [SUB-OUT] P.J. Washington from DAL\n",
      "2025-09-05 18:45:29,788 - INFO - [SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,789 - INFO - [PERIOD 3] Reset to starters\n",
      "2025-09-05 18:45:29,790 - INFO - [FIRST-ACTION] Steven Adams -> HOU (msg: 4)\n",
      "2025-09-05 18:45:29,790 - INFO - [AUTO-OUT] Dillon Brooks from HOU (idle: 298.0s)\n",
      "2025-09-05 18:45:29,790 - INFO - [FIRST-ACTION] Max Christie -> DAL (msg: 1)\n",
      "2025-09-05 18:45:29,791 - INFO - [AUTO-OUT] Kyrie Irving from DAL (idle: 273.0s)\n",
      "2025-09-05 18:45:29,792 - INFO - [FIRST-ACTION] Dillon Brooks -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,793 - INFO - [AUTO-OUT] Tari Eason from HOU (idle: 166.2s)\n",
      "2025-09-05 18:45:29,793 - INFO - [SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,794 - INFO - [SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,795 - INFO - [SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,796 - INFO - [SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,797 - INFO - [FIRST-ACTION] Kyrie Irving -> DAL (msg: 2)\n",
      "2025-09-05 18:45:29,797 - INFO - [AUTO-OUT] P.J. Washington from DAL (idle: 391.0s)\n",
      "2025-09-05 18:45:29,798 - INFO - [SUB-OUT] Max Christie from DAL\n",
      "2025-09-05 18:45:29,799 - INFO - [SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,800 - INFO - [SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,801 - INFO - [SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,802 - INFO - [SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,803 - INFO - [SUB-IN] Cam Whitmore to HOU\n",
      "2025-09-05 18:45:29,804 - INFO - [SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,804 - INFO - [SUB-IN] Max Christie to DAL\n",
      "2025-09-05 18:45:29,805 - INFO - [SUB-OUT] Dante Exum from DAL\n",
      "2025-09-05 18:45:29,806 - INFO - [SUB-IN] Daniel Gafford to DAL\n",
      "2025-09-05 18:45:29,807 - INFO - [SUB-OUT] Anthony Davis from DAL\n",
      "2025-09-05 18:45:29,807 - INFO - [SUB-IN] Kyrie Irving to DAL\n",
      "2025-09-05 18:45:29,808 - INFO - [SUB-OUT] Dillon Brooks from HOU\n",
      "2025-09-05 18:45:29,809 - INFO - [SUB-IN] Aaron Holiday to HOU\n",
      "2025-09-05 18:45:29,810 - INFO - [PERIOD 4] Continue lineups\n",
      "2025-09-05 18:45:29,810 - INFO - [FIRST-ACTION] Steven Adams -> HOU (msg: 4)\n",
      "2025-09-05 18:45:29,811 - INFO - [AUTO-OUT] Jalen Green from HOU (idle: 201.0s)\n",
      "2025-09-05 18:45:29,812 - INFO - [FIRST-ACTION] Klay Thompson -> DAL (msg: 4)\n",
      "2025-09-05 18:45:29,812 - INFO - [AUTO-OUT] Spencer Dinwiddie from DAL (idle: 183.0s)\n",
      "2025-09-05 18:45:29,813 - INFO - [FIRST-ACTION] Amen Thompson -> HOU (msg: 2)\n",
      "2025-09-05 18:45:29,813 - INFO - [AUTO-OUT] Alperen Sengun from HOU (idle: 245.0s)\n",
      "2025-09-05 18:45:29,813 - INFO - [FIRST-ACTION] Spencer Dinwiddie -> DAL (msg: 2)\n",
      "2025-09-05 18:45:29,813 - INFO - [AUTO-OUT] Naji Marshall from DAL (idle: 177.3s)\n",
      "2025-09-05 18:45:29,814 - INFO - [SUB-OUT] Aaron Holiday from HOU\n",
      "2025-09-05 18:45:29,814 - INFO - [SUB-IN] Jalen Green to HOU\n",
      "2025-09-05 18:45:29,815 - INFO - [SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,815 - INFO - [SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,816 - INFO - [SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,816 - INFO - [SUB-IN] Alperen Sengun to HOU\n",
      "2025-09-05 18:45:29,817 - INFO - [SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,817 - INFO - [SUB-IN] Dillon Brooks to HOU\n",
      "2025-09-05 18:45:29,818 - INFO - [SUB-OUT] Klay Thompson from DAL\n",
      "2025-09-05 18:45:29,818 - INFO - [SUB-IN] Naji Marshall to DAL\n",
      "2025-09-05 18:45:29,819 - INFO - [SUB-OUT] Dante Exum from DAL\n",
      "2025-09-05 18:45:29,819 - INFO - [SUB-IN] Spencer Dinwiddie to DAL\n",
      "2025-09-05 18:45:29,820 - INFO - [SUB-OUT] Amen Thompson from HOU\n",
      "2025-09-05 18:45:29,820 - INFO - [SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,820 - INFO - [SUB-OUT] Cam Whitmore from HOU\n",
      "2025-09-05 18:45:29,821 - INFO - [SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,822 - INFO - [SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,823 - INFO - [SUB-IN] Amen Thompson to HOU\n",
      "2025-09-05 18:45:29,824 - INFO - [SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,824 - INFO - [SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,825 - INFO - [SUB-OUT] Spencer Dinwiddie from DAL\n",
      "2025-09-05 18:45:29,825 - INFO - [SUB-IN] Dante Exum to DAL\n",
      "2025-09-05 18:45:29,826 - INFO - [SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,826 - INFO - [SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,827 - INFO - [SUB-OUT] Steven Adams from HOU\n",
      "2025-09-05 18:45:29,828 - INFO - [SUB-IN] Tari Eason to HOU\n",
      "2025-09-05 18:45:29,828 - INFO - [SUB-OUT] Kyrie Irving from DAL\n",
      "2025-09-05 18:45:29,829 - INFO - [SUB-IN] Olivier-Maxence Prosper to DAL\n",
      "2025-09-05 18:45:29,829 - INFO - [SUB-OUT] Daniel Gafford from DAL\n",
      "2025-09-05 18:45:29,829 - INFO - [SUB-IN] Kessler Edwards to DAL\n",
      "2025-09-05 18:45:29,830 - INFO - [SUB-OUT] Tari Eason from HOU\n",
      "2025-09-05 18:45:29,830 - INFO - [SUB-IN] Steven Adams to HOU\n",
      "2025-09-05 18:45:29,835 - WARNING - CORRECTED: Minutes validation: 4 players exceed 120s tolerance\n",
      "2025-09-05 18:45:29,835 - WARNING -   Dante Exum (DAL): calc=1503.0s vs box=1261.0s (diff=242.0s)\n",
      "2025-09-05 18:45:29,835 - WARNING -   Kyrie Irving (DAL): calc=2273.6s vs box=2496.0s (diff=222.4s)\n",
      "2025-09-05 18:45:29,836 - WARNING -   Naji Marshall (DAL): calc=1668.0s vs box=1500.0s (diff=168.0s)\n",
      "2025-09-05 18:45:29,836 - WARNING -   Reed Sheppard (HOU): calc=307.0s vs box=433.0s (diff=126.0s)\n",
      "2025-09-05 18:45:29,836 - INFO - SUBSTITUTION SUMMARY: 46 subs, 15 first-actions, 16 auto-outs\n",
      "2025-09-05 18:45:29,837 - INFO - [PASS] Enhanced Lineups & Rim Analytics: Enhanced engine: 506 events, 46 subs, 15 first-actions. Validation: 4/21 offenders; 5-on-floor fixes: 1\n",
      "2025-09-05 18:45:29,837 - WARNING - [WARN] Enhanced Lineups & Rim Analytics: 4 players exceed 120s tolerance\n",
      "2025-09-05 18:45:29,851 - INFO - [UPDATED COMPARISON] Traditional Data-Driven: 16 offenders, Enhanced: 8 offenders\n",
      "2025-09-05 18:45:29,851 - INFO - [UPDATED COMPARISON] Improvement: 8 fewer offenders\n",
      "2025-09-05 18:45:29,852 - INFO - [UPDATED COMPARISON] Traditional flags: 873, Enhanced flags: 114\n",
      "2025-09-05 18:45:29,852 - INFO - [UPDATED COMPARISON] Traditional lineup size distribution: {'4': 509, '5': 377, '3': 88, '6': 42}\n",
      "2025-09-05 18:45:29,853 - INFO - [PASS] Compare Traditional vs Enhanced (Updated): Updated comparison complete: Traditional Data-Driven (16 offenders) vs Enhanced (8 offenders). Improvement: 8 fewer offenders. Traditional flagged 873 issues, Enhanced flagged 114 issues.\n",
      "2025-09-05 18:45:29,872 - INFO - [PASS] Compare Minutes: minutes_compare built (21 rows). Basic within 10%: 3/21.\n",
      "2025-09-05 18:45:29,883 - INFO - [FAIL] Dataset Compliance Validation: Dataset compliance check: 2 issues found\n",
      "2025-09-05 18:45:29,884 - WARNING - [WARN] Dataset Compliance Validation: Traditional method: 30.0% compliance with 5-man requirement (FAILS)\n",
      "2025-09-05 18:45:29,884 - WARNING - [WARN] Dataset Compliance Validation: Minutes validation: 4 players exceed 120s tolerance\n",
      "2025-09-05 18:45:29,884 - INFO - Creating project submission artifacts using enhanced method...\n",
      "2025-09-05 18:45:29,891 - INFO - [PASS] Create Submission Artifacts: Created final submission artifacts: 42 lineups, 19 players\n",
      "2025-09-05 18:45:29,945 - INFO - ==============================================================================\n",
      "2025-09-05 18:45:29,946 - INFO - UNIQUE LINEUPS — TRADITIONAL (5-man)\n",
      "2025-09-05 18:45:29,946 - INFO - ==============================================================================\n",
      "2025-09-05 18:45:29,947 - INFO - DAL: 3 unique lineups\n",
      "2025-09-05 18:45:29,947 - INFO -    1. [162] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, P.J. Washington, Daniel Gafford\n",
      "2025-09-05 18:45:29,947 - INFO -    2. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Spencer Dinwiddie, P.J. Washington\n",
      "2025-09-05 18:45:29,949 - INFO -    3. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Dante Exum, P.J. Washington\n",
      "2025-09-05 18:45:29,949 - INFO - HOU: 4 unique lineups\n",
      "2025-09-05 18:45:29,950 - INFO -    1. [135] size=5  Dillon Brooks, Jalen Green, Alperen Sengun, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,951 - INFO -    2. [66] size=5  Steven Adams, Dillon Brooks, Jalen Green, Alperen Sengun, Amen Thompson\n",
      "2025-09-05 18:45:29,951 - INFO -    3. [11] size=5  Steven Adams, Dillon Brooks, Aaron Holiday, Alperen Sengun, Amen Thompson\n",
      "2025-09-05 18:45:29,951 - INFO -    4. [1] size=5  Steven Adams, Jalen Green, Jae'Sean Tate, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,952 - INFO - ------------------------------------------------------------------------------\n",
      "2025-09-05 18:45:29,952 - INFO - ==============================================================================\n",
      "2025-09-05 18:45:29,952 - INFO - UNIQUE LINEUPS — TRADITIONAL (ALL sizes)\n",
      "2025-09-05 18:45:29,953 - INFO - ==============================================================================\n",
      "2025-09-05 18:45:29,953 - INFO - DAL: 11 unique lineups (sizes: [4, 5, 6])\n",
      "2025-09-05 18:45:29,954 - INFO -    1. [115] size=4  Klay Thompson, Anthony Davis, Spencer Dinwiddie, P.J. Washington\n",
      "2025-09-05 18:45:29,954 - INFO -    2. [93] size=4  Klay Thompson, Anthony Davis, Dante Exum, P.J. Washington\n",
      "2025-09-05 18:45:29,955 - INFO -    3. [55] size=4  Klay Thompson, P.J. Washington, Daniel Gafford, Max Christie\n",
      "2025-09-05 18:45:29,955 - INFO -    4. [23] size=4  Klay Thompson, Spencer Dinwiddie, P.J. Washington, Daniel Gafford\n",
      "2025-09-05 18:45:29,955 - INFO -    5. [15] size=4  Klay Thompson, Spencer Dinwiddie, Daniel Gafford, Max Christie\n",
      "2025-09-05 18:45:29,956 - INFO -    6. [1] size=4  Kyrie Irving, Klay Thompson, Anthony Davis, P.J. Washington\n",
      "2025-09-05 18:45:29,956 - INFO -    7. [162] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, P.J. Washington, Daniel Gafford\n",
      "2025-09-05 18:45:29,956 - INFO -    8. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Spencer Dinwiddie, P.J. Washington\n",
      "2025-09-05 18:45:29,956 - INFO -    9. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Dante Exum, P.J. Washington\n",
      "2025-09-05 18:45:29,956 - INFO -   10. [30] size=6  Kyrie Irving, Klay Thompson, Anthony Davis, P.J. Washington, Daniel Gafford, Max Christie\n",
      "2025-09-05 18:45:29,958 - INFO -   11. [12] size=6  Kyrie Irving, Klay Thompson, Anthony Davis, Spencer Dinwiddie, P.J. Washington, Daniel Gafford\n",
      "2025-09-05 18:45:29,958 - INFO - HOU: 15 unique lineups (sizes: [3, 4, 5])\n",
      "2025-09-05 18:45:29,959 - INFO -    1. [40] size=3  Steven Adams, Aaron Holiday, Cam Whitmore\n",
      "2025-09-05 18:45:29,959 - INFO -    2. [24] size=3  Aaron Holiday, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,960 - INFO -    3. [23] size=3  Aaron Holiday, Amen Thompson, Cam Whitmore\n",
      "2025-09-05 18:45:29,960 - INFO -    4. [1] size=3  Aaron Holiday, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,961 - INFO -    5. [78] size=4  Jalen Green, Alperen Sengun, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,961 - INFO -    6. [46] size=4  Steven Adams, Jae'Sean Tate, Cam Whitmore, Reed Sheppard\n",
      "2025-09-05 18:45:29,961 - INFO -    7. [30] size=4  Steven Adams, Jae'Sean Tate, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,962 - INFO -    8. [26] size=4  Steven Adams, Aaron Holiday, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,963 - INFO -    9. [25] size=4  Steven Adams, Jalen Green, Jae'Sean Tate, Tari Eason\n",
      "2025-09-05 18:45:29,963 - INFO -   10. [1] size=4  Jalen Green, Jae'Sean Tate, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,964 - INFO -   11. [1] size=4  Steven Adams, Dillon Brooks, Aaron Holiday, Amen Thompson\n",
      "2025-09-05 18:45:29,964 - INFO -   12. [135] size=5  Dillon Brooks, Jalen Green, Alperen Sengun, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,965 - INFO -   13. [66] size=5  Steven Adams, Dillon Brooks, Jalen Green, Alperen Sengun, Amen Thompson\n",
      "2025-09-05 18:45:29,965 - INFO -   14. [11] size=5  Steven Adams, Dillon Brooks, Aaron Holiday, Alperen Sengun, Amen Thompson\n",
      "2025-09-05 18:45:29,965 - INFO -   15. [1] size=5  Steven Adams, Jalen Green, Jae'Sean Tate, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,966 - INFO - ------------------------------------------------------------------------------\n",
      "2025-09-05 18:45:29,966 - INFO - ==============================================================================\n",
      "2025-09-05 18:45:29,966 - INFO - UNIQUE LINEUPS — ENHANCED (5-man)\n",
      "2025-09-05 18:45:29,966 - INFO - ==============================================================================\n",
      "2025-09-05 18:45:29,967 - INFO - DAL: 24 unique lineups\n",
      "2025-09-05 18:45:29,967 - INFO -    1. [10] size=5  Kyrie Irving, Klay Thompson, Dante Exum, Daniel Gafford, Max Christie\n",
      "2025-09-05 18:45:29,968 - INFO -    2. [8] size=5  Kyrie Irving, Spencer Dinwiddie, Daniel Gafford, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,968 - INFO -    3. [5] size=5  Kyrie Irving, Anthony Davis, Spencer Dinwiddie, Dante Exum, Naji Marshall\n",
      "2025-09-05 18:45:29,969 - INFO -    4. [5] size=5  Kyrie Irving, Dante Exum, Daniel Gafford, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,969 - INFO -    5. [5] size=5  Anthony Davis, Spencer Dinwiddie, Dante Exum, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,970 - INFO -    6. [3] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Spencer Dinwiddie, P.J. Washington\n",
      "2025-09-05 18:45:29,970 - INFO -    7. [2] size=5  Kyrie Irving, Klay Thompson, Daniel Gafford, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,971 - INFO -    8. [2] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Dante Exum, Max Christie\n",
      "2025-09-05 18:45:29,972 - INFO -    9. [2] size=5  Kyrie Irving, Klay Thompson, Spencer Dinwiddie, Daniel Gafford, Max Christie\n",
      "2025-09-05 18:45:29,972 - INFO -   10. [2] size=5  Kyrie Irving, Anthony Davis, Dante Exum, P.J. Washington, Max Christie\n",
      "2025-09-05 18:45:29,973 - INFO -   11. [2] size=5  Kyrie Irving, Anthony Davis, Dante Exum, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,974 - INFO -   12. [2] size=5  Klay Thompson, Anthony Davis, P.J. Washington, Daniel Gafford, Max Christie\n",
      "2025-09-05 18:45:29,974 - INFO -   13. [2] size=5  Dante Exum, Naji Marshall, Kessler Edwards, Max Christie, Olivier-Maxence Prosper\n",
      "2025-09-05 18:45:29,975 - INFO -   14. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, P.J. Washington, Daniel Gafford\n",
      "2025-09-05 18:45:29,975 - INFO -   15. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Spencer Dinwiddie, Naji Marshall\n",
      "2025-09-05 18:45:29,976 - INFO -   16. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Spencer Dinwiddie, Dante Exum\n",
      "2025-09-05 18:45:29,976 - INFO -   17. [1] size=5  Kyrie Irving, Klay Thompson, Anthony Davis, Dante Exum, P.J. Washington\n",
      "2025-09-05 18:45:29,977 - INFO -   18. [1] size=5  Kyrie Irving, Anthony Davis, Dante Exum, P.J. Washington, Naji Marshall\n",
      "2025-09-05 18:45:29,977 - INFO -   19. [1] size=5  Klay Thompson, Anthony Davis, P.J. Washington, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,977 - INFO -   20. [1] size=5  Anthony Davis, Spencer Dinwiddie, Daniel Gafford, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,978 - INFO -   21. [1] size=5  Anthony Davis, Spencer Dinwiddie, Dante Exum, P.J. Washington, Max Christie\n",
      "2025-09-05 18:45:29,978 - INFO -   22. [1] size=5  Anthony Davis, Dante Exum, P.J. Washington, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,979 - INFO -   23. [1] size=5  Spencer Dinwiddie, Dante Exum, Daniel Gafford, Naji Marshall, Max Christie\n",
      "2025-09-05 18:45:29,979 - INFO -   24. [1] size=5  Dante Exum, Daniel Gafford, Naji Marshall, Max Christie, Olivier-Maxence Prosper\n",
      "2025-09-05 18:45:29,980 - INFO - HOU: 23 unique lineups\n",
      "2025-09-05 18:45:29,981 - INFO -    1. [11] size=5  Dillon Brooks, Jalen Green, Alperen Sengun, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,982 - INFO -    2. [8] size=5  Steven Adams, Dillon Brooks, Jalen Green, Alperen Sengun, Amen Thompson\n",
      "2025-09-05 18:45:29,982 - INFO -    3. [4] size=5  Dillon Brooks, Jalen Green, Alperen Sengun, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,983 - INFO -    4. [4] size=5  Dillon Brooks, Jalen Green, Alperen Sengun, Amen Thompson, Cam Whitmore\n",
      "2025-09-05 18:45:29,983 - INFO -    5. [4] size=5  Jalen Green, Alperen Sengun, Tari Eason, Amen Thompson, Cam Whitmore\n",
      "2025-09-05 18:45:29,984 - INFO -    6. [4] size=5  Steven Adams, Dillon Brooks, Jalen Green, Jae'Sean Tate, Cam Whitmore\n",
      "2025-09-05 18:45:29,984 - INFO -    7. [3] size=5  Dillon Brooks, Jalen Green, Alperen Sengun, Amen Thompson, Reed Sheppard\n",
      "2025-09-05 18:45:29,985 - INFO -    8. [3] size=5  Alperen Sengun, Tari Eason, Amen Thompson, Cam Whitmore, Reed Sheppard\n",
      "2025-09-05 18:45:29,986 - INFO -    9. [2] size=5  Steven Adams, Aaron Holiday, Alperen Sengun, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,986 - INFO -   10. [2] size=5  Steven Adams, Aaron Holiday, Tari Eason, Amen Thompson, Cam Whitmore\n",
      "2025-09-05 18:45:29,986 - INFO -   11. [2] size=5  Steven Adams, Jalen Green, Alperen Sengun, Tari Eason, Amen Thompson\n",
      "2025-09-05 18:45:29,987 - INFO -   12. [2] size=5  Steven Adams, Jalen Green, Tari Eason, Amen Thompson, Cam Whitmore\n",
      "2025-09-05 18:45:29,987 - INFO -   13. [1] size=5  Dillon Brooks, Jae'Sean Tate, Tari Eason, Cam Whitmore, Reed Sheppard\n",
      "2025-09-05 18:45:29,988 - INFO -   14. [1] size=5  Dillon Brooks, Alperen Sengun, Tari Eason, Amen Thompson, Cam Whitmore\n",
      "2025-09-05 18:45:29,988 - INFO -   15. [1] size=5  Dillon Brooks, Alperen Sengun, Tari Eason, Amen Thompson, Reed Sheppard\n",
      "2025-09-05 18:45:29,989 - INFO -   16. [1] size=5  Dillon Brooks, Alperen Sengun, Tari Eason, Cam Whitmore, Reed Sheppard\n",
      "2025-09-05 18:45:29,989 - INFO -   17. [1] size=5  Aaron Holiday, Jalen Green, Alperen Sengun, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,989 - INFO -   18. [1] size=5  Steven Adams, Dillon Brooks, Jalen Green, Alperen Sengun, Tari Eason\n",
      "2025-09-05 18:45:29,990 - INFO -   19. [1] size=5  Steven Adams, Dillon Brooks, Jalen Green, Alperen Sengun, Cam Whitmore\n",
      "2025-09-05 18:45:29,990 - INFO -   20. [1] size=5  Steven Adams, Dillon Brooks, Jalen Green, Tari Eason, Cam Whitmore\n",
      "2025-09-05 18:45:29,991 - INFO -   21. [1] size=5  Steven Adams, Dillon Brooks, Jalen Green, Amen Thompson, Cam Whitmore\n",
      "2025-09-05 18:45:29,991 - INFO -   22. [1] size=5  Steven Adams, Dillon Brooks, Jae'Sean Tate, Tari Eason, Reed Sheppard\n",
      "2025-09-05 18:45:29,991 - INFO -   23. [1] size=5  Steven Adams, Dillon Brooks, Tari Eason, Cam Whitmore, Reed Sheppard\n",
      "2025-09-05 18:45:29,992 - INFO - ------------------------------------------------------------------------------\n",
      "2025-09-05 18:45:30,001 - INFO - [PASS] Write Final Report: Report written: minutes_validation_full.csv, minutes_offenders.csv, basic_lineup_state.csv, basic_lineup_flags.csv, minutes_basic.csv, enhanced_lineup_state.csv, enhanced_lineup_flags.csv, minutes_enhanced.csv, minutes_compare.csv, traditional_vs_enhanced_comparison.csv, comprehensive_flags_analysis.csv, unique_lineups_traditional_5.csv, unique_lineups_traditional_all.csv, unique_lineups_enhanced_5.csv, run_summary.json\n",
      "2025-09-05 18:45:30,118 - WARNING - Step 1 completed with validation warnings but core data loaded successfully\n",
      "2025-09-05 18:45:30,118 - INFO - ✅ Step 1 completed in 0.81s\n",
      "2025-09-05 18:45:30,118 - INFO - STEP 2: Extracting Entities...\n",
      "2025-09-05 18:45:30,132 - INFO - Step 3a: Extracting unique players...\n",
      "2025-09-05 18:45:30,133 - INFO - Extracting unique players from box score...\n",
      "2025-09-05 18:45:30,135 - INFO - [PASS] Extract Unique Players: Extracted 21 players across 2 teams. Team distribution: {'DAL': 11, 'HOU': 10}\n",
      "2025-09-05 18:45:30,136 - INFO - Step 3b: Extracting starters from box score...\n",
      "2025-09-05 18:45:30,136 - INFO - Extracting starting lineups...\n",
      "2025-09-05 18:45:30,138 - INFO - Team DAL starters: ['Kyrie Irving', 'Anthony Davis', 'Daniel Gafford', 'Klay Thompson', 'P.J. Washington']\n",
      "2025-09-05 18:45:30,138 - INFO - Team HOU starters: ['Jalen Green', 'Alperen Sengun', 'Amen Thompson', 'Dillon Brooks', 'Tari Eason']\n",
      "2025-09-05 18:45:30,139 - INFO - [PASS] Extract Starters: Extracted starters for 2 teams, 10 total starters\n",
      "2025-09-05 18:45:30,139 - INFO - Step 3c: Creating team mapping...\n",
      "2025-09-05 18:45:30,139 - INFO - Creating team mapping...\n",
      "2025-09-05 18:45:30,142 - INFO - [PASS] Extract Team Mapping: Created mapping for 2 teams: {1610612742: 'DAL', 1610612745: 'HOU'}. Home: DAL, Away: HOU\n",
      "2025-09-05 18:45:30,142 - INFO - Step 3d: Extracting game info...\n",
      "2025-09-05 18:45:30,143 - INFO - Extracting game information...\n",
      "2025-09-05 18:45:30,144 - INFO - [PASS] Extract Game Info: Game: HOU @ DAL, 21.0 players, 10.0 starters\n",
      "2025-09-05 18:45:30,144 - INFO - Step 3e: Creating canonical tables...\n",
      "2025-09-05 18:45:30,146 - INFO - Creating canonical entity tables...\n",
      "2025-09-05 18:45:30,166 - INFO - [PASS] Create Canonical Tables: Created canonical tables: players (view), starters (10), teams (2)\n",
      "2025-09-05 18:45:30,167 - INFO - Step 3f: Final validation...\n",
      "2025-09-05 18:45:30,167 - INFO - Performing final entity validation...\n",
      "2025-09-05 18:45:30,168 - INFO - [PASS] Entity Completeness: Entity validation: 0 errors, 0 warnings. Starter counts: {'DAL': 5, 'HOU': 5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENHANCED NBA PIPELINE - DATA LOADING SUMMARY\n",
      "================================================================================\n",
      "BOX SCORE:\n",
      "   Original rows: 35\n",
      "   Active players: 28\n",
      "   Final rows: 21\n",
      "   Teams: HOU, DAL\n",
      "   Starters per team: {'HOU': 5, 'DAL': 5}\n",
      "\n",
      "PLAY-BY-PLAY:\n",
      "   Original rows: 507\n",
      "   Game events: 506\n",
      "   Final rows: 506\n",
      "   Total shots: 183\n",
      "   Shots with coordinates: 183\n",
      "   Rim attempts: 52\n",
      "   Average distance: 12.1 ft\n",
      "\n",
      "LINEUP ENGINE:\n",
      "   Substitutions: 46\n",
      "   First-actions auto-IN: 15\n",
      "   Inactivity auto-OUTs: 16\n",
      "   5-on-floor fixes: 1\n",
      "   Minutes tolerance: ±120s\n",
      "   Minutes offenders: 4/21\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NBA PIPELINE VALIDATION SUMMARY\n",
      "================================================================================\n",
      "OVERALL STATUS: 14/15 tests passed\n",
      "TOTAL VALIDATION TIME: 0.66 seconds\n",
      "TOTAL WARNINGS: 7\n",
      "\n",
      "[PASS] Load Box Score\n",
      "   Details: Processed box score: 35 → 28 active → 21 final rows. Teams: ['HOU', 'DAL'], Starters: {'HOU': 5, 'DAL': 5}\n",
      "   Data Count: 21\n",
      "   Time: 0.022s\n",
      "   [WARN] Removed 7 players with no playing time\n",
      "\n",
      "[PASS] Load PBP\n",
      "   Details: Processed PBP: 507 → 506 game events → 506 final rows. Shots: 183, Rim attempts: 52\n",
      "   Data Count: 506\n",
      "   Time: 0.028s\n",
      "   [WARN] 1 shots beyond 35.0 feet (max: 41.0ft)\n",
      "   [WARN] Flagged 1 extreme-distance shots (> 35.0 ft)\n",
      "\n",
      "[PASS] Data Relationships\n",
      "   Details: Relationship validation: Box teams: 2, PBP teams: 2, Final PBP events: 506\n",
      "   Data Count: 0\n",
      "   Time: 0.008s\n",
      "   [WARN] 3 players in PBP not in box score: players = {1627963.0, 1628954.0, 101283.0}\n",
      "\n",
      "[PASS] Create Lookup Views\n",
      "   Details: Created/Replaced 3 lookup tables: pbp_event_msg_types, pbp_action_types, pbp_option_types\n",
      "   Data Count: 0\n",
      "   Time: 0.018s\n",
      "\n",
      "[PASS] Create Dimensions\n",
      "   Details: dim_players: 21 rows; pbp_only_players: 0 rows; dim_officials: 3 rows\n",
      "   Data Count: 0\n",
      "   Time: 0.056s\n",
      "\n",
      "[PASS] Create PBP Enriched View\n",
      "   Details: Created view pbp_enriched with 506 rows (matches pbp)\n",
      "   Data Count: 0\n",
      "   Time: 0.029s\n",
      "\n",
      "[PASS] Traditional Data-Driven Lineups\n",
      "   Details: Traditional data-driven tracking: 1016 states, 873 flags, 37.1% correct lineup size\n",
      "   Data Count: 0\n",
      "   Time: 0.100s\n",
      "\n",
      "[PASS] Enhanced Substitution Tracking with Flags\n",
      "   Details: Enhanced tracking complete: 46 subs, 15 first-actions, 114 total flags\n",
      "   Data Count: 0\n",
      "   Time: 0.103s\n",
      "\n",
      "[PASS] Missing Player Report\n",
      "   Details: Built missing_player_report with 0 rows\n",
      "   Data Count: 0\n",
      "   Time: 0.042s\n",
      "\n",
      "[PASS] Enhanced Lineups & Rim Analytics\n",
      "   Details: Enhanced engine: 506 events, 46 subs, 15 first-actions. Validation: 4/21 offenders; 5-on-floor fixes: 1\n",
      "   Data Count: 0\n",
      "   Time: 0.096s\n",
      "   [WARN] 4 players exceed 120s tolerance\n",
      "\n",
      "[PASS] Compare Traditional vs Enhanced (Updated)\n",
      "   Details: Updated comparison complete: Traditional Data-Driven (16 offenders) vs Enhanced (8 offenders). Improvement: 8 fewer offenders. Traditional flagged 873 issues, Enhanced flagged 114 issues.\n",
      "   Data Count: 0\n",
      "   Time: 0.017s\n",
      "\n",
      "[PASS] Compare Minutes\n",
      "   Details: minutes_compare built (21 rows). Basic within 10%: 3/21.\n",
      "   Data Count: 0\n",
      "   Time: 0.018s\n",
      "\n",
      "[FAIL] Dataset Compliance Validation\n",
      "   Details: Dataset compliance check: 2 issues found\n",
      "   Data Count: 0\n",
      "   Time: 0.010s\n",
      "   [WARN] Traditional method: 30.0% compliance with 5-man requirement (FAILS)\n",
      "   [WARN] Minutes validation: 4 players exceed 120s tolerance\n",
      "\n",
      "[PASS] Create Submission Artifacts\n",
      "   Details: Created final submission artifacts: 42 lineups, 19 players\n",
      "   Data Count: 61\n",
      "   Time: 0.008s\n",
      "\n",
      "[PASS] Write Final Report\n",
      "   Details: Report written: minutes_validation_full.csv, minutes_offenders.csv, basic_lineup_state.csv, basic_lineup_flags.csv, minutes_basic.csv, enhanced_lineup_state.csv, enhanced_lineup_flags.csv, minutes_enhanced.csv, minutes_compare.csv, traditional_vs_enhanced_comparison.csv, comprehensive_flags_analysis.csv, unique_lineups_traditional_5.csv, unique_lineups_traditional_all.csv, unique_lineups_enhanced_5.csv, run_summary.json\n",
      "   Data Count: 0\n",
      "   Time: 0.109s\n",
      "\n",
      "================================================================================\n",
      "🏀 NBA Pipeline - Robust Entity Extraction\n",
      "==================================================\n",
      "\n",
      "================================================================================\n",
      "ROBUST NBA ENTITY EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "🏀 GAME: HOU @ DAL\n",
      "   Teams: DAL, HOU\n",
      "   Players: 21\n",
      "   Starters: 10\n",
      "\n",
      "👥 PLAYERS BY TEAM:\n",
      "   DAL: 11 players (5 starters)\n",
      "   HOU: 10 players (5 starters)\n",
      "\n",
      "🏆 STARTING LINEUPS:\n",
      "   DAL:\n",
      "     1. Kyrie Irving (#11, 41:36, 13pts)\n",
      "     2. Anthony Davis (#3, 30:56, 26pts)\n",
      "     3. Daniel Gafford (#21, 25:48, 5pts)\n",
      "     4. Klay Thompson (#31, 21:48, 13pts)\n",
      "     5. P.J. Washington (#25, 16:12, 6pts)\n",
      "   HOU:\n",
      "     1. Jalen Green (#4, 41:11, 24pts)\n",
      "     2. Alperen Sengun (#28, 37:45, 30pts)\n",
      "     3. Amen Thompson (#1, 36:55, 20pts)\n",
      "     4. Dillon Brooks (#9, 36:43, 9pts)\n",
      "     5. Tari Eason (#17, 29:38, 8pts)\n",
      "\n",
      "🏟️  TEAM MAPPING:\n",
      "   1610612742 → DAL 🏠\n",
      "   1610612745 → HOU ✈️\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NBA PIPELINE VALIDATION SUMMARY\n",
      "================================================================================\n",
      "OVERALL STATUS: 6/6 tests passed\n",
      "TOTAL VALIDATION TIME: 0.03 seconds\n",
      "TOTAL WARNINGS: 0\n",
      "\n",
      "[PASS] Extract Unique Players\n",
      "   Details: Extracted 21 players across 2 teams. Team distribution: {'DAL': 11, 'HOU': 10}\n",
      "   Data Count: 21\n",
      "   Time: 0.003s\n",
      "\n",
      "[PASS] Extract Starters\n",
      "   Details: Extracted starters for 2 teams, 10 total starters\n",
      "   Data Count: 10\n",
      "   Time: 0.004s\n",
      "\n",
      "[PASS] Extract Team Mapping\n",
      "   Details: Created mapping for 2 teams: {1610612742: 'DAL', 1610612745: 'HOU'}. Home: DAL, Away: HOU\n",
      "   Data Count: 2\n",
      "   Time: 0.003s\n",
      "\n",
      "[PASS] Extract Game Info\n",
      "   Details: Game: HOU @ DAL, 21.0 players, 10.0 starters\n",
      "   Data Count: 1\n",
      "   Time: 0.001s\n",
      "\n",
      "[PASS] Create Canonical Tables\n",
      "   Details: Created canonical tables: players (view), starters (10), teams (2)\n",
      "   Data Count: 12\n",
      "   Time: 0.021s\n",
      "\n",
      "[PASS] Entity Completeness\n",
      "   Details: Entity validation: 0 errors, 0 warnings. Starter counts: {'DAL': 5, 'HOU': 5}\n",
      "   Data Count: 0\n",
      "   Time: 0.001s\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 18:45:30,203 - INFO - ✅ Step 2 completed in 0.08s\n",
      "2025-09-05 18:45:30,204 - INFO - STEP 3: Processing PBP Data...\n",
      "2025-09-05 18:45:30,227 - INFO - Step 4a: Initializing lineups for both tracking methods...\n",
      "2025-09-05 18:45:30,227 - INFO - Initializing lineups for both tracking methods...\n",
      "2025-09-05 18:45:30,228 - INFO - Initialized DAL starters: [202681, 202691, 203076, 1629023, 1629655]\n",
      "2025-09-05 18:45:30,228 - INFO - Initialized HOU starters: [1628415, 1630224, 1630578, 1631106, 1641708]\n",
      "2025-09-05 18:45:30,229 - INFO - [PASS] Initialize Lineups: Initialized lineups for both tracking methods: 2 teams\n",
      "2025-09-05 18:45:30,229 - INFO - Step 4b: Loading PBP events with Step 2 classification...\n",
      "2025-09-05 18:45:30,229 - INFO - Loading PBP events with Step 2 classification...\n",
      "2025-09-05 18:45:30,258 - INFO - [PASS] Load PBP Events: Loaded 506 events with Step 2 classification\n",
      "2025-09-05 18:45:30,258 - INFO - Step 4c: Processing events with both Traditional and Enhanced methods...\n",
      "2025-09-05 18:45:30,258 - INFO - Processing 506 events with both methods...\n",
      "2025-09-05 18:45:30,259 - INFO - Processing period 1\n",
      "2025-09-05 18:45:30,259 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,260 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,261 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,261 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,262 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,262 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,263 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,264 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,264 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,265 - INFO - Substitution P1: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,266 - INFO - Processing period 2\n",
      "2025-09-05 18:45:30,266 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,266 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,267 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,267 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,267 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,267 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,267 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,268 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,268 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,268 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,268 - INFO - Substitution P2: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,270 - INFO - Processing period 3\n",
      "2025-09-05 18:45:30,270 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,271 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,271 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,271 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,272 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,272 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,273 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,273 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,274 - INFO - Substitution P3: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,274 - INFO - Processing period 4\n",
      "2025-09-05 18:45:30,274 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,275 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,275 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,275 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,275 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,275 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,277 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,277 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,277 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,277 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,278 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,278 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,278 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,278 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,278 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,279 - INFO - Substitution P4: Traditional=True, Enhanced=True\n",
      "2025-09-05 18:45:30,280 - INFO - [PASS] Process All Events: Processed 506 events. Traditional: 46 subs, 68 flags, 31 size deviations. Enhanced: 46 subs, 91 first-actions, 98 auto-outs.\n",
      "2025-09-05 18:45:30,280 - INFO - Step 4d: Creating Step 4 output tables...\n",
      "2025-09-05 18:45:30,281 - INFO - Creating Step 4 output tables with both tracking methods...\n",
      "2025-09-05 18:45:30,306 - INFO - [VALIDATE] Step 4 schema columns: ['pbp_id', 'period', 'pbp_order', 'wall_clock_int', 'description', 'msg_type', 'action_type', 'off_team_id', 'def_team_id', 'player_id_1', 'player_id_2', 'player_id_3', 'is_shot', 'is_rim_attempt', 'is_rim_make', 'distance_ft', 'is_substitution', 'points', 'traditional_off_lineup', 'traditional_def_lineup', 'enhanced_off_lineup', 'enhanced_def_lineup']\n",
      "2025-09-05 18:45:30,310 - INFO - [CONTRACT] Step 4 stamped version 'dual_lineups_v1' for table 'step4_processed_events' (506 rows)\n",
      "2025-09-05 18:45:30,310 - INFO - [PASS] Create Step 4 Output Tables: Created Step 4 output tables: processed_events (506 rows), traditional_flags (68 rows), enhanced_flags (236 rows), method_comparison\n",
      "2025-09-05 18:45:30,355 - INFO - ✅ Step 3 completed in 0.15s\n",
      "2025-09-05 18:45:30,355 - INFO - STEP 4: Running Dual-Method Possession Engine...\n",
      "2025-09-05 18:45:30,377 - INFO - Step 5a: Running pipeline diagnostic...\n",
      "2025-09-05 18:45:30,377 - INFO - === PIPELINE DIAGNOSTIC ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA Pipeline - Step 4: Integrated PBP Processing (Updated with Step 2)\n",
      "===========================================================================\n",
      "output tables results=============== ValidationResult(step_name='Create Step 4 Output Tables', passed=True, details='Created Step 4 output tables: processed_events (506 rows), traditional_flags (68 rows), enhanced_flags (236 rows), method_comparison', data_count=506, processing_time=0.029732942581176758, warnings=[])\n",
      "\n",
      "================================================================================\n",
      "NBA PIPELINE - STEP 4 SUMMARY (INTEGRATED WITH STEP 2)\n",
      "================================================================================\n",
      "TRADITIONAL DATA-DRIVEN METHOD:\n",
      "  Substitutions Processed: 46\n",
      "  Flags Generated: 68\n",
      "  Lineup Size Deviations: 31\n",
      "  Current Lineup Sizes:\n",
      "    DAL: 6 players\n",
      "    HOU: 5 players\n",
      "\n",
      "ENHANCED ESTIMATION METHOD:\n",
      "  Substitutions Processed: 46\n",
      "  First-Action Injections: 91\n",
      "  Auto-Out Corrections: 98\n",
      "  Flags Generated: 236\n",
      "  Current Lineup Sizes:\n",
      "    DAL: 5 players\n",
      "    HOU: 5 players\n",
      "\n",
      "TOTAL EVENTS PROCESSED: 506\n",
      "LINEUP SIZE ACCURACY:\n",
      "  Traditional: 1/2 teams have 5-man lineups (50.0%)\n",
      "  Enhanced: 2/2 teams have 5-man lineups (100.0%)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NBA PIPELINE VALIDATION SUMMARY\n",
      "================================================================================\n",
      "OVERALL STATUS: 4/4 tests passed\n",
      "TOTAL VALIDATION TIME: 0.08 seconds\n",
      "TOTAL WARNINGS: 0\n",
      "\n",
      "[PASS] Initialize Lineups\n",
      "   Details: Initialized lineups for both tracking methods: 2 teams\n",
      "   Data Count: 0\n",
      "   Time: 0.002s\n",
      "\n",
      "[PASS] Load PBP Events\n",
      "   Details: Loaded 506 events with Step 2 classification\n",
      "   Data Count: 506\n",
      "   Time: 0.029s\n",
      "\n",
      "[PASS] Process All Events\n",
      "   Details: Processed 506 events. Traditional: 46 subs, 68 flags, 31 size deviations. Enhanced: 46 subs, 91 first-actions, 98 auto-outs.\n",
      "   Data Count: 506\n",
      "   Time: 0.022s\n",
      "\n",
      "[PASS] Create Step 4 Output Tables\n",
      "   Details: Created Step 4 output tables: processed_events (506 rows), traditional_flags (68 rows), enhanced_flags (236 rows), method_comparison\n",
      "   Data Count: 506\n",
      "   Time: 0.030s\n",
      "\n",
      "================================================================================\n",
      "NBA Pipeline - UPDATED Step 5: Dual-Method Possession Engine\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 18:45:30,394 - INFO - Diagnostic results: {'all_tables': ['action_types', 'box_score', 'canonical_pbp', 'canonical_players', 'canonical_starters', 'canonical_teams', 'dim_officials', 'dim_players', 'dim_teams', 'enhanced_flags', 'enhanced_lineup_flags', 'enhanced_lineup_state', 'enhanced_violation_report', 'event_types', 'final_dual_lineups', 'final_dual_players', 'final_lineups', 'final_players', 'final_players_rim', 'method_comparison_summary', 'minutes_basic', 'minutes_compare', 'minutes_enhanced', 'minutes_offenders', 'minutes_traditional', 'minutes_validation_full', 'missing_player_report', 'option_types', 'pbp', 'pbp_action_types', 'pbp_enriched', 'pbp_event_msg_types', 'pbp_only_players', 'pbp_option_types', 'pipeline_contract', 'processed_events', 'project1_lineups', 'project2_players', 'step4_enhanced_flags', 'step4_method_comparison', 'step4_processed_events', 'step4_traditional_flags', 'team_summary', 'traditional_lineup_flags', 'traditional_lineup_state', 'traditional_violation_report', 'traditional_vs_enhanced_comparison_updated'], 'step_requirements': {'step4_processed_events': True, 'traditional_lineup_state': True, 'enhanced_lineup_state': True, 'traditional_lineup_flags': True, 'enhanced_lineup_flags': True}, 'alternative_tables': {'step4_traditional_flags': True, 'step4_enhanced_flags': True, 'processed_events': True, 'traditional_violation_report': True, 'enhanced_violation_report': True}, 'table_counts': {'step4_processed_events': 506, 'traditional_lineup_state': 1016, 'enhanced_lineup_state': 122, 'traditional_lineup_flags': 873, 'enhanced_lineup_flags': 67, 'step4_traditional_flags': 68, 'step4_enhanced_flags': 236, 'traditional_violation_report': 873, 'enhanced_violation_report': 67}, 'sample_data': {'step4_processed_events': [{'pbp_id': 4, 'period': 1, 'pbp_order': 2, 'wall_clock_int': 403050, 'description': 'Jump Ball Gafford vs. Eason: Tip to Washington', 'msg_type': 10, 'action_type': 0, 'off_team_id': 1610612742, 'def_team_id': 1610612745, 'player_id_1': 1629655.0, 'player_id_2': 1631106.0, 'player_id_3': 1629023.0, 'is_shot': False, 'is_rim_attempt': False, 'is_rim_make': False, 'distance_ft': nan, 'is_substitution': False, 'points': 0, 'traditional_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'traditional_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'enhanced_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 7, 'period': 1, 'pbp_order': 3, 'wall_clock_int': 403180, 'description': \"Gafford 1' Alley Oop Dunk (2 PTS) (Davis 1 AST)\", 'msg_type': 1, 'action_type': 52, 'off_team_id': 1610612742, 'def_team_id': 1610612745, 'player_id_1': 1629655.0, 'player_id_2': 203076.0, 'player_id_3': nan, 'is_shot': True, 'is_rim_attempt': True, 'is_rim_make': True, 'distance_ft': 1.2041594578792296, 'is_substitution': False, 'points': 2, 'traditional_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'traditional_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'enhanced_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 9, 'period': 1, 'pbp_order': 4, 'wall_clock_int': 403390, 'description': 'Thompson 3PT Jump Shot (3 PTS) (Eason 1 AST)', 'msg_type': 1, 'action_type': 1, 'off_team_id': 1610612745, 'def_team_id': 1610612742, 'player_id_1': 1641708.0, 'player_id_2': 1631106.0, 'player_id_3': nan, 'is_shot': True, 'is_rim_attempt': False, 'is_rim_make': False, 'distance_ft': 22.83703133071372, 'is_substitution': False, 'points': 3, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'traditional_def_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_def_lineup': '[202681, 202691, 203076, 1629023, 1629655]'}], 'traditional_lineup_state': [{'period': 1, 'pbp_order': 2, 'abs_time': 0.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 202691, 203076, 1629023, 1629655]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Daniel Gafford\", \"Klay Thompson\", \"Kyrie Irving\", \"P.J. Washington\"]', 'event_desc': 'Period 1 start - reset to starters', 'event_type': 'PERIOD_START'}, {'period': 1, 'pbp_order': 2, 'abs_time': 0.0, 'team_id': 1610612745, 'team_abbrev': 'HOU', 'lineup_size': 5, 'lineup_player_ids_json': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'lineup_player_names_json': '[\"Alperen Sengun\", \"Amen Thompson\", \"Dillon Brooks\", \"Jalen Green\", \"Tari Eason\"]', 'event_desc': 'Period 1 start - reset to starters', 'event_type': 'PERIOD_START'}, {'period': 1, 'pbp_order': 2, 'abs_time': 0.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 202691, 203076, 1629023, 1629655]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Daniel Gafford\", \"Klay Thompson\", \"Kyrie Irving\", \"P.J. Washington\"]', 'event_desc': 'Jump Ball Gafford vs. Eason: Tip to Washington', 'event_type': 'OTHER'}], 'enhanced_lineup_state': [{'period': 1, 'pbp_order': 43, 'abs_time': 332.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 202691, 203076, 203957, 1629023]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Dante Exum\", \"Klay Thompson\", \"Kyrie Irving\", \"P.J. Washington\"]', 'event_desc': 'SUB: Exum FOR Gafford'}, {'period': 1, 'pbp_order': 43, 'abs_time': 332.0, 'team_id': 1610612745, 'team_abbrev': 'HOU', 'lineup_size': 5, 'lineup_player_ids_json': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'lineup_player_names_json': '[\"Alperen Sengun\", \"Amen Thompson\", \"Dillon Brooks\", \"Jalen Green\", \"Tari Eason\"]', 'event_desc': 'SUB: Exum FOR Gafford'}, {'period': 1, 'pbp_order': 44, 'abs_time': 332.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 203076, 203957, 1629023, 1631108]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Dante Exum\", \"Kyrie Irving\", \"Max Christie\", \"P.J. Washington\"]', 'event_desc': 'SUB: Christie FOR Thompson'}]}, 'step4_schema': {'columns': ['pbp_id', 'period', 'pbp_order', 'wall_clock_int', 'description', 'msg_type', 'action_type', 'off_team_id', 'def_team_id', 'player_id_1', 'player_id_2', 'player_id_3', 'is_shot', 'is_rim_attempt', 'is_rim_make', 'distance_ft', 'is_substitution', 'points', 'traditional_off_lineup', 'traditional_def_lineup', 'enhanced_off_lineup', 'enhanced_def_lineup'], 'has_legacy_lineups': False, 'has_traditional_lineups': True, 'has_enhanced_lineups': True, 'has_points': True, 'has_rim_flags': True}, 'step4_null_audit': {'traditional_off_lineup': 506, 'traditional_def_lineup': 506, 'enhanced_off_lineup': 506, 'enhanced_def_lineup': 506, 'points': 506, 'is_rim_attempt': 506, 'is_rim_make': 506}, 'contract': {'component': 'step4', 'version': 'dual_lineups_v1', 'table_name': 'step4_processed_events', 'columns_json': '[\"pbp_id\", \"period\", \"pbp_order\", \"wall_clock_int\", \"description\", \"msg_type\", \"action_type\", \"off_team_id\", \"def_team_id\", \"player_id_1\", \"player_id_2\", \"player_id_3\", \"is_shot\", \"is_rim_attempt\", \"is_rim_make\", \"distance_ft\", \"is_substitution\", \"points\", \"traditional_off_lineup\", \"traditional_def_lineup\", \"enhanced_off_lineup\", \"enhanced_def_lineup\"]', 'row_count': 506, 'created_at': Timestamp('2025-09-05 18:45:30.308000')}}\n",
      "2025-09-05 18:45:30,394 - INFO - Pipeline diagnostic completed: {'all_tables': ['action_types', 'box_score', 'canonical_pbp', 'canonical_players', 'canonical_starters', 'canonical_teams', 'dim_officials', 'dim_players', 'dim_teams', 'enhanced_flags', 'enhanced_lineup_flags', 'enhanced_lineup_state', 'enhanced_violation_report', 'event_types', 'final_dual_lineups', 'final_dual_players', 'final_lineups', 'final_players', 'final_players_rim', 'method_comparison_summary', 'minutes_basic', 'minutes_compare', 'minutes_enhanced', 'minutes_offenders', 'minutes_traditional', 'minutes_validation_full', 'missing_player_report', 'option_types', 'pbp', 'pbp_action_types', 'pbp_enriched', 'pbp_event_msg_types', 'pbp_only_players', 'pbp_option_types', 'pipeline_contract', 'processed_events', 'project1_lineups', 'project2_players', 'step4_enhanced_flags', 'step4_method_comparison', 'step4_processed_events', 'step4_traditional_flags', 'team_summary', 'traditional_lineup_flags', 'traditional_lineup_state', 'traditional_violation_report', 'traditional_vs_enhanced_comparison_updated'], 'step_requirements': {'step4_processed_events': True, 'traditional_lineup_state': True, 'enhanced_lineup_state': True, 'traditional_lineup_flags': True, 'enhanced_lineup_flags': True}, 'alternative_tables': {'step4_traditional_flags': True, 'step4_enhanced_flags': True, 'processed_events': True, 'traditional_violation_report': True, 'enhanced_violation_report': True}, 'table_counts': {'step4_processed_events': 506, 'traditional_lineup_state': 1016, 'enhanced_lineup_state': 122, 'traditional_lineup_flags': 873, 'enhanced_lineup_flags': 67, 'step4_traditional_flags': 68, 'step4_enhanced_flags': 236, 'traditional_violation_report': 873, 'enhanced_violation_report': 67}, 'sample_data': {'step4_processed_events': [{'pbp_id': 4, 'period': 1, 'pbp_order': 2, 'wall_clock_int': 403050, 'description': 'Jump Ball Gafford vs. Eason: Tip to Washington', 'msg_type': 10, 'action_type': 0, 'off_team_id': 1610612742, 'def_team_id': 1610612745, 'player_id_1': 1629655.0, 'player_id_2': 1631106.0, 'player_id_3': 1629023.0, 'is_shot': False, 'is_rim_attempt': False, 'is_rim_make': False, 'distance_ft': nan, 'is_substitution': False, 'points': 0, 'traditional_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'traditional_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'enhanced_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 7, 'period': 1, 'pbp_order': 3, 'wall_clock_int': 403180, 'description': \"Gafford 1' Alley Oop Dunk (2 PTS) (Davis 1 AST)\", 'msg_type': 1, 'action_type': 52, 'off_team_id': 1610612742, 'def_team_id': 1610612745, 'player_id_1': 1629655.0, 'player_id_2': 203076.0, 'player_id_3': nan, 'is_shot': True, 'is_rim_attempt': True, 'is_rim_make': True, 'distance_ft': 1.2041594578792296, 'is_substitution': False, 'points': 2, 'traditional_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'traditional_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'enhanced_def_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 9, 'period': 1, 'pbp_order': 4, 'wall_clock_int': 403390, 'description': 'Thompson 3PT Jump Shot (3 PTS) (Eason 1 AST)', 'msg_type': 1, 'action_type': 1, 'off_team_id': 1610612745, 'def_team_id': 1610612742, 'player_id_1': 1641708.0, 'player_id_2': 1631106.0, 'player_id_3': nan, 'is_shot': True, 'is_rim_attempt': False, 'is_rim_make': False, 'distance_ft': 22.83703133071372, 'is_substitution': False, 'points': 3, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'traditional_def_lineup': '[202681, 202691, 203076, 1629023, 1629655]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_def_lineup': '[202681, 202691, 203076, 1629023, 1629655]'}], 'traditional_lineup_state': [{'period': 1, 'pbp_order': 2, 'abs_time': 0.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 202691, 203076, 1629023, 1629655]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Daniel Gafford\", \"Klay Thompson\", \"Kyrie Irving\", \"P.J. Washington\"]', 'event_desc': 'Period 1 start - reset to starters', 'event_type': 'PERIOD_START'}, {'period': 1, 'pbp_order': 2, 'abs_time': 0.0, 'team_id': 1610612745, 'team_abbrev': 'HOU', 'lineup_size': 5, 'lineup_player_ids_json': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'lineup_player_names_json': '[\"Alperen Sengun\", \"Amen Thompson\", \"Dillon Brooks\", \"Jalen Green\", \"Tari Eason\"]', 'event_desc': 'Period 1 start - reset to starters', 'event_type': 'PERIOD_START'}, {'period': 1, 'pbp_order': 2, 'abs_time': 0.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 202691, 203076, 1629023, 1629655]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Daniel Gafford\", \"Klay Thompson\", \"Kyrie Irving\", \"P.J. Washington\"]', 'event_desc': 'Jump Ball Gafford vs. Eason: Tip to Washington', 'event_type': 'OTHER'}], 'enhanced_lineup_state': [{'period': 1, 'pbp_order': 43, 'abs_time': 332.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 202691, 203076, 203957, 1629023]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Dante Exum\", \"Klay Thompson\", \"Kyrie Irving\", \"P.J. Washington\"]', 'event_desc': 'SUB: Exum FOR Gafford'}, {'period': 1, 'pbp_order': 43, 'abs_time': 332.0, 'team_id': 1610612745, 'team_abbrev': 'HOU', 'lineup_size': 5, 'lineup_player_ids_json': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'lineup_player_names_json': '[\"Alperen Sengun\", \"Amen Thompson\", \"Dillon Brooks\", \"Jalen Green\", \"Tari Eason\"]', 'event_desc': 'SUB: Exum FOR Gafford'}, {'period': 1, 'pbp_order': 44, 'abs_time': 332.0, 'team_id': 1610612742, 'team_abbrev': 'DAL', 'lineup_size': 5, 'lineup_player_ids_json': '[202681, 203076, 203957, 1629023, 1631108]', 'lineup_player_names_json': '[\"Anthony Davis\", \"Dante Exum\", \"Kyrie Irving\", \"Max Christie\", \"P.J. Washington\"]', 'event_desc': 'SUB: Christie FOR Thompson'}]}, 'step4_schema': {'columns': ['pbp_id', 'period', 'pbp_order', 'wall_clock_int', 'description', 'msg_type', 'action_type', 'off_team_id', 'def_team_id', 'player_id_1', 'player_id_2', 'player_id_3', 'is_shot', 'is_rim_attempt', 'is_rim_make', 'distance_ft', 'is_substitution', 'points', 'traditional_off_lineup', 'traditional_def_lineup', 'enhanced_off_lineup', 'enhanced_def_lineup'], 'has_legacy_lineups': False, 'has_traditional_lineups': True, 'has_enhanced_lineups': True, 'has_points': True, 'has_rim_flags': True}, 'step4_null_audit': {'traditional_off_lineup': 506, 'traditional_def_lineup': 506, 'enhanced_off_lineup': 506, 'enhanced_def_lineup': 506, 'points': 506, 'is_rim_attempt': 506, 'is_rim_make': 506}, 'contract': {'component': 'step4', 'version': 'dual_lineups_v1', 'table_name': 'step4_processed_events', 'columns_json': '[\"pbp_id\", \"period\", \"pbp_order\", \"wall_clock_int\", \"description\", \"msg_type\", \"action_type\", \"off_team_id\", \"def_team_id\", \"player_id_1\", \"player_id_2\", \"player_id_3\", \"is_shot\", \"is_rim_attempt\", \"is_rim_make\", \"distance_ft\", \"is_substitution\", \"points\", \"traditional_off_lineup\", \"traditional_def_lineup\", \"enhanced_off_lineup\", \"enhanced_def_lineup\"]', 'row_count': 506, 'created_at': Timestamp('2025-09-05 18:45:30.308000')}}\n",
      "2025-09-05 18:45:30,395 - INFO - Step 5b: Loading dual-method data...\n",
      "2025-09-05 18:45:30,395 - INFO - Loading dual-method data from Step 2/4 integration...\n",
      "2025-09-05 18:45:30,399 - INFO - DEBUG: Available tables in database: ['action_types', 'box_score', 'canonical_pbp', 'canonical_players', 'canonical_starters', 'canonical_teams', 'dim_officials', 'dim_players', 'dim_teams', 'enhanced_flags', 'enhanced_lineup_flags', 'enhanced_lineup_state', 'enhanced_violation_report', 'event_types', 'final_dual_lineups', 'final_dual_players', 'final_lineups', 'final_players', 'final_players_rim', 'method_comparison_summary', 'minutes_basic', 'minutes_compare', 'minutes_enhanced', 'minutes_offenders', 'minutes_traditional', 'minutes_validation_full', 'missing_player_report', 'option_types', 'pbp', 'pbp_action_types', 'pbp_enriched', 'pbp_event_msg_types', 'pbp_only_players', 'pbp_option_types', 'pipeline_contract', 'processed_events', 'project1_lineups', 'project2_players', 'step4_enhanced_flags', 'step4_method_comparison', 'step4_processed_events', 'step4_traditional_flags', 'team_summary', 'traditional_lineup_flags', 'traditional_lineup_state', 'traditional_violation_report', 'traditional_vs_enhanced_comparison_updated']\n",
      "2025-09-05 18:45:30,400 - INFO - DEBUG: Optional tables present: ['traditional_lineup_state', 'enhanced_lineup_state', 'traditional_lineup_flags', 'enhanced_lineup_flags', 'step4_traditional_flags', 'step4_enhanced_flags', 'traditional_violation_report', 'enhanced_violation_report']\n",
      "2025-09-05 18:45:30,400 - INFO - DEBUG: Optional tables missing (non-blocking): []\n",
      "2025-09-05 18:45:30,402 - INFO - DEBUG: Using primary flags table 'traditional_lineup_flags'\n",
      "2025-09-05 18:45:30,405 - INFO - DEBUG: Loaded 873 rows from 'traditional_lineup_flags' for flags/violations\n",
      "2025-09-05 18:45:30,409 - INFO - DEBUG: Using primary flags table 'enhanced_lineup_flags'\n",
      "2025-09-05 18:45:30,411 - INFO - DEBUG: Loaded 67 rows from 'enhanced_lineup_flags' for flags/violations\n",
      "2025-09-05 18:45:30,415 - INFO - [PASS] Load Dual Method Data: Loaded Step 5 inputs. Flags: traditional=873, enhanced=67. Optional sources missing (non-blocking): [].\n",
      "2025-09-05 18:45:30,416 - INFO - Step 5c: Identifying dual-method possessions...\n",
      "2025-09-05 18:45:30,416 - INFO - Identifying possessions with dual-method lineup contexts (FIXED MODE)...\n",
      "2025-09-05 18:45:30,417 - INFO - DEBUG: step4_processed_events has 506 rows\n",
      "2025-09-05 18:45:30,419 - INFO - DEBUG: Retrieved 506 events with valid team IDs\n",
      "2025-09-05 18:45:30,432 - INFO - === FIXED POSSESSION DEBUG ANALYSIS ===\n",
      "2025-09-05 18:45:30,432 - INFO - Total Possessions Created: 274\n",
      "2025-09-05 18:45:30,434 - INFO - Total Possession Points: 221\n",
      "2025-09-05 18:45:30,434 - INFO - Total Scoring Events Processed: 122\n",
      "2025-09-05 18:45:30,434 - INFO - Total Event Points: 221\n",
      "2025-09-05 18:45:30,435 - INFO - FIXED TEAM-BY-TEAM ANALYSIS:\n",
      "2025-09-05 18:45:30,435 - INFO -   DAL:\n",
      "2025-09-05 18:45:30,435 - INFO -     Possession Points: 115\n",
      "2025-09-05 18:45:30,436 - INFO -     Event Points: 115\n",
      "2025-09-05 18:45:30,436 - INFO -     Difference: +0\n",
      "2025-09-05 18:45:30,437 - INFO -   HOU:\n",
      "2025-09-05 18:45:30,437 - INFO -     Possession Points: 106\n",
      "2025-09-05 18:45:30,438 - INFO -     Event Points: 106\n",
      "2025-09-05 18:45:30,438 - INFO -     Difference: +0\n",
      "2025-09-05 18:45:30,438 - INFO - [PASS] Identify Dual Possessions: FIXED: Identified 274 dual-method possessions, 221 possession points vs 221 event points.\n",
      "2025-09-05 18:45:30,439 - INFO - Step 5d: Calculating dual-method lineup statistics...\n",
      "2025-09-05 18:45:30,439 - INFO - Calculating dual-method lineup statistics...\n",
      "2025-09-05 18:45:30,440 - INFO - [PASS] Calculate Dual Lineup Stats: Calculated lineup stats: 20 traditional, 42 enhanced lineups\n",
      "2025-09-05 18:45:30,441 - INFO - Step 5e: Calculating dual-method player rim statistics...\n",
      "2025-09-05 18:45:30,441 - INFO - Calculating dual-method player rim defense statistics...\n",
      "2025-09-05 18:45:30,444 - INFO - [PASS] Calculate Dual Player Rim Stats: Calculated player rim stats: 18 traditional, 19 enhanced players with rim data\n",
      "2025-09-05 18:45:30,444 - INFO - Step 5f: Running comprehensive points flow debugging...\n",
      "2025-09-05 18:45:30,445 - INFO - === COMPREHENSIVE POINTS FLOW DEBUG ===\n",
      "2025-09-05 18:45:30,448 - INFO - POINTS FLOW ANALYSIS:\n",
      "2025-09-05 18:45:30,449 - INFO -   DAL:\n",
      "2025-09-05 18:45:30,450 - INFO -     Raw PBP: 115\n",
      "2025-09-05 18:45:30,450 - INFO -     Step4 Processed: 115 (diff: +0)\n",
      "2025-09-05 18:45:30,451 - INFO -     Possessions: 115 (diff: +0)\n",
      "2025-09-05 18:45:30,451 - INFO -     Traditional Lineups: 115 (diff: +0)\n",
      "2025-09-05 18:45:30,451 - INFO -     Enhanced Lineups: 115 (diff: +0)\n",
      "2025-09-05 18:45:30,452 - INFO -   HOU:\n",
      "2025-09-05 18:45:30,452 - INFO -     Raw PBP: 106\n",
      "2025-09-05 18:45:30,452 - INFO -     Step4 Processed: 106 (diff: +0)\n",
      "2025-09-05 18:45:30,452 - INFO -     Possessions: 106 (diff: +0)\n",
      "2025-09-05 18:45:30,453 - INFO -     Traditional Lineups: 106 (diff: +0)\n",
      "2025-09-05 18:45:30,453 - INFO -     Enhanced Lineups: 106 (diff: +0)\n",
      "2025-09-05 18:45:30,453 - INFO - Points flow debug completed: {'raw_pbp_points': {'DAL': {'points': 115, 'events': 64}, 'HOU': {'points': 106, 'events': 58}}, 'step4_processed_points': {'DAL': {'points': 115, 'events': 64}, 'HOU': {'points': 106, 'events': 58}}, 'possession_points': {'DAL': {'points': 115, 'possessions': 133}, 'HOU': {'points': 106, 'possessions': 141}}, 'lineup_points': {'traditional': {'DAL': {'points': 115, 'lineups': 8}, 'HOU': {'points': 106, 'lineups': 12}}, 'enhanced': {'DAL': {'points': 115, 'lineups': 22}, 'HOU': {'points': 106, 'lineups': 20}}}, 'discrepancies': [{'team': 'DAL', 'raw_pbp_points': 115, 'step4_processed_points': 115, 'possession_points': 115, 'traditional_lineup_points': 115, 'enhanced_lineup_points': 115, 'raw_to_step4_diff': 0, 'step4_to_possession_diff': 0, 'possession_to_traditional_diff': 0, 'possession_to_enhanced_diff': 0}, {'team': 'HOU', 'raw_pbp_points': 106, 'step4_processed_points': 106, 'possession_points': 106, 'traditional_lineup_points': 106, 'enhanced_lineup_points': 106, 'raw_to_step4_diff': 0, 'step4_to_possession_diff': 0, 'possession_to_traditional_diff': 0, 'possession_to_enhanced_diff': 0}], 'detailed_scoring_events': [{'pbp_id': 9, 'period': 1, 'pbp_order': 4, 'description': 'Thompson 3PT Jump Shot (3 PTS) (Eason 1 AST)', 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 25, 'period': 1, 'pbp_order': 15, 'description': \"Sengun 18' Jump Shot (2 PTS) (Ja. Green 1 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 33, 'period': 1, 'pbp_order': 21, 'description': \"Ja. Green 2' Driving Layup (2 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 6, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 55, 'period': 1, 'pbp_order': 37, 'description': \"Ja. Green 25' 3PT Pullup Jump Shot (5 PTS)\", 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 79, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 68, 'period': 1, 'pbp_order': 46, 'description': 'Ja. Green Free Throw 2 of 2 (6 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 12, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 70, 'period': 1, 'pbp_order': 48, 'description': \"Whitmore 1' Driving Dunk (2 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 9, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 80, 'period': 1, 'pbp_order': 56, 'description': \"Thompson 1' Cutting Dunk Shot (5 PTS) (Sengun 1 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 108, 'traditional_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 109, 'period': 1, 'pbp_order': 76, 'description': \"Ja. Green 10' Fadeaway Jumper (8 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 63, 'traditional_off_lineup': '[1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1630224, 1630578, 1631106, 1641708, 1641715]'}, {'pbp_id': 123, 'period': 1, 'pbp_order': 87, 'description': \"Brooks 28' 3PT Jump Shot (3 PTS) (Tate 1 AST)\", 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[1630224, 1630256, 1630578, 1641708, 1641715]'}, {'pbp_id': 159, 'period': 2, 'pbp_order': 110, 'description': 'Whitmore Free Throw 1 of 3 (3 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 13, 'traditional_off_lineup': '[1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[203500, 1628415, 1630256, 1641708, 1641715]'}, {'pbp_id': 160, 'period': 2, 'pbp_order': 111, 'description': 'Whitmore Free Throw 2 of 3 (4 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 14, 'traditional_off_lineup': '[1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[203500, 1628415, 1630256, 1641708, 1641715]'}, {'pbp_id': 161, 'period': 2, 'pbp_order': 112, 'description': 'Whitmore Free Throw 3 of 3 (5 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 15, 'traditional_off_lineup': '[1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[203500, 1628415, 1630256, 1641708, 1641715]'}, {'pbp_id': 186, 'period': 2, 'pbp_order': 133, 'description': 'Irving Free Throw Technical (1 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 16, 'traditional_off_lineup': '[1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[203500, 1628415, 1631106, 1641715, 1642263]'}, {'pbp_id': 201, 'period': 2, 'pbp_order': 147, 'description': 'Eason Free Throw 1 of 2 (1 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 11, 'traditional_off_lineup': '[1630224, 1630578, 1631106, 1641708]', 'enhanced_off_lineup': '[203500, 1628415, 1631106, 1641715, 1642263]'}, {'pbp_id': 206, 'period': 2, 'pbp_order': 150, 'description': 'Eason Free Throw 2 of 2 (2 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 12, 'traditional_off_lineup': '[203500, 1630224, 1630256, 1631106]', 'enhanced_off_lineup': '[203500, 1630256, 1631106, 1641715, 1642263]'}, {'pbp_id': 214, 'period': 2, 'pbp_order': 157, 'description': 'Sheppard Free Throw Technical (1 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 16, 'traditional_off_lineup': '[203500, 1630224, 1630256, 1631106]', 'enhanced_off_lineup': '[203500, 1630256, 1631106, 1641715, 1642263]'}, {'pbp_id': 224, 'period': 2, 'pbp_order': 165, 'description': \"Thompson 21' Jump Shot (7 PTS) (Whitmore 1 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630224, 1630256, 1631106]', 'enhanced_off_lineup': '[203500, 1630256, 1631106, 1641715, 1642263]'}, {'pbp_id': 241, 'period': 2, 'pbp_order': 176, 'description': 'Sengun Free Throw 2 of 2 (3 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 12, 'traditional_off_lineup': '[203500, 1630256, 1631106, 1641715]', 'enhanced_off_lineup': '[203500, 1630256, 1631106, 1641715, 1642263]'}, {'pbp_id': 244, 'period': 2, 'pbp_order': 179, 'description': \"Sheppard 14' Running Pull-Up Jump Shot (3 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 103, 'traditional_off_lineup': '[203500, 1630256, 1631106, 1641715]', 'enhanced_off_lineup': '[203500, 1630256, 1631106, 1641715, 1642263]'}, {'pbp_id': 258, 'period': 2, 'pbp_order': 188, 'description': 'Brooks Free Throw 2 of 2 (4 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 12, 'traditional_off_lineup': '[203500, 1630256, 1631106, 1641715]', 'enhanced_off_lineup': '[203500, 1630256, 1631106, 1641715, 1642263]'}, {'pbp_id': 288, 'period': 2, 'pbp_order': 212, 'description': \"Eason 21' Pullup Jump Shot (4 PTS) (Sengun 2 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 79, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630256, 1631106, 1641715, 1642263]'}, {'pbp_id': 292, 'period': 2, 'pbp_order': 215, 'description': \"Ja. Green 2' Running Layup (10 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 41, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630578, 1631106, 1641715, 1642263]'}, {'pbp_id': 295, 'period': 2, 'pbp_order': 218, 'description': \"Ja. Green 26' 3PT Pullup Jump Shot (13 PTS)\", 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 79, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1642263]'}, {'pbp_id': 311, 'period': 2, 'pbp_order': 231, 'description': \"Ja. Green 25' 3PT Jump Shot (16 PTS) (Thompson 1 AST)\", 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1642263]'}, {'pbp_id': 324, 'period': 2, 'pbp_order': 240, 'description': \"Sengun 16' Jump Shot (5 PTS) (Thompson 2 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1642263]'}, {'pbp_id': 327, 'period': 2, 'pbp_order': 242, 'description': 'Thompson Alley Oop Dunk (9 PTS) (Ja. Green 2 AST)', 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 52, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1642263]'}, {'pbp_id': 331, 'period': 2, 'pbp_order': 244, 'description': \"Thompson 1' Alley Oop Dunk (11 PTS) (Sengun 3 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 52, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 348, 'period': 3, 'pbp_order': 252, 'description': \"Sengun 9' Hook Shot (7 PTS) (Adams 1 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 3, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 358, 'period': 3, 'pbp_order': 259, 'description': 'Thompson 3PT Jump Shot (14 PTS) (Brooks 1 AST)', 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 366, 'period': 3, 'pbp_order': 264, 'description': \"Sengun 15' Jump Shot (9 PTS) (Ja. Green 3 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 384, 'period': 3, 'pbp_order': 277, 'description': 'Brooks 3PT Jump Shot (7 PTS) (Ja. Green 4 AST)', 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1630224, 1630578, 1631106, 1641708]'}, {'pbp_id': 388, 'period': 3, 'pbp_order': 280, 'description': \"Sengun 5' Driving Hook Shot (11 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 57, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 396, 'period': 3, 'pbp_order': 286, 'description': 'Sengun Free Throw 1 of 2 (12 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 11, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 399, 'period': 3, 'pbp_order': 288, 'description': 'Sengun Free Throw 2 of 2 (13 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 12, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 402, 'period': 3, 'pbp_order': 291, 'description': \"Sengun 26' 3PT Jump Shot (16 PTS) (Brooks 2 AST)\", 'points': 3, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 406, 'period': 3, 'pbp_order': 294, 'description': \"Ja. Green 1' Running Reverse Layup (18 PTS) (Brooks 3 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 74, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 420, 'period': 3, 'pbp_order': 305, 'description': \"Sengun 5' Turnaround Hook Shot (18 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 58, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 423, 'period': 3, 'pbp_order': 307, 'description': \"Brooks 19' Step Back Jump Shot (9 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 80, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 440, 'period': 3, 'pbp_order': 320, 'description': \"Thompson 6' Turnaround Jump Shot (16 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 47, 'traditional_off_lineup': '[203500, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 458, 'period': 3, 'pbp_order': 335, 'description': \"Ja. Green 1' Driving Dunk (20 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 9, 'traditional_off_lineup': '[203500, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 464, 'period': 3, 'pbp_order': 340, 'description': \"Sengun 1' Cutting Dunk Shot (20 PTS) (Ja. Green 5 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 108, 'traditional_off_lineup': '[203500, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[203500, 1630224, 1630578, 1641708, 1641715]'}, {'pbp_id': 475, 'period': 3, 'pbp_order': 347, 'description': \"Eason 14' Jump Shot (6 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 1, 'traditional_off_lineup': '[203500, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[1630224, 1630578, 1631106, 1641708, 1641715]'}, {'pbp_id': 486, 'period': 3, 'pbp_order': 355, 'description': \"Eason 1' Alley Oop Layup (8 PTS) (Ja. Green 6 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 43, 'traditional_off_lineup': '[203500, 1628415, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[1630224, 1630578, 1631106, 1641708, 1641715]'}, {'pbp_id': 504, 'period': 4, 'pbp_order': 364, 'description': \"Holiday 4' Driving Finger Roll Layup (2 PTS) (Thompson 3 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 75, 'traditional_off_lineup': '[203500, 1628415, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[203500, 1630224, 1630578, 1631106, 1641715]'}, {'pbp_id': 535, 'period': 4, 'pbp_order': 384, 'description': \"Thompson 11' Pullup Jump Shot (18 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 79, 'traditional_off_lineup': '[203500, 1628415, 1628988, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[203500, 1628988, 1630578, 1631106, 1641715]'}, {'pbp_id': 539, 'period': 4, 'pbp_order': 387, 'description': 'Whitmore Free Throw 1 of 2 (6 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 11, 'traditional_off_lineup': '[203500, 1628415, 1628988, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[203500, 1628988, 1631106, 1641708, 1641715]'}, {'pbp_id': 542, 'period': 4, 'pbp_order': 389, 'description': 'Whitmore Free Throw 2 of 2 (7 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 12, 'traditional_off_lineup': '[203500, 1628415, 1628988, 1630256, 1641708, 1642263]', 'enhanced_off_lineup': '[203500, 1628988, 1631106, 1641708, 1641715]'}, {'pbp_id': 571, 'period': 4, 'pbp_order': 410, 'description': \"Ja. Green 6' Driving Floating Bank Jump Shot (22 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 102, 'traditional_off_lineup': '[203500, 1628988, 1630256, 1631106, 1641708, 1642263]', 'enhanced_off_lineup': '[203500, 1628988, 1630578, 1641708, 1641715]'}, {'pbp_id': 581, 'period': 4, 'pbp_order': 417, 'description': 'Thompson Tip Layup Shot (20 PTS)', 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 97, 'traditional_off_lineup': '[203500, 1628988, 1630256, 1631106, 1641708, 1642263]', 'enhanced_off_lineup': '[1628988, 1630224, 1630578, 1641708, 1641715]'}, {'pbp_id': 597, 'period': 4, 'pbp_order': 427, 'description': \"Sengun 5' Turnaround Hook Shot (22 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 58, 'traditional_off_lineup': '[1628988, 1630256, 1641708, 1641715, 1642263]', 'enhanced_off_lineup': '[1628988, 1630224, 1630578, 1641708, 1641715]'}, {'pbp_id': 604, 'period': 4, 'pbp_order': 432, 'description': 'Sengun Free Throw 1 of 2 (23 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 11, 'traditional_off_lineup': '[1628988, 1630256, 1641708, 1641715, 1642263]', 'enhanced_off_lineup': '[1628988, 1630224, 1630578, 1641708, 1641715]'}, {'pbp_id': 605, 'period': 4, 'pbp_order': 433, 'description': 'Sengun Free Throw 2 of 2 (24 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 12, 'traditional_off_lineup': '[1628988, 1630256, 1641708, 1641715, 1642263]', 'enhanced_off_lineup': '[1628988, 1630224, 1630578, 1641708, 1641715]'}, {'pbp_id': 613, 'period': 4, 'pbp_order': 439, 'description': 'Adams Tip Dunk Shot (2 PTS)', 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 107, 'traditional_off_lineup': '[1628988, 1630256, 1641708, 1641715, 1642263]', 'enhanced_off_lineup': '[1628988, 1630224, 1630578, 1641708, 1641715]'}, {'pbp_id': 625, 'period': 4, 'pbp_order': 447, 'description': 'Sengun Free Throw 1 of 2 (25 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 11, 'traditional_off_lineup': '[1628988, 1630256, 1631106, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1630224, 1630578, 1631106, 1641715]'}, {'pbp_id': 632, 'period': 4, 'pbp_order': 453, 'description': \"Ja. Green 14' Step Back Jump Shot (24 PTS)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 80, 'traditional_off_lineup': '[203500, 1628988, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1630224, 1630578, 1631106, 1641715]'}, {'pbp_id': 672, 'period': 4, 'pbp_order': 482, 'description': 'Sengun Free Throw 1 of 2 (26 PTS)', 'points': 1, 'off_team_id': 1610612745, 'msg_type': 3, 'action_type': 11, 'traditional_off_lineup': '[203500, 1628988, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 678, 'period': 4, 'pbp_order': 487, 'description': \"Sengun 1' Driving Floating Jump Shot (28 PTS) (Thompson 4 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 101, 'traditional_off_lineup': '[203500, 1628988, 1630256, 1641715, 1642263]', 'enhanced_off_lineup': '[203500, 1628415, 1630224, 1630578, 1641708]'}, {'pbp_id': 697, 'period': 4, 'pbp_order': 500, 'description': \"Sengun 1' Cutting Dunk Shot (30 PTS) (Thompson 5 AST)\", 'points': 2, 'off_team_id': 1610612745, 'msg_type': 1, 'action_type': 108, 'traditional_off_lineup': '[1628988, 1630256, 1631106, 1641715, 1642263]', 'enhanced_off_lineup': '[1628415, 1630224, 1630578, 1631106, 1641708]'}]}\n",
      "2025-09-05 18:45:30,453 - INFO - Step 5g: Running detailed HOU scoring events analysis...\n",
      "2025-09-05 18:45:30,454 - INFO - === DETAILED HOU SCORING EVENTS ANALYSIS ===\n",
      "2025-09-05 18:45:30,460 - INFO - HOU Scoring Analysis:\n",
      "2025-09-05 18:45:30,460 - INFO -   Total Scoring Events: 58\n",
      "2025-09-05 18:45:30,461 - INFO -   Total Points from Events: 106\n",
      "2025-09-05 18:45:30,461 - INFO -   Total Points from Possessions: 106\n",
      "2025-09-05 18:45:30,461 - INFO -   Potential Issues Found: 0\n",
      "2025-09-05 18:45:30,462 - INFO - HOU analysis completed: {'total_hou_scoring_events': 58, 'total_hou_points_calculated': 106, 'scoring_events_by_type': {('points', 'count'): {1: 40, 3: 18}, ('points', 'sum'): {1: 88, 3: 18}, ('pbp_id', 'count'): {1: 40, 3: 18}}, 'potential_issues': [], 'event_details': [{'pbp_id': 9, 'period': 1, 'pbp_order': 4, 'description': 'Thompson 3PT Jump Shot (3 PTS) (Eason 1 AST)', 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 25, 'period': 1, 'pbp_order': 15, 'description': \"Sengun 18' Jump Shot (2 PTS) (Ja. Green 1 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 33, 'period': 1, 'pbp_order': 21, 'description': \"Ja. Green 2' Driving Layup (2 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 55, 'period': 1, 'pbp_order': 37, 'description': \"Ja. Green 25' 3PT Pullup Jump Shot (5 PTS)\", 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 68, 'period': 1, 'pbp_order': 46, 'description': 'Ja. Green Free Throw 2 of 2 (6 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 70, 'period': 1, 'pbp_order': 48, 'description': \"Whitmore 1' Driving Dunk (2 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 80, 'period': 1, 'pbp_order': 56, 'description': \"Thompson 1' Cutting Dunk Shot (5 PTS) (Sengun 1 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 109, 'period': 1, 'pbp_order': 76, 'description': \"Ja. Green 10' Fadeaway Jumper (8 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 123, 'period': 1, 'pbp_order': 87, 'description': \"Brooks 28' 3PT Jump Shot (3 PTS) (Tate 1 AST)\", 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 159, 'period': 2, 'pbp_order': 110, 'description': 'Whitmore Free Throw 1 of 3 (3 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 160, 'period': 2, 'pbp_order': 111, 'description': 'Whitmore Free Throw 2 of 3 (4 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 161, 'period': 2, 'pbp_order': 112, 'description': 'Whitmore Free Throw 3 of 3 (5 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 186, 'period': 2, 'pbp_order': 133, 'description': 'Irving Free Throw Technical (1 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 201, 'period': 2, 'pbp_order': 147, 'description': 'Eason Free Throw 1 of 2 (1 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 206, 'period': 2, 'pbp_order': 150, 'description': 'Eason Free Throw 2 of 2 (2 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 214, 'period': 2, 'pbp_order': 157, 'description': 'Sheppard Free Throw Technical (1 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 224, 'period': 2, 'pbp_order': 165, 'description': \"Thompson 21' Jump Shot (7 PTS) (Whitmore 1 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 241, 'period': 2, 'pbp_order': 176, 'description': 'Sengun Free Throw 2 of 2 (3 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 244, 'period': 2, 'pbp_order': 179, 'description': \"Sheppard 14' Running Pull-Up Jump Shot (3 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 258, 'period': 2, 'pbp_order': 188, 'description': 'Brooks Free Throw 2 of 2 (4 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 288, 'period': 2, 'pbp_order': 212, 'description': \"Eason 21' Pullup Jump Shot (4 PTS) (Sengun 2 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 292, 'period': 2, 'pbp_order': 215, 'description': \"Ja. Green 2' Running Layup (10 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 295, 'period': 2, 'pbp_order': 218, 'description': \"Ja. Green 26' 3PT Pullup Jump Shot (13 PTS)\", 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 311, 'period': 2, 'pbp_order': 231, 'description': \"Ja. Green 25' 3PT Jump Shot (16 PTS) (Thompson 1 AST)\", 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 324, 'period': 2, 'pbp_order': 240, 'description': \"Sengun 16' Jump Shot (5 PTS) (Thompson 2 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 327, 'period': 2, 'pbp_order': 242, 'description': 'Thompson Alley Oop Dunk (9 PTS) (Ja. Green 2 AST)', 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 331, 'period': 2, 'pbp_order': 244, 'description': \"Thompson 1' Alley Oop Dunk (11 PTS) (Sengun 3 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 348, 'period': 3, 'pbp_order': 252, 'description': \"Sengun 9' Hook Shot (7 PTS) (Adams 1 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 358, 'period': 3, 'pbp_order': 259, 'description': 'Thompson 3PT Jump Shot (14 PTS) (Brooks 1 AST)', 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 366, 'period': 3, 'pbp_order': 264, 'description': \"Sengun 15' Jump Shot (9 PTS) (Ja. Green 3 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 384, 'period': 3, 'pbp_order': 277, 'description': 'Brooks 3PT Jump Shot (7 PTS) (Ja. Green 4 AST)', 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 388, 'period': 3, 'pbp_order': 280, 'description': \"Sengun 5' Driving Hook Shot (11 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 396, 'period': 3, 'pbp_order': 286, 'description': 'Sengun Free Throw 1 of 2 (12 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 399, 'period': 3, 'pbp_order': 288, 'description': 'Sengun Free Throw 2 of 2 (13 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 402, 'period': 3, 'pbp_order': 291, 'description': \"Sengun 26' 3PT Jump Shot (16 PTS) (Brooks 2 AST)\", 'points': 3, 'original_points': 3, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 406, 'period': 3, 'pbp_order': 294, 'description': \"Ja. Green 1' Running Reverse Layup (18 PTS) (Brooks 3 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 420, 'period': 3, 'pbp_order': 305, 'description': \"Sengun 5' Turnaround Hook Shot (18 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 423, 'period': 3, 'pbp_order': 307, 'description': \"Brooks 19' Step Back Jump Shot (9 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 440, 'period': 3, 'pbp_order': 320, 'description': \"Thompson 6' Turnaround Jump Shot (16 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 458, 'period': 3, 'pbp_order': 335, 'description': \"Ja. Green 1' Driving Dunk (20 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 464, 'period': 3, 'pbp_order': 340, 'description': \"Sengun 1' Cutting Dunk Shot (20 PTS) (Ja. Green 5 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 475, 'period': 3, 'pbp_order': 347, 'description': \"Eason 14' Jump Shot (6 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 486, 'period': 3, 'pbp_order': 355, 'description': \"Eason 1' Alley Oop Layup (8 PTS) (Ja. Green 6 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 504, 'period': 4, 'pbp_order': 364, 'description': \"Holiday 4' Driving Finger Roll Layup (2 PTS) (Thompson 3 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 535, 'period': 4, 'pbp_order': 384, 'description': \"Thompson 11' Pullup Jump Shot (18 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 539, 'period': 4, 'pbp_order': 387, 'description': 'Whitmore Free Throw 1 of 2 (6 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 542, 'period': 4, 'pbp_order': 389, 'description': 'Whitmore Free Throw 2 of 2 (7 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 571, 'period': 4, 'pbp_order': 410, 'description': \"Ja. Green 6' Driving Floating Bank Jump Shot (22 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 581, 'period': 4, 'pbp_order': 417, 'description': 'Thompson Tip Layup Shot (20 PTS)', 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 597, 'period': 4, 'pbp_order': 427, 'description': \"Sengun 5' Turnaround Hook Shot (22 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 604, 'period': 4, 'pbp_order': 432, 'description': 'Sengun Free Throw 1 of 2 (23 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 605, 'period': 4, 'pbp_order': 433, 'description': 'Sengun Free Throw 2 of 2 (24 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 613, 'period': 4, 'pbp_order': 439, 'description': 'Adams Tip Dunk Shot (2 PTS)', 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 625, 'period': 4, 'pbp_order': 447, 'description': 'Sengun Free Throw 1 of 2 (25 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 632, 'period': 4, 'pbp_order': 453, 'description': \"Ja. Green 14' Step Back Jump Shot (24 PTS)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 672, 'period': 4, 'pbp_order': 482, 'description': 'Sengun Free Throw 1 of 2 (26 PTS)', 'points': 1, 'original_points': 1, 'msg_type': 3, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 678, 'period': 4, 'pbp_order': 487, 'description': \"Sengun 1' Driving Floating Jump Shot (28 PTS) (Thompson 4 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}, {'pbp_id': 697, 'period': 4, 'pbp_order': 500, 'description': \"Sengun 1' Cutting Dunk Shot (30 PTS) (Thompson 5 AST)\", 'points': 2, 'original_points': 2, 'msg_type': 1, 'has_traditional_lineup': True, 'has_enhanced_lineup': True}], 'possession_attribution': {'total_possession_points': 106, 'difference_from_events': 0}}\n",
      "2025-09-05 18:45:30,462 - INFO - Step 5f: Creating dual-method output tables...\n",
      "2025-09-05 18:45:30,462 - INFO - Creating dual-method output tables...\n",
      "2025-09-05 18:45:30,499 - INFO - [PASS] Create Dual Method Tables: Created dual-method tables: lineups(62), players(42), comparisons, violations\n",
      "2025-09-05 18:45:30,500 - INFO - Step 5g: Creating project deliverable tables (enhanced only)...\n",
      "2025-09-05 18:45:30,513 - INFO - [PASS] Create Project Output Tables: Project outputs created: project1_lineups(42), project2_players(21)\n",
      "2025-09-05 18:45:30,569 - INFO - ✅ Step 4 completed in 0.21s\n",
      "2025-09-05 18:45:30,570 - INFO - STEP 5: Running Final Export and Validation...\n",
      "2025-09-05 18:45:30,591 - INFO - Step 6a: Validating dual-method tables...\n",
      "2025-09-05 18:45:30,592 - INFO - Validating dual-method tables exist...\n",
      "2025-09-05 18:45:30,604 - INFO - Step 6b: Validating against box score (dual-method)...\n",
      "2025-09-05 18:45:30,605 - INFO - Validating dual-method results against box score (DEBUG MODE)...\n",
      "2025-09-05 18:45:30,618 - INFO - === DAL DEBUG ===\n",
      "2025-09-05 18:45:30,619 - INFO - Box Score: 116 points (11 players)\n",
      "2025-09-05 18:45:30,619 - INFO - Raw Events: 115 points (64 scoring events)\n",
      "2025-09-05 18:45:30,620 - INFO - Player Breakdown: Kyrie Irving:13, Max Christie:23, Anthony Davis:26, Daniel Gafford:5, Naji Marshall:16, Spencer Dinwiddie:10, Klay Thompson:13, Dante Exum:4, P.J. Washington:6, Olivier-Maxence Prosper:0, Kessler Edwards:0\n",
      "2025-09-05 18:45:30,621 - INFO - Traditional: 115 points (8 lineups, 133 poss)\n",
      "2025-09-05 18:45:30,621 - INFO -   vs Box: -1 | vs Raw Events: +0 | Rating: 116.9\n",
      "2025-09-05 18:45:30,622 - INFO - Enhanced: 115 points (22 lineups, 133 poss)\n",
      "2025-09-05 18:45:30,622 - INFO -   vs Box: -1 | vs Raw Events: +0 | Rating: 85.4\n",
      "2025-09-05 18:45:30,623 - INFO - \n",
      "2025-09-05 18:45:30,623 - INFO - === HOU DEBUG ===\n",
      "2025-09-05 18:45:30,624 - INFO - Box Score: 105 points (10 players)\n",
      "2025-09-05 18:45:30,624 - INFO - Raw Events: 106 points (58 scoring events)\n",
      "2025-09-05 18:45:30,624 - INFO - Player Breakdown: Jalen Green:24, Alperen Sengun:30, Amen Thompson:20, Dillon Brooks:9, Tari Eason:8, Steven Adams:2, Cam Whitmore:7, Reed Sheppard:3, Jae'Sean Tate:0, Aaron Holiday:2\n",
      "2025-09-05 18:45:30,624 - INFO - Traditional: 106 points (12 lineups, 141 poss)\n",
      "2025-09-05 18:45:30,624 - INFO -   vs Box: +1 | vs Raw Events: +0 | Rating: 76.8\n",
      "2025-09-05 18:45:30,625 - INFO - Enhanced: 106 points (20 lineups, 141 poss)\n",
      "2025-09-05 18:45:30,625 - INFO -   vs Box: +1 | vs Raw Events: +0 | Rating: 84.4\n",
      "2025-09-05 18:45:30,625 - INFO - \n",
      "2025-09-05 18:45:30,627 - INFO - Step 6b.1: Running points attribution audit (debug-only)...\n",
      "2025-09-05 18:45:30,627 - INFO - === ENHANCED POINTS ATTRIBUTION AUDIT ===\n",
      "2025-09-05 18:45:30,658 - INFO - === ENHANCED AUDIT FINDINGS ===\n",
      "2025-09-05 18:45:30,659 - INFO - DAL POINTS FLOW:\n",
      "2025-09-05 18:45:30,660 - INFO -   Box Score (Ground Truth): 116\n",
      "2025-09-05 18:45:30,660 - INFO -   Raw PBP: 115 (diff: -1)\n",
      "2025-09-05 18:45:30,661 - INFO -   Step4 Processed: 115 (diff: +0)\n",
      "2025-09-05 18:45:30,661 - INFO -   Traditional Lineups: 115 (diff: +0)\n",
      "2025-09-05 18:45:30,661 - INFO -   Enhanced Lineups: 115 (diff: +0)\n",
      "2025-09-05 18:45:30,662 - INFO -   FINAL DISCREPANCY vs Box: Traditional=-1, Enhanced=-1\n",
      "2025-09-05 18:45:30,662 - INFO - HOU POINTS FLOW:\n",
      "2025-09-05 18:45:30,662 - INFO -   Box Score (Ground Truth): 105\n",
      "2025-09-05 18:45:30,663 - INFO -   Raw PBP: 106 (diff: +1)\n",
      "2025-09-05 18:45:30,663 - INFO -   Step4 Processed: 106 (diff: +0)\n",
      "2025-09-05 18:45:30,664 - INFO -   Traditional Lineups: 106 (diff: +0)\n",
      "2025-09-05 18:45:30,664 - INFO -   Enhanced Lineups: 106 (diff: +0)\n",
      "2025-09-05 18:45:30,664 - INFO -   FINAL DISCREPANCY vs Box: Traditional=+1, Enhanced=+1\n",
      "2025-09-05 18:45:30,665 - INFO - Step 6c: Validating data completeness (dual-method)...\n",
      "2025-09-05 18:45:30,666 - INFO - Validating data completeness...\n",
      "2025-09-05 18:45:30,670 - INFO - Step 6d: Exporting project deliverables (dual-method)...\n",
      "2025-09-05 18:45:30,670 - INFO - Exporting project deliverables for both methods...\n",
      "2025-09-05 18:45:30,683 - INFO - Step 6e: Exporting violation reports...\n",
      "2025-09-05 18:45:30,683 - INFO - Exporting violation reports for traditional lineups...\n",
      "2025-09-05 18:45:30,694 - INFO - Step 6f: Exporting method comparison reports...\n",
      "2025-09-05 18:45:30,695 - INFO - Exporting method comparison reports...\n",
      "2025-09-05 18:45:30,699 - INFO - Step 6g: Generating comprehensive quality report...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NBA PIPELINE - STEP 5 DUAL-METHOD SUMMARY\n",
      "================================================================================\n",
      "POSSESSION ANALYSIS:\n",
      "  Total Dual Possessions: 274\n",
      "  Total Points: 221\n",
      "  Periods: 4\n",
      "\n",
      "TRADITIONAL METHOD RESULTS:\n",
      "  Unique Lineups: 20\n",
      "  5-Man Lineups: 6 (30.0%)\n",
      "  Violation Flags: 873\n",
      "\n",
      "ENHANCED METHOD RESULTS:\n",
      "  Unique Lineups: 42\n",
      "  5-Man Lineups: 42 (100.0%)\n",
      "  Violation Flags: 67\n",
      "\n",
      "PLAYER RIM DEFENSE:\n",
      "  Traditional Players with Rim Data: 18\n",
      "  Enhanced Players with Rim Data: 19\n",
      "\n",
      "METHOD EFFECTIVENESS:\n",
      "  Lineup Count Change: -22 (Enhanced has fewer unique lineups)\n",
      "  5-Man Accuracy: Traditional 30.0% vs Enhanced 100.0%\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NBA PIPELINE VALIDATION SUMMARY\n",
      "================================================================================\n",
      "OVERALL STATUS: 6/6 tests passed\n",
      "TOTAL VALIDATION TIME: 0.10 seconds\n",
      "TOTAL WARNINGS: 0\n",
      "\n",
      "[PASS] Load Dual Method Data\n",
      "   Details: Loaded Step 5 inputs. Flags: traditional=873, enhanced=67. Optional sources missing (non-blocking): [].\n",
      "   Data Count: 940\n",
      "   Time: 0.021s\n",
      "\n",
      "[PASS] Identify Dual Possessions\n",
      "   Details: FIXED: Identified 274 dual-method possessions, 221 possession points vs 221 event points.\n",
      "   Data Count: 274\n",
      "   Time: 0.021s\n",
      "\n",
      "[PASS] Calculate Dual Lineup Stats\n",
      "   Details: Calculated lineup stats: 20 traditional, 42 enhanced lineups\n",
      "   Data Count: 62\n",
      "   Time: 0.001s\n",
      "\n",
      "[PASS] Calculate Dual Player Rim Stats\n",
      "   Details: Calculated player rim stats: 18 traditional, 19 enhanced players with rim data\n",
      "   Data Count: 42\n",
      "   Time: 0.004s\n",
      "\n",
      "[PASS] Create Dual Method Tables\n",
      "   Details: Created dual-method tables: lineups(62), players(42), comparisons, violations\n",
      "   Data Count: 104\n",
      "   Time: 0.037s\n",
      "\n",
      "[PASS] Create Project Output Tables\n",
      "   Details: Project outputs created: project1_lineups(42), project2_players(21)\n",
      "   Data Count: 63\n",
      "   Time: 0.013s\n",
      "\n",
      "================================================================================\n",
      "NBA Pipeline - Step 6: Dual-Method Final Validation & Export\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 18:45:30,700 - INFO - Generating quality report...\n",
      "2025-09-05 18:45:30,707 - INFO - ✅ Step 5 completed in 0.14s\n",
      "2025-09-05 18:45:30,707 - INFO - Looking for exports in: c:\\docker_projects\\interview_hackathon\\api\\src\\airflow_project\\data\\mavs_data_engineer_2025\\exports\n",
      "2025-09-05 18:45:30,708 - INFO - Found output: project1_lineups_traditional.csv\n",
      "2025-09-05 18:45:30,708 - INFO - Found output: project1_lineups_enhanced.csv\n",
      "2025-09-05 18:45:30,709 - INFO - Found output: project2_players_traditional.csv\n",
      "2025-09-05 18:45:30,709 - INFO - Found output: project2_players_enhanced.csv\n",
      "2025-09-05 18:45:30,710 - INFO - Found output: traditional_lineup_violations.csv\n",
      "2025-09-05 18:45:30,710 - INFO - Found output: method_comparison_summary.csv\n",
      "2025-09-05 18:45:30,711 - INFO - Found output: quality_report.txt\n",
      "2025-09-05 18:45:30,711 - INFO - \n",
      "2025-09-05 18:45:30,711 - INFO - ================================================================================\n",
      "2025-09-05 18:45:30,711 - INFO - PIPELINE COMPLETED!\n",
      "2025-09-05 18:45:30,711 - INFO - ================================================================================\n",
      "2025-09-05 18:45:30,712 - INFO - Total execution time: 1.40 seconds\n",
      "2025-09-05 18:45:30,712 - INFO - Steps completed: 5\n",
      "2025-09-05 18:45:30,713 - INFO - Warnings: 1\n",
      "2025-09-05 18:45:30,713 - INFO - \n",
      "2025-09-05 18:45:30,714 - INFO - OUTPUTS GENERATED:\n",
      "2025-09-05 18:45:30,715 - INFO -   • project1_lineups_traditional: project1_lineups_traditional.csv (1.9 KB)\n",
      "2025-09-05 18:45:30,715 - INFO -   • project1_lineups_enhanced: project1_lineups_enhanced.csv (4.0 KB)\n",
      "2025-09-05 18:45:30,716 - INFO -   • project2_players_traditional: project2_players_traditional.csv (1.1 KB)\n",
      "2025-09-05 18:45:30,717 - INFO -   • project2_players_enhanced: project2_players_enhanced.csv (1.2 KB)\n",
      "2025-09-05 18:45:30,717 - INFO -   • violation_reports: traditional_lineup_violations.csv (218.3 KB)\n",
      "2025-09-05 18:45:30,718 - INFO -   • method_comparison: method_comparison_summary.csv (0.3 KB)\n",
      "2025-09-05 18:45:30,718 - INFO -   • quality_report: quality_report.txt (0.9 KB)\n",
      "2025-09-05 18:45:30,719 - INFO - \n",
      "2025-09-05 18:45:30,719 - INFO - WARNINGS:\n",
      "2025-09-05 18:45:30,719 - WARNING -   • Step 1 had validation warnings but core functionality works\n",
      "2025-09-05 18:45:30,720 - INFO - \n",
      "2025-09-05 18:45:30,720 - INFO - READY FOR PROJECT SUBMISSION!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "NBA PIPELINE - FINAL EXPORT & VALIDATION SUMMARY\n",
      "================================================================================\n",
      "EXPORTED FILES (12):\n",
      "   - base_dataset_violations.txt (0.9 KB)\n",
      "   - enhanced_method_flags.csv (5.9 KB)\n",
      "   - method_comparison_summary.csv (0.3 KB)\n",
      "   - method_effectiveness_report.txt (1.0 KB)\n",
      "   - project1_lineups_enhanced.csv (4.0 KB)\n",
      "   - project1_lineups_traditional.csv (1.9 KB)\n",
      "   - project1_lineups_traditional_with_6th_player.csv (2.0 KB)\n",
      "   - project2_players_enhanced.csv (1.2 KB)\n",
      "   - project2_players_traditional.csv (1.1 KB)\n",
      "   - quality_report.txt (0.9 KB)\n",
      "   - traditional_lineup_violations.csv (218.3 KB)\n",
      "   - violation_summary.txt (0.3 KB)\n",
      "\n",
      "FINAL RESULTS:\n",
      "   Unique Lineups: 28\n",
      "   Active Players: 21\n",
      "   Unable to retrieve final metrics\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "NBA Pipeline - Enhanced Data Loading & Validation\n",
      "============================================================\n",
      "\n",
      "✅ Pipeline completed successfully!\n",
      "Total time: 1.40s\n",
      "Generated 7 output files.\n",
      "\n",
      "Check the exports/ directory for output files:\n",
      "  📄 project1_lineups_traditional.csv\n",
      "  📄 project1_lineups_enhanced.csv\n",
      "  📄 project2_players_traditional.csv\n",
      "  📄 project2_players_enhanced.csv\n",
      "  📄 traditional_lineup_violations.csv\n",
      "  📄 method_comparison_summary.csv\n",
      "  📄 quality_report.txt\n",
      "\n",
      "⚠️  1 warnings (check logs for details)\n"
     ]
    }
   ],
   "source": [
    "# %%writefile api/src/airflow_project/run_complete_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "FIXED Complete NBA Pipeline Runner\n",
    "==================================\n",
    "\n",
    "This module runs the complete NBA pipeline from start to finish and outputs\n",
    "the 3 required datasets with FIXED file path handling and error management.\n",
    "\n",
    "FIXED ISSUES:\n",
    "1. Proper database path construction and validation\n",
    "2. Enhanced error handling and debugging\n",
    "3. Corrected argument parsing to avoid kernel file path confusion\n",
    "4. Added comprehensive validation at each step\n",
    "\n",
    "Usage:\n",
    "    python run_complete_pipeline.py [database_path]\n",
    "\n",
    "If no database path is provided, defaults to 'mavs_enhanced.duckdb'\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "# FIXED: Ensure proper working directory and path setup\n",
    "def setup_pipeline_environment():\n",
    "    \"\"\"FIXED: Setup the pipeline environment with proper paths\"\"\"\n",
    "    # Get current working directory\n",
    "    current_dir = Path.cwd()\n",
    "\n",
    "    # Check if we're in the right directory structure\n",
    "    if current_dir.name != \"airflow_project\":\n",
    "        # Look for airflow_project directory\n",
    "        airflow_project_paths = [\n",
    "            current_dir / \"api\" / \"src\" / \"airflow_project\",\n",
    "            current_dir / \"airflow_project\", \n",
    "            Path(\"api/src/airflow_project\")\n",
    "        ]\n",
    "\n",
    "        for path in airflow_project_paths:\n",
    "            if path.exists() and path.is_dir():\n",
    "                os.chdir(str(path))\n",
    "                print(f\"Changed working directory to: {path.absolute()}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: airflow_project directory not found. Current dir: {current_dir}\")\n",
    "\n",
    "    # Add current directory to Python path\n",
    "    sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "    return Path.cwd()\n",
    "\n",
    "# Setup environment before imports\n",
    "working_dir = setup_pipeline_environment()\n",
    "\n",
    "# Import all pipeline modules with error handling\n",
    "try:\n",
    "    from eda.data.nba_data_loader import load_all_data_enhanced\n",
    "    from eda.data.nba_pbp_processor import process_pbp_with_step2_integration\n",
    "    from eda.data.nba_entities_extractor import extract_all_entities_robust\n",
    "    from eda.data.nba_possession_engine import run_dual_method_possession_engine\n",
    "    from eda.data.nba_final_export import run_dual_method_final_export\n",
    "    print(\"[SUCCESS] All pipeline modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"[ERROR] Error importing pipeline modules: {e}\")\n",
    "    print(f\"Current working directory: {Path.cwd()}\")\n",
    "    print(f\"Python path: {sys.path[:3]}\")  # Show first 3 entries\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configure logging with FIXED paths\n",
    "def setup_logging(working_dir: Path):\n",
    "    \"\"\"Setup logging with proper file paths\"\"\"\n",
    "    logs_dir = working_dir / \"logs\"\n",
    "    logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    log_file = logs_dir / \"complete_pipeline.log\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler(str(log_file))\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging(working_dir)\n",
    "\n",
    "def validate_database_path(db_path: str) -> str:\n",
    "    \"\"\"FIXED: Validate and construct proper database path\"\"\"\n",
    "    # Convert to Path object for better handling\n",
    "    path = Path(db_path)\n",
    "\n",
    "    # Check if it looks like a kernel file (FIXED: detect and reject kernel paths)\n",
    "    if \"kernel\" in str(path).lower() or \"jupyter\" in str(path).lower():\n",
    "        logger.error(f\"Invalid database path detected (kernel file): {path}\")\n",
    "        logger.info(\"Using default database path instead\")\n",
    "        return \"mavs_enhanced.duckdb\"\n",
    "\n",
    "    # If relative path, make it relative to working directory\n",
    "    if not path.is_absolute():\n",
    "        path = working_dir / path\n",
    "\n",
    "    # Ensure .duckdb extension\n",
    "    if not str(path).endswith('.duckdb'):\n",
    "        path = path.with_suffix('.duckdb')\n",
    "\n",
    "    logger.info(f\"Using database path: {path.absolute()}\")\n",
    "    return str(path)\n",
    "\n",
    "def run_complete_pipeline(database_path: str = \"mavs_enhanced.duckdb\") -> Tuple[bool, dict]:\n",
    "    \"\"\"\n",
    "    FIXED: Run the complete NBA pipeline from start to finish.\n",
    "\n",
    "    Args:\n",
    "        database_path: Path to the database file\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (success: bool, results: dict)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # FIXED: Validate and construct proper database path\n",
    "    database_path = validate_database_path(database_path)\n",
    "\n",
    "    results = {\n",
    "        \"database_path\": database_path,\n",
    "        \"working_directory\": str(working_dir),\n",
    "        \"start_time\": start_time,\n",
    "        \"steps_completed\": [],\n",
    "        \"errors\": [],\n",
    "        \"warnings\": [],\n",
    "        \"outputs\": {},\n",
    "        \"total_time\": 0,\n",
    "        \"step_details\": {}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\" NBA COMPLETE PIPELINE RUNNER\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(f\"Database: {database_path}\")\n",
    "        logger.info(f\"Working Directory: {working_dir}\")\n",
    "        logger.info(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\")\n",
    "        logger.info(\"\")\n",
    "\n",
    "        # FIXED: Step 1 with enhanced error handling\n",
    "        logger.info(\"STEP 1: Loading NBA Data...\")\n",
    "        step1_start = time.time()\n",
    "        try:\n",
    "            # Check if data directory exists\n",
    "            data_dir = working_dir / \"data\" / \"mavs_data_engineer_2025\"\n",
    "            if not data_dir.exists():\n",
    "                raise Exception(f\"Data directory not found: {data_dir}\")\n",
    "\n",
    "            success, loader = load_all_data_enhanced(data_dir=None, db_path=database_path)\n",
    "            # FIXED: Be more tolerant of validation failures - check if core functionality works\n",
    "            if not success:\n",
    "                # Check if the database has the essential tables\n",
    "                import duckdb\n",
    "                conn = duckdb.connect(database_path)\n",
    "                essential_tables = ['pbp', 'box_score', 'pbp_event_msg_types']\n",
    "                missing_tables = []\n",
    "                for table in essential_tables:\n",
    "                    count = conn.execute(f\"SELECT COUNT(*) FROM information_schema.tables WHERE table_name='{table}'\").fetchone()[0]\n",
    "                    if count == 0:\n",
    "                        missing_tables.append(table)\n",
    "                conn.close()\n",
    "\n",
    "                if missing_tables:\n",
    "                    raise Exception(f\"Step 1 failed: Essential tables missing: {missing_tables}\")\n",
    "                else:\n",
    "                    logger.warning(\"Step 1 completed with validation warnings but core data loaded successfully\")\n",
    "                    results[\"warnings\"].append(\"Step 1 had validation warnings but core functionality works\")\n",
    "\n",
    "            step1_time = time.time() - step1_start\n",
    "            results[\"steps_completed\"].append(\"Step 1: Data Loading\")\n",
    "            results[\"step_details\"][\"step1\"] = {\"time\": step1_time, \"status\": \"success\" if success else \"warning\"}\n",
    "            logger.info(f\"✅ Step 1 completed in {step1_time:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step 1 failed: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            results[\"step_details\"][\"step1\"] = {\"time\": time.time() - step1_start, \"status\": \"failed\", \"error\": str(e)}\n",
    "            return False, results\n",
    "\n",
    "        # FIXED: Step 2 with validation\n",
    "        logger.info(\"STEP 2: Extracting Entities...\")\n",
    "        step2_start = time.time()\n",
    "        try:\n",
    "            success, entities = extract_all_entities_robust(database_path)\n",
    "            if not success:\n",
    "                raise Exception(\"Step 2 failed: Entity extraction returned False\")\n",
    "            if entities is None:\n",
    "                raise Exception(\"Step 2 failed: No entities returned\")\n",
    "\n",
    "            step2_time = time.time() - step2_start\n",
    "            results[\"steps_completed\"].append(\"Step 2: Entity Extraction\")\n",
    "            results[\"step_details\"][\"step2\"] = {\"time\": step2_time, \"status\": \"success\"}\n",
    "            logger.info(f\"✅ Step 2 completed in {step2_time:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step 2 failed: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            results[\"step_details\"][\"step2\"] = {\"time\": time.time() - step2_start, \"status\": \"failed\", \"error\": str(e)}\n",
    "            return False, results\n",
    "\n",
    "        # FIXED: Step 3 with proper entity passing\n",
    "        logger.info(\"STEP 3: Processing PBP Data...\")\n",
    "        step3_start = time.time()\n",
    "        try:\n",
    "            success, processor = process_pbp_with_step2_integration(db_path=database_path, entities=entities)\n",
    "            if not success:\n",
    "                raise Exception(\"Step 3 failed: PBP processing returned False\")\n",
    "\n",
    "            step3_time = time.time() - step3_start\n",
    "            results[\"steps_completed\"].append(\"Step 3: PBP Processing\")\n",
    "            results[\"step_details\"][\"step3\"] = {\"time\": step3_time, \"status\": \"success\"}\n",
    "            logger.info(f\"✅ Step 3 completed in {step3_time:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step 3 failed: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            results[\"step_details\"][\"step3\"] = {\"time\": time.time() - step3_start, \"status\": \"failed\", \"error\": str(e)}\n",
    "            return False, results\n",
    "\n",
    "        # FIXED: Step 4 with comprehensive validation\n",
    "        logger.info(\"STEP 4: Running Dual-Method Possession Engine...\")\n",
    "        step4_start = time.time()\n",
    "        try:\n",
    "            success, possession_engine = run_dual_method_possession_engine(db_path=database_path, entities=entities)\n",
    "            if not success:\n",
    "                raise Exception(\"Step 4 failed: Possession engine returned False\")\n",
    "            if possession_engine is None:\n",
    "                raise Exception(\"Step 4 failed: No possession engine returned\")\n",
    "\n",
    "            step4_time = time.time() - step4_start\n",
    "            results[\"steps_completed\"].append(\"Step 4: Possession Engine\")\n",
    "            results[\"step_details\"][\"step4\"] = {\"time\": step4_time, \"status\": \"success\"}\n",
    "            logger.info(f\"✅ Step 4 completed in {step4_time:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step 4 failed: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            results[\"step_details\"][\"step4\"] = {\"time\": time.time() - step4_start, \"status\": \"failed\", \"error\": str(e)}\n",
    "            return False, results\n",
    "\n",
    "        # FIXED: Step 5 with output validation\n",
    "        logger.info(\"STEP 5: Running Final Export and Validation...\")\n",
    "        step5_start = time.time()\n",
    "        try:\n",
    "            success, final_validator = run_dual_method_final_export(db_path=database_path)\n",
    "            if not success:\n",
    "                # Don't fail completely if exports have warnings but core functionality works\n",
    "                results[\"warnings\"].append(\"Step 5 had validation warnings but core exports succeeded\")\n",
    "                logger.warning(\"Step 5 completed with warnings\")\n",
    "\n",
    "            step5_time = time.time() - step5_start\n",
    "            results[\"steps_completed\"].append(\"Step 5: Final Export\")\n",
    "            results[\"step_details\"][\"step5\"] = {\"time\": step5_time, \"status\": \"success\" if success else \"warning\"}\n",
    "            logger.info(f\"✅ Step 5 completed in {step5_time:.2f}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Step 5 failed: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            results[\"errors\"].append(error_msg)\n",
    "            results[\"step_details\"][\"step5\"] = {\"time\": time.time() - step5_start, \"status\": \"failed\", \"error\": str(e)}\n",
    "            return False, results\n",
    "\n",
    "        # FIXED: Collect outputs with proper path validation\n",
    "        exports_dir = working_dir / \"data\" / \"mavs_data_engineer_2025\" / \"exports\"\n",
    "        if not exports_dir.exists():\n",
    "            exports_dir = working_dir / \"exports\"\n",
    "\n",
    "        logger.info(f\"Looking for exports in: {exports_dir}\")\n",
    "\n",
    "        potential_outputs = {\n",
    "            \"project1_lineups_traditional\": \"project1_lineups_traditional.csv\",\n",
    "            \"project1_lineups_enhanced\": \"project1_lineups_enhanced.csv\", \n",
    "            \"project2_players_traditional\": \"project2_players_traditional.csv\",\n",
    "            \"project2_players_enhanced\": \"project2_players_enhanced.csv\",\n",
    "            \"violation_reports\": \"traditional_lineup_violations.csv\",\n",
    "            \"method_comparison\": \"method_comparison_summary.csv\",\n",
    "            \"quality_report\": \"quality_report.txt\"\n",
    "        }\n",
    "\n",
    "        # Check which files actually exist\n",
    "        for name, filename in potential_outputs.items():\n",
    "            file_path = exports_dir / filename\n",
    "            if file_path.exists():\n",
    "                results[\"outputs\"][name] = str(file_path)\n",
    "                logger.info(f\"Found output: {filename}\")\n",
    "            else:\n",
    "                results[\"warnings\"].append(f\"Expected output file not found: {filename}\")\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        results[\"total_time\"] = total_time\n",
    "\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(\"PIPELINE COMPLETED!\")\n",
    "        logger.info(\"=\" * 80)\n",
    "        logger.info(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "        logger.info(f\"Steps completed: {len(results['steps_completed'])}\")\n",
    "        logger.info(f\"Warnings: {len(results['warnings'])}\")\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"OUTPUTS GENERATED:\")\n",
    "        for name, path in results[\"outputs\"].items():\n",
    "            file_size = Path(path).stat().st_size / 1024 if Path(path).exists() else 0\n",
    "            logger.info(f\"  • {name}: {Path(path).name} ({file_size:.1f} KB)\")\n",
    "\n",
    "        if results[\"warnings\"]:\n",
    "            logger.info(\"\")\n",
    "            logger.info(\"WARNINGS:\")\n",
    "            for warning in results[\"warnings\"]:\n",
    "                logger.warning(f\"  • {warning}\")\n",
    "\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"READY FOR PROJECT SUBMISSION!\")\n",
    "\n",
    "        return True, results\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Pipeline failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(f\"Error occurred in working directory: {working_dir}\")\n",
    "        results[\"errors\"].append(error_msg)\n",
    "        results[\"total_time\"] = time.time() - start_time\n",
    "        return False, results\n",
    "\n",
    "def main():\n",
    "    \"\"\"FIXED: Main entry point with proper argument handling.\"\"\"\n",
    "    # FIXED: Properly handle command line arguments\n",
    "    if len(sys.argv) > 1:\n",
    "        arg = sys.argv[1]\n",
    "        # FIXED: Filter out any jupyter/kernel related arguments\n",
    "        if \"--f=\" in arg or \"kernel\" in arg.lower() or \"jupyter\" in arg.lower():\n",
    "            logger.warning(f\"Ignoring invalid argument (appears to be kernel file): {arg}\")\n",
    "            database_path = \"mavs_enhanced.duckdb\"\n",
    "        else:\n",
    "            database_path = arg\n",
    "    else:\n",
    "        database_path = \"mavs_enhanced.duckdb\"\n",
    "\n",
    "    logger.info(f\"Starting complete pipeline with database: {database_path}\")\n",
    "\n",
    "    success, results = run_complete_pipeline(database_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if success:\n",
    "        print(\"NBA Pipeline - Enhanced Data Loading & Validation\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\")\n",
    "        print(\"✅ Pipeline completed successfully!\")\n",
    "        print(f\"Total time: {results['total_time']:.2f}s\")\n",
    "        print(f\"Generated {len(results['outputs'])} output files.\")\n",
    "        print(\"\")\n",
    "        print(\"Check the exports/ directory for output files:\")\n",
    "        for name, path in results['outputs'].items():\n",
    "            print(f\"  📄 {Path(path).name}\")\n",
    "\n",
    "        if results['warnings']:\n",
    "            print(f\"\\n⚠️  {len(results['warnings'])} warnings (check logs for details)\")\n",
    "\n",
    "    else:\n",
    "        print(\"NBA Pipeline - Enhanced Data Loading & Validation\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\")\n",
    "        print(\"❌ Pipeline failed!\")\n",
    "        print(f\"  Error: {results['errors'][-1] if results['errors'] else 'Unknown error'}\")\n",
    "        print(f\"  Total steps completed: {len(results['steps_completed'])}\")\n",
    "        print(f\"  Total time: {results['total_time']:.2f}s\")\n",
    "        print(\"\\nAn exception has occurred, use %tb to see the full traceback.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24214b99",
   "metadata": {},
   "source": [
    "# Real Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9460086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/eda/pipeline/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/eda/pipeline/__init__.py\n",
    "# Pipeline module for data processing workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd7d904",
   "metadata": {},
   "source": [
    "# Dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab5577c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/mavs_lineups_assetaware_dag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/mavs_lineups_assetaware_dag.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project modules are importable without changing global CWD\n",
    "THIS = Path(__file__).resolve()\n",
    "AIRFLOW_PROJECT_ROOT = THIS.parents[1]  # dags -> airflow_project\n",
    "if str(AIRFLOW_PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(AIRFLOW_PROJECT_ROOT))\n",
    "\n",
    "from utils import config as CFG\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "from airflow.exceptions import AirflowException\n",
    "\n",
    "\n",
    "def _supports_deferrable_filesensor() -> bool:\n",
    "    \"\"\"Return True if FileSensor accepts 'deferrable' (Airflow >=2.9).\"\"\"\n",
    "    try:\n",
    "        from airflow.sensors.filesystem import FileSensor  # type: ignore\n",
    "        return \"deferrable\" in inspect.signature(FileSensor.__init__).parameters\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _file_sensor_partial():\n",
    "    \"\"\"Return a FileSensor.partial(...) using config-defined knobs.\"\"\"\n",
    "    from airflow.sensors.filesystem import FileSensor  # late import\n",
    "    kwargs = dict(\n",
    "        task_id=\"wait_for_file\",\n",
    "        fs_conn_id=CFG.AIRFLOW_FS_CONN_ID,\n",
    "        poke_interval=CFG.FILE_SENSOR_POKE_SEC,\n",
    "        timeout=CFG.FILE_SENSOR_TIMEOUT_SEC,\n",
    "        soft_fail=False,\n",
    "    )\n",
    "    if _supports_deferrable_filesensor():\n",
    "        kwargs[\"deferrable\"] = True\n",
    "    return FileSensor.partial(**kwargs)\n",
    "\n",
    "\n",
    "@dag(\n",
    "    dag_id=\"nba_lineups_assetaware_v3\",\n",
    "    # IMPORTANT: cross-version compatible — use 'schedule', not 'timetable'\n",
    "    schedule=CFG.build_combined_schedule(),\n",
    "    default_args=CFG.airflow_default_args(),\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=[\"nba\", \"lineups\", \"duckdb\", \"asset-aware\"],\n",
    "    doc_md=\"\"\"\n",
    "### NBA Lineups DAG (Cron + optional Asset/Dataset triggers)\n",
    "- **Scheduling**: Cron and, when supported by this Airflow, Assets/Datasets.\n",
    "- **Inputs**: waits on required CSVs via (deferrable) FileSensor mapping.\n",
    "- **Pipeline**: calls `run_complete_pipeline.run_complete_pipeline()`.\n",
    "- **Validation**: asserts required CSV exports exist and are non-empty.\n",
    "\"\"\",\n",
    ")\n",
    "def lineup_dag():\n",
    "\n",
    "    required_files = [str(p) for p in CFG.required_input_files()]\n",
    "\n",
    "    # Wait for all inputs (mapped sensor, deferrable when supported)\n",
    "    with TaskGroup(group_id=\"wait_for_inputs\", tooltip=\"Wait for required input CSVs\") as wait_for_inputs:\n",
    "        _file_sensor_partial().expand(filepath=required_files)\n",
    "\n",
    "    @task\n",
    "    def print_config() -> None:\n",
    "        \"\"\"Emit configuration summary and write column_usage_report.md.\"\"\"\n",
    "        CFG.print_configuration_summary()\n",
    "\n",
    "    @task\n",
    "    def preflight_checks() -> None:\n",
    "        \"\"\"Fail fast if required files are missing or perf config is unsafe.\"\"\"\n",
    "        files_ok = CFG.validate_data_files()\n",
    "        perf_ok = CFG.validate_performance_config()\n",
    "        if not files_ok:\n",
    "            raise AirflowException(\"Preflight failed: required input files missing or empty.\")\n",
    "        if not perf_ok:\n",
    "            raise AirflowException(\"Preflight failed: performance configuration invalid.\")\n",
    "\n",
    "    @task\n",
    "    def run_pipeline() -> dict:\n",
    "        \"\"\"Run the complete pipeline; return compact results dict.\"\"\"\n",
    "        from run_complete_pipeline import run_complete_pipeline  # late import\n",
    "        ok, results = run_complete_pipeline(database_path=str(CFG.DUCKDB_PATH))\n",
    "        if not ok:\n",
    "            err = (results.get(\"errors\") or [\"unknown\"])[-1]\n",
    "            raise AirflowException(f\"Pipeline failed: {err}\")\n",
    "        return {\n",
    "            \"outputs\": results.get(\"outputs\", {}),\n",
    "            \"total_time\": results.get(\"total_time\", 0.0),\n",
    "            \"warnings\": results.get(\"warnings\", []),\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def validate_outputs(results: dict) -> dict:\n",
    "        \"\"\"Check required CSVs exist and are non-empty.\"\"\"\n",
    "        from pathlib import Path\n",
    "        outputs = results.get(\"outputs\", {})\n",
    "        required = [\n",
    "            \"project1_lineups_traditional\",\n",
    "            \"project2_players_traditional\",\n",
    "        ]\n",
    "        missing = [k for k in required if k not in outputs]\n",
    "        if missing:\n",
    "            raise AirflowException(f\"Missing expected outputs: {missing}\")\n",
    "        zero = []\n",
    "        for k in required:\n",
    "            p = Path(outputs[k])\n",
    "            if not p.exists() or p.stat().st_size == 0:\n",
    "                zero.append(p.name)\n",
    "        if zero:\n",
    "            raise AirflowException(f\"Zero-sized outputs: {zero}\")\n",
    "        return {\n",
    "            \"validated\": True,\n",
    "            \"export_dir\": str(CFG.EXPORTS_DIR),\n",
    "            \"count_required\": len(required),\n",
    "            \"warnings\": results.get(\"warnings\", []),\n",
    "            \"total_time\": float(results.get(\"total_time\", 0.0)),\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def write_run_report(summary: dict) -> str:\n",
    "        \"\"\"Persist a JSON run report and return its path.\"\"\"\n",
    "        import json\n",
    "        from datetime import datetime\n",
    "        CFG.EXPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        report = {\n",
    "            \"validated\": summary[\"validated\"],\n",
    "            \"count_required\": summary[\"count_required\"],\n",
    "            \"total_time_sec\": round(summary[\"total_time\"], 2),\n",
    "            \"warnings\": summary.get(\"warnings\", []),\n",
    "            \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        }\n",
    "        out = CFG.EXPORTS_DIR / \"run_summary.json\"\n",
    "        out.write_text(json.dumps(report, indent=2))\n",
    "        return str(out)\n",
    "\n",
    "    wfi = wait_for_inputs\n",
    "    pc = print_config()\n",
    "    pf = preflight_checks()\n",
    "    rp = run_pipeline()\n",
    "    vo = validate_outputs(rp)\n",
    "    wr = write_run_report(vo)\n",
    "\n",
    "    wfi >> pc >> pf >> rp >> vo >> wr\n",
    "\n",
    "\n",
    "dag = lineup_dag()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49753922",
   "metadata": {},
   "source": [
    "# Plugins\n",
    "- plugins: Add custom or community plugins for your project to this file. It is empty by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b572e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/plugins/custom_operator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/plugins/custom_operator.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
